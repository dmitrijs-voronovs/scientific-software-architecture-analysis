quality_attribute,keyword,matched_word,sentence,source,author,repo,version,wiki,url
Deployability,install,install,Unfortunately it's not clear from your post what might be going wrong here. Is this on a clean install of Ubuntu 16? We'd recommend starting there first to make sure everything is working and then moving to whatever environment you are running on.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/5#issuecomment-349829021
Usability,clear,clear,Unfortunately it's not clear from your post what might be going wrong here. Is this on a clean install of Ubuntu 16? We'd recommend starting there first to make sure everything is working and then moving to whatever environment you are running on.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/5#issuecomment-349829021
Testability,test,test,We added a note about needing the `gsutil` from Google Cloud SDK to our [Build and Test guide](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md). Let us know if you are still having issues.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/5#issuecomment-350478903
Usability,guid,guide,We added a note about needing the `gsutil` from Google Cloud SDK to our [Build and Test guide](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md). Let us know if you are still having issues.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/5#issuecomment-350478903
Deployability,pipeline,pipeline,"@ardoli, @ink1's suggestion is great, and there are two more:. * You can try [Udocker](https://blog.utar.co/blog/udocker) so that root privileges are not required for a Docker image.; * Tweak the DeepVariant source code to make it compile in user-space for your environment. It's a bit hairy, but can be done. The plus side is that you'll see DeepVariant is a pretty simple and modular pipeline, which you can tweak for your preferred analysis. Deep neural networks can be applied to many areas in the -Omics space besides just variant analysis.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6#issuecomment-372515026
Usability,simpl,simple,"@ardoli, @ink1's suggestion is great, and there are two more:. * You can try [Udocker](https://blog.utar.co/blog/udocker) so that root privileges are not required for a Docker image.; * Tweak the DeepVariant source code to make it compile in user-space for your environment. It's a bit hairy, but can be done. The plus side is that you'll see DeepVariant is a pretty simple and modular pipeline, which you can tweak for your preferred analysis. Deep neural networks can be applied to many areas in the -Omics space besides just variant analysis.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6#issuecomment-372515026
Modifiability,portab,portable,"Building a deepvariant Singularity image is indeed really quite simple and portable. I did it and test it on CentOS7 and MacOS X and it run in both case with deepvariant quick-test data. I will post the complete ""how-to"" when I'll have a couple of minutes.; Thank's ink1 for the idea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6#issuecomment-372953552
Testability,test,test,"Building a deepvariant Singularity image is indeed really quite simple and portable. I did it and test it on CentOS7 and MacOS X and it run in both case with deepvariant quick-test data. I will post the complete ""how-to"" when I'll have a couple of minutes.; Thank's ink1 for the idea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6#issuecomment-372953552
Usability,simpl,simple,"Building a deepvariant Singularity image is indeed really quite simple and portable. I did it and test it on CentOS7 and MacOS X and it run in both case with deepvariant quick-test data. I will post the complete ""how-to"" when I'll have a couple of minutes.; Thank's ink1 for the idea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6#issuecomment-372953552
Deployability,install,install,@depristo thank you for taking the time to help me out. . When I was trying to build DeepVariant on Ubuntu 16.04 it required that I install Google Cloud SDK because it needed the 'gsutil' command. Before installing Google Cloud SDK I started reading the 'deepvariant-quick-start.md' file and it talked a lot about using Google Cloud SDK and setting up a Google Cloud account and 'enable billing for you account'. . Based on your comment I am assuming that Google Cloud Platform is not needed and we can simply ignore 'gsutil' once the build process is complete?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9#issuecomment-351577610
Usability,simpl,simply,@depristo thank you for taking the time to help me out. . When I was trying to build DeepVariant on Ubuntu 16.04 it required that I install Google Cloud SDK because it needed the 'gsutil' command. Before installing Google Cloud SDK I started reading the 'deepvariant-quick-start.md' file and it talked a lot about using Google Cloud SDK and setting up a Google Cloud account and 'enable billing for you account'. . Based on your comment I am assuming that Google Cloud Platform is not needed and we can simply ignore 'gsutil' once the build process is complete?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9#issuecomment-351577610
Deployability,update,updated,"@MediciPrime We've updated the [quickstart guide](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md) based on your feedback to make it more clear that setting up a Cloud account and enabling billing isn't required to run DeepVariant. Take the new wording in that doc out for a spin and let us know what you think. Cheers,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9#issuecomment-352087449
Usability,guid,guide,"@MediciPrime We've updated the [quickstart guide](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md) based on your feedback to make it more clear that setting up a Cloud account and enabling billing isn't required to run DeepVariant. Take the new wording in that doc out for a spin and let us know what you think. Cheers,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9#issuecomment-352087449
Availability,avail,available,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:; ```; conda install -c conda-forge -c bioconda deepvariant; ```; It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9#issuecomment-354748344
Deployability,install,install,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:; ```; conda install -c conda-forge -c bioconda deepvariant; ```; It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9#issuecomment-354748344
Integrability,wrap,wrapper,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:; ```; conda install -c conda-forge -c bioconda deepvariant; ```; It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9#issuecomment-354748344
Modifiability,portab,portability,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:; ```; conda install -c conda-forge -c bioconda deepvariant; ```; It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9#issuecomment-354748344
Security,expose,expose,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:; ```; conda install -c conda-forge -c bioconda deepvariant; ```; It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9#issuecomment-354748344
Usability,feedback,feedback,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:; ```; conda install -c conda-forge -c bioconda deepvariant; ```; It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9#issuecomment-354748344
Usability,learn,learning,"If I remember correctly, wildcards like * and ? should work. We can probably improve the comment there.; But concatenating everything together works too. You can directly cat all `*tfrecord.gz` into another big all.tfrecord.gz file. I would suggest trying wildcard first though. In terms of how to set num_examples: for now if you know roughly how many examples you have (for example, I can't remember if make_examples print out that information), you can just set a rough number. It's only being used here: ; https://github.com/google/deepvariant/blob/r0.4/deepvariant/model_train.py#L211; It does affect the learning rate decay, but it doesn't have to be exact.; I'll see if I can come back with a better example to count examples later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10#issuecomment-351130156
Deployability,release,release,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13#issuecomment-351172185
Integrability,depend,depends,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13#issuecomment-351172185
Modifiability,extend,extending,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13#issuecomment-351172185
Testability,log,logic,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13#issuecomment-351172185
Usability,simpl,simplest,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13#issuecomment-351172185
Deployability,install,install,"It might not hurt to install a [Jenkins server](https://jenkins-ci.org/) - similar how the protobuf team does it - for continuous integration, as it also plays well with Github PRs and alleviates these simple headaches.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19#issuecomment-353393763
Integrability,integrat,integration,"It might not hurt to install a [Jenkins server](https://jenkins-ci.org/) - similar how the protobuf team does it - for continuous integration, as it also plays well with Github PRs and alleviates these simple headaches.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19#issuecomment-353393763
Usability,simpl,simple,"It might not hurt to install a [Jenkins server](https://jenkins-ci.org/) - similar how the protobuf team does it - for continuous integration, as it also plays well with Github PRs and alleviates these simple headaches.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19#issuecomment-353393763
Deployability,release,release,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash; $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'; 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a; 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2; $; ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19#issuecomment-353510712
Integrability,depend,dependencies,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash; $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'; 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a; 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2; $; ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19#issuecomment-353510712
Modifiability,evolve,evolve,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash; $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'; 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a; 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2; $; ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19#issuecomment-353510712
Testability,log,log,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash; $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'; 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a; 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2; $; ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19#issuecomment-353510712
Usability,simpl,simplify,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash; $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'; 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a; 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2; $; ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19#issuecomment-353510712
Availability,avail,available,"Hi Paul, thanks for mentioning this issue.; I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant).; These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for.; Would it be ok for you to build DeepVariant for your CPU by following [Building and testing; DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-353701703
Deployability,update,update,"Hi Paul, thanks for mentioning this issue.; I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant).; These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for.; Would it be ok for you to build DeepVariant for your CPU by following [Building and testing; DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-353701703
Performance,optimiz,optimization,"Hi Paul, thanks for mentioning this issue.; I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant).; These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for.; Would it be ok for you to build DeepVariant for your CPU by following [Building and testing; DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-353701703
Testability,test,testing,"Hi Paul, thanks for mentioning this issue.; I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant).; These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for.; Would it be ok for you to build DeepVariant for your CPU by following [Building and testing; DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-353701703
Usability,clear,clear,"Hi Paul, thanks for mentioning this issue.; I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant).; These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for.; Would it be ok for you to build DeepVariant for your CPU by following [Building and testing; DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-353701703
Deployability,release,releases,I'm closing this issue because we aren't likely to provide prebuilt binaries *without* AVX instructions. One reason is that the AVX instructions are critical to efficiently evaluate our deep learning model. Another is that TensorFlow itself will soon provide prebuilt binaries with AVX instructions (https://github.com/tensorflow/tensorflow/releases). . Users who need to run DeepVariant on pre-AVX instruction chipsets should build DeepVariant from sources.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-355389148
Energy Efficiency,efficient,efficiently,I'm closing this issue because we aren't likely to provide prebuilt binaries *without* AVX instructions. One reason is that the AVX instructions are critical to efficiently evaluate our deep learning model. Another is that TensorFlow itself will soon provide prebuilt binaries with AVX instructions (https://github.com/tensorflow/tensorflow/releases). . Users who need to run DeepVariant on pre-AVX instruction chipsets should build DeepVariant from sources.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-355389148
Usability,learn,learning,I'm closing this issue because we aren't likely to provide prebuilt binaries *without* AVX instructions. One reason is that the AVX instructions are critical to efficiently evaluate our deep learning model. Another is that TensorFlow itself will soon provide prebuilt binaries with AVX instructions (https://github.com/tensorflow/tensorflow/releases). . Users who need to run DeepVariant on pre-AVX instruction chipsets should build DeepVariant from sources.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-355389148
Performance,optimiz,optimizations-on-modern-intel-architecture,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl; * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-489377484
Security,access,access,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl; * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-489377484
Testability,test,test,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl; * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-489377484
Usability,guid,guide-instruction-set-specific-dispatching-on-intel-architectures,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl; * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-489377484
Availability,echo,echo,"e when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
Deployability,update,update,"Another update on CLIF dependency:; @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```; # Builds OSS CLIF binary for DeepVariant.; #; # This script should be run on a cloud VM. Known to work on some versions of; # Linux OS.; #; # OSS CLIF takes a very long time to build (10+ minutes) since it needs to; # compile parts of clang and LLVM. To save this build time, we use this script; # to build CLIF, install it in /usr/local/clif, and then packages up; # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz; # called oss_clif.latest.tgz.; #; # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant.; # Various versions that we built and released can be found under:; # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif; #; # We do recognize that this should be temporary, and will update when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
Integrability,depend,dependency,"Another update on CLIF dependency:; @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```; # Builds OSS CLIF binary for DeepVariant.; #; # This script should be run on a cloud VM. Known to work on some versions of; # Linux OS.; #; # OSS CLIF takes a very long time to build (10+ minutes) since it needs to; # compile parts of clang and LLVM. To save this build time, we use this script; # to build CLIF, install it in /usr/local/clif, and then packages up; # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz; # called oss_clif.latest.tgz.; #; # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant.; # Various versions that we built and released can be found under:; # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif; #; # We do recognize that this should be temporary, and will update when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
Modifiability,config,config,"e when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
Usability,resume,resumed,"Another update on CLIF dependency:; @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```; # Builds OSS CLIF binary for DeepVariant.; #; # This script should be run on a cloud VM. Known to work on some versions of; # Linux OS.; #; # OSS CLIF takes a very long time to build (10+ minutes) since it needs to; # compile parts of clang and LLVM. To save this build time, we use this script; # to build CLIF, install it in /usr/local/clif, and then packages up; # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz; # called oss_clif.latest.tgz.; #; # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant.; # Various versions that we built and released can be found under:; # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif; #; # We do recognize that this should be temporary, and will update when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
Availability,error,error,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:; ```; # Get a machine; gcloud beta compute instances create ""${USER}-centos6"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-6"" --image-project ""centos-cloud"" \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \; --zone ""us-west1-b"". # ssh into it; gcloud compute ssh ${USER}-centos6 --zone us-west1-b; ```. ```; ##### On the GCE instance #####; # Install Python 2.7; sudo yum install -y centos-release-SCL; sudo yum install -y python27; source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/; (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""); sudo ldconfig # Reload shared libraries.; ```; (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:; ```; $ /usr/local/clif/bin/pyclif; usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]; [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]; [--prepend PREPEND] [--include_paths INCLUDE_PATHS]; [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]; [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]; input_filename; pyclif: error: too few arguments; ```. Please let me know once you have a chance to try it.; CentOS 6 is tricky. It feels like everything is old :(; Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385864674
Deployability,update,update,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:; ```; # Get a machine; gcloud beta compute instances create ""${USER}-centos6"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-6"" --image-project ""centos-cloud"" \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \; --zone ""us-west1-b"". # ssh into it; gcloud compute ssh ${USER}-centos6 --zone us-west1-b; ```. ```; ##### On the GCE instance #####; # Install Python 2.7; sudo yum install -y centos-release-SCL; sudo yum install -y python27; source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/; (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""); sudo ldconfig # Reload shared libraries.; ```; (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:; ```; $ /usr/local/clif/bin/pyclif; usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]; [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]; [--prepend PREPEND] [--include_paths INCLUDE_PATHS]; [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]; [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]; input_filename; pyclif: error: too few arguments; ```. Please let me know once you have a chance to try it.; CentOS 6 is tricky. It feels like everything is old :(; Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385864674
Usability,usab,usable,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:; ```; # Get a machine; gcloud beta compute instances create ""${USER}-centos6"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-6"" --image-project ""centos-cloud"" \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \; --zone ""us-west1-b"". # ssh into it; gcloud compute ssh ${USER}-centos6 --zone us-west1-b; ```. ```; ##### On the GCE instance #####; # Install Python 2.7; sudo yum install -y centos-release-SCL; sudo yum install -y python27; source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/; (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""); sudo ldconfig # Reload shared libraries.; ```; (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:; ```; $ /usr/local/clif/bin/pyclif; usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]; [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]; [--prepend PREPEND] [--include_paths INCLUDE_PATHS]; [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]; [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]; input_filename; pyclif: error: too few arguments; ```. Please let me know once you have a chance to try it.; CentOS 6 is tricky. It feels like everything is old :(; Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385864674
Deployability,install,installing,"Thanks @machomachopadre for your report. Just so I'm 100% clear, you've got python 2.7 and 3.5 on the machine, and our build-prereqs.sh script is installing some packages into python 3.5 and some into 2.7? I don't think we've been clear before about this, but DeepVariant is intended for python 2.7 only, as we've never tested it using python3. . Can you confirm that you can install DeepVariant on a clean Ubuntu 16 instance in the cloud?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/30#issuecomment-355031587
Testability,test,tested,"Thanks @machomachopadre for your report. Just so I'm 100% clear, you've got python 2.7 and 3.5 on the machine, and our build-prereqs.sh script is installing some packages into python 3.5 and some into 2.7? I don't think we've been clear before about this, but DeepVariant is intended for python 2.7 only, as we've never tested it using python3. . Can you confirm that you can install DeepVariant on a clean Ubuntu 16 instance in the cloud?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/30#issuecomment-355031587
Usability,clear,clear,"Thanks @machomachopadre for your report. Just so I'm 100% clear, you've got python 2.7 and 3.5 on the machine, and our build-prereqs.sh script is installing some packages into python 3.5 and some into 2.7? I don't think we've been clear before about this, but DeepVariant is intended for python 2.7 only, as we've never tested it using python3. . Can you confirm that you can install DeepVariant on a clean Ubuntu 16 instance in the cloud?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/30#issuecomment-355031587
Usability,clear,clear,"To be 100% clear, are you saying you booted a clean ubuntu 16 instance and it failed to build there? Or is this on an already customized machine?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32#issuecomment-355348946
Availability,error,error,"To make it clearer, I put the path structure here.; ```; /deepvariant/core/; cloud_utils_test.py; math.py; ...; ```; And in `cloud_utils_test.py`:; ```; """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import; from __future__ import division; from __future__ import print_function. import httplib; ...; ```; Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. ; But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:; ```; >>> import httplib; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>; import mimetools; File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>; import tempfile; File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>; from random import Random as _Random; File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>; from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil; ***File ""math.py"", line 79, in <module>***; import numpy as np; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import add_newdocs; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>; from numpy.lib import add_newdoc; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>; from .type_check import *; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>; import numpy.core.numeric as _nx; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>; from numpy.testing.nosetester impo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32#issuecomment-355522771
Performance,load,load,"ttplib; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>; import mimetools; File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>; import tempfile; File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>; from random import Random as _Random; File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>; from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil; ***File ""math.py"", line 79, in <module>***; import numpy as np; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import add_newdocs; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>; from numpy.lib import add_newdoc; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>; from .type_check import *; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>; import numpy.core.numeric as _nx; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>; from numpy.testing.nosetester import _numpy_tester; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>; from . import decorators as dec; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>; from .utils import SkipTest, assert_warns; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>; from tempfile import mkdtemp, mkstemp; ImportError: cannot import name mkdtemp; >>> ; ```; As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module.; On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32#issuecomment-355522771
Testability,test,test,"To make it clearer, I put the path structure here.; ```; /deepvariant/core/; cloud_utils_test.py; math.py; ...; ```; And in `cloud_utils_test.py`:; ```; """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import; from __future__ import division; from __future__ import print_function. import httplib; ...; ```; Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. ; But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:; ```; >>> import httplib; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>; import mimetools; File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>; import tempfile; File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>; from random import Random as _Random; File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>; from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil; ***File ""math.py"", line 79, in <module>***; import numpy as np; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import add_newdocs; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>; from numpy.lib import add_newdoc; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>; from .type_check import *; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>; import numpy.core.numeric as _nx; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>; from numpy.testing.nosetester impo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32#issuecomment-355522771
Usability,clear,clearer,"To make it clearer, I put the path structure here.; ```; /deepvariant/core/; cloud_utils_test.py; math.py; ...; ```; And in `cloud_utils_test.py`:; ```; """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import; from __future__ import division; from __future__ import print_function. import httplib; ...; ```; Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. ; But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:; ```; >>> import httplib; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>; import mimetools; File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>; import tempfile; File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>; from random import Random as _Random; File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>; from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil; ***File ""math.py"", line 79, in <module>***; import numpy as np; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import add_newdocs; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>; from numpy.lib import add_newdoc; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>; from .type_check import *; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>; import numpy.core.numeric as _nx; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>; from numpy.testing.nosetester impo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32#issuecomment-355522771
Integrability,depend,dependencies,"Hi Attila,. Yeah it takes about a week to fully uncover how all the dependencies work together, which can be fun if you have the time. The side-benefit is that then you'll discover that DeepVariant is just a basic collections of tools (and data-structures) with many possibilities to tweak, simplify and expand on - which can be even more fun to play with :). ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41#issuecomment-361132599
Usability,simpl,simplify,"Hi Attila,. Yeah it takes about a week to fully uncover how all the dependencies work together, which can be fun if you have the time. The side-benefit is that then you'll discover that DeepVariant is just a basic collections of tools (and data-structures) with many possibilities to tweak, simplify and expand on - which can be even more fun to play with :). ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41#issuecomment-361132599
Integrability,depend,dependencies,"@pgrosu Thanks Paul, we appreciate the close eye you've given to the codebase. Unfortunately managing all of the dependencies is challenging, and we aren't super happy with how complex the build/run prereqs scripts are. And we agree with you that there's a lot of opportunity to explore extensions, simplifications, and optimizations of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41#issuecomment-361436703
Performance,optimiz,optimizations,"@pgrosu Thanks Paul, we appreciate the close eye you've given to the codebase. Unfortunately managing all of the dependencies is challenging, and we aren't super happy with how complex the build/run prereqs scripts are. And we agree with you that there's a lot of opportunity to explore extensions, simplifications, and optimizations of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41#issuecomment-361436703
Usability,simpl,simplifications,"@pgrosu Thanks Paul, we appreciate the close eye you've given to the codebase. Unfortunately managing all of the dependencies is challenging, and we aren't super happy with how complex the build/run prereqs scripts are. And we agree with you that there's a lot of opportunity to explore extensions, simplifications, and optimizations of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41#issuecomment-361436703
Usability,simpl,simple,"Hi Mark (@depristo),. There is a theme of elegance with the current implementation that I tend to appreciate, though I agree that streamlining it for support/growth is rich with opportunities to explore. Let me know if you would like to work on it together - or just bounce off ideas - as some could be low-hanging fruit with simple remedies. Best,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41#issuecomment-361455478
Performance,perform,performance,"At present we do not have a specific recommendation for joint genotyping DeepVariant gVCFs. Given the accurate genotype likelihood calibration of single-sample DeepVariant calls it may be better to simply merge calls without computing genotype posteriors based on population allelic frequencies and then altering the genotypes. We are actively investigating the performance of different methods to be able to provide a set of best practices. That said, the gVCF outputs of DeepVariant are syntactically and semantically equivalent to those produced by other tools like GATK, so can be used by any existing joint genotyping tools.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45#issuecomment-363913008
Usability,simpl,simply,"At present we do not have a specific recommendation for joint genotyping DeepVariant gVCFs. Given the accurate genotype likelihood calibration of single-sample DeepVariant calls it may be better to simply merge calls without computing genotype posteriors based on population allelic frequencies and then altering the genotypes. We are actively investigating the performance of different methods to be able to provide a set of best practices. That said, the gVCF outputs of DeepVariant are syntactically and semantically equivalent to those produced by other tools like GATK, so can be used by any existing joint genotyping tools.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45#issuecomment-363913008
Deployability,configurat,configuration,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49#issuecomment-366745899
Energy Efficiency,schedul,scheduler,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49#issuecomment-366745899
Modifiability,config,configuration,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49#issuecomment-366745899
Usability,simpl,simplest,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49#issuecomment-366745899
Deployability,configurat,configuration,"Is it possible to merge the tfrecords files though?. On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,; >; > Since your WDL workflow is using Docker, the simplest approach is to; > include a Docker-specific argument for --cpuset-cpus, or change the; > Session configuration which I've detailed at, the following location:; >; > #42 (comment); > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>; >; > For information regarding the --cpuset-cpus here's a reference:; >; > https://docs.docker.com/config/containers/resource_; > constraints/#configure-the-default-cfs-scheduler; >; > There are many ways to change DeepVariant, but I think this will will get; > you the quickest results for the issue you're facing.; >; > Hope it helps,; > Paul; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49#issuecomment-366748047
Energy Efficiency,schedul,scheduler,"Is it possible to merge the tfrecords files though?. On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,; >; > Since your WDL workflow is using Docker, the simplest approach is to; > include a Docker-specific argument for --cpuset-cpus, or change the; > Session configuration which I've detailed at, the following location:; >; > #42 (comment); > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>; >; > For information regarding the --cpuset-cpus here's a reference:; >; > https://docs.docker.com/config/containers/resource_; > constraints/#configure-the-default-cfs-scheduler; >; > There are many ways to change DeepVariant, but I think this will will get; > you the quickest results for the issue you're facing.; >; > Hope it helps,; > Paul; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49#issuecomment-366748047
Modifiability,config,configuration,"Is it possible to merge the tfrecords files though?. On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,; >; > Since your WDL workflow is using Docker, the simplest approach is to; > include a Docker-specific argument for --cpuset-cpus, or change the; > Session configuration which I've detailed at, the following location:; >; > #42 (comment); > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>; >; > For information regarding the --cpuset-cpus here's a reference:; >; > https://docs.docker.com/config/containers/resource_; > constraints/#configure-the-default-cfs-scheduler; >; > There are many ways to change DeepVariant, but I think this will will get; > you the quickest results for the issue you're facing.; >; > Hope it helps,; > Paul; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49#issuecomment-366748047
Usability,simpl,simplest,"Is it possible to merge the tfrecords files though?. On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,; >; > Since your WDL workflow is using Docker, the simplest approach is to; > include a Docker-specific argument for --cpuset-cpus, or change the; > Session configuration which I've detailed at, the following location:; >; > #42 (comment); > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>; >; > For information regarding the --cpuset-cpus here's a reference:; >; > https://docs.docker.com/config/containers/resource_; > constraints/#configure-the-default-cfs-scheduler; >; > There are many ways to change DeepVariant, but I think this will will get; > you the quickest results for the issue you're facing.; >; > Hope it helps,; > Paul; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49#issuecomment-366748047
Usability,simpl,simply,"If you're looking for simply merging the tfrecord files (without having to touch Python code) to one, you can actually just concatenate tfrecord files together.; Something like:; `cat shard.*.tfrecord > merged.tfrecord`. or you can also concatenate zipped tfrecord files:; `cat shard.*.tfrecord.gz > merged.tfrecord.gz`; will work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49#issuecomment-366848143
Usability,simpl,simple,Can you produce a small snippet of your BAM file and an associated command line that reproduces the issue and share it with us? We'd be happy to debug but it'd be great to have a simple example that causes the problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-370805635
Availability,error,error,"oticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```; paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; HTTP request sent, awaiting response... 200 OK; Length: 357342653 (341M) [binary/octet-stream]; Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; samtools index: failed to create index for ""ENCFF528VXT.bam""; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; WARNING: Logging before flag parsing goes to stderr.; I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; Traceback (most recent call last):; File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371293506
Performance,cache,cache,"46 bytes; samtools index: failed to create index for ""ENCFF528VXT.bam""; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; WARNING: Logging before flag parsing goes to stderr.; I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; Traceback (most recent call last):; File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options; with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:; File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371293506
Security,access,accessible,"42653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; samtools index: failed to create index for ""ENCFF528VXT.bam""; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; WARNING: Logging before flag parsing goes to stderr.; I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; Traceback (most recent call last):; File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_op",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371293506
Testability,test,test,"I was suspicious something else might be the issue. So I did a simple test to see if there is an issue with Luisa's BAM file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```; paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; HTTP request sent, awaiting response... 200 OK; Length: 357342653 (341M) [binary/octet-stream]; Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; samtools index: failed to create index for ""ENCFF528VXT.bam""; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; WARNING: Logging before flag parsing goes to stderr.; I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::bam_hdr_read] EOF marker is absent",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371293506
Usability,simpl,simple,"I was suspicious something else might be the issue. So I did a simple test to see if there is an issue with Luisa's BAM file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```; paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; HTTP request sent, awaiting response... 200 OK; Length: 357342653 (341M) [binary/octet-stream]; Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; samtools index: failed to create index for ""ENCFF528VXT.bam""; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; WARNING: Logging before flag parsing goes to stderr.; I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::bam_hdr_read] EOF marker is absent",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371293506
Availability,error,error,"Indeed the file was truncated, sorry about that. I am still testing locally; with other even smaller files ( like : wget; http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam; which is public and smaller and not truncated ) and I get the same exact; error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test; > to see if there is an issue with Luisa's BAM file, and noticed that I; > cannot even create an index - which would naturally make even the; > prerequisite make_examples not complete properly:; >; > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; > HTTP request sent, awaiting response... 200 OK; > Length: 357342653 (341M) [binary/octet-stream]; > Saving to: âENCFF528VXT.bamâ; >; > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s; >; > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]; >; > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; > samtools index: failed to create index for ""ENCFF528VXT.bam""; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075
Performance,cache,cache,"ndex: failed to create index for ""ENCFF528VXT.bam""; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; > WARNING: Logging before flag parsing goes to stderr.; > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > Traceback (most recent call last):; > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>; > tf.app.run(); > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; > _sys.exit(main(_sys.argv[:1] + flags_passthrough)); > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main; > options = default_options(add_flags=True, flags_obj=FLAGS); > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options; > with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:; > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075
Security,access,accessible,"l@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; > samtools index: failed to create index for ""ENCFF528VXT.bam""; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; > WARNING: Logging before flag parsing goes to stderr.; > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > Traceback (most recent call last):; > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>; > tf.app.run(); > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; > _sys.exit(main(_sys.argv[:1] + flags_passthrough)); > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main; > options = default_options(add_flags=True, flags_obj=FLAGS); > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075
Testability,test,testing,"Indeed the file was truncated, sorry about that. I am still testing locally; with other even smaller files ( like : wget; http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam; which is public and smaller and not truncated ) and I get the same exact; error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test; > to see if there is an issue with Luisa's BAM file, and noticed that I; > cannot even create an index - which would naturally make even the; > prerequisite make_examples not complete properly:; >; > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; > HTTP request sent, awaiting response... 200 OK; > Length: 357342653 (341M) [binary/octet-stream]; > Saving to: âENCFF528VXT.bamâ; >; > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s; >; > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]; >; > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; > samtools index: failed to create index for ""ENCFF528VXT.bam""; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075
Usability,simpl,simple,"Indeed the file was truncated, sorry about that. I am still testing locally; with other even smaller files ( like : wget; http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam; which is public and smaller and not truncated ) and I get the same exact; error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test; > to see if there is an issue with Luisa's BAM file, and noticed that I; > cannot even create an index - which would naturally make even the; > prerequisite make_examples not complete properly:; >; > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; > HTTP request sent, awaiting response... 200 OK; > Length: 357342653 (341M) [binary/octet-stream]; > Saving to: âENCFF528VXT.bamâ; >; > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s; >; > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]; >; > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; > samtools index: failed to create index for ""ENCFF528VXT.bam""; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075
Usability,simpl,simply,"Hi again,; I didn't read carefully so I missed that you said you want to __train__ a model.; If you want to get `make_examples` to create more candidates, the other flags you need to consider are: `vsc_min_count_snps`, `vsc_min_count_indels`, `vsc_min_fraction_snps`, `vsc_min_fraction_indels`. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-379110341
Availability,error,error-free,"ad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,; I didn't read carefully so I missed that you said you want to train a model.; If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-379857500
Integrability,message,message,"ad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,; I didn't read carefully so I missed that you said you want to train a model.; If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-379857500
Safety,safe,safe,"Thanks, I’m giving that a try today. vsc_min_count_snps, vsc_min_count_indels are already small numbers (2), so I changed only the fraction flags from their defaults, 0.12, to 0.01 which the fraction I want. Is that reasonable?. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Thursday, April 5, 2018 6:56 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,; I didn't read carefully so I missed that you said you want to train a model.; If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercept",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-379857500
Security,confidential,confidential,"ad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,; I didn't read carefully so I missed that you said you want to train a model.; If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-379857500
Usability,simpl,simply,"t_indels are already small numbers (2), so I changed only the fraction flags from their defaults, 0.12, to 0.01 which the fraction I want. Is that reasonable?. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Thursday, April 5, 2018 6:56 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,; I didn't read carefully so I missed that you said you want to train a model.; If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contai",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-379857500
Availability,error,error,"Hello,. With make_examples I believe I have made examples I can use in model_train. The 64 files are named like this: 5PRR-RD_S86.examples.tfrecord-00000-of-00064. My protobuffer file contains this:. name: ""my-training-dataset""; tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord""; num_examples: 64. When I run model_train I see this error:. ValueError: Cannot find matching files with the pattern ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". How should I specify the tfrecord_path to get model_train to use the files?. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Thursday, April 5, 2018 6:56 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,; I didn't read carefully so I missed that you said you want to train a model.; If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual name",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380158183
Integrability,message,message,"ad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,; I didn't read carefully so I missed that you said you want to train a model.; If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380158183
Safety,safe,safe,"Hello,. With make_examples I believe I have made examples I can use in model_train. The 64 files are named like this: 5PRR-RD_S86.examples.tfrecord-00000-of-00064. My protobuffer file contains this:. name: ""my-training-dataset""; tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord""; num_examples: 64. When I run model_train I see this error:. ValueError: Cannot find matching files with the pattern ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". How should I specify the tfrecord_path to get model_train to use the files?. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Thursday, April 5, 2018 6:56 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,; I didn't read carefully so I missed that you said you want to train a model.; If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual name",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380158183
Security,confidential,confidential,"ad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,; I didn't read carefully so I missed that you said you want to train a model.; If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380158183
Usability,simpl,simply,"les with the pattern ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". How should I specify the tfrecord_path to get model_train to use the files?. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Thursday, April 5, 2018 6:56 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,; I didn't read carefully so I missed that you said you want to train a model.; If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contai",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380158183
Deployability,install,installation,"I'm not completely sure about your setting. Our installation guide currently is done on Ubuntu 16. ; In your case, it seems like you're unable to install python-wheel? I think that's outside the scope of DeepVariant support. A few possible ways to get unstuck : maybe you can see whether you can skip installing python-wheel, and see what you actually need to install to proceed to the next step. I don't think we can be of more help on this issue. I'm closing this issue for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/66#issuecomment-403636246
Usability,guid,guide,"I'm not completely sure about your setting. Our installation guide currently is done on Ubuntu 16. ; In your case, it seems like you're unable to install python-wheel? I think that's outside the scope of DeepVariant support. A few possible ways to get unstuck : maybe you can see whether you can skip installing python-wheel, and see what you actually need to install to proceed to the next step. I don't think we can be of more help on this issue. I'm closing this issue for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/66#issuecomment-403636246
Deployability,pipeline,pipeline,"Hi,. I see, that actually makes a lot more sense, thank you for helping clear that up. As for the training, I understand that there isn't really any official documentation on adding classes, however I was wondering if I could be pointed towards some of the files/functions in charge of calculating the result of the CNN, to see what needs to be changed for adding additional classes. Another option I was considering that is easier to perform, however is much more computationally heavy, is to train a new model from scratch for each of the classes I wish to add, where I would modify the VCF file given to the 'make_examples' script to make the labels be : 0/0 = undefined, 0/1 = yes, 0/2 = no, and to do this for every new class I wish to add. So I would run the vanilla DeepVariant on my input BAM, then do further runs with each model to further categorize variants that would fall within each of the classes I am examining (can possibly even process the initial output VCF such that the additional runs will only run for within those regions that were initially classified as a variant). Is such an approach possible? If all that is being done is converting each input into 6 channels and have it run through the Inception Pipeline then I believe my approach should be possible. I guess my question specifically is, is there anything within the DeepVariant pipeline that will prevent me from training it to identify completely new classes instead of the default HOM-REF, HET, and HOM-ALT?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/67#issuecomment-383347052
Energy Efficiency,charge,charge,"Hi,. I see, that actually makes a lot more sense, thank you for helping clear that up. As for the training, I understand that there isn't really any official documentation on adding classes, however I was wondering if I could be pointed towards some of the files/functions in charge of calculating the result of the CNN, to see what needs to be changed for adding additional classes. Another option I was considering that is easier to perform, however is much more computationally heavy, is to train a new model from scratch for each of the classes I wish to add, where I would modify the VCF file given to the 'make_examples' script to make the labels be : 0/0 = undefined, 0/1 = yes, 0/2 = no, and to do this for every new class I wish to add. So I would run the vanilla DeepVariant on my input BAM, then do further runs with each model to further categorize variants that would fall within each of the classes I am examining (can possibly even process the initial output VCF such that the additional runs will only run for within those regions that were initially classified as a variant). Is such an approach possible? If all that is being done is converting each input into 6 channels and have it run through the Inception Pipeline then I believe my approach should be possible. I guess my question specifically is, is there anything within the DeepVariant pipeline that will prevent me from training it to identify completely new classes instead of the default HOM-REF, HET, and HOM-ALT?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/67#issuecomment-383347052
Performance,perform,perform,"Hi,. I see, that actually makes a lot more sense, thank you for helping clear that up. As for the training, I understand that there isn't really any official documentation on adding classes, however I was wondering if I could be pointed towards some of the files/functions in charge of calculating the result of the CNN, to see what needs to be changed for adding additional classes. Another option I was considering that is easier to perform, however is much more computationally heavy, is to train a new model from scratch for each of the classes I wish to add, where I would modify the VCF file given to the 'make_examples' script to make the labels be : 0/0 = undefined, 0/1 = yes, 0/2 = no, and to do this for every new class I wish to add. So I would run the vanilla DeepVariant on my input BAM, then do further runs with each model to further categorize variants that would fall within each of the classes I am examining (can possibly even process the initial output VCF such that the additional runs will only run for within those regions that were initially classified as a variant). Is such an approach possible? If all that is being done is converting each input into 6 channels and have it run through the Inception Pipeline then I believe my approach should be possible. I guess my question specifically is, is there anything within the DeepVariant pipeline that will prevent me from training it to identify completely new classes instead of the default HOM-REF, HET, and HOM-ALT?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/67#issuecomment-383347052
Usability,clear,clear,"Hi,. I see, that actually makes a lot more sense, thank you for helping clear that up. As for the training, I understand that there isn't really any official documentation on adding classes, however I was wondering if I could be pointed towards some of the files/functions in charge of calculating the result of the CNN, to see what needs to be changed for adding additional classes. Another option I was considering that is easier to perform, however is much more computationally heavy, is to train a new model from scratch for each of the classes I wish to add, where I would modify the VCF file given to the 'make_examples' script to make the labels be : 0/0 = undefined, 0/1 = yes, 0/2 = no, and to do this for every new class I wish to add. So I would run the vanilla DeepVariant on my input BAM, then do further runs with each model to further categorize variants that would fall within each of the classes I am examining (can possibly even process the initial output VCF such that the additional runs will only run for within those regions that were initially classified as a variant). Is such an approach possible? If all that is being done is converting each input into 6 channels and have it run through the Inception Pipeline then I believe my approach should be possible. I guess my question specifically is, is there anything within the DeepVariant pipeline that will prevent me from training it to identify completely new classes instead of the default HOM-REF, HET, and HOM-ALT?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/67#issuecomment-383347052
Availability,down,down,"Hi masgouri@, . Thanks for the excellent question and for sharing that you've been having good experiences with DeepVariant. We are always interested in user stories so if you feel like sharing more about your experiences with DeepVariant please send them our way. There are a few separate issues here; let me try to unpack them. -- There are some engineering constraints in DeepVariant that assume we train only 3 classes. One example is https://github.com/google/deepvariant/search?utf8=%E2%9C%93&q=DEFAULT_NUM_CLASSES&type= but there may be others. If you want to change the number of classes you'll need to track these down and generalize them. If you do that please send us a CL and we'll make sure to incorporate the changes into the master codebase here. Once you do that it should be possible to train as with as many labels as you like. -- You'll need to write your own labeler algorithm to provide a label for each training variant. These classes are in deepvariant/labeler. There are two (a PositionalLabeler and a HaplotypeLabeler) so you can use these as templates to hook up your own labeling algorithm. At the core these just need to return an integer label (0 ... N) for each example. Once you get that going it should be simple to label your own examples. Technically the labels can be anything. For example, if you emitted the reference base value as 0 for A, 1 for T, etc I'm pretty sure the machinery can learn to call the reference base from the image with perfect accuracy. -- The machinery of training really doesn't know anything about NGS data. It just takes the examples and tries to learn to predict the label. Once you get your labels in and the machinery doesn't expect just 3 it should be possible to train anything you want. You can use standard evaluation machinery to decide if your DeepVariant training run is successfully learning something. -- You'll likely need to write your own version of postprocess_variants (or extend it in some way) to bridge from the predic",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/67#issuecomment-383764665
Integrability,bridg,bridge,"et me try to unpack them. -- There are some engineering constraints in DeepVariant that assume we train only 3 classes. One example is https://github.com/google/deepvariant/search?utf8=%E2%9C%93&q=DEFAULT_NUM_CLASSES&type= but there may be others. If you want to change the number of classes you'll need to track these down and generalize them. If you do that please send us a CL and we'll make sure to incorporate the changes into the master codebase here. Once you do that it should be possible to train as with as many labels as you like. -- You'll need to write your own labeler algorithm to provide a label for each training variant. These classes are in deepvariant/labeler. There are two (a PositionalLabeler and a HaplotypeLabeler) so you can use these as templates to hook up your own labeling algorithm. At the core these just need to return an integer label (0 ... N) for each example. Once you get that going it should be simple to label your own examples. Technically the labels can be anything. For example, if you emitted the reference base value as 0 for A, 1 for T, etc I'm pretty sure the machinery can learn to call the reference base from the image with perfect accuracy. -- The machinery of training really doesn't know anything about NGS data. It just takes the examples and tries to learn to predict the label. Once you get your labels in and the machinery doesn't expect just 3 it should be possible to train anything you want. You can use standard evaluation machinery to decide if your DeepVariant training run is successfully learning something. -- You'll likely need to write your own version of postprocess_variants (or extend it in some way) to bridge from the predicted labels and the information you want to export in the VCF (or other file format). In germline DeepVariant we learn to predict 0, 1, and 2 which mean hom-ref, het, and hom-alt, respectively, which is encoded in postprocess_variants. You'll need your own equivalent to this code. Hope this helps!. Mark",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/67#issuecomment-383764665
Modifiability,extend,extend,"et me try to unpack them. -- There are some engineering constraints in DeepVariant that assume we train only 3 classes. One example is https://github.com/google/deepvariant/search?utf8=%E2%9C%93&q=DEFAULT_NUM_CLASSES&type= but there may be others. If you want to change the number of classes you'll need to track these down and generalize them. If you do that please send us a CL and we'll make sure to incorporate the changes into the master codebase here. Once you do that it should be possible to train as with as many labels as you like. -- You'll need to write your own labeler algorithm to provide a label for each training variant. These classes are in deepvariant/labeler. There are two (a PositionalLabeler and a HaplotypeLabeler) so you can use these as templates to hook up your own labeling algorithm. At the core these just need to return an integer label (0 ... N) for each example. Once you get that going it should be simple to label your own examples. Technically the labels can be anything. For example, if you emitted the reference base value as 0 for A, 1 for T, etc I'm pretty sure the machinery can learn to call the reference base from the image with perfect accuracy. -- The machinery of training really doesn't know anything about NGS data. It just takes the examples and tries to learn to predict the label. Once you get your labels in and the machinery doesn't expect just 3 it should be possible to train anything you want. You can use standard evaluation machinery to decide if your DeepVariant training run is successfully learning something. -- You'll likely need to write your own version of postprocess_variants (or extend it in some way) to bridge from the predicted labels and the information you want to export in the VCF (or other file format). In germline DeepVariant we learn to predict 0, 1, and 2 which mean hom-ref, het, and hom-alt, respectively, which is encoded in postprocess_variants. You'll need your own equivalent to this code. Hope this helps!. Mark",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/67#issuecomment-383764665
Safety,predict,predict,"et me try to unpack them. -- There are some engineering constraints in DeepVariant that assume we train only 3 classes. One example is https://github.com/google/deepvariant/search?utf8=%E2%9C%93&q=DEFAULT_NUM_CLASSES&type= but there may be others. If you want to change the number of classes you'll need to track these down and generalize them. If you do that please send us a CL and we'll make sure to incorporate the changes into the master codebase here. Once you do that it should be possible to train as with as many labels as you like. -- You'll need to write your own labeler algorithm to provide a label for each training variant. These classes are in deepvariant/labeler. There are two (a PositionalLabeler and a HaplotypeLabeler) so you can use these as templates to hook up your own labeling algorithm. At the core these just need to return an integer label (0 ... N) for each example. Once you get that going it should be simple to label your own examples. Technically the labels can be anything. For example, if you emitted the reference base value as 0 for A, 1 for T, etc I'm pretty sure the machinery can learn to call the reference base from the image with perfect accuracy. -- The machinery of training really doesn't know anything about NGS data. It just takes the examples and tries to learn to predict the label. Once you get your labels in and the machinery doesn't expect just 3 it should be possible to train anything you want. You can use standard evaluation machinery to decide if your DeepVariant training run is successfully learning something. -- You'll likely need to write your own version of postprocess_variants (or extend it in some way) to bridge from the predicted labels and the information you want to export in the VCF (or other file format). In germline DeepVariant we learn to predict 0, 1, and 2 which mean hom-ref, het, and hom-alt, respectively, which is encoded in postprocess_variants. You'll need your own equivalent to this code. Hope this helps!. Mark",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/67#issuecomment-383764665
Usability,simpl,simple,"nt please send them our way. There are a few separate issues here; let me try to unpack them. -- There are some engineering constraints in DeepVariant that assume we train only 3 classes. One example is https://github.com/google/deepvariant/search?utf8=%E2%9C%93&q=DEFAULT_NUM_CLASSES&type= but there may be others. If you want to change the number of classes you'll need to track these down and generalize them. If you do that please send us a CL and we'll make sure to incorporate the changes into the master codebase here. Once you do that it should be possible to train as with as many labels as you like. -- You'll need to write your own labeler algorithm to provide a label for each training variant. These classes are in deepvariant/labeler. There are two (a PositionalLabeler and a HaplotypeLabeler) so you can use these as templates to hook up your own labeling algorithm. At the core these just need to return an integer label (0 ... N) for each example. Once you get that going it should be simple to label your own examples. Technically the labels can be anything. For example, if you emitted the reference base value as 0 for A, 1 for T, etc I'm pretty sure the machinery can learn to call the reference base from the image with perfect accuracy. -- The machinery of training really doesn't know anything about NGS data. It just takes the examples and tries to learn to predict the label. Once you get your labels in and the machinery doesn't expect just 3 it should be possible to train anything you want. You can use standard evaluation machinery to decide if your DeepVariant training run is successfully learning something. -- You'll likely need to write your own version of postprocess_variants (or extend it in some way) to bridge from the predicted labels and the information you want to export in the VCF (or other file format). In germline DeepVariant we learn to predict 0, 1, and 2 which mean hom-ref, het, and hom-alt, respectively, which is encoded in postprocess_variants. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/67#issuecomment-383764665
Usability,simpl,simply,Thank you - I'm attaching my BED file which simply defines the entire length of the chromosome (I am working on a bacterium):; NC_000962.3 0	4411531; Is this an issue with a non-human genome?. (Saved with a .txt so that I could upload.); [confidence.bed.txt](https://github.com/google/deepvariant/files/1986525/confidence.bed.txt),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/71#issuecomment-387627131
Usability,clear,clear,This seems like a genuine bug. Is there any way you can share the genome reference and reads along with a command line that reproduces the issue? It's not clear to me what the actual issue is and that would help a lot debugging the actual problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/71#issuecomment-387816424
Availability,error,error,"Further to this question, I am trying to run 2 chromosome for the whole genome case study. . The user guide said that we can do ```--regions "" chr20 chr21"" ``` to run both chromosome, however, I get the following error when I run the make sample with ``` --regions ""chr20 chr21"" ```. I have also tried ```--regions ""20 21""```, but none of these works. . ```; E0814 18:04:05.051175 140174423590656 errors.py:64] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_D92FlN/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '21']"".; parallel: This job failed:; python /HD_disk/real-case-study-chr/input/bin/make_examples.zip --mode calling --ref /HD_disk/real-case-study-chr/input/data/hs37d5.fa.gz --reads /HD_disk/real-case-study-chr/input/data/HG002_NIST_150bp_50x.bam --examples /HD_disk/real-case-study-chr/output_1_2_3/HG002.examples.tfrecord@8.gz --regions 20 21 --gvcf /HD_disk/real-case-study-chr/output_1_2_3/HG002.gvcf.tfrecord@8.gz --task 2; ```. Would you please suggest how I should specify the region to get multiple chromosome running at the same time. For single chromosome, ``` --regions ""20"" ``` works fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/72#issuecomment-412946034
Usability,guid,guide,"Further to this question, I am trying to run 2 chromosome for the whole genome case study. . The user guide said that we can do ```--regions "" chr20 chr21"" ``` to run both chromosome, however, I get the following error when I run the make sample with ``` --regions ""chr20 chr21"" ```. I have also tried ```--regions ""20 21""```, but none of these works. . ```; E0814 18:04:05.051175 140174423590656 errors.py:64] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_D92FlN/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '21']"".; parallel: This job failed:; python /HD_disk/real-case-study-chr/input/bin/make_examples.zip --mode calling --ref /HD_disk/real-case-study-chr/input/data/hs37d5.fa.gz --reads /HD_disk/real-case-study-chr/input/data/HG002_NIST_150bp_50x.bam --examples /HD_disk/real-case-study-chr/output_1_2_3/HG002.examples.tfrecord@8.gz --regions 20 21 --gvcf /HD_disk/real-case-study-chr/output_1_2_3/HG002.gvcf.tfrecord@8.gz --task 2; ```. Would you please suggest how I should specify the region to get multiple chromosome running at the same time. For single chromosome, ``` --regions ""20"" ``` works fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/72#issuecomment-412946034
Deployability,release,release,"Hi,; In the latest release we added this page:; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details-training-data.md; As well as a training tutorial:; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md. If you decide to train a model, we would love to hear your feedback as detailed as you are willing to provide us. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/80#issuecomment-415562625
Usability,feedback,feedback,"Hi,; In the latest release we added this page:; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details-training-data.md; As well as a training tutorial:; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md. If you decide to train a model, we would love to hear your feedback as detailed as you are willing to provide us. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/80#issuecomment-415562625
Usability,simpl,simple,"Hi,; I understand that it's a common use case to be compatible with GATK. We'll consider potentially adding a flag for that conversion. But since we're following the spec (using the VCF v4.3 spec: page 25 of this doc https://samtools.github.io/hts-specs/VCFv4.3.pdf), this won't be of high priority. For now the simple substitution you're doing is correct. I filed a bug internally to track. I'll close this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/83#issuecomment-403616978
Availability,avail,available,"directories in the tree flowing naturally from it. A minimal number of obvious scripts (with associated documentation) should naturally transition them from source code, through the build phase, and as quickly to the analysis process. Basically less is more, and you do not want to add more to the main directory as you begin to approach version 1.0. So ideally what would be nice is to have one build script - let's call it `build` - that reads a `config/build` directory - and does the following:. 1. Analyzes hidden build-generated files under `.build/...`, and detects if the `prereq` and `build-and-test` phases have been successfully processed. Otherwise, it would report back to the user the build status, and query the user with a recommendation on the next required step to run. For example:. ```; $ build status. Checking DeepVariant build prerequisites... OK; Checking DeepVariant test environment compatibility [GPU]... OK; Checking DeepVariant build binaries... MISSING. Proceed with building binaries [Y/n]?. ```. 2. Once the builds have been successfully completed, then it would tell the user the available options to run (i.e. `make_examples`, `call_variants`, `postprocess_variants`, `pkrusche/hap.py`, etc.) as well as any analysis that has been completed if a `results` or `analysis-results` folder is present. 3. For Cloud Build, that would also be part of the `build` script, when they provide the `help` argument:. ```; $ ./build help; ...; cloudbuild Builds Docker images of DeepVariant, and ; pushes them on the Google Container Registry (gcr.io) ; ... $ ./build cloudbuild help; CPU Builds a DeepVariant Docker image for CPU usage.; GPU Builds a DeepVariant Docker image for GPU usage.; Runner Builds a DeepVariant Docker image for large-scale analysis run; using the Genomics Pipelines API. $; ```. Even `Runner` is a bit too general, so maybe calling it `LargeScaleAnalysis`, or something which should be instantly recognizable as to its intended use. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/87#issuecomment-413745335
Modifiability,config,config,"d, and has a very practical use-case for many users that would like to automate their analysis. I'm sort of hinting at something else. So just imagine you are pitching this to some Bioinformatician that has been doing their analysis on an internal cluster for years. S/he has heard from others of the benefits of DeepVariant through Google Cloud resources, and now has a little bit of free time to try it out to convince their boss to use it. The idea is that the root directory should be as simple as possible, so users don't get overwhelmed or confused - with all the other directories in the tree flowing naturally from it. A minimal number of obvious scripts (with associated documentation) should naturally transition them from source code, through the build phase, and as quickly to the analysis process. Basically less is more, and you do not want to add more to the main directory as you begin to approach version 1.0. So ideally what would be nice is to have one build script - let's call it `build` - that reads a `config/build` directory - and does the following:. 1. Analyzes hidden build-generated files under `.build/...`, and detects if the `prereq` and `build-and-test` phases have been successfully processed. Otherwise, it would report back to the user the build status, and query the user with a recommendation on the next required step to run. For example:. ```; $ build status. Checking DeepVariant build prerequisites... OK; Checking DeepVariant test environment compatibility [GPU]... OK; Checking DeepVariant build binaries... MISSING. Proceed with building binaries [Y/n]?. ```. 2. Once the builds have been successfully completed, then it would tell the user the available options to run (i.e. `make_examples`, `call_variants`, `postprocess_variants`, `pkrusche/hap.py`, etc.) as well as any analysis that has been completed if a `results` or `analysis-results` folder is present. 3. For Cloud Build, that would also be part of the `build` script, when they provide the `help",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/87#issuecomment-413745335
Safety,detect,detects," Bioinformatician that has been doing their analysis on an internal cluster for years. S/he has heard from others of the benefits of DeepVariant through Google Cloud resources, and now has a little bit of free time to try it out to convince their boss to use it. The idea is that the root directory should be as simple as possible, so users don't get overwhelmed or confused - with all the other directories in the tree flowing naturally from it. A minimal number of obvious scripts (with associated documentation) should naturally transition them from source code, through the build phase, and as quickly to the analysis process. Basically less is more, and you do not want to add more to the main directory as you begin to approach version 1.0. So ideally what would be nice is to have one build script - let's call it `build` - that reads a `config/build` directory - and does the following:. 1. Analyzes hidden build-generated files under `.build/...`, and detects if the `prereq` and `build-and-test` phases have been successfully processed. Otherwise, it would report back to the user the build status, and query the user with a recommendation on the next required step to run. For example:. ```; $ build status. Checking DeepVariant build prerequisites... OK; Checking DeepVariant test environment compatibility [GPU]... OK; Checking DeepVariant build binaries... MISSING. Proceed with building binaries [Y/n]?. ```. 2. Once the builds have been successfully completed, then it would tell the user the available options to run (i.e. `make_examples`, `call_variants`, `postprocess_variants`, `pkrusche/hap.py`, etc.) as well as any analysis that has been completed if a `results` or `analysis-results` folder is present. 3. For Cloud Build, that would also be part of the `build` script, when they provide the `help` argument:. ```; $ ./build help; ...; cloudbuild Builds Docker images of DeepVariant, and ; pushes them on the Google Container Registry (gcr.io) ; ... $ ./build cloudbuild help;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/87#issuecomment-413745335
Testability,test,test," Bioinformatician that has been doing their analysis on an internal cluster for years. S/he has heard from others of the benefits of DeepVariant through Google Cloud resources, and now has a little bit of free time to try it out to convince their boss to use it. The idea is that the root directory should be as simple as possible, so users don't get overwhelmed or confused - with all the other directories in the tree flowing naturally from it. A minimal number of obvious scripts (with associated documentation) should naturally transition them from source code, through the build phase, and as quickly to the analysis process. Basically less is more, and you do not want to add more to the main directory as you begin to approach version 1.0. So ideally what would be nice is to have one build script - let's call it `build` - that reads a `config/build` directory - and does the following:. 1. Analyzes hidden build-generated files under `.build/...`, and detects if the `prereq` and `build-and-test` phases have been successfully processed. Otherwise, it would report back to the user the build status, and query the user with a recommendation on the next required step to run. For example:. ```; $ build status. Checking DeepVariant build prerequisites... OK; Checking DeepVariant test environment compatibility [GPU]... OK; Checking DeepVariant build binaries... MISSING. Proceed with building binaries [Y/n]?. ```. 2. Once the builds have been successfully completed, then it would tell the user the available options to run (i.e. `make_examples`, `call_variants`, `postprocess_variants`, `pkrusche/hap.py`, etc.) as well as any analysis that has been completed if a `results` or `analysis-results` folder is present. 3. For Cloud Build, that would also be part of the `build` script, when they provide the `help` argument:. ```; $ ./build help; ...; cloudbuild Builds Docker images of DeepVariant, and ; pushes them on the Google Container Registry (gcr.io) ; ... $ ./build cloudbuild help;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/87#issuecomment-413745335
Usability,simpl,simple,"Hi Pichuan,. I understand the nature of Cloud Build, and has a very practical use-case for many users that would like to automate their analysis. I'm sort of hinting at something else. So just imagine you are pitching this to some Bioinformatician that has been doing their analysis on an internal cluster for years. S/he has heard from others of the benefits of DeepVariant through Google Cloud resources, and now has a little bit of free time to try it out to convince their boss to use it. The idea is that the root directory should be as simple as possible, so users don't get overwhelmed or confused - with all the other directories in the tree flowing naturally from it. A minimal number of obvious scripts (with associated documentation) should naturally transition them from source code, through the build phase, and as quickly to the analysis process. Basically less is more, and you do not want to add more to the main directory as you begin to approach version 1.0. So ideally what would be nice is to have one build script - let's call it `build` - that reads a `config/build` directory - and does the following:. 1. Analyzes hidden build-generated files under `.build/...`, and detects if the `prereq` and `build-and-test` phases have been successfully processed. Otherwise, it would report back to the user the build status, and query the user with a recommendation on the next required step to run. For example:. ```; $ build status. Checking DeepVariant build prerequisites... OK; Checking DeepVariant test environment compatibility [GPU]... OK; Checking DeepVariant build binaries... MISSING. Proceed with building binaries [Y/n]?. ```. 2. Once the builds have been successfully completed, then it would tell the user the available options to run (i.e. `make_examples`, `call_variants`, `postprocess_variants`, `pkrusche/hap.py`, etc.) as well as any analysis that has been completed if a `results` or `analysis-results` folder is present. 3. For Cloud Build, that would also be part ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/87#issuecomment-413745335
Deployability,update,updated,"Hi @pgrosu , thanks for your feedback!; Thanks to @nmousavi 's work, the Cloud runner page is now updated:; https://cloud.google.com/genomics/docs/tutorials/deepvariant with 0.7.0, and 0.7.0 deepvariant_runner image is now tagged as latest. In terms of our GitHub page --; I fixed a few small thing such as the typo (and ran spell checking and fixed a few more!) Also fixed the `0.4.1` issue. I'll also address the contributing document at some point soon. These changes are right now still just in our internal codebase. I'll get it out when I have a chance to push out some documentation fixes. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/87#issuecomment-415643215
Usability,feedback,feedback,"Hi @pgrosu , thanks for your feedback!; Thanks to @nmousavi 's work, the Cloud runner page is now updated:; https://cloud.google.com/genomics/docs/tutorials/deepvariant with 0.7.0, and 0.7.0 deepvariant_runner image is now tagged as latest. In terms of our GitHub page --; I fixed a few small thing such as the typo (and ran spell checking and fixed a few more!) Also fixed the `0.4.1` issue. I'll also address the contributing document at some point soon. These changes are right now still just in our internal codebase. I'll get it out when I have a chance to push out some documentation fixes. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/87#issuecomment-415643215
Usability,clear,clear,"Hi @pgrosu , I filed an internal issue to track your suggestions in https://github.com/google/deepvariant/issues/87#issuecomment-413745335; I'll need to digest it a bit more, and probably talk to to a few users in more detail to design this experience better. On a high level, I think I'll need to at least think about overhauling the GitHub page to make it super clear how to run - even just with our docker image, it's apparently still not obvious for users to find (because it's hidden in the docs). And, the suggestion of making things as simple as possible is great one - both in terms of the directory structure, and also the output that our programs output. I think I have stared DeepVariant for too long that I'm losing empathy for people who look at it for the first time. I'll close this issue on GitHub, but will plan to think about this in more detail and hope to get back with at least some more plan later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/87#issuecomment-416830583
Deployability,update,updated,"Hi, we recently updated the quick start and case studies to use docker.; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md. It seems like I should rearrange and simplify our documentation to make it more clear.; Can you tell me where is the place you first read? Is it the main github page, or did you clone the codebase and directly start from there?; I will try to make some improvement next week.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-415984031
Usability,simpl,simplify,"Hi, we recently updated the quick start and case studies to use docker.; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md. It seems like I should rearrange and simplify our documentation to make it more clear.; Can you tell me where is the place you first read? Is it the main github page, or did you clone the codebase and directly start from there?; I will try to make some improvement next week.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-415984031
Availability,down,download,"i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://storage.googleapis.com/bazel-apt/dists/stable/InRelease Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
Deployability,update,updates,"@pichuan hi, i got the zip file and ra run-prereq.sh and this was the output:. solokopi@solokopi-All-Series:~/Desktop/DeepVariant-0.7.0+cl-208818123$ sudo bash run-prereq.sh; [sudo] password for solokopi: ; ========== Load config settings.; ========== [2018年 08月 28日 星期二 10:31:05 CST] Stage 'Misc setup' starting; Ign:1 http://dl.google.com/linux/chrome/deb stable InRelease; Hit:2 http://mirrors.aliyun.com/ubuntu xenial InRelease ; Hit:3 http://dl.google.com/linux/chrome/deb stable Release ; Hit:4 http://mirrors.aliyun.com/ubuntu xenial-updates InRelease ; Get:6 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB] ; Hit:7 http://mirrors.aliyun.com/ubuntu xenial-security InRelease ; Hit:8 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Err:9 http://storage.googleapis.com/bazel-apt stable InRelease ; Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; Err:10 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
Integrability,depend,dependency,"t.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:43:08 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Package",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
Modifiability,config,config,"@pichuan hi, i got the zip file and ra run-prereq.sh and this was the output:. solokopi@solokopi-All-Series:~/Desktop/DeepVariant-0.7.0+cl-208818123$ sudo bash run-prereq.sh; [sudo] password for solokopi: ; ========== Load config settings.; ========== [2018年 08月 28日 星期二 10:31:05 CST] Stage 'Misc setup' starting; Ign:1 http://dl.google.com/linux/chrome/deb stable InRelease; Hit:2 http://mirrors.aliyun.com/ubuntu xenial InRelease ; Hit:3 http://dl.google.com/linux/chrome/deb stable Release ; Hit:4 http://mirrors.aliyun.com/ubuntu xenial-updates InRelease ; Get:6 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB] ; Hit:7 http://mirrors.aliyun.com/ubuntu xenial-security InRelease ; Hit:8 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Err:9 http://storage.googleapis.com/bazel-apt stable InRelease ; Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; Err:10 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
Performance,cache,cachetools,"b/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018年 08月 28日 星期二 10:55:17 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf-nightly-gpu as it is not installed.; Skipping tensorflow-gpu as it is not installed.; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:02:09 --:--:-- 0curl: (7) Fa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
Security,password,password,"@pichuan hi, i got the zip file and ra run-prereq.sh and this was the output:. solokopi@solokopi-All-Series:~/Desktop/DeepVariant-0.7.0+cl-208818123$ sudo bash run-prereq.sh; [sudo] password for solokopi: ; ========== Load config settings.; ========== [2018年 08月 28日 星期二 10:31:05 CST] Stage 'Misc setup' starting; Ign:1 http://dl.google.com/linux/chrome/deb stable InRelease; Hit:2 http://mirrors.aliyun.com/ubuntu xenial InRelease ; Hit:3 http://dl.google.com/linux/chrome/deb stable Release ; Hit:4 http://mirrors.aliyun.com/ubuntu xenial-updates InRelease ; Get:6 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB] ; Hit:7 http://mirrors.aliyun.com/ubuntu xenial-security InRelease ; Hit:8 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Err:9 http://storage.googleapis.com/bazel-apt stable InRelease ; Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; Err:10 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
Testability,mock,mock,"-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-date: pip in /usr/local/lib/python2.7/dist-packages (18.0); ========== [2018年 08月 28日 星期二 10:55:12 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/local/lib/python2.7/dist-packages (0.5.5); Requirement already satisfied: enum34 in /usr/local/lib/python2.7/dist-packages (1.1.6); Requirement already satisfied: sortedcontainers==1.5.3 in /usr/local/lib/python2.7/dist-packages (1.5.3); Requirement already satisfied: intervaltree in /usr/local/lib/python2.7/dist-packages (2.1.0); Requirement already satisfied: sortedcontainers in /usr/local/lib/python2.7/dist-packages (from intervaltree) (1.5.3); Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (2.0.0); Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (4.2.0); Requirement already satisfied: funcsigs>=1; python_version < ""3.3"" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.0.2); Requirement already satisfied: six>=1.9 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.11.0); Requirement already satisfied: numpy==1.14 in /usr/local/lib/python2.7/dist-packages (1.14.0); Requirement already satisfied: requests>=2.18 in /usr/local/lib/python2.7/dist-packages (2.19.1); Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (2018.8.13); Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (3.0.4); Requirement already satisfied: urllib3<1.24,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
Usability,learn,learn,"isfied: oauth2client>=4.0.0 in /usr/local/lib/python2.7/dist-packages (4.1.2); Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0) (0.4.4); Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0) (0.2.2); Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0) (1.11.0); Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0) (0.11.3); Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0) (3.4.2); Requirement already satisfied: crcmod>=1.7 in /usr/local/lib/python2.7/dist-packages (1.7); Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (1.11.0); Requirement already satisfied: sklearn in /usr/local/lib/python2.7/dist-packages (0.0); Requirement already satisfied: scikit-learn in /usr/local/lib/python2.7/dist-packages (from sklearn) (0.19.2); Requirement already satisfied: pandas in /usr/local/lib/python2.7/dist-packages (0.23.4); Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (1.14.0); Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (2.7.3); Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas) (2018.5); Requirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0); Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.7); Requirement already up-to-date: google-api-python-client in /usr/local/lib/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
Usability,feedback,feedback,@pichuan and @pgrosu : Thank you for your feedback. Will keep you posted on the issue.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/90#issuecomment-418211657
Testability,log,log,"Hi,; As you can see from our training case study, for larger datasets, we shuffled it on a distributed runner (dataflow). I think Beam also supports other things like Spark, but I have not tried it myself.; This shuffling code achieves two things: 1. Shuffle the examples, 2. Count the number of examples. Shuffling is important for training a model. Internally our tensorflow code does some extra shuffling in each batch, but that might not be enough on a global scale. I think a global shuffling is a better practice from a machine learning perspective. But empirically it might also be ok to not shuffle globally.; The other thing that you'll need is the total number of training examples, which I think you can find in the log when you made the examples (but you'll need to sum them up across the shards). The number of examples is used in the calculation of learning rate. You can certainly try without shuffling, but it's not the best practice I'd recommend.; Let me know if it works for you. If you find a good way to shuffle on Spark or other distributed system that you'd like to share, that will be great! Please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/91#issuecomment-418172888
Usability,learn,learning,"Hi,; As you can see from our training case study, for larger datasets, we shuffled it on a distributed runner (dataflow). I think Beam also supports other things like Spark, but I have not tried it myself.; This shuffling code achieves two things: 1. Shuffle the examples, 2. Count the number of examples. Shuffling is important for training a model. Internally our tensorflow code does some extra shuffling in each batch, but that might not be enough on a global scale. I think a global shuffling is a better practice from a machine learning perspective. But empirically it might also be ok to not shuffle globally.; The other thing that you'll need is the total number of training examples, which I think you can find in the log when you made the examples (but you'll need to sum them up across the shards). The number of examples is used in the calculation of learning rate. You can certainly try without shuffling, but it's not the best practice I'd recommend.; Let me know if it works for you. If you find a good way to shuffle on Spark or other distributed system that you'd like to share, that will be great! Please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/91#issuecomment-418172888
Usability,guid,guide,@ssm0808 FYI on how to shuffle on Spark: http://people.apache.org/~pwendell/spark-nightly/spark-master-docs/latest/rdd-programming-guide.html#shuffle-operations,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/91#issuecomment-423040145
Integrability,protocol,protocolbuffers,"Hi Ram,. I see what's happening. You have protobuf 3.5.1 in your include paths, but this is trying to compile the protobuf 3.6 version. You will notice in this line:. ```; new (initial_block_) Block(options_.initial_block_size, NULL);; ```. Which is only part of `3.6.x`:. https://github.com/protocolbuffers/protobuf/blob/3.6.x/src/google/protobuf/arena.cc#L77. Basically a bunch of conflicts between declarations and use. So to simplify things, could you remove your protobuf 3.5.1 from your paths. If you are on a university cluster, it's usually something like `module unload MODULE_NAME`. After that try rerunning it again. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-422619015
Usability,simpl,simplify,"Hi Ram,. I see what's happening. You have protobuf 3.5.1 in your include paths, but this is trying to compile the protobuf 3.6 version. You will notice in this line:. ```; new (initial_block_) Block(options_.initial_block_size, NULL);; ```. Which is only part of `3.6.x`:. https://github.com/protocolbuffers/protobuf/blob/3.6.x/src/google/protobuf/arena.cc#L77. Basically a bunch of conflicts between declarations and use. So to simplify things, could you remove your protobuf 3.5.1 from your paths. If you are on a university cluster, it's usually something like `module unload MODULE_NAME`. After that try rerunning it again. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-422619015
Availability,error,error,"Hi Paul,. I simplified my setup a bit and have removed protobuf in my PATHs. I was able to complete the build, but most of the tests seem to be failing with the below error. TypeError: __new__() got an unexpected keyword argument 'serialized_options'. Attached is the log with verbose failures. Please let me know if you have any clue.; [log2.txt](https://github.com/google/deepvariant/files/2397845/log2.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-422863842
Testability,test,tests,"Hi Paul,. I simplified my setup a bit and have removed protobuf in my PATHs. I was able to complete the build, but most of the tests seem to be failing with the below error. TypeError: __new__() got an unexpected keyword argument 'serialized_options'. Attached is the log with verbose failures. Please let me know if you have any clue.; [log2.txt](https://github.com/google/deepvariant/files/2397845/log2.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-422863842
Usability,simpl,simplified,"Hi Paul,. I simplified my setup a bit and have removed protobuf in my PATHs. I was able to complete the build, but most of the tests seem to be failing with the below error. TypeError: __new__() got an unexpected keyword argument 'serialized_options'. Attached is the log with verbose failures. Please let me know if you have any clue.; [log2.txt](https://github.com/google/deepvariant/files/2397845/log2.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-422863842
Testability,log,logs,"That's great news Ram, and glad to hear it all worked out. The logs were basically guiding me through rules of implication, and it looked like we were getting close. Let us know if you run into any other issues, as we would be happy to help you out. @depristo Thank you Mark for the nice compliments, that means quite a lot. I always enjoy helping out folks and the team. I find exploring complex and challenging problems interesting and fun.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-423408772
Usability,guid,guiding,"That's great news Ram, and glad to hear it all worked out. The logs were basically guiding me through rules of implication, and it looked like we were getting close. Let us know if you run into any other issues, as we would be happy to help you out. @depristo Thank you Mark for the nice compliments, that means quite a lot. I always enjoy helping out folks and the team. I find exploring complex and challenging problems interesting and fun.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-423408772
Deployability,install,installs,"Phil and Pi-Chuan;; That's exactly right, `dv_make_examples.py` is a simple wrapper for `make_examples.zip` to make it directly callable and is part of the conda packaging process, not a default script from the Google team. If it's not useful, feel free to call directly at the standard installs, it just tries to avoid needing to know the full install path and other details about the parallelization so the command line syntax is simplified. For your example you'd use it like:; ```; dv_make_examples.py; --ref chr20.fa.gz \; --reads test.bam \; --examples shardedExamples/examples.tfrecord@2.gz \; --regions regions.bed \; --sample test \; --logdir location/to/place/logfiles; --cores 1; ```; Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/101#issuecomment-430171385
Integrability,wrap,wrapper,"Phil and Pi-Chuan;; That's exactly right, `dv_make_examples.py` is a simple wrapper for `make_examples.zip` to make it directly callable and is part of the conda packaging process, not a default script from the Google team. If it's not useful, feel free to call directly at the standard installs, it just tries to avoid needing to know the full install path and other details about the parallelization so the command line syntax is simplified. For your example you'd use it like:; ```; dv_make_examples.py; --ref chr20.fa.gz \; --reads test.bam \; --examples shardedExamples/examples.tfrecord@2.gz \; --regions regions.bed \; --sample test \; --logdir location/to/place/logfiles; --cores 1; ```; Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/101#issuecomment-430171385
Safety,avoid,avoid,"Phil and Pi-Chuan;; That's exactly right, `dv_make_examples.py` is a simple wrapper for `make_examples.zip` to make it directly callable and is part of the conda packaging process, not a default script from the Google team. If it's not useful, feel free to call directly at the standard installs, it just tries to avoid needing to know the full install path and other details about the parallelization so the command line syntax is simplified. For your example you'd use it like:; ```; dv_make_examples.py; --ref chr20.fa.gz \; --reads test.bam \; --examples shardedExamples/examples.tfrecord@2.gz \; --regions regions.bed \; --sample test \; --logdir location/to/place/logfiles; --cores 1; ```; Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/101#issuecomment-430171385
Testability,test,test,"Phil and Pi-Chuan;; That's exactly right, `dv_make_examples.py` is a simple wrapper for `make_examples.zip` to make it directly callable and is part of the conda packaging process, not a default script from the Google team. If it's not useful, feel free to call directly at the standard installs, it just tries to avoid needing to know the full install path and other details about the parallelization so the command line syntax is simplified. For your example you'd use it like:; ```; dv_make_examples.py; --ref chr20.fa.gz \; --reads test.bam \; --examples shardedExamples/examples.tfrecord@2.gz \; --regions regions.bed \; --sample test \; --logdir location/to/place/logfiles; --cores 1; ```; Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/101#issuecomment-430171385
Usability,simpl,simple,"Phil and Pi-Chuan;; That's exactly right, `dv_make_examples.py` is a simple wrapper for `make_examples.zip` to make it directly callable and is part of the conda packaging process, not a default script from the Google team. If it's not useful, feel free to call directly at the standard installs, it just tries to avoid needing to know the full install path and other details about the parallelization so the command line syntax is simplified. For your example you'd use it like:; ```; dv_make_examples.py; --ref chr20.fa.gz \; --reads test.bam \; --examples shardedExamples/examples.tfrecord@2.gz \; --regions regions.bed \; --sample test \; --logdir location/to/place/logfiles; --cores 1; ```; Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/101#issuecomment-430171385
Availability,mask,mask,"@mclaugsf Thanks for the update! ; 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release.; For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like?; In my run for the WGS casestudy, it converges to something like:. ```; I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]; I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]; I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]; ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105#issuecomment-430711053
Deployability,update,update,"@mclaugsf Thanks for the update! ; 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release.; For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like?; In my run for the WGS casestudy, it converges to something like:. ```; I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]; I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]; I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]; ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105#issuecomment-430711053
Testability,log,log,"@mclaugsf Thanks for the update! ; 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release.; For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like?; In my run for the WGS casestudy, it converges to something like:. ```; I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]; I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]; I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]; ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105#issuecomment-430711053
Usability,clear,clear,"@mclaugsf Thanks for the update! ; 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release.; For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like?; In my run for the WGS casestudy, it converges to something like:. ```; I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]; I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]; I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]; ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105#issuecomment-430711053
Availability,avail,available,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105#issuecomment-430736386
Energy Efficiency,efficient,efficient,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105#issuecomment-430736386
Usability,learn,learning,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105#issuecomment-430736386
Safety,predict,prediction,@pichuan Can you inspect more closely the convolutions in inception v3 to trace out what is happening? How about building an ensemble model where the prediction runs through multiple learned models by either majority vote or a more fine-grained decision boundary.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109#issuecomment-431125198
Usability,learn,learned,@pichuan Can you inspect more closely the convolutions in inception v3 to trace out what is happening? How about building an ensemble model where the prediction runs through multiple learned models by either majority vote or a more fine-grained decision boundary.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109#issuecomment-431125198
Performance,perform,performs,"This is a very interesting question, and the answers to it are complex. I assume for the sake of the question that you have FASTQ data from sequencing a single bacterial colony (so this is not a metagenomics question). Let's divide the question between ""can it technically be done"" and ""will the answers be scientifically valid"". To the question ""can it technically be done"", the answer is probably yes. I am not aware that we have specifically attempted this in bacteria. But if you have a FASTA file with a reference for a species and FASTQ reads, you should be able to generate variant calls for it. To the question ""will the answers be scientifically valid"", it is important to note calling variants in bacterial genomes is an area of open research. Using DeepVariant is reasonable, but I don't think you'll able to consider the output of any method (DeepVariant or other) as certain to give you fully correct results on this problem right out of the box. You'll want to use a few methods (use Freebayes and GATK) and compare between them with metrics you can independently validate, then decide what works and doesn't for your use case. One way to do this could be that for a clonal lineage you expect variants to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin used this measure in a similar way to compare DeepVariant and other methods on inbred rice strains from the 3000 Rice Genomes Project. We would be quite interested to receive your feedback on how DeepVariant performs in this use case, as this may help us understand the value of DeepVariant and improve it for the community.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114#issuecomment-434889612
Security,validat,validate,"This is a very interesting question, and the answers to it are complex. I assume for the sake of the question that you have FASTQ data from sequencing a single bacterial colony (so this is not a metagenomics question). Let's divide the question between ""can it technically be done"" and ""will the answers be scientifically valid"". To the question ""can it technically be done"", the answer is probably yes. I am not aware that we have specifically attempted this in bacteria. But if you have a FASTA file with a reference for a species and FASTQ reads, you should be able to generate variant calls for it. To the question ""will the answers be scientifically valid"", it is important to note calling variants in bacterial genomes is an area of open research. Using DeepVariant is reasonable, but I don't think you'll able to consider the output of any method (DeepVariant or other) as certain to give you fully correct results on this problem right out of the box. You'll want to use a few methods (use Freebayes and GATK) and compare between them with metrics you can independently validate, then decide what works and doesn't for your use case. One way to do this could be that for a clonal lineage you expect variants to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin used this measure in a similar way to compare DeepVariant and other methods on inbred rice strains from the 3000 Rice Genomes Project. We would be quite interested to receive your feedback on how DeepVariant performs in this use case, as this may help us understand the value of DeepVariant and improve it for the community.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114#issuecomment-434889612
Usability,feedback,feedback,"This is a very interesting question, and the answers to it are complex. I assume for the sake of the question that you have FASTQ data from sequencing a single bacterial colony (so this is not a metagenomics question). Let's divide the question between ""can it technically be done"" and ""will the answers be scientifically valid"". To the question ""can it technically be done"", the answer is probably yes. I am not aware that we have specifically attempted this in bacteria. But if you have a FASTA file with a reference for a species and FASTQ reads, you should be able to generate variant calls for it. To the question ""will the answers be scientifically valid"", it is important to note calling variants in bacterial genomes is an area of open research. Using DeepVariant is reasonable, but I don't think you'll able to consider the output of any method (DeepVariant or other) as certain to give you fully correct results on this problem right out of the box. You'll want to use a few methods (use Freebayes and GATK) and compare between them with metrics you can independently validate, then decide what works and doesn't for your use case. One way to do this could be that for a clonal lineage you expect variants to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin used this measure in a similar way to compare DeepVariant and other methods on inbred rice strains from the 3000 Rice Genomes Project. We would be quite interested to receive your feedback on how DeepVariant performs in this use case, as this may help us understand the value of DeepVariant and improve it for the community.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114#issuecomment-434889612
Availability,error,error,"Hi Andrew,; Thanks for answering my questions.It is very informative.; Currently I am using the GATK pipeline to identify any novel strains for a; particular bacteria genus. So far it makes sense, meaning the known mutated; locations on the genome can be found using the pipeline. However, there are; a lot of possible false positives, which could be either potential new; SNPs, or just sequencing error. That's what prompted me to explore; DeepVarient in the first place, to minimize false positives from sequencing; error.; As I understand that GATK has a step to incorporate known variants to; correct for downstream analysis. But bacteria in general do not have those; information. I am wondering do you have any toolbox on this topic?; I'd love to perform some comparisons between different variant detection of; bacteria genomes in the near future. Will keep you posted if I find; anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>; wrote:. > This is a very interesting question, and the answers to it are complex. I; > assume for the sake of the question that you have FASTQ data from; > sequencing a single bacterial colony (so this is not a metagenomics; > question).; >; > Let's divide the question between ""can it technically be done"" and ""will; > the answers be scientifically valid""; >; > To the question ""can it technically be done"", the answer is probably yes.; > I am not aware that we have specifically attempted this in bacteria. But if; > you have a FASTA file with a reference for a species and FASTQ reads, you; > should be able to generate variant calls for it.; >; > To the question ""will the answers be scientifically valid"", it is; > important to note calling variants in bacterial genomes is an area of open; > research. Using DeepVariant is reasonable, but I don't think you'll able to; > consider the output of any method (DeepVariant or other) as certain to give; > you fully correct results on this problem right out of the bo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114#issuecomment-435084063
Deployability,pipeline,pipeline,"Hi Andrew,; Thanks for answering my questions.It is very informative.; Currently I am using the GATK pipeline to identify any novel strains for a; particular bacteria genus. So far it makes sense, meaning the known mutated; locations on the genome can be found using the pipeline. However, there are; a lot of possible false positives, which could be either potential new; SNPs, or just sequencing error. That's what prompted me to explore; DeepVarient in the first place, to minimize false positives from sequencing; error.; As I understand that GATK has a step to incorporate known variants to; correct for downstream analysis. But bacteria in general do not have those; information. I am wondering do you have any toolbox on this topic?; I'd love to perform some comparisons between different variant detection of; bacteria genomes in the near future. Will keep you posted if I find; anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>; wrote:. > This is a very interesting question, and the answers to it are complex. I; > assume for the sake of the question that you have FASTQ data from; > sequencing a single bacterial colony (so this is not a metagenomics; > question).; >; > Let's divide the question between ""can it technically be done"" and ""will; > the answers be scientifically valid""; >; > To the question ""can it technically be done"", the answer is probably yes.; > I am not aware that we have specifically attempted this in bacteria. But if; > you have a FASTA file with a reference for a species and FASTQ reads, you; > should be able to generate variant calls for it.; >; > To the question ""will the answers be scientifically valid"", it is; > important to note calling variants in bacterial genomes is an area of open; > research. Using DeepVariant is reasonable, but I don't think you'll able to; > consider the output of any method (DeepVariant or other) as certain to give; > you fully correct results on this problem right out of the bo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114#issuecomment-435084063
Performance,perform,perform,"Hi Andrew,; Thanks for answering my questions.It is very informative.; Currently I am using the GATK pipeline to identify any novel strains for a; particular bacteria genus. So far it makes sense, meaning the known mutated; locations on the genome can be found using the pipeline. However, there are; a lot of possible false positives, which could be either potential new; SNPs, or just sequencing error. That's what prompted me to explore; DeepVarient in the first place, to minimize false positives from sequencing; error.; As I understand that GATK has a step to incorporate known variants to; correct for downstream analysis. But bacteria in general do not have those; information. I am wondering do you have any toolbox on this topic?; I'd love to perform some comparisons between different variant detection of; bacteria genomes in the near future. Will keep you posted if I find; anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>; wrote:. > This is a very interesting question, and the answers to it are complex. I; > assume for the sake of the question that you have FASTQ data from; > sequencing a single bacterial colony (so this is not a metagenomics; > question).; >; > Let's divide the question between ""can it technically be done"" and ""will; > the answers be scientifically valid""; >; > To the question ""can it technically be done"", the answer is probably yes.; > I am not aware that we have specifically attempted this in bacteria. But if; > you have a FASTA file with a reference for a species and FASTQ reads, you; > should be able to generate variant calls for it.; >; > To the question ""will the answers be scientifically valid"", it is; > important to note calling variants in bacterial genomes is an area of open; > research. Using DeepVariant is reasonable, but I don't think you'll able to; > consider the output of any method (DeepVariant or other) as certain to give; > you fully correct results on this problem right out of the bo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114#issuecomment-435084063
Safety,detect,detection,"Hi Andrew,; Thanks for answering my questions.It is very informative.; Currently I am using the GATK pipeline to identify any novel strains for a; particular bacteria genus. So far it makes sense, meaning the known mutated; locations on the genome can be found using the pipeline. However, there are; a lot of possible false positives, which could be either potential new; SNPs, or just sequencing error. That's what prompted me to explore; DeepVarient in the first place, to minimize false positives from sequencing; error.; As I understand that GATK has a step to incorporate known variants to; correct for downstream analysis. But bacteria in general do not have those; information. I am wondering do you have any toolbox on this topic?; I'd love to perform some comparisons between different variant detection of; bacteria genomes in the near future. Will keep you posted if I find; anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>; wrote:. > This is a very interesting question, and the answers to it are complex. I; > assume for the sake of the question that you have FASTQ data from; > sequencing a single bacterial colony (so this is not a metagenomics; > question).; >; > Let's divide the question between ""can it technically be done"" and ""will; > the answers be scientifically valid""; >; > To the question ""can it technically be done"", the answer is probably yes.; > I am not aware that we have specifically attempted this in bacteria. But if; > you have a FASTA file with a reference for a species and FASTQ reads, you; > should be able to generate variant calls for it.; >; > To the question ""will the answers be scientifically valid"", it is; > important to note calling variants in bacterial genomes is an area of open; > research. Using DeepVariant is reasonable, but I don't think you'll able to; > consider the output of any method (DeepVariant or other) as certain to give; > you fully correct results on this problem right out of the bo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114#issuecomment-435084063
Security,validat,validate,"on, and the answers to it are complex. I; > assume for the sake of the question that you have FASTQ data from; > sequencing a single bacterial colony (so this is not a metagenomics; > question).; >; > Let's divide the question between ""can it technically be done"" and ""will; > the answers be scientifically valid""; >; > To the question ""can it technically be done"", the answer is probably yes.; > I am not aware that we have specifically attempted this in bacteria. But if; > you have a FASTA file with a reference for a species and FASTQ reads, you; > should be able to generate variant calls for it.; >; > To the question ""will the answers be scientifically valid"", it is; > important to note calling variants in bacterial genomes is an area of open; > research. Using DeepVariant is reasonable, but I don't think you'll able to; > consider the output of any method (DeepVariant or other) as certain to give; > you fully correct results on this problem right out of the box. You'll want; > to use a few methods (use Freebayes and GATK) and compare between them with; > metrics you can independently validate, then decide what works and doesn't; > for your use case.; >; > One way to do this could be that for a clonal lineage you expect variants; > to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin; > used this measure in a similar way to compare DeepVariant and other methods; > on inbred rice strains from the 3000 Rice Genomes Project.; >; > We would be quite interested to receive your feedback on how DeepVariant; > performs in this use case, as this may help us understand the value of; > DeepVariant and improve it for the community.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/114#issuecomment-434889612>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AHCP08xLJBFnbpQzdBn9MXeFKOyacKs9ks5uqjzDgaJpZM4YEtb1>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114#issuecomment-435084063
Usability,feedback,feedback,"on, and the answers to it are complex. I; > assume for the sake of the question that you have FASTQ data from; > sequencing a single bacterial colony (so this is not a metagenomics; > question).; >; > Let's divide the question between ""can it technically be done"" and ""will; > the answers be scientifically valid""; >; > To the question ""can it technically be done"", the answer is probably yes.; > I am not aware that we have specifically attempted this in bacteria. But if; > you have a FASTA file with a reference for a species and FASTQ reads, you; > should be able to generate variant calls for it.; >; > To the question ""will the answers be scientifically valid"", it is; > important to note calling variants in bacterial genomes is an area of open; > research. Using DeepVariant is reasonable, but I don't think you'll able to; > consider the output of any method (DeepVariant or other) as certain to give; > you fully correct results on this problem right out of the box. You'll want; > to use a few methods (use Freebayes and GATK) and compare between them with; > metrics you can independently validate, then decide what works and doesn't; > for your use case.; >; > One way to do this could be that for a clonal lineage you expect variants; > to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin; > used this measure in a similar way to compare DeepVariant and other methods; > on inbred rice strains from the 3000 Rice Genomes Project.; >; > We would be quite interested to receive your feedback on how DeepVariant; > performs in this use case, as this may help us understand the value of; > DeepVariant and improve it for the community.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/114#issuecomment-434889612>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AHCP08xLJBFnbpQzdBn9MXeFKOyacKs9ks5uqjzDgaJpZM4YEtb1>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114#issuecomment-435084063
Usability,learn,learning,"Hi @melkerdawy ,; the DeepVariant codebase is currently designed to for DNA data only. The underlying tool and principle (of converting genomic data into a machine learning problem) could be generalized. But the existing tool as is isn't designed or used for RNA-seq data. In another word - it could work, but it will be open-ended research. I'd recommend you looking into how DeepVariant is done, and look into the [Nucleus](https://github.com/google/nucleus) library as well. We just recently announced a pip package for Nucleus. . Feel free to share your experimental results and discuss any issues you've encountered. We'll try our best to answer and discuss with you here. (Closing for now. Feel free to re-open)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115#issuecomment-436861158
Performance,perform,performance,"Hi @AldoCP,. We have not comprehensively investigated performance on RNA-Seq samples yet. Because some components of the RNA-Seq problem more closely resemble variant calling in DNA-Seq exomes, I recommend that you use the exome model instead of the WGS one. A proper investigation would probably involve looking at the variant calls that result from the same sample with both DNA-Seq and RNA-Seq and investigating the concordance of calls. I expect that sort of comparison is probably worth a paper. . In general, we observe that when DeepVariant is applied in other domains beyond its training data, it tends to undercall as opposed to call false positive events, so if you are getting calls in DeepVariant and not HaplotypeCaller, those are worth looking into. If you do so, we would be interested in your feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115#issuecomment-457857605
Usability,feedback,feedback,"Hi @AldoCP,. We have not comprehensively investigated performance on RNA-Seq samples yet. Because some components of the RNA-Seq problem more closely resemble variant calling in DNA-Seq exomes, I recommend that you use the exome model instead of the WGS one. A proper investigation would probably involve looking at the variant calls that result from the same sample with both DNA-Seq and RNA-Seq and investigating the concordance of calls. I expect that sort of comparison is probably worth a paper. . In general, we observe that when DeepVariant is applied in other domains beyond its training data, it tends to undercall as opposed to call false positive events, so if you are getting calls in DeepVariant and not HaplotypeCaller, those are worth looking into. If you do so, we would be interested in your feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115#issuecomment-457857605
Safety,detect,detected,"Thanks for the feedback. I went back to my files and just realized that my previous comment was inaccurate: the locus I analyzed on RNASeq was ""chr20:10,000,000-10,040,000""; the same exonic variant (chr20:10019093) was detected by both GATK and DeepVariant (WGS model) in my sample. As mentioned, I didn't do extensive tests at all (it was just that one locus) -- I'm happy to do further analysis if relevant,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115#issuecomment-462075143
Testability,test,tests,"Thanks for the feedback. I went back to my files and just realized that my previous comment was inaccurate: the locus I analyzed on RNASeq was ""chr20:10,000,000-10,040,000""; the same exonic variant (chr20:10019093) was detected by both GATK and DeepVariant (WGS model) in my sample. As mentioned, I didn't do extensive tests at all (it was just that one locus) -- I'm happy to do further analysis if relevant,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115#issuecomment-462075143
Usability,feedback,feedback,"Thanks for the feedback. I went back to my files and just realized that my previous comment was inaccurate: the locus I analyzed on RNASeq was ""chr20:10,000,000-10,040,000""; the same exonic variant (chr20:10019093) was detected by both GATK and DeepVariant (WGS model) in my sample. As mentioned, I didn't do extensive tests at all (it was just that one locus) -- I'm happy to do further analysis if relevant,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115#issuecomment-462075143
Availability,failure,failure,"Hi @DiableJambe ,. Sorry for my late response, I do not have a docker image yet. And I attached the detailed steps here for your reference. It's a bit long :). # DeepVariant. ## Environment. ```bash; # Power8 environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment; source /etc/profile; module load at11.0; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages; export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python; export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages; ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash; # download source code; wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz; tar -zxvf cmake-3.13.3.tar.gz; cd cmake-3.13.3. # build scirpt; ./bootstrap; make -j20; make -j20 install; export PATH=/usr/local/bin:$PATH; ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$C",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
Deployability,install,install,"Hi @DiableJambe ,. Sorry for my late response, I do not have a docker image yet. And I attached the detailed steps here for your reference. It's a bit long :). # DeepVariant. ## Environment. ```bash; # Power8 environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment; source /etc/profile; module load at11.0; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages; export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python; export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages; ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash; # download source code; wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz; tar -zxvf cmake-3.13.3.tar.gz; cd cmake-3.13.3. # build scirpt; ./bootstrap; make -j20; make -j20 install; export PATH=/usr/local/bin:$PATH; ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$C",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
Energy Efficiency,monitor,monitor," ""$@""//deepvariant:resources_test; # use lscpu to show the actual CPU number; ################################################################################; python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160; python -c ""import psutil;print(p/sutil.cpu_count; ())"" #160. vim deepvariant/resources.py; --------------------------------; def _get_cpu_count():; """"""Gets the number of physical cores in this machine.; Returns:; int >= 1 if the call to get the cpu_count succeeded, or 0 if not.; """"""; # return psutil.cpu_count(logical=False) or 0 ==> comment; return 20; --------------------------------. vim deepvariant/resources_test.py; --------------------------------; def test_metrics_is_ok_when_cpu_count_returns_none(self):; # Some psutil functions, such as cpu_freq(), can return None depending on; # the environment; make sure we don't crash when that occurs.; with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):; with resources.ResourceMonitor() as monitor:; #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment; self.assertEqual(monitor.metrics().physical_core_count, 20); --------------------------------. ##########################################################################; # //deepvariant/realigner/allele_count_linear:generate_trained_model_test; # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8; ##########################################################################; # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python; python -c ""import numpy"" # prequests of TF 1.12.0; python -c ""import scipy"" # prequests of TF 1.12.0; pip install Cython --force-reinstall --no-deos; pip install scikit-learn --force-reinstall --no-deos; # build from source; wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz; tar zxvf 0.20.2.tar.gz; cd scikit-learn-0.20.2; python setup.py bdist_wheel; # ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
Integrability,protocol,protocolbuffers,"r/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment; source /etc/profile; module load at11.0; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages; export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python; export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages; ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash; # download source code; wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz; tar -zxvf cmake-3.13.3.tar.gz; cd cmake-3.13.3. # build scirpt; ./bootstrap; make -j20; make -j20 install; export PATH=/usr/local/bin:$PATH; ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static; make clean; make -j20; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
Modifiability,config,configure,"ke -j20 install; export PATH=/usr/local/bin:$PATH; ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static; make clean; make -j20; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash; # download source code; wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip; mkdir bazel-0.15.0; unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0; cd bazel-0.15.0. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export JAVA_HOME=/usr/lib/jvm/java-1.8.0; export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
Performance,load,load,"Hi @DiableJambe ,. Sorry for my late response, I do not have a docker image yet. And I attached the detailed steps here for your reference. It's a bit long :). # DeepVariant. ## Environment. ```bash; # Power8 environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment; source /etc/profile; module load at11.0; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages; export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python; export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages; ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash; # download source code; wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz; tar -zxvf cmake-3.13.3.tar.gz; cd cmake-3.13.3. # build scirpt; ./bootstrap; make -j20; make -j20 install; export PATH=/usr/local/bin:$PATH; ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$C",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
Security,certificate,certificate," 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash; # download source code ; wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15. # environment; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which gcc. # build; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make -j20; make install. # set environment; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15; echo ""$(python --version)"". # Pip 19.0.2; wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate; $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst; #pip install --upgrade --force-reinstall pip; echo ""$(pip --version)""; pip install setuptools nose asv cython future protobuf==3.6.1 six mock; pip install --upgrade setuptools; ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
Testability,mock,mock,"n.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15. # environment; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which gcc. # build; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make -j20; make install. # set environment; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15; echo ""$(python --version)"". # Pip 19.0.2; wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate; $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst; #pip install --upgrade --force-reinstall pip; echo ""$(pip --version)""; pip install setuptools nose asv cython future protobuf==3.6.1 six mock; pip install --upgrade setuptools; ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)"";",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
Usability,learn,learn,"t(logical=False) or 0 ==> comment; return 20; --------------------------------. vim deepvariant/resources_test.py; --------------------------------; def test_metrics_is_ok_when_cpu_count_returns_none(self):; # Some psutil functions, such as cpu_freq(), can return None depending on; # the environment; make sure we don't crash when that occurs.; with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):; with resources.ResourceMonitor() as monitor:; #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment; self.assertEqual(monitor.metrics().physical_core_count, 20); --------------------------------. ##########################################################################; # //deepvariant/realigner/allele_count_linear:generate_trained_model_test; # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8; ##########################################################################; # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python; python -c ""import numpy"" # prequests of TF 1.12.0; python -c ""import scipy"" # prequests of TF 1.12.0; pip install Cython --force-reinstall --no-deos; pip install scikit-learn --force-reinstall --no-deos; # build from source; wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz; tar zxvf 0.20.2.tar.gz; cd scikit-learn-0.20.2; python setup.py bdist_wheel; # verify; python -c ""from sklearn.externals import joblib"". ##########################################################################; # //deepvariant/labeler:haplotype_labeler_test; # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant/labeler:haplotype_labeler_test --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH; ##########################################################################; # fail due to mock data, open an issue in github; https://github.com/google/deepvariant/issues/154. ##########",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
Availability,checkpoint,checkpoints,"Thank you so much.; In my current understanding, if I use the pre-trained parameters for DeepVariant in Keras or other deep learning frameworks, building the equivalent model in each library is required.; Anyway, I'll try which approach is the easiest for my aim. Please let another confirmation.; For saving the checkpoints ""DeepVariant-inception_v3-0.7.0+data-wgs_standard"", did you used the function in this?:; https://github.com/google/deepvariant/blob/3c43de4541c45673e30d14daef742fca68fdf69b/deepvariant/testing/tf_test_utils.py#L48. Best.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/127#issuecomment-446044386
Testability,test,testing,"Thank you so much.; In my current understanding, if I use the pre-trained parameters for DeepVariant in Keras or other deep learning frameworks, building the equivalent model in each library is required.; Anyway, I'll try which approach is the easiest for my aim. Please let another confirmation.; For saving the checkpoints ""DeepVariant-inception_v3-0.7.0+data-wgs_standard"", did you used the function in this?:; https://github.com/google/deepvariant/blob/3c43de4541c45673e30d14daef742fca68fdf69b/deepvariant/testing/tf_test_utils.py#L48. Best.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/127#issuecomment-446044386
Usability,learn,learning,"Thank you so much.; In my current understanding, if I use the pre-trained parameters for DeepVariant in Keras or other deep learning frameworks, building the equivalent model in each library is required.; Anyway, I'll try which approach is the easiest for my aim. Please let another confirmation.; For saving the checkpoints ""DeepVariant-inception_v3-0.7.0+data-wgs_standard"", did you used the function in this?:; https://github.com/google/deepvariant/blob/3c43de4541c45673e30d14daef742fca68fdf69b/deepvariant/testing/tf_test_utils.py#L48. Best.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/127#issuecomment-446044386
Usability,learn,learning,"Hi @pichuan,. Thank you very much. I got to understand your excellent codes of deepvariant by learning tensorflow slim and Estimator API.; Although I still think that this topic is very useful for many users (deep learning beginners), please close this issue if your prioritization in list of things is low. Best regards,. Masaru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/127#issuecomment-449729322
Usability,usab,usability,"Hi Masaru,; I've filed an internal issue to track - we'll keep usability for beginners in mind for future API change.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/127#issuecomment-450001402
Usability,feedback,feedback,Assigning to @nmousavi for some feedback on why this path changed.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/132#issuecomment-453148437
Usability,feedback,feedback,"Thanks @nmousavi @chrisfleisch @melkerdawy ; For now, please see if the workaround that @nmousavi suggested can be used. @chrisfleisch I'm actually already planning to take a closer look at the way we build the docker image. Thanks for your feedback. I will take this into account when I do that. I'll add the information in your last comment to our internal tracking bug. Note that this might be a little low on my priority list. But I'll try to get to it soon, and I might check back with you directly to make sure things work for you.; I'll leave this GitHub issue open!!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/132#issuecomment-458349387
Deployability,release,released,"@chrisfleisch ; We have released v0.8.0 today :). This time, I made sure to try out Singularity as well. Here are some my personal notes when I tried to pull our Docker image and build Singularity image, based on the suggestion on your comment above. I'm still new to Singularity, so I'd really appreciate more feedback if any of the following doesn't make sense, or if any of these can be improved to be more similar to what users are used to. Once this becomes more mature (and if they become a popular use case), our team can consider adding them for official support in the future as well. ------. # @pichuan 's notes on building DeepVariant Singularity images for CPU and GPU. If you don't have singularity on your computer, install it first:; https://singularity.lbl.gov/install-linux. Once you do, you can pull the DeepVariant Docker image and convert it to a Singularity image. ## CPU image. ```; sudo docker pull gcr.io/deepvariant-docker/deepvariant:0.8.0; sudo docker tag gcr.io/deepvariant-docker/deepvariant:0.8.0 localhost:5000/deepvariant:latest; sudo docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo docker push localhost:5000/deepvariant:latest; SINGULARITY_NOHTTPS=1 singularity build deepvariant.simg docker://localhost:5000/deepvariant:latest; ```. To run [Quick Start](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md), instead of using the docker command, you can use this command instead:. ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; ```; (The extra flags for singularity was added because of the locale issue: https://github.com/BioContainers/containers/issues/206#issuecomment-448698033). ## GPU image; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/132#issuecomment-482430728
Integrability,message,messages,"ant:latest; sudo docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo docker push localhost:5000/deepvariant:latest; SINGULARITY_NOHTTPS=1 singularity build deepvariant.simg docker://localhost:5000/deepvariant:latest; ```. To run [Quick Start](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md), instead of using the docker command, you can use this command instead:. ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; ```; (The extra flags for singularity was added because of the locale issue: https://github.com/BioContainers/containers/issues/206#issuecomment-448698033). ## GPU image; ```; sudo nvidia-docker pull gcr.io/deepvariant-docker/deepvariant_gpu:0.8.0; sudo nvidia-docker tag gcr.io/deepvariant-docker/deepvariant_gpu:0.8.0 localhost:5000/deepvariant_gpu:latest; sudo nvidia-docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo nvidia-docker push localhost:5000/deepvariant_gpu:latest; SINGULARITY_NOHTTPS=1 singularity build deepvariant_gpu.simg docker://localhost:5000/deepvariant_gpu:latest; ```. Running through Quick Start just to make sure nothing wrong:; ```; singularity -s exec --nv -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant_gpu.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; ```. I do see this issue though:; https://github.com/sylabs/singularity/issues/1916; with the; ```; awk: warning: escape sequence `\.' treated as plain `.'; ```; messages.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/132#issuecomment-482430728
Usability,feedback,feedback,"@chrisfleisch ; We have released v0.8.0 today :). This time, I made sure to try out Singularity as well. Here are some my personal notes when I tried to pull our Docker image and build Singularity image, based on the suggestion on your comment above. I'm still new to Singularity, so I'd really appreciate more feedback if any of the following doesn't make sense, or if any of these can be improved to be more similar to what users are used to. Once this becomes more mature (and if they become a popular use case), our team can consider adding them for official support in the future as well. ------. # @pichuan 's notes on building DeepVariant Singularity images for CPU and GPU. If you don't have singularity on your computer, install it first:; https://singularity.lbl.gov/install-linux. Once you do, you can pull the DeepVariant Docker image and convert it to a Singularity image. ## CPU image. ```; sudo docker pull gcr.io/deepvariant-docker/deepvariant:0.8.0; sudo docker tag gcr.io/deepvariant-docker/deepvariant:0.8.0 localhost:5000/deepvariant:latest; sudo docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo docker push localhost:5000/deepvariant:latest; SINGULARITY_NOHTTPS=1 singularity build deepvariant.simg docker://localhost:5000/deepvariant:latest; ```. To run [Quick Start](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md), instead of using the docker command, you can use this command instead:. ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; ```; (The extra flags for singularity was added because of the locale issue: https://github.com/BioContainers/containers/issues/206#issuecomment-448698033). ## GPU image; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/132#issuecomment-482430728
Deployability,install,install,@pgrosu ; I am sorry if I was not clear enough. I tried to say that I could not install glibc locally on my system. ; I started the contact with the system administrator to see what they can do.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/137#issuecomment-454145983
Usability,clear,clear,@pgrosu ; I am sorry if I was not clear enough. I tried to say that I could not install glibc locally on my system. ; I started the contact with the system administrator to see what they can do.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/137#issuecomment-454145983
Deployability,update,update,"@frapaport ; an update on Singularity - I've tested our latest setting (which will come out in the next release) by converting it in to a Singularity image. It seems to work fine for me. So, if you would be able to install singularity, that will be an easier way forward once our next release is out.; I'll still come back and revisit the usability of our bioconda installation. But might take a while.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/137#issuecomment-480563989
Testability,test,tested,"@frapaport ; an update on Singularity - I've tested our latest setting (which will come out in the next release) by converting it in to a Singularity image. It seems to work fine for me. So, if you would be able to install singularity, that will be an easier way forward once our next release is out.; I'll still come back and revisit the usability of our bioconda installation. But might take a while.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/137#issuecomment-480563989
Usability,usab,usability,"@frapaport ; an update on Singularity - I've tested our latest setting (which will come out in the next release) by converting it in to a Singularity image. It seems to work fine for me. So, if you would be able to install singularity, that will be an easier way forward once our next release is out.; I'll still come back and revisit the usability of our bioconda installation. But might take a while.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/137#issuecomment-480563989
Availability,reliab,reliably,"Hello @mosh305 . I would like to learn more about what you would like to do with this sample, and, if possible, propose some alternatives that are more likely to succeed. I don't believe that the NA12878 Mt.Sinai set here can be reliably processed. This is a non-CCS PacBio dataset, so there will be far too many candidate examples generated to process efficiently. Also, the DeepVariant models are not trained for non-CCS PacBio reads. May I recommend that instead you consider the CCS dataset for HG002 that was submitted to genome in a bottle:. ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/PacBio_CCS_15kb/. If this does sound interesting to you, we can provide to you a model trained for the CCS data type. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/138#issuecomment-458681829
Energy Efficiency,efficient,efficiently,"Hello @mosh305 . I would like to learn more about what you would like to do with this sample, and, if possible, propose some alternatives that are more likely to succeed. I don't believe that the NA12878 Mt.Sinai set here can be reliably processed. This is a non-CCS PacBio dataset, so there will be far too many candidate examples generated to process efficiently. Also, the DeepVariant models are not trained for non-CCS PacBio reads. May I recommend that instead you consider the CCS dataset for HG002 that was submitted to genome in a bottle:. ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/PacBio_CCS_15kb/. If this does sound interesting to you, we can provide to you a model trained for the CCS data type. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/138#issuecomment-458681829
Usability,learn,learn,"Hello @mosh305 . I would like to learn more about what you would like to do with this sample, and, if possible, propose some alternatives that are more likely to succeed. I don't believe that the NA12878 Mt.Sinai set here can be reliably processed. This is a non-CCS PacBio dataset, so there will be far too many candidate examples generated to process efficiently. Also, the DeepVariant models are not trained for non-CCS PacBio reads. May I recommend that instead you consider the CCS dataset for HG002 that was submitted to genome in a bottle:. ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/PacBio_CCS_15kb/. If this does sound interesting to you, we can provide to you a model trained for the CCS data type. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/138#issuecomment-458681829
Deployability,install,install,"Hi @andrewrech ; I'll be closing this issue.; To add on the previous answer about `TF_CUDA_VERSION`: currently run-prereq.sh has multiple paths to install tensorflow. If you end up building tensorflow from scratch it self, the env variable `TF_CUDA_VERSION` might be picked up by that. Internally we don't really use that code path anymore so I'm not sure if it actually still works. I'll make a note to simplify and clean up run-prereq.sh in the future. Please feel free to open another bug if you have more questions. If you have more suggestions regarding this particular issue, feel free to follow up here as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145#issuecomment-467081816
Modifiability,variab,variable,"Hi @andrewrech ; I'll be closing this issue.; To add on the previous answer about `TF_CUDA_VERSION`: currently run-prereq.sh has multiple paths to install tensorflow. If you end up building tensorflow from scratch it self, the env variable `TF_CUDA_VERSION` might be picked up by that. Internally we don't really use that code path anymore so I'm not sure if it actually still works. I'll make a note to simplify and clean up run-prereq.sh in the future. Please feel free to open another bug if you have more questions. If you have more suggestions regarding this particular issue, feel free to follow up here as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145#issuecomment-467081816
Usability,simpl,simplify,"Hi @andrewrech ; I'll be closing this issue.; To add on the previous answer about `TF_CUDA_VERSION`: currently run-prereq.sh has multiple paths to install tensorflow. If you end up building tensorflow from scratch it self, the env variable `TF_CUDA_VERSION` might be picked up by that. Internally we don't really use that code path anymore so I'm not sure if it actually still works. I'll make a note to simplify and clean up run-prereq.sh in the future. Please feel free to open another bug if you have more questions. If you have more suggestions regarding this particular issue, feel free to follow up here as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145#issuecomment-467081816
Deployability,configurat,configuration,"@gunjanbaid I think you're right about it being a configuration for a different pipeline. Thanks, I'll try using bam.bai instead of simply .bai and see if that works.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149#issuecomment-461160956
Modifiability,config,configuration,"@gunjanbaid I think you're right about it being a configuration for a different pipeline. Thanks, I'll try using bam.bai instead of simply .bai and see if that works.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149#issuecomment-461160956
Usability,simpl,simply,"@gunjanbaid I think you're right about it being a configuration for a different pipeline. Thanks, I'll try using bam.bai instead of simply .bai and see if that works.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149#issuecomment-461160956
Usability,simpl,simply,I think it actually does work with simply reads.bam and reads.bai. Yep!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149#issuecomment-461539447
Usability,feedback,feedback,"@pichuan Thanks for the feedback -- I have altered my WDL (I'm building this as a method on FireCloud) to include the lscpu command, and have run with 1, 4 and 64 cores/shards so hopefully I will get a better sense of what's going on and report back here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150#issuecomment-461030330
Deployability,pipeline,pipeline,"Oh I thought I was using a consistent model and codebase — thanks for the links. *Edit*: I was using the correct model, it was simply that it was not localized in the correct folder apparently. > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:; > ; > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path?; > ; > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:; > ; > https://cloud.google.com/genomics/docs/tutorials/deepvariant; > ; > Is there any reason why you don't use cloud runner?; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151#issuecomment-461399048
Testability,test,test,"Oh I thought I was using a consistent model and codebase — thanks for the links. *Edit*: I was using the correct model, it was simply that it was not localized in the correct folder apparently. > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:; > ; > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path?; > ; > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:; > ; > https://cloud.google.com/genomics/docs/tutorials/deepvariant; > ; > Is there any reason why you don't use cloud runner?; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151#issuecomment-461399048
Usability,simpl,simply,"Oh I thought I was using a consistent model and codebase — thanks for the links. *Edit*: I was using the correct model, it was simply that it was not localized in the correct folder apparently. > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:; > ; > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path?; > ; > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:; > ; > https://cloud.google.com/genomics/docs/tutorials/deepvariant; > ; > Is there any reason why you don't use cloud runner?; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151#issuecomment-461399048
Availability,failure,failure,"/hs37d5.fa.gzi; ```. Then, I ran `make_examples` similar to the way you did in your original post:; ```; ## Run `make_examples`; ( time seq 0 $((N_SHARDS-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""hs37d5.fa.gz"" \; --reads ""151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam"" \; --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \; --regions ""agilent_sureselect_human_all_exon_v5_b37_targets.bed"" \; --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; ```; This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:; ```; ls HG002.examples.tfrecord*.gz | wc -l; ```; I see 64 of them here.; A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail.; Common failure modes I've seen before:; - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted.; - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:; ```; ## Run `call_variants`; ( time \; /opt/deepvariant/bin/call_variants \; --outfile ""HG002.cvo.tfrecord.gz"" \; --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \; --checkpoint ""model.ckpt"" \; ) 2>&1 | tee ""call_variants.log"" &; ```; When I run this on the same 64-core, 128GB RA",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151#issuecomment-461411282
Deployability,update,update,"ke sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out.; ```; sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2; ```. 2. Inside the interactive mode, run the following:; ```; MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard""; DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata; N_SHARDS=""64"". ## Download extra packages; sudo apt-get -y update; sudo apt-get -y install parallel; sudo apt-get -y install aria2; ## Download models, and test data; # Copy the model files to your local disk.; aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001; aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index; aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data; aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai; aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam; aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed; aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz; aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai; aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi; aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi; ```. Then, I ran `make_examples` similar to the way you did in your original post:; ```; ## Run `make_examples`; ( time seq",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151#issuecomment-461411282
Safety,abort,abort,"/hs37d5.fa.gzi; ```. Then, I ran `make_examples` similar to the way you did in your original post:; ```; ## Run `make_examples`; ( time seq 0 $((N_SHARDS-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""hs37d5.fa.gz"" \; --reads ""151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam"" \; --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \; --regions ""agilent_sureselect_human_all_exon_v5_b37_targets.bed"" \; --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; ```; This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:; ```; ls HG002.examples.tfrecord*.gz | wc -l; ```; I see 64 of them here.; A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail.; Common failure modes I've seen before:; - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted.; - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:; ```; ## Run `call_variants`; ( time \; /opt/deepvariant/bin/call_variants \; --outfile ""HG002.cvo.tfrecord.gz"" \; --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \; --checkpoint ""model.ckpt"" \; ) 2>&1 | tee ""call_variants.log"" &; ```; When I run this on the same 64-core, 128GB RA",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151#issuecomment-461411282
Testability,test,test,"Hi @ekofman ,. a few things:. (1) The `@64` format correspond to `000??-of-00064`. So, in your case, it is expected that `--examples ""${sample_id}.examples.tfrecord@${numShards}.gz""` in the `make_examples` step generated ; `test.examples.tfrecord-000??-of-00064.gz` (shards from 00 to 63). (2) From the command you pasted, it seems like you're directly running inside docker.; If that is the case, you don't need the [-v flags](https://docs.docker.com/storage/volumes/), but you still need to make sure that the files are actually visible when/where you run call_variants.; If you're still running call_variants inside docker (like you did for your make_examples step), please make sure when you can see `test.examples.tfrecord-000??-of-00064.gz` from where you run the command. So, `ls test.examples.tfrecord-000??-of-00064.gz` should see the files. The command you pasted from our run_wgs_case_study_docker.sh example is actually different from the make_examples command you posted first. If you're using that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out.; ```; sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2; ```. 2. Inside the interactive mode, run the following:; ```; MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard""; DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata; N_SHARDS=""64"". ## Download extra packages; sudo apt-get -y update; sudo apt-get -y in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151#issuecomment-461411282
Usability,feedback,feedback,"ut you didn't notice, then the next step will fail.; Common failure modes I've seen before:; - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted.; - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:; ```; ## Run `call_variants`; ( time \; /opt/deepvariant/bin/call_variants \; --outfile ""HG002.cvo.tfrecord.gz"" \; --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \; --checkpoint ""model.ckpt"" \; ) 2>&1 | tee ""call_variants.log"" &; ```; When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:; ```; ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz""; ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151#issuecomment-461411282
Deployability,install,installing,"@ekofman Currently, the case studies (and corresponding scripts) are used to show an example of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc).; If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU.; We have also documented it here:; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run.; If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --; Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151#issuecomment-461426712
Integrability,depend,depending,"@ekofman Currently, the case studies (and corresponding scripts) are used to show an example of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc).; If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU.; We have also documented it here:; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run.; If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --; Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151#issuecomment-461426712
Modifiability,config,configure," to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc).; If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU.; We have also documented it here:; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run.; If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --; Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If you have more suggestions on how to organize the documentation better in the future, please let me know. Even now it's already a bit messy a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151#issuecomment-461426712
Performance,perform,performance," on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc).; If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU.; We have also documented it here:; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run.; If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --; Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If you have more suggestions on how to organize the documentation better in the future, please let me know. Even now it's already a bit messy and I would like to simplify it further. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151#issuecomment-461426712
Usability,learn,learn," on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc).; If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU.; We have also documented it here:; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run.; If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --; Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If you have more suggestions on how to organize the documentation better in the future, please let me know. Even now it's already a bit messy and I would like to simplify it further. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151#issuecomment-461426712
Deployability,upgrade,upgrade,"Hi @qili93 ; Thank you for looking into this! We have an internal issue to track Python 3 upgrade. I don't have a timeline for this now, but will let you know when we have more clear plans. ; Closing this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154#issuecomment-464963634
Usability,clear,clear,"Hi @qili93 ; Thank you for looking into this! We have an internal issue to track Python 3 upgrade. I don't have a timeline for this now, but will let you know when we have more clear plans. ; Closing this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154#issuecomment-464963634
Availability,error,error,"Hi @internalsensor , please see my answer below.; (And, can you give us some feedback on why you don't use the Docker version, but decided to use the prebuilt binaries instead? Thank you!). ===; Hi @internalsensor , ; I think it's likely what @pgrosu said about multiple versions installed. It'll be useful to figure out what actually got used. I was not able to reproduce the error you found.; In case it's useful, here is what I did to try to reproduce. I used a modified version of the WES script, in which I made sure I pip install a pinned version of intervaltree, and I downloaded the prebuilt binaries instead of rebuilding on my own:. ```; git clone https://github.com/google/deepvariant.git; cd deepvariant; wget https://gist.githubusercontent.com/pichuan/5a1216e080ccaf286810af062d6cb7a2/raw/20f6b3e9473b8f2e7d520edace1f2bb7a7231f06/run_wes_case_study_prebuilt_binaries.sh; bash -x run_wes_case_study_prebuilt_binaries.sh; ```. This worked with the prebuilt binaries v0.7.2. So I wasn't able to reproduce your error.; You can also use `pip show intervaltree` to double check what pip package you have.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155#issuecomment-464534790
Deployability,install,installed,"Hi @internalsensor , please see my answer below.; (And, can you give us some feedback on why you don't use the Docker version, but decided to use the prebuilt binaries instead? Thank you!). ===; Hi @internalsensor , ; I think it's likely what @pgrosu said about multiple versions installed. It'll be useful to figure out what actually got used. I was not able to reproduce the error you found.; In case it's useful, here is what I did to try to reproduce. I used a modified version of the WES script, in which I made sure I pip install a pinned version of intervaltree, and I downloaded the prebuilt binaries instead of rebuilding on my own:. ```; git clone https://github.com/google/deepvariant.git; cd deepvariant; wget https://gist.githubusercontent.com/pichuan/5a1216e080ccaf286810af062d6cb7a2/raw/20f6b3e9473b8f2e7d520edace1f2bb7a7231f06/run_wes_case_study_prebuilt_binaries.sh; bash -x run_wes_case_study_prebuilt_binaries.sh; ```. This worked with the prebuilt binaries v0.7.2. So I wasn't able to reproduce your error.; You can also use `pip show intervaltree` to double check what pip package you have.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155#issuecomment-464534790
Usability,feedback,feedback,"Hi @internalsensor , please see my answer below.; (And, can you give us some feedback on why you don't use the Docker version, but decided to use the prebuilt binaries instead? Thank you!). ===; Hi @internalsensor , ; I think it's likely what @pgrosu said about multiple versions installed. It'll be useful to figure out what actually got used. I was not able to reproduce the error you found.; In case it's useful, here is what I did to try to reproduce. I used a modified version of the WES script, in which I made sure I pip install a pinned version of intervaltree, and I downloaded the prebuilt binaries instead of rebuilding on my own:. ```; git clone https://github.com/google/deepvariant.git; cd deepvariant; wget https://gist.githubusercontent.com/pichuan/5a1216e080ccaf286810af062d6cb7a2/raw/20f6b3e9473b8f2e7d520edace1f2bb7a7231f06/run_wes_case_study_prebuilt_binaries.sh; bash -x run_wes_case_study_prebuilt_binaries.sh; ```. This worked with the prebuilt binaries v0.7.2. So I wasn't able to reproduce your error.; You can also use `pip show intervaltree` to double check what pip package you have.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155#issuecomment-464534790
Deployability,release,release,"Hi @kokyriakidis ; MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release.; We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157#issuecomment-465225098
Integrability,interface,interface,"Hi @kokyriakidis ; MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release.; We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157#issuecomment-465225098
Modifiability,config,configuration,"Hi @kokyriakidis ; MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release.; We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157#issuecomment-465225098
Performance,perform,performance,"Hi @kokyriakidis ; MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release.; We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157#issuecomment-465225098
Usability,learn,learn,"Hi @kokyriakidis ; MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release.; We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157#issuecomment-465225098
Deployability,install,install,"Hi @aditya-88, I looked into this issue a bit further. Unfortunately, I don't think using a `disutils.spawn`-style solution will be possible here. There are a few different things to point out. First, we need not just the `pyclif` binary, but other files that get created when you install CLIF. Here are all the files present in my `clif` directory:. ```; $ ls clif; bin clang examples include lib local pip-selfcheck.json python share; ```. Second, even if you did obtain all the needed CLIF files, we still cannot check for them to present at other paths. In our [`WORKSPACE` file](https://github.com/google/deepvariant/blob/r0.7/WORKSPACE#L78) for Bazel, we create a `new_local_repository` for CLIF. Creating this `new_local_repository` requires specifying a full absolute path in advance (more details in [the docs](https://docs.bazel.build/versions/master/be/workspace.html#new_local_repository)). Two solutions for you to get around this would be:; 1. Create a symlink to your CLIF directory via `ln -s $SOME_PATH/clif /usr/local/clif`; OR; 2. Modify the path in the [`WORKSPACE` file](https://github.com/google/deepvariant/blob/r0.7/WORKSPACE#L81). For example, if your files are at `/home/my_account/clif/`, you can change the path to `/home/my_account`. We can definitely make this more clear in our documentation in the future.; @pichuan Feel free to add on if I missed anything.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160#issuecomment-472209625
Usability,clear,clear,"Hi @aditya-88, I looked into this issue a bit further. Unfortunately, I don't think using a `disutils.spawn`-style solution will be possible here. There are a few different things to point out. First, we need not just the `pyclif` binary, but other files that get created when you install CLIF. Here are all the files present in my `clif` directory:. ```; $ ls clif; bin clang examples include lib local pip-selfcheck.json python share; ```. Second, even if you did obtain all the needed CLIF files, we still cannot check for them to present at other paths. In our [`WORKSPACE` file](https://github.com/google/deepvariant/blob/r0.7/WORKSPACE#L78) for Bazel, we create a `new_local_repository` for CLIF. Creating this `new_local_repository` requires specifying a full absolute path in advance (more details in [the docs](https://docs.bazel.build/versions/master/be/workspace.html#new_local_repository)). Two solutions for you to get around this would be:; 1. Create a symlink to your CLIF directory via `ln -s $SOME_PATH/clif /usr/local/clif`; OR; 2. Modify the path in the [`WORKSPACE` file](https://github.com/google/deepvariant/blob/r0.7/WORKSPACE#L81). For example, if your files are at `/home/my_account/clif/`, you can change the path to `/home/my_account`. We can definitely make this more clear in our documentation in the future.; @pichuan Feel free to add on if I missed anything.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160#issuecomment-472209625
Availability,error,error,"artially owned by Google). 2) In the PrecisionFDA Truth Challenge, the HG002 truth set was characterized by NIST but these calls were fully with-held. As a result, this challenge represents the only time that a well-characterized genome was hidden from all developers and offered a unique opportunity to avoid over-fitting. To preserve the validity of this, we never train on HG002 in Illumina data. As noted above, the version of DeepVariant submitted in PrecisionFDA was early, and subsequent improvements improved accuracy both on this sample and for other instruments and PCR preparations. With these accuracy improvements, DeepVariant unambiguously outperforms other entries in both SNP and Indel:. There were 1689 SNP FN and 832 SNP FP. The same sample with the current version of DeepVariant has 1328 SNP FN and 749 SNP FP. . For Indels, the PrecisionFDA submission has 4175 Indel FN and 2839 Indel FP. The current version of DeepVariant has 1428 Indel FN and 924 Indel FP, a reduction in error of almost 50% compared to the most accurate Indel entry in Precision FDA Truth Challenge. The DeepVariant paper has the evaluation numbers for the first open source version (https://www.nature.com/articles/nbt.4235) and compares these results of this with the PrecisionFDA entries. 3) There are good other checks which can provide an indirect estimate of quality and which do not require a particular characterized samples. For example, you can call the same sample with GATK and DeepVariant and take the calls only made in one sample or the other. Comparison of the TiTv for those calls present on one or the other can tell you which (on average) has higher quality (indicated by higher TiTv in the singletons for that caller). We perform these evaluations internally as well and would welcome feedback about a similar analysis from you on your own samples. . 4) When DeepVariant evaluates a candidate, it can call it as a homozygous variant, heterozygous variant, or indicate that it believes tha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-476392585
Performance,perform,perform," There were 1689 SNP FN and 832 SNP FP. The same sample with the current version of DeepVariant has 1328 SNP FN and 749 SNP FP. . For Indels, the PrecisionFDA submission has 4175 Indel FN and 2839 Indel FP. The current version of DeepVariant has 1428 Indel FN and 924 Indel FP, a reduction in error of almost 50% compared to the most accurate Indel entry in Precision FDA Truth Challenge. The DeepVariant paper has the evaluation numbers for the first open source version (https://www.nature.com/articles/nbt.4235) and compares these results of this with the PrecisionFDA entries. 3) There are good other checks which can provide an indirect estimate of quality and which do not require a particular characterized samples. For example, you can call the same sample with GATK and DeepVariant and take the calls only made in one sample or the other. Comparison of the TiTv for those calls present on one or the other can tell you which (on average) has higher quality (indicated by higher TiTv in the singletons for that caller). We perform these evaluations internally as well and would welcome feedback about a similar analysis from you on your own samples. . 4) When DeepVariant evaluates a candidate, it can call it as a homozygous variant, heterozygous variant, or indicate that it believes that although there is evidence for a variant at a position, the true call for this position is reference (0/0). In the paper referenced, I believe that these reference calls were considered as failing a filter. However, it is not the case that these are variant calls that were made and had to be removed. Directly taking the genotype for each call would arrive at the same number of variants. In effect, these were not really variant calls to filter. They were rows in the VCF that already did not indicate variation. . We would be enthusiastic to collaborate with you to benchmark DeepVariant against other methods on your own samples with various preparations and coverages if you like. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-476392585
Safety,avoid,avoid,"Hello Charles,. Let me take your questions point by point. 1) The PrecisionFDA entry is rpoplin-dv42. The corresponds to a substantially earlier version of DeepVariant, but the core elements (deep neural network classification of examples) is the same. Ryan Poplin and Mark DePristo were at Verily at the time, and since transferred to Google Brain. (Verily is partially owned by Google). 2) In the PrecisionFDA Truth Challenge, the HG002 truth set was characterized by NIST but these calls were fully with-held. As a result, this challenge represents the only time that a well-characterized genome was hidden from all developers and offered a unique opportunity to avoid over-fitting. To preserve the validity of this, we never train on HG002 in Illumina data. As noted above, the version of DeepVariant submitted in PrecisionFDA was early, and subsequent improvements improved accuracy both on this sample and for other instruments and PCR preparations. With these accuracy improvements, DeepVariant unambiguously outperforms other entries in both SNP and Indel:. There were 1689 SNP FN and 832 SNP FP. The same sample with the current version of DeepVariant has 1328 SNP FN and 749 SNP FP. . For Indels, the PrecisionFDA submission has 4175 Indel FN and 2839 Indel FP. The current version of DeepVariant has 1428 Indel FN and 924 Indel FP, a reduction in error of almost 50% compared to the most accurate Indel entry in Precision FDA Truth Challenge. The DeepVariant paper has the evaluation numbers for the first open source version (https://www.nature.com/articles/nbt.4235) and compares these results of this with the PrecisionFDA entries. 3) There are good other checks which can provide an indirect estimate of quality and which do not require a particular characterized samples. For example, you can call the same sample with GATK and DeepVariant and take the calls only made in one sample or the other. Comparison of the TiTv for those calls present on one or the other can tell you which (o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-476392585
Testability,benchmark,benchmark," There were 1689 SNP FN and 832 SNP FP. The same sample with the current version of DeepVariant has 1328 SNP FN and 749 SNP FP. . For Indels, the PrecisionFDA submission has 4175 Indel FN and 2839 Indel FP. The current version of DeepVariant has 1428 Indel FN and 924 Indel FP, a reduction in error of almost 50% compared to the most accurate Indel entry in Precision FDA Truth Challenge. The DeepVariant paper has the evaluation numbers for the first open source version (https://www.nature.com/articles/nbt.4235) and compares these results of this with the PrecisionFDA entries. 3) There are good other checks which can provide an indirect estimate of quality and which do not require a particular characterized samples. For example, you can call the same sample with GATK and DeepVariant and take the calls only made in one sample or the other. Comparison of the TiTv for those calls present on one or the other can tell you which (on average) has higher quality (indicated by higher TiTv in the singletons for that caller). We perform these evaluations internally as well and would welcome feedback about a similar analysis from you on your own samples. . 4) When DeepVariant evaluates a candidate, it can call it as a homozygous variant, heterozygous variant, or indicate that it believes that although there is evidence for a variant at a position, the true call for this position is reference (0/0). In the paper referenced, I believe that these reference calls were considered as failing a filter. However, it is not the case that these are variant calls that were made and had to be removed. Directly taking the genotype for each call would arrive at the same number of variants. In effect, these were not really variant calls to filter. They were rows in the VCF that already did not indicate variation. . We would be enthusiastic to collaborate with you to benchmark DeepVariant against other methods on your own samples with various preparations and coverages if you like. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-476392585
Usability,feedback,feedback," There were 1689 SNP FN and 832 SNP FP. The same sample with the current version of DeepVariant has 1328 SNP FN and 749 SNP FP. . For Indels, the PrecisionFDA submission has 4175 Indel FN and 2839 Indel FP. The current version of DeepVariant has 1428 Indel FN and 924 Indel FP, a reduction in error of almost 50% compared to the most accurate Indel entry in Precision FDA Truth Challenge. The DeepVariant paper has the evaluation numbers for the first open source version (https://www.nature.com/articles/nbt.4235) and compares these results of this with the PrecisionFDA entries. 3) There are good other checks which can provide an indirect estimate of quality and which do not require a particular characterized samples. For example, you can call the same sample with GATK and DeepVariant and take the calls only made in one sample or the other. Comparison of the TiTv for those calls present on one or the other can tell you which (on average) has higher quality (indicated by higher TiTv in the singletons for that caller). We perform these evaluations internally as well and would welcome feedback about a similar analysis from you on your own samples. . 4) When DeepVariant evaluates a candidate, it can call it as a homozygous variant, heterozygous variant, or indicate that it believes that although there is evidence for a variant at a position, the true call for this position is reference (0/0). In the paper referenced, I believe that these reference calls were considered as failing a filter. However, it is not the case that these are variant calls that were made and had to be removed. Directly taking the genotype for each call would arrive at the same number of variants. In effect, these were not really variant calls to filter. They were rows in the VCF that already did not indicate variation. . We would be enthusiastic to collaborate with you to benchmark DeepVariant against other methods on your own samples with various preparations and coverages if you like. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-476392585
Deployability,pipeline,pipelines,"Hi Andrew,. Thank you very much for your kind and prompt response. For _point 1_ (and part of _point 2_), thank you very much for that additional information. I think my question was a simpler one: you can see multiple submissions for some groups, and I was trying to see if I understood all the submissions that used DeepVariant. Since there is only one ""rpoplin"" label (and no other entries from Verily Life Sciences), I'll assume that is the only DeepVariant benchmarks (in contrast to the there being multiple groups using GATK, in pipelines that gave varying results). I am also assuming that no one else was using DeepVariant at that time. However, please correct me if I am wrong. For _point 2_, I apologize: it is bad form to critique something without having tested it yourself. I sometimes worry that frequent use of deep learning may represent something that is popular (where many applications may not remain in common use in the long-term), but I need to assess each situation individually. So, I am very sorry about my tone in my initial message. Because of this post, I am now using DeepVariant as a way to practice learning some new skills in my free-time (such as using cloud computing options), but that makes it harder for me to provide a timely response. While the practice is something that I would like to gain on my own (I believe that I will lose some intuition about the results if I don't run the analysis myself), you are certainly welcome to work with any of the underlying data that I have uploaded to my [PGP page](https://my.pgp-hms.org/profile/hu832966). For _point 3_, I apologize that I need to take more time to read other papers carefully before citing them. For example, I have pretty much always seen a drop in accuracy for indels versus SNPs. However, if filtering for regions expected to have good concordance, I the loss in indel accuracy wasn't as bad as you might expect from [Figure 4](https://www.nature.com/articles/s41587-019-0054-x/tables/4) in that pap",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-476428247
Integrability,message,message," for your kind and prompt response. For _point 1_ (and part of _point 2_), thank you very much for that additional information. I think my question was a simpler one: you can see multiple submissions for some groups, and I was trying to see if I understood all the submissions that used DeepVariant. Since there is only one ""rpoplin"" label (and no other entries from Verily Life Sciences), I'll assume that is the only DeepVariant benchmarks (in contrast to the there being multiple groups using GATK, in pipelines that gave varying results). I am also assuming that no one else was using DeepVariant at that time. However, please correct me if I am wrong. For _point 2_, I apologize: it is bad form to critique something without having tested it yourself. I sometimes worry that frequent use of deep learning may represent something that is popular (where many applications may not remain in common use in the long-term), but I need to assess each situation individually. So, I am very sorry about my tone in my initial message. Because of this post, I am now using DeepVariant as a way to practice learning some new skills in my free-time (such as using cloud computing options), but that makes it harder for me to provide a timely response. While the practice is something that I would like to gain on my own (I believe that I will lose some intuition about the results if I don't run the analysis myself), you are certainly welcome to work with any of the underlying data that I have uploaded to my [PGP page](https://my.pgp-hms.org/profile/hu832966). For _point 3_, I apologize that I need to take more time to read other papers carefully before citing them. For example, I have pretty much always seen a drop in accuracy for indels versus SNPs. However, if filtering for regions expected to have good concordance, I the loss in indel accuracy wasn't as bad as you might expect from [Figure 4](https://www.nature.com/articles/s41587-019-0054-x/tables/4) in that paper (however, the concordance in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-476428247
Testability,benchmark,benchmarks,"Hi Andrew,. Thank you very much for your kind and prompt response. For _point 1_ (and part of _point 2_), thank you very much for that additional information. I think my question was a simpler one: you can see multiple submissions for some groups, and I was trying to see if I understood all the submissions that used DeepVariant. Since there is only one ""rpoplin"" label (and no other entries from Verily Life Sciences), I'll assume that is the only DeepVariant benchmarks (in contrast to the there being multiple groups using GATK, in pipelines that gave varying results). I am also assuming that no one else was using DeepVariant at that time. However, please correct me if I am wrong. For _point 2_, I apologize: it is bad form to critique something without having tested it yourself. I sometimes worry that frequent use of deep learning may represent something that is popular (where many applications may not remain in common use in the long-term), but I need to assess each situation individually. So, I am very sorry about my tone in my initial message. Because of this post, I am now using DeepVariant as a way to practice learning some new skills in my free-time (such as using cloud computing options), but that makes it harder for me to provide a timely response. While the practice is something that I would like to gain on my own (I believe that I will lose some intuition about the results if I don't run the analysis myself), you are certainly welcome to work with any of the underlying data that I have uploaded to my [PGP page](https://my.pgp-hms.org/profile/hu832966). For _point 3_, I apologize that I need to take more time to read other papers carefully before citing them. For example, I have pretty much always seen a drop in accuracy for indels versus SNPs. However, if filtering for regions expected to have good concordance, I the loss in indel accuracy wasn't as bad as you might expect from [Figure 4](https://www.nature.com/articles/s41587-019-0054-x/tables/4) in that pap",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-476428247
Usability,simpl,simpler,"Hi Andrew,. Thank you very much for your kind and prompt response. For _point 1_ (and part of _point 2_), thank you very much for that additional information. I think my question was a simpler one: you can see multiple submissions for some groups, and I was trying to see if I understood all the submissions that used DeepVariant. Since there is only one ""rpoplin"" label (and no other entries from Verily Life Sciences), I'll assume that is the only DeepVariant benchmarks (in contrast to the there being multiple groups using GATK, in pipelines that gave varying results). I am also assuming that no one else was using DeepVariant at that time. However, please correct me if I am wrong. For _point 2_, I apologize: it is bad form to critique something without having tested it yourself. I sometimes worry that frequent use of deep learning may represent something that is popular (where many applications may not remain in common use in the long-term), but I need to assess each situation individually. So, I am very sorry about my tone in my initial message. Because of this post, I am now using DeepVariant as a way to practice learning some new skills in my free-time (such as using cloud computing options), but that makes it harder for me to provide a timely response. While the practice is something that I would like to gain on my own (I believe that I will lose some intuition about the results if I don't run the analysis myself), you are certainly welcome to work with any of the underlying data that I have uploaded to my [PGP page](https://my.pgp-hms.org/profile/hu832966). For _point 3_, I apologize that I need to take more time to read other papers carefully before citing them. For example, I have pretty much always seen a drop in accuracy for indels versus SNPs. However, if filtering for regions expected to have good concordance, I the loss in indel accuracy wasn't as bad as you might expect from [Figure 4](https://www.nature.com/articles/s41587-019-0054-x/tables/4) in that pap",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-476428247
Availability,recover,recovery,"led filters for DeepVariant, then that is still only about 9 _million_ sites. However, I wanted to wait until I had my own DeepVariant results before I said anything else, since it is always possible there may have been something that I wouldn’t expecting in the VCF from DeepVariant. ***So, in terms of additional feedback***:. **5)** I compared DeepVariant on Google Cloud (v0.7.2, using code similar to [this](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_gcp_deepvariant_runner.sh)) Exome versus WGS variant calls with [a script that I wrote](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/VCF_recovery.pl) as well as precisionFDA. My script filters out “complex variants” (with more than one variant at a position), but .vcf files containing those variants weren’t flagged by precisionFDA (even though I did have to do some file re-processing). So, starting for my test (checking recovery of my CDS Exome variants in my WGS dataset), **the recovery numbers were very close to what I originally expected (98-99% for SNPs, ~90% for indels)**:. **Provided Exome on Provided WGS .bam Alignment**:; ```; 68759 / 72556 (94.8%) full SNP recovery; 71276 / 72556 (98.2%) partial SNP recovery; 3027 / 3648 (83.0%) full insertion recovery; 3413 / 3648 (93.6%) partial insertion recovery; 3119 / 3911 (79.7%) full deletion recovery; 3596 / 3911 (91.9%) partial deletion recovery; ```. **BWA-MEM Exome on WGS (for only on-target alignments)**:; ```; 51417 / 54229 (94.8%) full SNP recovery; 53116 / 54229 (97.9%) partial SNP recovery; 1964 / 2391 (82.1%) full insertion recovery; 2242 / 2391 (93.8%) partial insertion recovery; 2058 / 2537 (81.1%) full deletion recovery; 2349 / 2537 (92.6%) partial deletion recovery; ```. For comparison, you can see what is reported from precisionFDA (when running DeepVariant on my samples) for the [provided .bam alignment](https://precision.fda.gov/comparisons/3435) or the [BWA-MEM re-aligned comparison](https://pre",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894
Integrability,interface,interface,"lieve the ""winner"" designations from the precisionFDA challenge can be a bit misleading (even though, to be fair, they do color similar high percentiles, even though they also award a designation to one group per category). If I was the winner of a precisionFDA challenge, I would probably want to mention that somewhere. However, I don't typically see sections like ""Why DeepVariant"" at the top of most program READMEs. So, along with some observations about [run-time and cost](https://github.com/google/deepvariant/issues/171#issuecomment-483903505), I think it may respectfully be worth considering trimming back some of that information (**while continuing to provide excellent support on the issues section of GitHub!**). **7)** My understanding is that there is not a DeepVariant App on precisionFDA. I think they use AWS, and I may be able to create something unofficial using code similar to [my AWS test](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) (relating to issues #166 and #167). However, perhaps at some point, you could consider offering something that can be more officially (and better) supported by DeepVariant developers? This would be free to the users (since the FDA is covering the costs of using the DNAnexus-based interface), but there are some unique differences (like I had to change the chromosome formatting for my .vcf files, and there was an issue with my [Veritas WGS header](https://www.biostars.org/p/361415/#366669) that I had to fix). I am currently uploading my .fastq files (the .bam alignments are up there and public, but I think the chr format may cause issue with variant calling comparisons). However, all relevant information for these two samples will be publicly available in precisionFDA (from my [charles.warden](https://precision.fda.gov/users/charles.warden) account). You don’t have to re-open the ticket, but I would certainly welcome any feedback / thoughts that you might have. Sincerely,; Charles",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894
Performance,perform,performed,"fda.gov/challenges/truth/results-explore)"", and select “**func_cds**” (which is where I would expect most of the clinically actionable variants to be) DeepVariant is **not** top ranked for either F-score, Recall, or Precision (although the values are high, like the other variant calling results). I also created a [discussion group](https://precision.fda.gov/discussions/55-hg002-truth-dataset) on precisionFDA asking about how the “truth” set was defined. They mentioned that they **focused on regions where variants could be made most confidently** (genome-wide), and I’m assuming that is why most of the numbers are so high. Otherwise, they are more in the range of using [my same WGS sample and variant caller (DeepVariant) while only changing the alignment](https://precision.fda.gov/comparisons/3437), which isn’t really an independent verification (matching my original concern that the percentages being reported seemed unrealistically high). **In other words, I would say DeepVariant performed well, *along with other strategies tested***. I genuinely believe it is good to have a variety of freely available programs to use, but I believe the ""winner"" designations from the precisionFDA challenge can be a bit misleading (even though, to be fair, they do color similar high percentiles, even though they also award a designation to one group per category). If I was the winner of a precisionFDA challenge, I would probably want to mention that somewhere. However, I don't typically see sections like ""Why DeepVariant"" at the top of most program READMEs. So, along with some observations about [run-time and cost](https://github.com/google/deepvariant/issues/171#issuecomment-483903505), I think it may respectfully be worth considering trimming back some of that information (**while continuing to provide excellent support on the issues section of GitHub!**). **7)** My understanding is that there is not a DeepVariant App on precisionFDA. I think they use AWS, and I may be able to creat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894
Safety,recover,recovery,"led filters for DeepVariant, then that is still only about 9 _million_ sites. However, I wanted to wait until I had my own DeepVariant results before I said anything else, since it is always possible there may have been something that I wouldn’t expecting in the VCF from DeepVariant. ***So, in terms of additional feedback***:. **5)** I compared DeepVariant on Google Cloud (v0.7.2, using code similar to [this](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_gcp_deepvariant_runner.sh)) Exome versus WGS variant calls with [a script that I wrote](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/VCF_recovery.pl) as well as precisionFDA. My script filters out “complex variants” (with more than one variant at a position), but .vcf files containing those variants weren’t flagged by precisionFDA (even though I did have to do some file re-processing). So, starting for my test (checking recovery of my CDS Exome variants in my WGS dataset), **the recovery numbers were very close to what I originally expected (98-99% for SNPs, ~90% for indels)**:. **Provided Exome on Provided WGS .bam Alignment**:; ```; 68759 / 72556 (94.8%) full SNP recovery; 71276 / 72556 (98.2%) partial SNP recovery; 3027 / 3648 (83.0%) full insertion recovery; 3413 / 3648 (93.6%) partial insertion recovery; 3119 / 3911 (79.7%) full deletion recovery; 3596 / 3911 (91.9%) partial deletion recovery; ```. **BWA-MEM Exome on WGS (for only on-target alignments)**:; ```; 51417 / 54229 (94.8%) full SNP recovery; 53116 / 54229 (97.9%) partial SNP recovery; 1964 / 2391 (82.1%) full insertion recovery; 2242 / 2391 (93.8%) partial insertion recovery; 2058 / 2537 (81.1%) full deletion recovery; 2349 / 2537 (92.6%) partial deletion recovery; ```. For comparison, you can see what is reported from precisionFDA (when running DeepVariant on my samples) for the [provided .bam alignment](https://precision.fda.gov/comparisons/3435) or the [BWA-MEM re-aligned comparison](https://pre",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894
Testability,test,testing,"Hi Again,. I’ve had a chance to do some testing with my own samples, so I thought I should report back:. Going back to _point 4_, I noticed that you could choose to output a gVCF file in DeepVariant. However, that is not what is done by default (and it’s not what I did), and I can also tell that’s probably not what they did either. Namely, there are over 3 _billion_ base pairs in the human genome: if you add up the passed filters and failed filters for DeepVariant, then that is still only about 9 _million_ sites. However, I wanted to wait until I had my own DeepVariant results before I said anything else, since it is always possible there may have been something that I wouldn’t expecting in the VCF from DeepVariant. ***So, in terms of additional feedback***:. **5)** I compared DeepVariant on Google Cloud (v0.7.2, using code similar to [this](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_gcp_deepvariant_runner.sh)) Exome versus WGS variant calls with [a script that I wrote](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/VCF_recovery.pl) as well as precisionFDA. My script filters out “complex variants” (with more than one variant at a position), but .vcf files containing those variants weren’t flagged by precisionFDA (even though I did have to do some file re-processing). So, starting for my test (checking recovery of my CDS Exome variants in my WGS dataset), **the recovery numbers were very close to what I originally expected (98-99% for SNPs, ~90% for indels)**:. **Provided Exome on Provided WGS .bam Alignment**:; ```; 68759 / 72556 (94.8%) full SNP recovery; 71276 / 72556 (98.2%) partial SNP recovery; 3027 / 3648 (83.0%) full insertion recovery; 3413 / 3648 (93.6%) partial insertion recovery; 3119 / 3911 (79.7%) full deletion recovery; 3596 / 3911 (91.9%) partial deletion recovery; ```. **BWA-MEM Exome on WGS (for only on-target alignments)**:; ```; 51417 / 54229 (94.8%) full SNP recovery; 53116 / 54229 (97.9%) pa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894
Usability,feedback,feedback,"Hi Again,. I’ve had a chance to do some testing with my own samples, so I thought I should report back:. Going back to _point 4_, I noticed that you could choose to output a gVCF file in DeepVariant. However, that is not what is done by default (and it’s not what I did), and I can also tell that’s probably not what they did either. Namely, there are over 3 _billion_ base pairs in the human genome: if you add up the passed filters and failed filters for DeepVariant, then that is still only about 9 _million_ sites. However, I wanted to wait until I had my own DeepVariant results before I said anything else, since it is always possible there may have been something that I wouldn’t expecting in the VCF from DeepVariant. ***So, in terms of additional feedback***:. **5)** I compared DeepVariant on Google Cloud (v0.7.2, using code similar to [this](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_gcp_deepvariant_runner.sh)) Exome versus WGS variant calls with [a script that I wrote](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/VCF_recovery.pl) as well as precisionFDA. My script filters out “complex variants” (with more than one variant at a position), but .vcf files containing those variants weren’t flagged by precisionFDA (even though I did have to do some file re-processing). So, starting for my test (checking recovery of my CDS Exome variants in my WGS dataset), **the recovery numbers were very close to what I originally expected (98-99% for SNPs, ~90% for indels)**:. **Provided Exome on Provided WGS .bam Alignment**:; ```; 68759 / 72556 (94.8%) full SNP recovery; 71276 / 72556 (98.2%) partial SNP recovery; 3027 / 3648 (83.0%) full insertion recovery; 3413 / 3648 (93.6%) partial insertion recovery; 3119 / 3911 (79.7%) full deletion recovery; 3596 / 3911 (91.9%) partial deletion recovery; ```. **BWA-MEM Exome on WGS (for only on-target alignments)**:; ```; 51417 / 54229 (94.8%) full SNP recovery; 53116 / 54229 (97.9%) pa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894
Availability,error,error,"Thank you very much for your prompt response, particularly on the weekend!. I can see that I overlooked the line `MODEL=""${HOME}/${MODEL_NAME}/model.ckpt""` in this [quick-start](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) guide. I apologize about that. So, if I use `/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard/model.ckpt`, then that works!. If I only put the prefix with ""model,"" I get another error mentioning a checkpoint, but I am assuming that is what the .ckpt extension stands for. Thank you again. **Update (4/1)**: I just remembered that I am supposed to close the issues on GitHub, which is what I did :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166#issuecomment-478414504
Usability,guid,guide,"Thank you very much for your prompt response, particularly on the weekend!. I can see that I overlooked the line `MODEL=""${HOME}/${MODEL_NAME}/model.ckpt""` in this [quick-start](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) guide. I apologize about that. So, if I use `/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard/model.ckpt`, then that works!. If I only put the prefix with ""model,"" I get another error mentioning a checkpoint, but I am assuming that is what the .ckpt extension stands for. Thank you again. **Update (4/1)**: I just remembered that I am supposed to close the issues on GitHub, which is what I did :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166#issuecomment-478414504
Availability,error,error-prone,"Yup that's right. Sorry, I meant `model.ckpt`, not just `model`.; Feel free to open another issue if you have other questions. In the next release, I'm hoping to make it easier to use. One thing I'm looking into is to not have to specify a super long model path, which is error-prone. When the next release comes out, it'll be good to have your feedback again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166#issuecomment-478851260
Deployability,release,release,"Yup that's right. Sorry, I meant `model.ckpt`, not just `model`.; Feel free to open another issue if you have other questions. In the next release, I'm hoping to make it easier to use. One thing I'm looking into is to not have to specify a super long model path, which is error-prone. When the next release comes out, it'll be good to have your feedback again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166#issuecomment-478851260
Usability,feedback,feedback,"Yup that's right. Sorry, I meant `model.ckpt`, not just `model`.; Feel free to open another issue if you have other questions. In the next release, I'm hoping to make it easier to use. One thing I'm looking into is to not have to specify a super long model path, which is error-prone. When the next release comes out, it'll be good to have your feedback again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166#issuecomment-478851260
Deployability,install,installed,"Also, to answer your earlier question:. I may be still in the learning phase about some AWS things, but _m5.4xlarge_ (which is what I was using when I submitted the ticket) is listed as having 16 cores and 64 GB of RAM. I am currently testing running on an _r4.8xlarge_ EC2 instance (part of an ECS cluster, which has Docker installed) is listed as having 32 cores and 244 GB of RAM. If that doesn't work, I think I need to take more time to either look into other strategies (like keep trying to get things set up to run in AWS ""Batch"" and/or seeing if there might be a Docker configuration issue that I can change/fix) and/or go ahead and start my Google Cloud comparison. While I want to be fair and clear about having some costs associated with a learning curve, I've already spent more than $250 in ~2 weeks trying to test DeepVariant in the AWS system. So, in order for this to be a viable option over GATK (which I can run on my local computer for WGS data with 4 cores and 8 GB of ram), I need to be able to confirm that I can in fact perform Exome analysis on Google Cloud for $0.20 (and while ~25 min is not essential, I think you would need to update your page if those two metrics didn't apply to typical usage). Still running that **postprocess_variants** command on AWS, but I will keep you posted until the issue is fixed!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479522653
Modifiability,config,configuration,"Also, to answer your earlier question:. I may be still in the learning phase about some AWS things, but _m5.4xlarge_ (which is what I was using when I submitted the ticket) is listed as having 16 cores and 64 GB of RAM. I am currently testing running on an _r4.8xlarge_ EC2 instance (part of an ECS cluster, which has Docker installed) is listed as having 32 cores and 244 GB of RAM. If that doesn't work, I think I need to take more time to either look into other strategies (like keep trying to get things set up to run in AWS ""Batch"" and/or seeing if there might be a Docker configuration issue that I can change/fix) and/or go ahead and start my Google Cloud comparison. While I want to be fair and clear about having some costs associated with a learning curve, I've already spent more than $250 in ~2 weeks trying to test DeepVariant in the AWS system. So, in order for this to be a viable option over GATK (which I can run on my local computer for WGS data with 4 cores and 8 GB of ram), I need to be able to confirm that I can in fact perform Exome analysis on Google Cloud for $0.20 (and while ~25 min is not essential, I think you would need to update your page if those two metrics didn't apply to typical usage). Still running that **postprocess_variants** command on AWS, but I will keep you posted until the issue is fixed!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479522653
Performance,perform,perform,"Also, to answer your earlier question:. I may be still in the learning phase about some AWS things, but _m5.4xlarge_ (which is what I was using when I submitted the ticket) is listed as having 16 cores and 64 GB of RAM. I am currently testing running on an _r4.8xlarge_ EC2 instance (part of an ECS cluster, which has Docker installed) is listed as having 32 cores and 244 GB of RAM. If that doesn't work, I think I need to take more time to either look into other strategies (like keep trying to get things set up to run in AWS ""Batch"" and/or seeing if there might be a Docker configuration issue that I can change/fix) and/or go ahead and start my Google Cloud comparison. While I want to be fair and clear about having some costs associated with a learning curve, I've already spent more than $250 in ~2 weeks trying to test DeepVariant in the AWS system. So, in order for this to be a viable option over GATK (which I can run on my local computer for WGS data with 4 cores and 8 GB of ram), I need to be able to confirm that I can in fact perform Exome analysis on Google Cloud for $0.20 (and while ~25 min is not essential, I think you would need to update your page if those two metrics didn't apply to typical usage). Still running that **postprocess_variants** command on AWS, but I will keep you posted until the issue is fixed!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479522653
Testability,test,testing,"Also, to answer your earlier question:. I may be still in the learning phase about some AWS things, but _m5.4xlarge_ (which is what I was using when I submitted the ticket) is listed as having 16 cores and 64 GB of RAM. I am currently testing running on an _r4.8xlarge_ EC2 instance (part of an ECS cluster, which has Docker installed) is listed as having 32 cores and 244 GB of RAM. If that doesn't work, I think I need to take more time to either look into other strategies (like keep trying to get things set up to run in AWS ""Batch"" and/or seeing if there might be a Docker configuration issue that I can change/fix) and/or go ahead and start my Google Cloud comparison. While I want to be fair and clear about having some costs associated with a learning curve, I've already spent more than $250 in ~2 weeks trying to test DeepVariant in the AWS system. So, in order for this to be a viable option over GATK (which I can run on my local computer for WGS data with 4 cores and 8 GB of ram), I need to be able to confirm that I can in fact perform Exome analysis on Google Cloud for $0.20 (and while ~25 min is not essential, I think you would need to update your page if those two metrics didn't apply to typical usage). Still running that **postprocess_variants** command on AWS, but I will keep you posted until the issue is fixed!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479522653
Usability,learn,learning,"Also, to answer your earlier question:. I may be still in the learning phase about some AWS things, but _m5.4xlarge_ (which is what I was using when I submitted the ticket) is listed as having 16 cores and 64 GB of RAM. I am currently testing running on an _r4.8xlarge_ EC2 instance (part of an ECS cluster, which has Docker installed) is listed as having 32 cores and 244 GB of RAM. If that doesn't work, I think I need to take more time to either look into other strategies (like keep trying to get things set up to run in AWS ""Batch"" and/or seeing if there might be a Docker configuration issue that I can change/fix) and/or go ahead and start my Google Cloud comparison. While I want to be fair and clear about having some costs associated with a learning curve, I've already spent more than $250 in ~2 weeks trying to test DeepVariant in the AWS system. So, in order for this to be a viable option over GATK (which I can run on my local computer for WGS data with 4 cores and 8 GB of ram), I need to be able to confirm that I can in fact perform Exome analysis on Google Cloud for $0.20 (and while ~25 min is not essential, I think you would need to update your page if those two metrics didn't apply to typical usage). Still running that **postprocess_variants** command on AWS, but I will keep you posted until the issue is fixed!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479522653
Availability,error,error,"Hi Charles,. In addition to what Pi-Chuan said. We run a case study for exome on 128G instance (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-exome-case-study.md). postprocess_variants simply converts variants from internal format to VCF format. This error is very generic and from the information you provided there is no way to say it is related to TensorFlow.; It would be helpful if you could provide logs for postprocess_variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479584377
Testability,log,logs,"Hi Charles,. In addition to what Pi-Chuan said. We run a case study for exome on 128G instance (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-exome-case-study.md). postprocess_variants simply converts variants from internal format to VCF format. This error is very generic and from the information you provided there is no way to say it is related to TensorFlow.; It would be helpful if you could provide logs for postprocess_variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479584377
Usability,simpl,simply,"Hi Charles,. In addition to what Pi-Chuan said. We run a case study for exome on 128G instance (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-exome-case-study.md). postprocess_variants simply converts variants from internal format to VCF format. This error is very generic and from the information you provided there is no way to say it is related to TensorFlow.; It would be helpful if you could provide logs for postprocess_variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479584377
Security,access,access,"Thank you for the reply about TensorFlow. I'll double-check when I get home (I have limited IP access to my AWS Cloud account), but the call_variants file seemed much smaller than I would expect for a file format conversion at the next step. However, I think is probably that it is a binary file. However, the proportions of run time in that case study does match my own experience (the ""call_variants"" step ran the most quickly). I apologize that it will take me a little while to look into all of these things, but I do plan on comparing Google Cloud at some point (possibly for this particular issue). Total time / cost is important, but I am not currently certain what I would recommend to be as clear as possible to users. Using Docker made a huge difference for me for usability. However, at a later point, I am very grateful that you have all of the code open-source (so, if I wanted, I could use DeepVariant to better understand how to use TensorFlow in other applications). For example, even if they don't use COHCAP directly, you can use the [source code](https://github.com/cwarden45/COHCAP/tree/master/src) to see how the Boost libraries for [statistical distributions](https://www.boost.org/doc/libs/1_67_0/libs/math/doc/html/dist.html) (rather than parallel operation) can substantially decrease the run time (relative to the R-base functions). _[I apologize for being a bit off topic, but that is the best analogy that I can think of]_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479627680
Usability,clear,clear,"Thank you for the reply about TensorFlow. I'll double-check when I get home (I have limited IP access to my AWS Cloud account), but the call_variants file seemed much smaller than I would expect for a file format conversion at the next step. However, I think is probably that it is a binary file. However, the proportions of run time in that case study does match my own experience (the ""call_variants"" step ran the most quickly). I apologize that it will take me a little while to look into all of these things, but I do plan on comparing Google Cloud at some point (possibly for this particular issue). Total time / cost is important, but I am not currently certain what I would recommend to be as clear as possible to users. Using Docker made a huge difference for me for usability. However, at a later point, I am very grateful that you have all of the code open-source (so, if I wanted, I could use DeepVariant to better understand how to use TensorFlow in other applications). For example, even if they don't use COHCAP directly, you can use the [source code](https://github.com/cwarden45/COHCAP/tree/master/src) to see how the Boost libraries for [statistical distributions](https://www.boost.org/doc/libs/1_67_0/libs/math/doc/html/dist.html) (rather than parallel operation) can substantially decrease the run time (relative to the R-base functions). _[I apologize for being a bit off topic, but that is the best analogy that I can think of]_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479627680
Testability,test,test,Thank you - I think these commands will be helpful as I learn about using Google Cloud. Please don't rush the AWS test - I am guessing I will have to wait until at least Sunday to be able to set up an account and confirm that using Google Cloud can resolve this issue.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480271688
Usability,learn,learn,Thank you - I think these commands will be helpful as I learn about using Google Cloud. Please don't rush the AWS test - I am guessing I will have to wait until at least Sunday to be able to set up an account and confirm that using Google Cloud can resolve this issue.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480271688
Availability,error,error,"Thank you again for your reply, especially on the weekend. While I'd like to have a way to run all of the steps, thank you for this feedback. As a sort of similar test, I had a m3.large instance open, so I tested running the command from my home directory. For some reason, I don't get the same error message when I do this, but I also don't get any output and no output files are created. Nevertheless, this did gives me some ideas of other things that I can try. In general, maybe there are a couple things that I need to explain:. **1)** I am using an EC2 instance launched from ECS: https://aws.amazon.com/ecs/. This means Docker is already installed. When I create an instance through ECS, I don't think I have the option to create a micro instance (but that isn't my biggest problem). I thought I created the instance with an extra 100 GB of storage (just in case something extra was needed on the local hard drive, beyond RAM). But I don't think this should be the issue for this last step. There may be some other issue that I am not understanding, but I could run the 1st two steps this way (although with an admittedly longer run-time than I expected), and **postprocess_variants** is what I can't get to work. I also received a reply about converting the regular docker image to an[ ECR](https://aws.amazon.com/ecr/) image (submitted on 4/2), to run in AWS [Batch](https://aws.amazon.com/batch/). If that ends up being helpful, I will let you know. If they at least resolve that issue, the next ticket that I submitted (on 4/4) was for this exact issue (but I haven't heard any feedback from that yet). **2)** I want to be able to have my files (.bam, .fastq, etc.) accessible between instances. While I think Google created a way to do with with S3 buckets, my understanding was that I was supposed to do this with an EFS file system: https://aws.amazon.com/efs/. So, _/mnt/efs-genome_ is the EFS file system that contains my .fastq and .bam files (and DeepVariant output). I have to mount",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480616982
Deployability,install,installed,"Thank you again for your reply, especially on the weekend. While I'd like to have a way to run all of the steps, thank you for this feedback. As a sort of similar test, I had a m3.large instance open, so I tested running the command from my home directory. For some reason, I don't get the same error message when I do this, but I also don't get any output and no output files are created. Nevertheless, this did gives me some ideas of other things that I can try. In general, maybe there are a couple things that I need to explain:. **1)** I am using an EC2 instance launched from ECS: https://aws.amazon.com/ecs/. This means Docker is already installed. When I create an instance through ECS, I don't think I have the option to create a micro instance (but that isn't my biggest problem). I thought I created the instance with an extra 100 GB of storage (just in case something extra was needed on the local hard drive, beyond RAM). But I don't think this should be the issue for this last step. There may be some other issue that I am not understanding, but I could run the 1st two steps this way (although with an admittedly longer run-time than I expected), and **postprocess_variants** is what I can't get to work. I also received a reply about converting the regular docker image to an[ ECR](https://aws.amazon.com/ecr/) image (submitted on 4/2), to run in AWS [Batch](https://aws.amazon.com/batch/). If that ends up being helpful, I will let you know. If they at least resolve that issue, the next ticket that I submitted (on 4/4) was for this exact issue (but I haven't heard any feedback from that yet). **2)** I want to be able to have my files (.bam, .fastq, etc.) accessible between instances. While I think Google created a way to do with with S3 buckets, my understanding was that I was supposed to do this with an EFS file system: https://aws.amazon.com/efs/. So, _/mnt/efs-genome_ is the EFS file system that contains my .fastq and .bam files (and DeepVariant output). I have to mount",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480616982
Integrability,message,message,"Thank you again for your reply, especially on the weekend. While I'd like to have a way to run all of the steps, thank you for this feedback. As a sort of similar test, I had a m3.large instance open, so I tested running the command from my home directory. For some reason, I don't get the same error message when I do this, but I also don't get any output and no output files are created. Nevertheless, this did gives me some ideas of other things that I can try. In general, maybe there are a couple things that I need to explain:. **1)** I am using an EC2 instance launched from ECS: https://aws.amazon.com/ecs/. This means Docker is already installed. When I create an instance through ECS, I don't think I have the option to create a micro instance (but that isn't my biggest problem). I thought I created the instance with an extra 100 GB of storage (just in case something extra was needed on the local hard drive, beyond RAM). But I don't think this should be the issue for this last step. There may be some other issue that I am not understanding, but I could run the 1st two steps this way (although with an admittedly longer run-time than I expected), and **postprocess_variants** is what I can't get to work. I also received a reply about converting the regular docker image to an[ ECR](https://aws.amazon.com/ecr/) image (submitted on 4/2), to run in AWS [Batch](https://aws.amazon.com/batch/). If that ends up being helpful, I will let you know. If they at least resolve that issue, the next ticket that I submitted (on 4/4) was for this exact issue (but I haven't heard any feedback from that yet). **2)** I want to be able to have my files (.bam, .fastq, etc.) accessible between instances. While I think Google created a way to do with with S3 buckets, my understanding was that I was supposed to do this with an EFS file system: https://aws.amazon.com/efs/. So, _/mnt/efs-genome_ is the EFS file system that contains my .fastq and .bam files (and DeepVariant output). I have to mount",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480616982
Security,access,accessible,"ve a way to run all of the steps, thank you for this feedback. As a sort of similar test, I had a m3.large instance open, so I tested running the command from my home directory. For some reason, I don't get the same error message when I do this, but I also don't get any output and no output files are created. Nevertheless, this did gives me some ideas of other things that I can try. In general, maybe there are a couple things that I need to explain:. **1)** I am using an EC2 instance launched from ECS: https://aws.amazon.com/ecs/. This means Docker is already installed. When I create an instance through ECS, I don't think I have the option to create a micro instance (but that isn't my biggest problem). I thought I created the instance with an extra 100 GB of storage (just in case something extra was needed on the local hard drive, beyond RAM). But I don't think this should be the issue for this last step. There may be some other issue that I am not understanding, but I could run the 1st two steps this way (although with an admittedly longer run-time than I expected), and **postprocess_variants** is what I can't get to work. I also received a reply about converting the regular docker image to an[ ECR](https://aws.amazon.com/ecr/) image (submitted on 4/2), to run in AWS [Batch](https://aws.amazon.com/batch/). If that ends up being helpful, I will let you know. If they at least resolve that issue, the next ticket that I submitted (on 4/4) was for this exact issue (but I haven't heard any feedback from that yet). **2)** I want to be able to have my files (.bam, .fastq, etc.) accessible between instances. While I think Google created a way to do with with S3 buckets, my understanding was that I was supposed to do this with an EFS file system: https://aws.amazon.com/efs/. So, _/mnt/efs-genome_ is the EFS file system that contains my .fastq and .bam files (and DeepVariant output). I have to mount it whenever I create a new instance, but I don't have to re-upload the files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480616982
Testability,test,test,"Thank you again for your reply, especially on the weekend. While I'd like to have a way to run all of the steps, thank you for this feedback. As a sort of similar test, I had a m3.large instance open, so I tested running the command from my home directory. For some reason, I don't get the same error message when I do this, but I also don't get any output and no output files are created. Nevertheless, this did gives me some ideas of other things that I can try. In general, maybe there are a couple things that I need to explain:. **1)** I am using an EC2 instance launched from ECS: https://aws.amazon.com/ecs/. This means Docker is already installed. When I create an instance through ECS, I don't think I have the option to create a micro instance (but that isn't my biggest problem). I thought I created the instance with an extra 100 GB of storage (just in case something extra was needed on the local hard drive, beyond RAM). But I don't think this should be the issue for this last step. There may be some other issue that I am not understanding, but I could run the 1st two steps this way (although with an admittedly longer run-time than I expected), and **postprocess_variants** is what I can't get to work. I also received a reply about converting the regular docker image to an[ ECR](https://aws.amazon.com/ecr/) image (submitted on 4/2), to run in AWS [Batch](https://aws.amazon.com/batch/). If that ends up being helpful, I will let you know. If they at least resolve that issue, the next ticket that I submitted (on 4/4) was for this exact issue (but I haven't heard any feedback from that yet). **2)** I want to be able to have my files (.bam, .fastq, etc.) accessible between instances. While I think Google created a way to do with with S3 buckets, my understanding was that I was supposed to do this with an EFS file system: https://aws.amazon.com/efs/. So, _/mnt/efs-genome_ is the EFS file system that contains my .fastq and .bam files (and DeepVariant output). I have to mount",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480616982
Usability,feedback,feedback,"Thank you again for your reply, especially on the weekend. While I'd like to have a way to run all of the steps, thank you for this feedback. As a sort of similar test, I had a m3.large instance open, so I tested running the command from my home directory. For some reason, I don't get the same error message when I do this, but I also don't get any output and no output files are created. Nevertheless, this did gives me some ideas of other things that I can try. In general, maybe there are a couple things that I need to explain:. **1)** I am using an EC2 instance launched from ECS: https://aws.amazon.com/ecs/. This means Docker is already installed. When I create an instance through ECS, I don't think I have the option to create a micro instance (but that isn't my biggest problem). I thought I created the instance with an extra 100 GB of storage (just in case something extra was needed on the local hard drive, beyond RAM). But I don't think this should be the issue for this last step. There may be some other issue that I am not understanding, but I could run the 1st two steps this way (although with an admittedly longer run-time than I expected), and **postprocess_variants** is what I can't get to work. I also received a reply about converting the regular docker image to an[ ECR](https://aws.amazon.com/ecr/) image (submitted on 4/2), to run in AWS [Batch](https://aws.amazon.com/batch/). If that ends up being helpful, I will let you know. If they at least resolve that issue, the next ticket that I submitted (on 4/4) was for this exact issue (but I haven't heard any feedback from that yet). **2)** I want to be able to have my files (.bam, .fastq, etc.) accessible between instances. While I think Google created a way to do with with S3 buckets, my understanding was that I was supposed to do this with an EFS file system: https://aws.amazon.com/efs/. So, _/mnt/efs-genome_ is the EFS file system that contains my .fastq and .bam files (and DeepVariant output). I have to mount",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480616982
Availability,error,error,"Also, while I am still doing some extra testing, I figured out the cause of my problem (which was actually very simple):. **1)** The error message changed a little bit (so, I was focusing on the memory, when there was actually another error):. ```; terminate called after throwing an instance of 'std::system_error'; what(): Resource temporarily unavailable; ```. **2)** I was providing the relevant parts of the code on GitHub without realizing that everything from step 2 (**call_variants**) was commented out, which _included_ the CALL_VARIANTS_OUTPUT variable (which is the _input_ file for **postprocess_variants**). After uncommenting that line, the file reformatting ran within a minute (on an AWS ECS m5.2xlarge instance). I am very sorry that it took so many messages (and time) for me to eventually figure out this problem, but I hope this helps with other people seeing a similar error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-482393946
Integrability,message,message,"Also, while I am still doing some extra testing, I figured out the cause of my problem (which was actually very simple):. **1)** The error message changed a little bit (so, I was focusing on the memory, when there was actually another error):. ```; terminate called after throwing an instance of 'std::system_error'; what(): Resource temporarily unavailable; ```. **2)** I was providing the relevant parts of the code on GitHub without realizing that everything from step 2 (**call_variants**) was commented out, which _included_ the CALL_VARIANTS_OUTPUT variable (which is the _input_ file for **postprocess_variants**). After uncommenting that line, the file reformatting ran within a minute (on an AWS ECS m5.2xlarge instance). I am very sorry that it took so many messages (and time) for me to eventually figure out this problem, but I hope this helps with other people seeing a similar error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-482393946
Modifiability,variab,variable,"Also, while I am still doing some extra testing, I figured out the cause of my problem (which was actually very simple):. **1)** The error message changed a little bit (so, I was focusing on the memory, when there was actually another error):. ```; terminate called after throwing an instance of 'std::system_error'; what(): Resource temporarily unavailable; ```. **2)** I was providing the relevant parts of the code on GitHub without realizing that everything from step 2 (**call_variants**) was commented out, which _included_ the CALL_VARIANTS_OUTPUT variable (which is the _input_ file for **postprocess_variants**). After uncommenting that line, the file reformatting ran within a minute (on an AWS ECS m5.2xlarge instance). I am very sorry that it took so many messages (and time) for me to eventually figure out this problem, but I hope this helps with other people seeing a similar error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-482393946
Testability,test,testing,"Also, while I am still doing some extra testing, I figured out the cause of my problem (which was actually very simple):. **1)** The error message changed a little bit (so, I was focusing on the memory, when there was actually another error):. ```; terminate called after throwing an instance of 'std::system_error'; what(): Resource temporarily unavailable; ```. **2)** I was providing the relevant parts of the code on GitHub without realizing that everything from step 2 (**call_variants**) was commented out, which _included_ the CALL_VARIANTS_OUTPUT variable (which is the _input_ file for **postprocess_variants**). After uncommenting that line, the file reformatting ran within a minute (on an AWS ECS m5.2xlarge instance). I am very sorry that it took so many messages (and time) for me to eventually figure out this problem, but I hope this helps with other people seeing a similar error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-482393946
Usability,simpl,simple,"Also, while I am still doing some extra testing, I figured out the cause of my problem (which was actually very simple):. **1)** The error message changed a little bit (so, I was focusing on the memory, when there was actually another error):. ```; terminate called after throwing an instance of 'std::system_error'; what(): Resource temporarily unavailable; ```. **2)** I was providing the relevant parts of the code on GitHub without realizing that everything from step 2 (**call_variants**) was commented out, which _included_ the CALL_VARIANTS_OUTPUT variable (which is the _input_ file for **postprocess_variants**). After uncommenting that line, the file reformatting ran within a minute (on an AWS ECS m5.2xlarge instance). I am very sorry that it took so many messages (and time) for me to eventually figure out this problem, but I hope this helps with other people seeing a similar error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-482393946
Usability,clear,clear,"@xunjieli yes, I can. I only extracted the variants where the analyzed sample had the alternative allele (genotype 1/1,0/1,./1). I did this for the single vcf obtained from deepvariant and for the combined vcf (obtained from gatk using the gvcf created with deepvariant).; Then, I compared only the variants (chromosome, position, reference and alternate allele) of this two sets. Is it clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/170#issuecomment-482316115
Usability,feedback,feedback,"When you say ""_when we use another tool to process the gvcf created by deepvariant, some information are changed_,"" it might worth figuring out what exactly is changed. Without that info, the feedback isn't actionable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/170#issuecomment-482327182
Usability,feedback,feedback,"Thanks @jaqueytw . I don't believe we're currently encouraging our users to use GATK tools to process our files. But if you find any documentation that mentioned/encouraged that, please do let me know and I'd like to fix it. We're actively working on coming up with our own recommendation for best practice. Your analysis and feedback is very valuable. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/170#issuecomment-482328482
Availability,error,error,"Hi Charles,. Thank you for the machine information. For the AWS instance, the 8-core machine has 8-fold fewer CPUs than the 64-cores used in the case study. Given this, the total number of core-hours is 144. This is actually quite efficient for a DeepVariant run. For more similar time to the case study, you could consider the m5.12xlarge or m5.24xlarge, with 48 and 96 cores respectively. The 96-core instance will likely be faster than the case study, though with worse economics. In all cases, the use of spot instances will be far more cost-favorable. For the GCP run posted, this looks to be the GCP cost-optimized execution framework. This framework is built by the Google Cloud, so although I cannot speak as directly to this, I can refer your question to the right place. That GitHub repository is here: https://github.com/googlegenomics/gcp-deepvariant-runner/issues. For your Docker question, I believe that this error reflects that you are not mounting the volumes in the proper way in your script. In your .sh script, you may want to try to more closely align the way your script mounts and creates input and output directories with the case study. I don't feel that I can clearly communicate the details of mounting the Docker volumes. It may be valuable to read Docker's description of this: https://docs.docker.com/storage/volumes/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483500506
Energy Efficiency,efficient,efficient,"Hi Charles,. Thank you for the machine information. For the AWS instance, the 8-core machine has 8-fold fewer CPUs than the 64-cores used in the case study. Given this, the total number of core-hours is 144. This is actually quite efficient for a DeepVariant run. For more similar time to the case study, you could consider the m5.12xlarge or m5.24xlarge, with 48 and 96 cores respectively. The 96-core instance will likely be faster than the case study, though with worse economics. In all cases, the use of spot instances will be far more cost-favorable. For the GCP run posted, this looks to be the GCP cost-optimized execution framework. This framework is built by the Google Cloud, so although I cannot speak as directly to this, I can refer your question to the right place. That GitHub repository is here: https://github.com/googlegenomics/gcp-deepvariant-runner/issues. For your Docker question, I believe that this error reflects that you are not mounting the volumes in the proper way in your script. In your .sh script, you may want to try to more closely align the way your script mounts and creates input and output directories with the case study. I don't feel that I can clearly communicate the details of mounting the Docker volumes. It may be valuable to read Docker's description of this: https://docs.docker.com/storage/volumes/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483500506
Performance,optimiz,optimized,"Hi Charles,. Thank you for the machine information. For the AWS instance, the 8-core machine has 8-fold fewer CPUs than the 64-cores used in the case study. Given this, the total number of core-hours is 144. This is actually quite efficient for a DeepVariant run. For more similar time to the case study, you could consider the m5.12xlarge or m5.24xlarge, with 48 and 96 cores respectively. The 96-core instance will likely be faster than the case study, though with worse economics. In all cases, the use of spot instances will be far more cost-favorable. For the GCP run posted, this looks to be the GCP cost-optimized execution framework. This framework is built by the Google Cloud, so although I cannot speak as directly to this, I can refer your question to the right place. That GitHub repository is here: https://github.com/googlegenomics/gcp-deepvariant-runner/issues. For your Docker question, I believe that this error reflects that you are not mounting the volumes in the proper way in your script. In your .sh script, you may want to try to more closely align the way your script mounts and creates input and output directories with the case study. I don't feel that I can clearly communicate the details of mounting the Docker volumes. It may be valuable to read Docker's description of this: https://docs.docker.com/storage/volumes/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483500506
Usability,clear,clearly,"Hi Charles,. Thank you for the machine information. For the AWS instance, the 8-core machine has 8-fold fewer CPUs than the 64-cores used in the case study. Given this, the total number of core-hours is 144. This is actually quite efficient for a DeepVariant run. For more similar time to the case study, you could consider the m5.12xlarge or m5.24xlarge, with 48 and 96 cores respectively. The 96-core instance will likely be faster than the case study, though with worse economics. In all cases, the use of spot instances will be far more cost-favorable. For the GCP run posted, this looks to be the GCP cost-optimized execution framework. This framework is built by the Google Cloud, so although I cannot speak as directly to this, I can refer your question to the right place. That GitHub repository is here: https://github.com/googlegenomics/gcp-deepvariant-runner/issues. For your Docker question, I believe that this error reflects that you are not mounting the volumes in the proper way in your script. In your .sh script, you may want to try to more closely align the way your script mounts and creates input and output directories with the case study. I don't feel that I can clearly communicate the details of mounting the Docker volumes. It may be valuable to read Docker's description of this: https://docs.docker.com/storage/volumes/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483500506
Usability,simpl,simplicity,"Hi @zstephens, thanks for reaching out! The goal of `run_deepvariant` was to provide users with a single command they could use to run DeepVariant using Docker. For the sake of simplicity, this approach only accepts [a few different flags](https://github.com/google/deepvariant/blob/r0.8/scripts/run_deepvariant.py#L53). We still do support the `sample_name` flag for `make_examples`, but it is not possible to use with `run_deepvariant`. To use this and other flags, I would suggest using separate commands for each step of DeepVariant (`make_examples`, `call_variants`, or `postprocess_variants`). Each of these binaries is included in the Docker image and can be run using a command such as below (with any additional desired flags). ```; sudo docker run gcr.io/deepvariant-docker/deepvariant:0.8.0\ ; /opt/deepvariant/bin/make_examples; ```. I'll close this issue for now, but feel free to reopen if you have any other questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/173#issuecomment-483410868
Security,expose,exposed,"I just checked the code, and you're right that the temp file names will be the same:; https://github.com/google/deepvariant/blob/r0.9/scripts/run_deepvariant.py#L264-L266. For now, please pass in different `intermediate_results_dir` for each run. For example:; `--intermediate_results_dir=""/tmp/deepvariant_tmp_output/chr1""` for chr1, and so on. I'll think about how we want to improve this in the future. I can think of a few options for future improvements, such as :. 1. Use a random name for the internal /tmp files. Given that these are not exposed to the users anyway.; 2. Use a unique name derived from the output VCF file, instead of calling all temp files the same name. For now, using the `--intermediate_results_dir` should hopefully resolve your issue. Let me know if it works. If you have a suggestion on what's the best future improvement for better user experience, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/175#issuecomment-560625427
Usability,user experience,user experience,"I just checked the code, and you're right that the temp file names will be the same:; https://github.com/google/deepvariant/blob/r0.9/scripts/run_deepvariant.py#L264-L266. For now, please pass in different `intermediate_results_dir` for each run. For example:; `--intermediate_results_dir=""/tmp/deepvariant_tmp_output/chr1""` for chr1, and so on. I'll think about how we want to improve this in the future. I can think of a few options for future improvements, such as :. 1. Use a random name for the internal /tmp files. Given that these are not exposed to the users anyway.; 2. Use a unique name derived from the output VCF file, instead of calling all temp files the same name. For now, using the `--intermediate_results_dir` should hopefully resolve your issue. Let me know if it works. If you have a suggestion on what's the best future improvement for better user experience, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/175#issuecomment-560625427
Integrability,contract,contraction,"Hi @njbernstein . Performance on STR will be a function of the size of the event. In Illumina data, DeepVariant will likely stop calling events as they start to reach 100bp in size and larger. DeepVariant will call STR events below this size (for example, here is a HET call repeat expansion in one allele and repeat contraction in the other from a DeepVariant HG002 WGS VCF):. 10	50527727	.	CTATATATATATATATATATATATATATATATATATATATA	C,CTATATATATATATATATATATATATATATATATATATATATA	37.1	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:17:56:2,22,29:0.392857,0.517857:37,20,52,20,0,40. I don't have stratified accuracy metrics for STR performance, nor do I have comparisons of this to dedicated STR tools. I would imagine that dedicated STR callers perform better for the long (100bp+) events, due to specific approaches for that class of problem, below 100bp, I do not have an intuition as to which approach will perform better. For complex variants, do the extent these are in a size range callable by DeepVariant, DeepVariant will represent the sequence-resolved candidates found for variation. Here is an example from a DeepVariant HG002 WGS VCF):. 1 67310873 . CAAAAAAAAAAAAAAAAAAAGAAAAATTAAA C,CAAAAAAAAAAAAAAAAAAAAAGAAAAATTAAA 45.4 PASS . GT:GQ:DP:AD:VAF:PL 1/2:18:36:2,26,6:0.722222,0.166667:28,4,35,4,0,2. The second ALT allele has insertions of A at multiple places, so that this doesn't cleanly fit into a single contiguous set of inserted or deleted bases. In practice, these complex events will be rare in the size range that DeepVariant is designed to address as a small variant caller. Accidentally (because we did not design or train DeepVariant to do so), DeepVariant will call much larger insertion events in PacBio CCS data. Finally, with respect to your haplotype-aware question. Conceptually the first two steps are quite similar. For the first step, identifying which regions to reassemble, DeepVariant employs a relatively simple model which identifies regions that will benefit from reassembly. The",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/180#issuecomment-488147736
Performance,perform,performance,"Hi @njbernstein . Performance on STR will be a function of the size of the event. In Illumina data, DeepVariant will likely stop calling events as they start to reach 100bp in size and larger. DeepVariant will call STR events below this size (for example, here is a HET call repeat expansion in one allele and repeat contraction in the other from a DeepVariant HG002 WGS VCF):. 10	50527727	.	CTATATATATATATATATATATATATATATATATATATATA	C,CTATATATATATATATATATATATATATATATATATATATATA	37.1	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:17:56:2,22,29:0.392857,0.517857:37,20,52,20,0,40. I don't have stratified accuracy metrics for STR performance, nor do I have comparisons of this to dedicated STR tools. I would imagine that dedicated STR callers perform better for the long (100bp+) events, due to specific approaches for that class of problem, below 100bp, I do not have an intuition as to which approach will perform better. For complex variants, do the extent these are in a size range callable by DeepVariant, DeepVariant will represent the sequence-resolved candidates found for variation. Here is an example from a DeepVariant HG002 WGS VCF):. 1 67310873 . CAAAAAAAAAAAAAAAAAAAGAAAAATTAAA C,CAAAAAAAAAAAAAAAAAAAAAGAAAAATTAAA 45.4 PASS . GT:GQ:DP:AD:VAF:PL 1/2:18:36:2,26,6:0.722222,0.166667:28,4,35,4,0,2. The second ALT allele has insertions of A at multiple places, so that this doesn't cleanly fit into a single contiguous set of inserted or deleted bases. In practice, these complex events will be rare in the size range that DeepVariant is designed to address as a small variant caller. Accidentally (because we did not design or train DeepVariant to do so), DeepVariant will call much larger insertion events in PacBio CCS data. Finally, with respect to your haplotype-aware question. Conceptually the first two steps are quite similar. For the first step, identifying which regions to reassemble, DeepVariant employs a relatively simple model which identifies regions that will benefit from reassembly. The",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/180#issuecomment-488147736
Testability,log,logic,"lem, below 100bp, I do not have an intuition as to which approach will perform better. For complex variants, do the extent these are in a size range callable by DeepVariant, DeepVariant will represent the sequence-resolved candidates found for variation. Here is an example from a DeepVariant HG002 WGS VCF):. 1 67310873 . CAAAAAAAAAAAAAAAAAAAGAAAAATTAAA C,CAAAAAAAAAAAAAAAAAAAAAGAAAAATTAAA 45.4 PASS . GT:GQ:DP:AD:VAF:PL 1/2:18:36:2,26,6:0.722222,0.166667:28,4,35,4,0,2. The second ALT allele has insertions of A at multiple places, so that this doesn't cleanly fit into a single contiguous set of inserted or deleted bases. In practice, these complex events will be rare in the size range that DeepVariant is designed to address as a small variant caller. Accidentally (because we did not design or train DeepVariant to do so), DeepVariant will call much larger insertion events in PacBio CCS data. Finally, with respect to your haplotype-aware question. Conceptually the first two steps are quite similar. For the first step, identifying which regions to reassemble, DeepVariant employs a relatively simple model which identifies regions that will benefit from reassembly. The specific implementation differs from GATK (and from the linked description, the GATK logic sounds more complex). Benchmarks reassembling all regions with DeepVariant consistently show the same accuracy to the region selection version. For the second step, conceptually, the methods are very similar. Both construct a de Bruijn graph of reference and alternate contigs. The same authors of these GATK methods are authors of DeepVariant, so apart from writing in C++ for speed, I expect these two to be conceptually similar. For the third step the methods are entirely different. This is where DeepVariant applies a trained convolutional neural network, looking directly at the raw information across the reads, whereas GATK applies a PairHMM to calulate the likelihood of candidate full haplotypes based on their support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/180#issuecomment-488147736
Usability,intuit,intuition,"Hi @njbernstein . Performance on STR will be a function of the size of the event. In Illumina data, DeepVariant will likely stop calling events as they start to reach 100bp in size and larger. DeepVariant will call STR events below this size (for example, here is a HET call repeat expansion in one allele and repeat contraction in the other from a DeepVariant HG002 WGS VCF):. 10	50527727	.	CTATATATATATATATATATATATATATATATATATATATA	C,CTATATATATATATATATATATATATATATATATATATATATA	37.1	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:17:56:2,22,29:0.392857,0.517857:37,20,52,20,0,40. I don't have stratified accuracy metrics for STR performance, nor do I have comparisons of this to dedicated STR tools. I would imagine that dedicated STR callers perform better for the long (100bp+) events, due to specific approaches for that class of problem, below 100bp, I do not have an intuition as to which approach will perform better. For complex variants, do the extent these are in a size range callable by DeepVariant, DeepVariant will represent the sequence-resolved candidates found for variation. Here is an example from a DeepVariant HG002 WGS VCF):. 1 67310873 . CAAAAAAAAAAAAAAAAAAAGAAAAATTAAA C,CAAAAAAAAAAAAAAAAAAAAAGAAAAATTAAA 45.4 PASS . GT:GQ:DP:AD:VAF:PL 1/2:18:36:2,26,6:0.722222,0.166667:28,4,35,4,0,2. The second ALT allele has insertions of A at multiple places, so that this doesn't cleanly fit into a single contiguous set of inserted or deleted bases. In practice, these complex events will be rare in the size range that DeepVariant is designed to address as a small variant caller. Accidentally (because we did not design or train DeepVariant to do so), DeepVariant will call much larger insertion events in PacBio CCS data. Finally, with respect to your haplotype-aware question. Conceptually the first two steps are quite similar. For the first step, identifying which regions to reassemble, DeepVariant employs a relatively simple model which identifies regions that will benefit from reassembly. The",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/180#issuecomment-488147736
Performance,perform,performance,Hi @oschwengers . Thank you for your suggestion. I am curious if you have tried to run DeepVariant on haploid bacteria yet? We have investigated in-bred rice strains and found that it has a strong tendency to call homozygous sites as expected from the genome structure. I was curious if you have evaluated running DeepVariant on bacterial sequencing and seeing whether the results are reasonable. This could be valuable feedback to understand the differences in current performance versus expectations of the field. There may be some additional complexity in understanding how to express subclonal variants (these might manifest as HET calls). I don't have an intuition about the preferences of the field.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/183#issuecomment-490179510
Usability,feedback,feedback,Hi @oschwengers . Thank you for your suggestion. I am curious if you have tried to run DeepVariant on haploid bacteria yet? We have investigated in-bred rice strains and found that it has a strong tendency to call homozygous sites as expected from the genome structure. I was curious if you have evaluated running DeepVariant on bacterial sequencing and seeing whether the results are reasonable. This could be valuable feedback to understand the differences in current performance versus expectations of the field. There may be some additional complexity in understanding how to express subclonal variants (these might manifest as HET calls). I don't have an intuition about the preferences of the field.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/183#issuecomment-490179510
Modifiability,evolve,evolved,"Dear @oschwengers,. I will soon have to call variants from E.coli bacteria genomes and ONT SUP reads and wonder if I can use the newly introduced haploid option to tell Deepvariant that my bacterial reference genome is haploid, like shown in [this page for X and Y](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-xy-calling-case-study.md). `--haploid_contigs ""<Ecoli_chromosome>"" `. Also, will the _ONT_R104_ model be affected by this extra argument?. The paper referred to above by @mbhall88 dates from 2020 and may not be accurate anymore for the haploid aspect of variant calling in bacteria if DeepVariant has evolved in that domain. Thanks for your feedback",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/183#issuecomment-1824632486
Usability,feedback,feedback,"Dear @oschwengers,. I will soon have to call variants from E.coli bacteria genomes and ONT SUP reads and wonder if I can use the newly introduced haploid option to tell Deepvariant that my bacterial reference genome is haploid, like shown in [this page for X and Y](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-xy-calling-case-study.md). `--haploid_contigs ""<Ecoli_chromosome>"" `. Also, will the _ONT_R104_ model be affected by this extra argument?. The paper referred to above by @mbhall88 dates from 2020 and may not be accurate anymore for the haploid aspect of variant calling in bacteria if DeepVariant has evolved in that domain. Thanks for your feedback",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/183#issuecomment-1824632486
Availability,error,errors,"Hi @charlesfeigin,. It's great that things are being run as a non-root user. So to simplify the steps for reaching a solution, I just need three things from you:. 1. The complete set of commands that were typed for launching DeepVariant.; 2. The directories where the input files are located.; 3. The complete output of errors that were seen after DeepVariant was launched. This way we can reconstruct a runnable state. If you don't know everything, that's fine but at least steps 1 & 3 are necessary to begin to reconstruct step 2. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1699480255
Usability,simpl,simplify,"Hi @charlesfeigin,. It's great that things are being run as a non-root user. So to simplify the steps for reaching a solution, I just need three things from you:. 1. The complete set of commands that were typed for launching DeepVariant.; 2. The directories where the input files are located.; 3. The complete output of errors that were seen after DeepVariant was launched. This way we can reconstruct a runnable state. If you don't know everything, that's fine but at least steps 1 & 3 are necessary to begin to reconstruct step 2. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1699480255
Availability,error,error,"ility of non-root users to run deepvariant, I can do that, I just don’t know enough to fix this specific issue myself. 1) The following gives me ; docker run --mount type=bind,source=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs"",target=""/input"" google/deepvariant:""1.5.0"" ls -l /input. docker: Error response from daemon: invalid mount config for type ""bind"": bind source path does not exist: /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling.; See 'docker run --help'. 2) I’ve run the following, but confess I don’t understand what is going on past the second command and then deleting what was done at the end. If creating volumes can be skirted by changing some admin permission that is doable on my end. docker volume create --name dv-vol; dv-vol. docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" touch /input/a-new-file. docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" ls -l /input; total 0; -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file. docker container create --name dv-cp-cont -v dv-vol:/input-path-cont hello-world; 4d2015a9c2dbe3fc6d27854c440ce8505222ed6a8f2a3c945d36315081a832b6. docker cp dv-cp-cont:/input-path-cont . ls input-path-cont. a-new-file; touch input-path-cont/file-2. -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file; -rw-rw-r-- 1 1005 1010 0 Sep 1 00:26 file-2. docker cp input-path-cont/file-2 dv-cp-cont:/input-path-cont . docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" ls -l /input; total 0; -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file; -rw-rw-r-- 1 1005 1010 0 Sep 1 00:26 file-2. docker rmi --force hello-world:latest. Untagged: hello-world:latest; Untagged: hello-world@sha256:dcba6daec718f547568c562956fa47e1b03673dd010fe6ee58ca806767031d1c; Deleted: sha256:9c7a54a9a43cca047013b82af109fe963fde787f63f9e016fdc3384500c2823d. This gave the error you indicated, and the fix worked as well; docker volume rm dv-vol. Best,; Charles",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1701949973
Modifiability,config,config,"Hi Paul,. Thank you again for your replies and sorry mine are so slow (I think we are in very different time zones). So I’m administering this system but am not really a system administrator (I’m a biologist). Since I have root access, if there is something that can be done (or undone) to re-establish the ability of non-root users to run deepvariant, I can do that, I just don’t know enough to fix this specific issue myself. 1) The following gives me ; docker run --mount type=bind,source=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs"",target=""/input"" google/deepvariant:""1.5.0"" ls -l /input. docker: Error response from daemon: invalid mount config for type ""bind"": bind source path does not exist: /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling.; See 'docker run --help'. 2) I’ve run the following, but confess I don’t understand what is going on past the second command and then deleting what was done at the end. If creating volumes can be skirted by changing some admin permission that is doable on my end. docker volume create --name dv-vol; dv-vol. docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" touch /input/a-new-file. docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" ls -l /input; total 0; -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file. docker container create --name dv-cp-cont -v dv-vol:/input-path-cont hello-world; 4d2015a9c2dbe3fc6d27854c440ce8505222ed6a8f2a3c945d36315081a832b6. docker cp dv-cp-cont:/input-path-cont . ls input-path-cont. a-new-file; touch input-path-cont/file-2. -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file; -rw-rw-r-- 1 1005 1010 0 Sep 1 00:26 file-2. docker cp input-path-cont/file-2 dv-cp-cont:/input-path-cont . docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" ls -l /input; total 0; -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file; -rw-rw-r-- 1 1005 1010 0 Sep 1 00:26 file-2. docker rmi --force hello-world:latest. U",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1701949973
Security,access,access,"Hi Paul,. Thank you again for your replies and sorry mine are so slow (I think we are in very different time zones). So I’m administering this system but am not really a system administrator (I’m a biologist). Since I have root access, if there is something that can be done (or undone) to re-establish the ability of non-root users to run deepvariant, I can do that, I just don’t know enough to fix this specific issue myself. 1) The following gives me ; docker run --mount type=bind,source=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs"",target=""/input"" google/deepvariant:""1.5.0"" ls -l /input. docker: Error response from daemon: invalid mount config for type ""bind"": bind source path does not exist: /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling.; See 'docker run --help'. 2) I’ve run the following, but confess I don’t understand what is going on past the second command and then deleting what was done at the end. If creating volumes can be skirted by changing some admin permission that is doable on my end. docker volume create --name dv-vol; dv-vol. docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" touch /input/a-new-file. docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" ls -l /input; total 0; -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file. docker container create --name dv-cp-cont -v dv-vol:/input-path-cont hello-world; 4d2015a9c2dbe3fc6d27854c440ce8505222ed6a8f2a3c945d36315081a832b6. docker cp dv-cp-cont:/input-path-cont . ls input-path-cont. a-new-file; touch input-path-cont/file-2. -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file; -rw-rw-r-- 1 1005 1010 0 Sep 1 00:26 file-2. docker cp input-path-cont/file-2 dv-cp-cont:/input-path-cont . docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" ls -l /input; total 0; -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file; -rw-rw-r-- 1 1005 1010 0 Sep 1 00:26 file-2. docker rmi --force hello-world:latest. U",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1701949973
Usability,undo,undone,"Hi Paul,. Thank you again for your replies and sorry mine are so slow (I think we are in very different time zones). So I’m administering this system but am not really a system administrator (I’m a biologist). Since I have root access, if there is something that can be done (or undone) to re-establish the ability of non-root users to run deepvariant, I can do that, I just don’t know enough to fix this specific issue myself. 1) The following gives me ; docker run --mount type=bind,source=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs"",target=""/input"" google/deepvariant:""1.5.0"" ls -l /input. docker: Error response from daemon: invalid mount config for type ""bind"": bind source path does not exist: /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling.; See 'docker run --help'. 2) I’ve run the following, but confess I don’t understand what is going on past the second command and then deleting what was done at the end. If creating volumes can be skirted by changing some admin permission that is doable on my end. docker volume create --name dv-vol; dv-vol. docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" touch /input/a-new-file. docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" ls -l /input; total 0; -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file. docker container create --name dv-cp-cont -v dv-vol:/input-path-cont hello-world; 4d2015a9c2dbe3fc6d27854c440ce8505222ed6a8f2a3c945d36315081a832b6. docker cp dv-cp-cont:/input-path-cont . ls input-path-cont. a-new-file; touch input-path-cont/file-2. -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file; -rw-rw-r-- 1 1005 1010 0 Sep 1 00:26 file-2. docker cp input-path-cont/file-2 dv-cp-cont:/input-path-cont . docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" ls -l /input; total 0; -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file; -rw-rw-r-- 1 1005 1010 0 Sep 1 00:26 file-2. docker rmi --force hello-world:latest. U",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1701949973
Deployability,install,install,"e under some circumstances. . So let's try the `mount` approach. The reason we used `--mount` instead of `-v` is because it is more granular, telling us specifically which directory it is having an issue with. In this case it is getting stuck on the `variant_calling` directory, and somehow Docker is not able to recognize it. So let's try to determine what type of directory, and what permissions it has (including above and below it). $`1)`$ So if you could please type the following commands, then we can inspect the output to determine what might be the issue:. ```; stat /tiger/home/ajp1/analysis/demography/tasmanian_devil. stat /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling. stat /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs. ls -l /tiger/home/ajp1/analysis/demography | grep tasmanian_devil. ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/ | grep variant_calling. ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling | grep inputs; ```. $`2)`$ Now I have some minor questions about your system and Docker, in order to determine if there could be other underlying causes. For the following questions, it's okay if you don't remember everything: . * What operating system and version are your running? ; * How long ago did you install Docker?; * What commands did you use to install Docker? ; * What version of Docker are you running? (The command is in the commands below.). You previously mention that `ajp1` is part of the docker group, so we can use that as guidance when we inspect the output from the commands above. Below is a set of commands if you could please run, to answer some of the questions above (except for the installation ones). For Linux usually the commands are as follows (some will provide you information, as I included them for multiple operating systems):. ```; uname -a. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. docker --version; ```. Thank you,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1702017107
Usability,guid,guidance,"e under some circumstances. . So let's try the `mount` approach. The reason we used `--mount` instead of `-v` is because it is more granular, telling us specifically which directory it is having an issue with. In this case it is getting stuck on the `variant_calling` directory, and somehow Docker is not able to recognize it. So let's try to determine what type of directory, and what permissions it has (including above and below it). $`1)`$ So if you could please type the following commands, then we can inspect the output to determine what might be the issue:. ```; stat /tiger/home/ajp1/analysis/demography/tasmanian_devil. stat /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling. stat /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs. ls -l /tiger/home/ajp1/analysis/demography | grep tasmanian_devil. ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/ | grep variant_calling. ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling | grep inputs; ```. $`2)`$ Now I have some minor questions about your system and Docker, in order to determine if there could be other underlying causes. For the following questions, it's okay if you don't remember everything: . * What operating system and version are your running? ; * How long ago did you install Docker?; * What commands did you use to install Docker? ; * What version of Docker are you running? (The command is in the commands below.). You previously mention that `ajp1` is part of the docker group, so we can use that as guidance when we inspect the output from the commands above. Below is a set of commands if you could please run, to answer some of the questions above (except for the installation ones). For Linux usually the commands are as follows (some will provide you information, as I included them for multiple operating systems):. ```; uname -a. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. docker --version; ```. Thank you,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1702017107
Availability,checkpoint,checkpoint,"Hi @jumpyknight ,. We have also noticed that when warmstarting from a checkpoint, there seems to be dip in accuracy at the beginning. You can see an example like this in this plot in our training case study from r0.7:; https://raw.githubusercontent.com/google/deepvariant/r0.7/docs/images/TensorBoardAccuracy.png. And specifically, for a while it was puzzling to us why model.ckpt-0 was much less accurate. After looking much closely into TensorFlow and the behavior or warmstarting, here is what we found:. It turns out that there's some subtlety to what exactly gets loaded in warm starting. It's documented here:; https://www.tensorflow.org/api_docs/python/tf/train/warm_start; Specifically, this:; ```; vars_to_warm_start: [Optional] One of the following:; * A regular expression (string) that captures which variables to warm-start (see tf.get_collection). This expression will only consider variables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509
Modifiability,variab,variables,"Hi @jumpyknight ,. We have also noticed that when warmstarting from a checkpoint, there seems to be dip in accuracy at the beginning. You can see an example like this in this plot in our training case study from r0.7:; https://raw.githubusercontent.com/google/deepvariant/r0.7/docs/images/TensorBoardAccuracy.png. And specifically, for a while it was puzzling to us why model.ckpt-0 was much less accurate. After looking much closely into TensorFlow and the behavior or warmstarting, here is what we found:. It turns out that there's some subtlety to what exactly gets loaded in warm starting. It's documented here:; https://www.tensorflow.org/api_docs/python/tf/train/warm_start; Specifically, this:; ```; vars_to_warm_start: [Optional] One of the following:; * A regular expression (string) that captures which variables to warm-start (see tf.get_collection). This expression will only consider variables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509
Performance,load,loaded,"Hi @jumpyknight ,. We have also noticed that when warmstarting from a checkpoint, there seems to be dip in accuracy at the beginning. You can see an example like this in this plot in our training case study from r0.7:; https://raw.githubusercontent.com/google/deepvariant/r0.7/docs/images/TensorBoardAccuracy.png. And specifically, for a while it was puzzling to us why model.ckpt-0 was much less accurate. After looking much closely into TensorFlow and the behavior or warmstarting, here is what we found:. It turns out that there's some subtlety to what exactly gets loaded in warm starting. It's documented here:; https://www.tensorflow.org/api_docs/python/tf/train/warm_start; Specifically, this:; ```; vars_to_warm_start: [Optional] One of the following:; * A regular expression (string) that captures which variables to warm-start (see tf.get_collection). This expression will only consider variables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509
Safety,predict,prediction,"uch closely into TensorFlow and the behavior or warmstarting, here is what we found:. It turns out that there's some subtlety to what exactly gets loaded in warm starting. It's documented here:; https://www.tensorflow.org/api_docs/python/tf/train/warm_start; Specifically, this:; ```; vars_to_warm_start: [Optional] One of the following:; * A regular expression (string) that captures which variables to warm-start (see tf.get_collection). This expression will only consider variables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring here. If try with the small code change above and observe the log -- you'll see that a lot more vars are being used in warm-starting now. (But note that you'll likely want to exclude some variables such as global_step, because those will affect your learning rate). Note that even when warmstarting from an existing checkpoint, we don't currently recommend ""fine-tuning"" with very few steps. Even with our small TPU train",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509
Testability,log,log,"iables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring here. If try with the small code change above and observe the log -- you'll see that a lot more vars are being used in warm-starting now. (But note that you'll likely want to exclude some variables such as global_step, because those will affect your learning rate). Note that even when warmstarting from an existing checkpoint, we don't currently recommend ""fine-tuning"" with very few steps. Even with our small TPU training case study, we showed an example to train up to 50k steps before we pick a model.; So if you want to use our code to fine tune on your data, please proceed with caution and make sure you carefully benchmark the accuracy and robustness of the resulting model. . Thanks for sharing your findings here. If you have more questions or anything you want to discuss, please feel free to bring up here. (And, adding my teammate @emschorsch who has been looking into warmstarting. )",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509
Usability,learn,learning,"iables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring here. If try with the small code change above and observe the log -- you'll see that a lot more vars are being used in warm-starting now. (But note that you'll likely want to exclude some variables such as global_step, because those will affect your learning rate). Note that even when warmstarting from an existing checkpoint, we don't currently recommend ""fine-tuning"" with very few steps. Even with our small TPU training case study, we showed an example to train up to 50k steps before we pick a model.; So if you want to use our code to fine tune on your data, please proceed with caution and make sure you carefully benchmark the accuracy and robustness of the resulting model. . Thanks for sharing your findings here. If you have more questions or anything you want to discuss, please feel free to bring up here. (And, adding my teammate @emschorsch who has been looking into warmstarting. )",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509
Availability,error,error,"Hi Phil,; as you can see from the log you posted, the error actually came from:; ```; File ""/opt/conda/envs/nf-core-deepvariant-1.0/lib/python2.7/site-packages/psutil/_pslinux.py"", line 701, in cpu_freq; ""can't find current frequency file""); ```; If I read this correctly, ""frequency"" isn't referring to anything DeepVariant related, but is referring to ""cpu_freq"". I also did a search in the DeepVariant codebase for the string ""can't find current frequency file"" and I can confirm that error message does not come from us. It is still possible that this is related to some interaction with the DeepVariant code, but from the information you provide, it's not clear to me how.; And, given that this is at the system level, I don't think it's related to what organism you're providing as the input data. To understand whether your input data works on DeepVariant code or not, the best way is probably to try it with just the DeepVariant code. If that doesn't work, it'll be more clear whether there might be different issues. If the data you're running on is publicly accessible, I'm also happy to try running DeepVariant on it. Let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/191#issuecomment-504481029
Integrability,message,message,"Hi Phil,; as you can see from the log you posted, the error actually came from:; ```; File ""/opt/conda/envs/nf-core-deepvariant-1.0/lib/python2.7/site-packages/psutil/_pslinux.py"", line 701, in cpu_freq; ""can't find current frequency file""); ```; If I read this correctly, ""frequency"" isn't referring to anything DeepVariant related, but is referring to ""cpu_freq"". I also did a search in the DeepVariant codebase for the string ""can't find current frequency file"" and I can confirm that error message does not come from us. It is still possible that this is related to some interaction with the DeepVariant code, but from the information you provide, it's not clear to me how.; And, given that this is at the system level, I don't think it's related to what organism you're providing as the input data. To understand whether your input data works on DeepVariant code or not, the best way is probably to try it with just the DeepVariant code. If that doesn't work, it'll be more clear whether there might be different issues. If the data you're running on is publicly accessible, I'm also happy to try running DeepVariant on it. Let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/191#issuecomment-504481029
Security,access,accessible,"Hi Phil,; as you can see from the log you posted, the error actually came from:; ```; File ""/opt/conda/envs/nf-core-deepvariant-1.0/lib/python2.7/site-packages/psutil/_pslinux.py"", line 701, in cpu_freq; ""can't find current frequency file""); ```; If I read this correctly, ""frequency"" isn't referring to anything DeepVariant related, but is referring to ""cpu_freq"". I also did a search in the DeepVariant codebase for the string ""can't find current frequency file"" and I can confirm that error message does not come from us. It is still possible that this is related to some interaction with the DeepVariant code, but from the information you provide, it's not clear to me how.; And, given that this is at the system level, I don't think it's related to what organism you're providing as the input data. To understand whether your input data works on DeepVariant code or not, the best way is probably to try it with just the DeepVariant code. If that doesn't work, it'll be more clear whether there might be different issues. If the data you're running on is publicly accessible, I'm also happy to try running DeepVariant on it. Let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/191#issuecomment-504481029
Testability,log,log,"Hi Phil,; as you can see from the log you posted, the error actually came from:; ```; File ""/opt/conda/envs/nf-core-deepvariant-1.0/lib/python2.7/site-packages/psutil/_pslinux.py"", line 701, in cpu_freq; ""can't find current frequency file""); ```; If I read this correctly, ""frequency"" isn't referring to anything DeepVariant related, but is referring to ""cpu_freq"". I also did a search in the DeepVariant codebase for the string ""can't find current frequency file"" and I can confirm that error message does not come from us. It is still possible that this is related to some interaction with the DeepVariant code, but from the information you provide, it's not clear to me how.; And, given that this is at the system level, I don't think it's related to what organism you're providing as the input data. To understand whether your input data works on DeepVariant code or not, the best way is probably to try it with just the DeepVariant code. If that doesn't work, it'll be more clear whether there might be different issues. If the data you're running on is publicly accessible, I'm also happy to try running DeepVariant on it. Let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/191#issuecomment-504481029
Usability,clear,clear,"Hi Phil,; as you can see from the log you posted, the error actually came from:; ```; File ""/opt/conda/envs/nf-core-deepvariant-1.0/lib/python2.7/site-packages/psutil/_pslinux.py"", line 701, in cpu_freq; ""can't find current frequency file""); ```; If I read this correctly, ""frequency"" isn't referring to anything DeepVariant related, but is referring to ""cpu_freq"". I also did a search in the DeepVariant codebase for the string ""can't find current frequency file"" and I can confirm that error message does not come from us. It is still possible that this is related to some interaction with the DeepVariant code, but from the information you provide, it's not clear to me how.; And, given that this is at the system level, I don't think it's related to what organism you're providing as the input data. To understand whether your input data works on DeepVariant code or not, the best way is probably to try it with just the DeepVariant code. If that doesn't work, it'll be more clear whether there might be different issues. If the data you're running on is publicly accessible, I'm also happy to try running DeepVariant on it. Let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/191#issuecomment-504481029
Availability,checkpoint,checkpoint,The number is referring to the number of steps in training when this checkpoint is saved.; You can see https://www.tensorflow.org/guide/checkpoints for more information.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/192#issuecomment-506979241
Usability,guid,guide,The number is referring to the number of steps in training when this checkpoint is saved.; You can see https://www.tensorflow.org/guide/checkpoints for more information.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/192#issuecomment-506979241
Usability,simpl,simple,"> Is there a way that I could just modify the gcp_deepvariant_runner.py script . Yes, you can easily make deepvariant_runner to generate gVCF output by using `--gvcf_outfile` flag, please refer to [our documentation](https://cloud.google.com/genomics/docs/tutorials/deepvariant#genomic_vcf_gvcf_configuration) for more details. > I'm guessing I would need to fork the gcp-deepvariant-runner repo. That's one way to do it, however, the easier way is to use our docker image; in that case launching DeepVariant will be as simple as running a `gcloud ...` command. Again please refer to [our documentations](https://cloud.google.com/genomics/docs/tutorials/deepvariant#before_you_begin) for more details.; The only issue at the moment is that deepvariant_runner is still working with previous version of DeepVariant (0.7.2) and we are in the process of releasing a new docker image that will be compatible with the latest DeepVairant (0.8.8). Please let me know if you have any difficulties with deepvariant_runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/193#issuecomment-508203315
Usability,simpl,simple,"It's normal for the loss curve to start flattening out, although yours does change pretty abruptly. What exactly did you change (you mentioned using a `very simple topology`, not sure if that means you changed the architecture or something)? Also the commands you used would be helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/194#issuecomment-509337028
Usability,simpl,simple,"Thanks for the comments. I tried to train a simple 10-cnn-layer architecture and got the above results. Although the final loss still seems to be rather high, the accuracy is acceptable for the simple cnn network. I will close this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/194#issuecomment-513082707
Availability,error,error,"That paper is an early version of the code - so some things changed and some didn't. So let's parse this out:. 1) In the blog, if you look at [Jason's Jupyter notebooks](https://github.com/cschin/DeepDiveDeepVariant), you'll see him using `--norealign_reads` which skips the preprocessing step described in the paper. That's why he's getting the `T` which a is a possible sequencing error. 2) The general realignment heuristic is the following: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/realigner.py#L439-L454. 3) The ploidy dictates the most likely haplotypes which is 2 :). 3) The haploytype realignment also been optimized a bit since that paper. Take a look at this section of the code: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/fast_pass_aligner.cc#L117-L166. 4) Yes, the realignment is basically used to update the reads' interpretation via the best-representative haplotype, trying to approach what might be closest-truth if the sequencer were relatively perfect. It's basically a way to correct. 5) Variant calling comes later, and is performed via the following, which is called from [make_examples.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/make_examples.py):. https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_caller.py ; https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_calling.cc. 6) The [predict(...) in call_variants.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/call_variants.py#L374-L378) emits the `genotype_probabilities`, which Jason used to demonstrate the low probability of `T` as a sequencing error. Just like any machine-learning trained model, if it's not seen the pattern before it will not predict it with a high match - no real magic here :). 7) It might help to read the [deepvariant.proto](https://github.com/google/deepvariant/blob/r0.8/deepvariant/protos/deepvariant.proto), which you can treat like a spec-design document of the genera",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/197#issuecomment-512112524
Deployability,update,update,"That paper is an early version of the code - so some things changed and some didn't. So let's parse this out:. 1) In the blog, if you look at [Jason's Jupyter notebooks](https://github.com/cschin/DeepDiveDeepVariant), you'll see him using `--norealign_reads` which skips the preprocessing step described in the paper. That's why he's getting the `T` which a is a possible sequencing error. 2) The general realignment heuristic is the following: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/realigner.py#L439-L454. 3) The ploidy dictates the most likely haplotypes which is 2 :). 3) The haploytype realignment also been optimized a bit since that paper. Take a look at this section of the code: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/fast_pass_aligner.cc#L117-L166. 4) Yes, the realignment is basically used to update the reads' interpretation via the best-representative haplotype, trying to approach what might be closest-truth if the sequencer were relatively perfect. It's basically a way to correct. 5) Variant calling comes later, and is performed via the following, which is called from [make_examples.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/make_examples.py):. https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_caller.py ; https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_calling.cc. 6) The [predict(...) in call_variants.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/call_variants.py#L374-L378) emits the `genotype_probabilities`, which Jason used to demonstrate the low probability of `T` as a sequencing error. Just like any machine-learning trained model, if it's not seen the pattern before it will not predict it with a high match - no real magic here :). 7) It might help to read the [deepvariant.proto](https://github.com/google/deepvariant/blob/r0.8/deepvariant/protos/deepvariant.proto), which you can treat like a spec-design document of the genera",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/197#issuecomment-512112524
Performance,optimiz,optimized,"That paper is an early version of the code - so some things changed and some didn't. So let's parse this out:. 1) In the blog, if you look at [Jason's Jupyter notebooks](https://github.com/cschin/DeepDiveDeepVariant), you'll see him using `--norealign_reads` which skips the preprocessing step described in the paper. That's why he's getting the `T` which a is a possible sequencing error. 2) The general realignment heuristic is the following: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/realigner.py#L439-L454. 3) The ploidy dictates the most likely haplotypes which is 2 :). 3) The haploytype realignment also been optimized a bit since that paper. Take a look at this section of the code: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/fast_pass_aligner.cc#L117-L166. 4) Yes, the realignment is basically used to update the reads' interpretation via the best-representative haplotype, trying to approach what might be closest-truth if the sequencer were relatively perfect. It's basically a way to correct. 5) Variant calling comes later, and is performed via the following, which is called from [make_examples.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/make_examples.py):. https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_caller.py ; https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_calling.cc. 6) The [predict(...) in call_variants.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/call_variants.py#L374-L378) emits the `genotype_probabilities`, which Jason used to demonstrate the low probability of `T` as a sequencing error. Just like any machine-learning trained model, if it's not seen the pattern before it will not predict it with a high match - no real magic here :). 7) It might help to read the [deepvariant.proto](https://github.com/google/deepvariant/blob/r0.8/deepvariant/protos/deepvariant.proto), which you can treat like a spec-design document of the genera",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/197#issuecomment-512112524
Safety,predict,predict,"/github.com/cschin/DeepDiveDeepVariant), you'll see him using `--norealign_reads` which skips the preprocessing step described in the paper. That's why he's getting the `T` which a is a possible sequencing error. 2) The general realignment heuristic is the following: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/realigner.py#L439-L454. 3) The ploidy dictates the most likely haplotypes which is 2 :). 3) The haploytype realignment also been optimized a bit since that paper. Take a look at this section of the code: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/fast_pass_aligner.cc#L117-L166. 4) Yes, the realignment is basically used to update the reads' interpretation via the best-representative haplotype, trying to approach what might be closest-truth if the sequencer were relatively perfect. It's basically a way to correct. 5) Variant calling comes later, and is performed via the following, which is called from [make_examples.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/make_examples.py):. https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_caller.py ; https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_calling.cc. 6) The [predict(...) in call_variants.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/call_variants.py#L374-L378) emits the `genotype_probabilities`, which Jason used to demonstrate the low probability of `T` as a sequencing error. Just like any machine-learning trained model, if it's not seen the pattern before it will not predict it with a high match - no real magic here :). 7) It might help to read the [deepvariant.proto](https://github.com/google/deepvariant/blob/r0.8/deepvariant/protos/deepvariant.proto), which you can treat like a spec-design document of the general idea behind DeepVariant. The [details doc](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-details.md) might help, but it's a bit general. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/197#issuecomment-512112524
Usability,learn,learning,"/github.com/cschin/DeepDiveDeepVariant), you'll see him using `--norealign_reads` which skips the preprocessing step described in the paper. That's why he's getting the `T` which a is a possible sequencing error. 2) The general realignment heuristic is the following: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/realigner.py#L439-L454. 3) The ploidy dictates the most likely haplotypes which is 2 :). 3) The haploytype realignment also been optimized a bit since that paper. Take a look at this section of the code: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/fast_pass_aligner.cc#L117-L166. 4) Yes, the realignment is basically used to update the reads' interpretation via the best-representative haplotype, trying to approach what might be closest-truth if the sequencer were relatively perfect. It's basically a way to correct. 5) Variant calling comes later, and is performed via the following, which is called from [make_examples.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/make_examples.py):. https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_caller.py ; https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_calling.cc. 6) The [predict(...) in call_variants.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/call_variants.py#L374-L378) emits the `genotype_probabilities`, which Jason used to demonstrate the low probability of `T` as a sequencing error. Just like any machine-learning trained model, if it's not seen the pattern before it will not predict it with a high match - no real magic here :). 7) It might help to read the [deepvariant.proto](https://github.com/google/deepvariant/blob/r0.8/deepvariant/protos/deepvariant.proto), which you can treat like a spec-design document of the general idea behind DeepVariant. The [details doc](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-details.md) might help, but it's a bit general. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/197#issuecomment-512112524
Performance,perform,performing,"@pichuan @pgrosu This improves my understanding substantially. After reading the paper, my initial impression was that DeepVariant consists of two parts - one part following traditional callers performing local assembly, haplotype detection and candidate allele generation based on the top-two haplotypes, and a second part which is a DNN used to perform candidate filtration. In such a scheme, the second part would be presented a single image per site and it makes a determination among homozygous-ref, heterozygous, and homozygous-alt in an allele-agnostic way - hence it needs a single image per site. In such a caller, 3 allele candidates at a site wouldn't be possible. I was also surprised that such a method could outperform all others, since this approach simply (approximately) replaces the VQSR step in GATK or the RandomForest in Strelka, and a lot would be riding on heuristic-based algorithms for candidate selection. I understand now that all (reasonable) candidates at a site are evaluated and scored against each other using the DNN, which seems to me to be a much more satisfying approach. I am still interested in how different predictions are combined into a single variant quality score, since they are not normalized; hence my request for guidance towards the code source that combines multiple predictions into a variant call. I will go through these resources over the next day. Thanks a lot! Much appreciated.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/197#issuecomment-512127253
Safety,detect,detection,"@pichuan @pgrosu This improves my understanding substantially. After reading the paper, my initial impression was that DeepVariant consists of two parts - one part following traditional callers performing local assembly, haplotype detection and candidate allele generation based on the top-two haplotypes, and a second part which is a DNN used to perform candidate filtration. In such a scheme, the second part would be presented a single image per site and it makes a determination among homozygous-ref, heterozygous, and homozygous-alt in an allele-agnostic way - hence it needs a single image per site. In such a caller, 3 allele candidates at a site wouldn't be possible. I was also surprised that such a method could outperform all others, since this approach simply (approximately) replaces the VQSR step in GATK or the RandomForest in Strelka, and a lot would be riding on heuristic-based algorithms for candidate selection. I understand now that all (reasonable) candidates at a site are evaluated and scored against each other using the DNN, which seems to me to be a much more satisfying approach. I am still interested in how different predictions are combined into a single variant quality score, since they are not normalized; hence my request for guidance towards the code source that combines multiple predictions into a variant call. I will go through these resources over the next day. Thanks a lot! Much appreciated.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/197#issuecomment-512127253
Usability,simpl,simply,"@pichuan @pgrosu This improves my understanding substantially. After reading the paper, my initial impression was that DeepVariant consists of two parts - one part following traditional callers performing local assembly, haplotype detection and candidate allele generation based on the top-two haplotypes, and a second part which is a DNN used to perform candidate filtration. In such a scheme, the second part would be presented a single image per site and it makes a determination among homozygous-ref, heterozygous, and homozygous-alt in an allele-agnostic way - hence it needs a single image per site. In such a caller, 3 allele candidates at a site wouldn't be possible. I was also surprised that such a method could outperform all others, since this approach simply (approximately) replaces the VQSR step in GATK or the RandomForest in Strelka, and a lot would be riding on heuristic-based algorithms for candidate selection. I understand now that all (reasonable) candidates at a site are evaluated and scored against each other using the DNN, which seems to me to be a much more satisfying approach. I am still interested in how different predictions are combined into a single variant quality score, since they are not normalized; hence my request for guidance towards the code source that combines multiple predictions into a variant call. I will go through these resources over the next day. Thanks a lot! Much appreciated.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/197#issuecomment-512127253
Availability,checkpoint,checkpoints,"Hi,. Evaluation is done using [Estimator class](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#evaluate). You may create a simple script calling Estimator.evaluate in a loop passing different checkpoints. > checkpoint_path: Path of a specific checkpoint to evaluate. If None, the latest checkpoint in model_dir is used. If there are no checkpoints in model_dir, evaluation is run with newly initialized Variables instead of ones restored from checkpoint. Hope it helps.; Alexey",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/198#issuecomment-512031369
Usability,simpl,simple,"Hi,. Evaluation is done using [Estimator class](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#evaluate). You may create a simple script calling Estimator.evaluate in a loop passing different checkpoints. > checkpoint_path: Path of a specific checkpoint to evaluate. If None, the latest checkpoint in model_dir is used. If there are no checkpoints in model_dir, evaluation is run with newly initialized Variables instead of ones restored from checkpoint. Hope it helps.; Alexey",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/198#issuecomment-512031369
Performance,perform,perform,"Hi @fengcong3 . DeepVariant has been applied to plant species. In the case of rice, there was good evidence of high accuracy superior [some results in this blog](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant). However, these rice genomes were diploid and with a similar variant density of humans. DeepVariant is currently written to be a diploid variant caller. So in cases with polyploid organisms (which includes wheat), it is not yet clear how DeepVariant will perform. However, I am also not sure how other variant callers perform on polyploid samples. If you (or anyone) knows current polyploid callers for wheat, I would like to know and to run benchmarks between the two. Finally, it is possible to train DeepVariant models for a specific genome. We have a previous example of this in mosquitos [see our blog](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). We hope to explore training a general plant model or a general non-human model in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/200#issuecomment-513411694
Testability,benchmark,benchmarks,"Hi @fengcong3 . DeepVariant has been applied to plant species. In the case of rice, there was good evidence of high accuracy superior [some results in this blog](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant). However, these rice genomes were diploid and with a similar variant density of humans. DeepVariant is currently written to be a diploid variant caller. So in cases with polyploid organisms (which includes wheat), it is not yet clear how DeepVariant will perform. However, I am also not sure how other variant callers perform on polyploid samples. If you (or anyone) knows current polyploid callers for wheat, I would like to know and to run benchmarks between the two. Finally, it is possible to train DeepVariant models for a specific genome. We have a previous example of this in mosquitos [see our blog](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). We hope to explore training a general plant model or a general non-human model in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/200#issuecomment-513411694
Usability,clear,clear,"Hi @fengcong3 . DeepVariant has been applied to plant species. In the case of rice, there was good evidence of high accuracy superior [some results in this blog](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant). However, these rice genomes were diploid and with a similar variant density of humans. DeepVariant is currently written to be a diploid variant caller. So in cases with polyploid organisms (which includes wheat), it is not yet clear how DeepVariant will perform. However, I am also not sure how other variant callers perform on polyploid samples. If you (or anyone) knows current polyploid callers for wheat, I would like to know and to run benchmarks between the two. Finally, it is possible to train DeepVariant models for a specific genome. We have a previous example of this in mosquitos [see our blog](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). We hope to explore training a general plant model or a general non-human model in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/200#issuecomment-513411694
Testability,log,logic,"Hi @aderzelle ; First of all, when I first think of ""deterministic"" behavior, I usually think about whether the exact same input yields the same results. Which we've made effort to make sure that running on the same input on the same machine should have deterministic behavior. If you have noticed non-deterministic behavior on the same sample, please do let us know. That said, you're absolutely right. These two represent the exact same event, but simple comparison scripts will not understand those events are in fact identical. ```; 90123; TGGGT; T--GTTC <-- Sample 1; TGTTC <-- Sample 2; ```. I think other users have reported to us in the past. So far we haven't looked closely into the code logic yet, because we mostly rely on tools such as `hap.py` for normalizing these during evaluation. So, this is a known behavior but haven't been investigated yet. And I do agree that it'll be good if we can behavior more consistently. . I will file an internal issue to track this (if we don't have one already). I can't guarantee when it'll be addressed though. If you have more suggestions on whether one representation might be better than the other, let me know. I'm closing this issue, but feel free to add your thoughts here. Thank you for reporting this!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/202#issuecomment-517123277
Usability,simpl,simple,"Hi @aderzelle ; First of all, when I first think of ""deterministic"" behavior, I usually think about whether the exact same input yields the same results. Which we've made effort to make sure that running on the same input on the same machine should have deterministic behavior. If you have noticed non-deterministic behavior on the same sample, please do let us know. That said, you're absolutely right. These two represent the exact same event, but simple comparison scripts will not understand those events are in fact identical. ```; 90123; TGGGT; T--GTTC <-- Sample 1; TGTTC <-- Sample 2; ```. I think other users have reported to us in the past. So far we haven't looked closely into the code logic yet, because we mostly rely on tools such as `hap.py` for normalizing these during evaluation. So, this is a known behavior but haven't been investigated yet. And I do agree that it'll be good if we can behavior more consistently. . I will file an internal issue to track this (if we don't have one already). I can't guarantee when it'll be addressed though. If you have more suggestions on whether one representation might be better than the other, let me know. I'm closing this issue, but feel free to add your thoughts here. Thank you for reporting this!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/202#issuecomment-517123277
Usability,learn,learning,I'm not sure I see how the learning rate or batch_size would affect those metrics -- could you provide more information on your setup? And the two cases you are seeing -- are those randomly occurring or have you pinpointed why it flips one way or the other?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/203#issuecomment-517389773
Availability,checkpoint,checkpoints,"Hi @melkerdawy , thanks for reporting this issue.; In general it is difficult for us (as the developers of DeepVariant) to give advice on the training behavior of other people's data. I can, however, from a perspective of a fellow ML researcher, ask a few more questions here to see if we can help you make progress:. (1) You said ""Regardless of how low we set the learning rate or the batch size or saving the intervals, the value of either the values of (TNs/All) and (FNs/All) is set to 0. or the values of (TPs/All) and (FPs/All) is set to zero."" --> The wording you have is confusing. I believe what you see here means that the `model_eval` code takes the checkpoints generated by `model_train`, and evaluated on the tuning data you've generated. And, based on the labeled tuning data, if the model thinks there are either no TNs(True Negatives) or FNs (False Negatives) --> this seems to indicate that it's likely at this point, the model might just don't call any negatives at all. But it's just my guess based on what you observe. (2) Before we even dig deeper into the training behavior, can you check this:; What is the distribution of your labeled data? For example, in the regular DeepVariant formulation, we have 3 classes, ""0"" -- HOM_REF, ""1"" -- HET, ""2"" --HOM_ALT. HET and HOM_ALT are the ones that are consider germline variants, while HOM_REF calls will result in `RefCall` in the final DeepVariant VCF files.; To give you an example, in our [0.8 release](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-details-training-data.md#deepvariant-training-data), our total number of examples for WGS was 346,505,686. And the distribution was:; ```; class 0, count: 101,679,899; class 1, count: 145,911,730; class 2, count: 98,914,057; ```; There is no fixed recommendation of what the ratio should be. It depends on a lot of factors such as what is your BAM file like, what is the threshold you're using in the first round of very sensitive caller (which picks the candidat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/203#issuecomment-518462111
Deployability,release,release,"All) is set to 0. or the values of (TPs/All) and (FPs/All) is set to zero."" --> The wording you have is confusing. I believe what you see here means that the `model_eval` code takes the checkpoints generated by `model_train`, and evaluated on the tuning data you've generated. And, based on the labeled tuning data, if the model thinks there are either no TNs(True Negatives) or FNs (False Negatives) --> this seems to indicate that it's likely at this point, the model might just don't call any negatives at all. But it's just my guess based on what you observe. (2) Before we even dig deeper into the training behavior, can you check this:; What is the distribution of your labeled data? For example, in the regular DeepVariant formulation, we have 3 classes, ""0"" -- HOM_REF, ""1"" -- HET, ""2"" --HOM_ALT. HET and HOM_ALT are the ones that are consider germline variants, while HOM_REF calls will result in `RefCall` in the final DeepVariant VCF files.; To give you an example, in our [0.8 release](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-details-training-data.md#deepvariant-training-data), our total number of examples for WGS was 346,505,686. And the distribution was:; ```; class 0, count: 101,679,899; class 1, count: 145,911,730; class 2, count: 98,914,057; ```; There is no fixed recommendation of what the ratio should be. It depends on a lot of factors such as what is your BAM file like, what is the threshold you're using in the first round of very sensitive caller (which picks the candidates that feed into the classifier), and many many other details that are specific to your data. Which is why I said our team cannot help debug the details of your case. But hopefully by examining your own distribution, you can first see if the training (and tuning) data makes sense or not. If the data has very skewed distribution, there are also other techniques that the ML community uses to improve the accuracy. But I won't be able to get into that. It's also not what D",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/203#issuecomment-518462111
Integrability,depend,depends,"call any negatives at all. But it's just my guess based on what you observe. (2) Before we even dig deeper into the training behavior, can you check this:; What is the distribution of your labeled data? For example, in the regular DeepVariant formulation, we have 3 classes, ""0"" -- HOM_REF, ""1"" -- HET, ""2"" --HOM_ALT. HET and HOM_ALT are the ones that are consider germline variants, while HOM_REF calls will result in `RefCall` in the final DeepVariant VCF files.; To give you an example, in our [0.8 release](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-details-training-data.md#deepvariant-training-data), our total number of examples for WGS was 346,505,686. And the distribution was:; ```; class 0, count: 101,679,899; class 1, count: 145,911,730; class 2, count: 98,914,057; ```; There is no fixed recommendation of what the ratio should be. It depends on a lot of factors such as what is your BAM file like, what is the threshold you're using in the first round of very sensitive caller (which picks the candidates that feed into the classifier), and many many other details that are specific to your data. Which is why I said our team cannot help debug the details of your case. But hopefully by examining your own distribution, you can first see if the training (and tuning) data makes sense or not. If the data has very skewed distribution, there are also other techniques that the ML community uses to improve the accuracy. But I won't be able to get into that. It's also not what DeepVariant designed for.; DeepVariant is wrapped around TensorFlow, which is a much more general purpose ML tool. If there are functionalities that we don't provide, please also look into TensorFlow to see if they have something useful for you. I'm closing this issue now because this is not really a DeepVariant issue. But if you believe this actually reveals some bugs in our codebase, I'm happy to discuss further if you can show a reproducible example that demonstrate the error.; Cl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/203#issuecomment-518462111
Usability,learn,learning,"Hi @melkerdawy , thanks for reporting this issue.; In general it is difficult for us (as the developers of DeepVariant) to give advice on the training behavior of other people's data. I can, however, from a perspective of a fellow ML researcher, ask a few more questions here to see if we can help you make progress:. (1) You said ""Regardless of how low we set the learning rate or the batch size or saving the intervals, the value of either the values of (TNs/All) and (FNs/All) is set to 0. or the values of (TPs/All) and (FPs/All) is set to zero."" --> The wording you have is confusing. I believe what you see here means that the `model_eval` code takes the checkpoints generated by `model_train`, and evaluated on the tuning data you've generated. And, based on the labeled tuning data, if the model thinks there are either no TNs(True Negatives) or FNs (False Negatives) --> this seems to indicate that it's likely at this point, the model might just don't call any negatives at all. But it's just my guess based on what you observe. (2) Before we even dig deeper into the training behavior, can you check this:; What is the distribution of your labeled data? For example, in the regular DeepVariant formulation, we have 3 classes, ""0"" -- HOM_REF, ""1"" -- HET, ""2"" --HOM_ALT. HET and HOM_ALT are the ones that are consider germline variants, while HOM_REF calls will result in `RefCall` in the final DeepVariant VCF files.; To give you an example, in our [0.8 release](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-details-training-data.md#deepvariant-training-data), our total number of examples for WGS was 346,505,686. And the distribution was:; ```; class 0, count: 101,679,899; class 1, count: 145,911,730; class 2, count: 98,914,057; ```; There is no fixed recommendation of what the ratio should be. It depends on a lot of factors such as what is your BAM file like, what is the threshold you're using in the first round of very sensitive caller (which picks the candidat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/203#issuecomment-518462111
Deployability,release,release,"Hi,; Thanks for your question! In our [blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) you mentioned, we showed an example of further training a model to perform better on mosquito data, which has higher variant density than human. Since then, internally we have continued to investigate what properties of the human genome and population structure DeepVariant learns during training. We’re hoping to come up with a suggested non-human model soon, but do not yet have a specific timeframe for releasing such a model. In terms of having a ""universal"" model, that is a good question too! We are currently investigating whether we can reduce the number of models we release while maintaining the high accuracy. Ideally if we can train one model that works well for all scenarios, we will certainly do that. Currently we’re optimizing our model accuracy for each common application, while keeping the number of released models as low as we can.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/204#issuecomment-518518311
Energy Efficiency,reduce,reduce,"Hi,; Thanks for your question! In our [blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) you mentioned, we showed an example of further training a model to perform better on mosquito data, which has higher variant density than human. Since then, internally we have continued to investigate what properties of the human genome and population structure DeepVariant learns during training. We’re hoping to come up with a suggested non-human model soon, but do not yet have a specific timeframe for releasing such a model. In terms of having a ""universal"" model, that is a good question too! We are currently investigating whether we can reduce the number of models we release while maintaining the high accuracy. Ideally if we can train one model that works well for all scenarios, we will certainly do that. Currently we’re optimizing our model accuracy for each common application, while keeping the number of released models as low as we can.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/204#issuecomment-518518311
Performance,perform,perform,"Hi,; Thanks for your question! In our [blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) you mentioned, we showed an example of further training a model to perform better on mosquito data, which has higher variant density than human. Since then, internally we have continued to investigate what properties of the human genome and population structure DeepVariant learns during training. We’re hoping to come up with a suggested non-human model soon, but do not yet have a specific timeframe for releasing such a model. In terms of having a ""universal"" model, that is a good question too! We are currently investigating whether we can reduce the number of models we release while maintaining the high accuracy. Ideally if we can train one model that works well for all scenarios, we will certainly do that. Currently we’re optimizing our model accuracy for each common application, while keeping the number of released models as low as we can.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/204#issuecomment-518518311
Usability,learn,learns,"Hi,; Thanks for your question! In our [blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) you mentioned, we showed an example of further training a model to perform better on mosquito data, which has higher variant density than human. Since then, internally we have continued to investigate what properties of the human genome and population structure DeepVariant learns during training. We’re hoping to come up with a suggested non-human model soon, but do not yet have a specific timeframe for releasing such a model. In terms of having a ""universal"" model, that is a good question too! We are currently investigating whether we can reduce the number of models we release while maintaining the high accuracy. Ideally if we can train one model that works well for all scenarios, we will certainly do that. Currently we’re optimizing our model accuracy for each common application, while keeping the number of released models as low as we can.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/204#issuecomment-518518311
Availability,avail,available,"Hi @aderzelle . Today we released DeepVariant v0.9, which contains several changes to code and training models. As part of this release, we have introduced changes which fix the issue for the BAM snippets presented, and which we think will generally fix the issue that you observed in other cases. To briefly summarize what we believe to be the cause - in candidate generation, a de Bruijn graph of variant and reference haplotypes is constructed. In rare cases, some graph paths are created in which local connections are valid, but no individual read supports the entire path. In your case, this caused two similar representations to generate candidates at different positions, each of which could be locally supported. In our fix, we require at least some support for the constructed graph of the candidate haplotype. We also noticed a separate fix that resolves your case. Specifically, your case was sensitive to the kmer length used to construct the graph. By default, this is 10, but we noticed that increasing to 15 also resolved your issue. We think this may reflect local repetitiveness. We have exposed this parameter in make_examples as: --dbg_min_k. This is available when running make_examples directly, but not in the Docker image. Since the issue should be resolved in v0.9 without this change, this is mostly for your information if you want to experiment with other tweaks. We would be interested to hear your feedback confirming this case is resolved in v0.9. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/209#issuecomment-553647872
Deployability,release,released,"Hi @aderzelle . Today we released DeepVariant v0.9, which contains several changes to code and training models. As part of this release, we have introduced changes which fix the issue for the BAM snippets presented, and which we think will generally fix the issue that you observed in other cases. To briefly summarize what we believe to be the cause - in candidate generation, a de Bruijn graph of variant and reference haplotypes is constructed. In rare cases, some graph paths are created in which local connections are valid, but no individual read supports the entire path. In your case, this caused two similar representations to generate candidates at different positions, each of which could be locally supported. In our fix, we require at least some support for the constructed graph of the candidate haplotype. We also noticed a separate fix that resolves your case. Specifically, your case was sensitive to the kmer length used to construct the graph. By default, this is 10, but we noticed that increasing to 15 also resolved your issue. We think this may reflect local repetitiveness. We have exposed this parameter in make_examples as: --dbg_min_k. This is available when running make_examples directly, but not in the Docker image. Since the issue should be resolved in v0.9 without this change, this is mostly for your information if you want to experiment with other tweaks. We would be interested to hear your feedback confirming this case is resolved in v0.9. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/209#issuecomment-553647872
Security,expose,exposed,"Hi @aderzelle . Today we released DeepVariant v0.9, which contains several changes to code and training models. As part of this release, we have introduced changes which fix the issue for the BAM snippets presented, and which we think will generally fix the issue that you observed in other cases. To briefly summarize what we believe to be the cause - in candidate generation, a de Bruijn graph of variant and reference haplotypes is constructed. In rare cases, some graph paths are created in which local connections are valid, but no individual read supports the entire path. In your case, this caused two similar representations to generate candidates at different positions, each of which could be locally supported. In our fix, we require at least some support for the constructed graph of the candidate haplotype. We also noticed a separate fix that resolves your case. Specifically, your case was sensitive to the kmer length used to construct the graph. By default, this is 10, but we noticed that increasing to 15 also resolved your issue. We think this may reflect local repetitiveness. We have exposed this parameter in make_examples as: --dbg_min_k. This is available when running make_examples directly, but not in the Docker image. Since the issue should be resolved in v0.9 without this change, this is mostly for your information if you want to experiment with other tweaks. We would be interested to hear your feedback confirming this case is resolved in v0.9. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/209#issuecomment-553647872
Usability,feedback,feedback,"Hi @aderzelle . Today we released DeepVariant v0.9, which contains several changes to code and training models. As part of this release, we have introduced changes which fix the issue for the BAM snippets presented, and which we think will generally fix the issue that you observed in other cases. To briefly summarize what we believe to be the cause - in candidate generation, a de Bruijn graph of variant and reference haplotypes is constructed. In rare cases, some graph paths are created in which local connections are valid, but no individual read supports the entire path. In your case, this caused two similar representations to generate candidates at different positions, each of which could be locally supported. In our fix, we require at least some support for the constructed graph of the candidate haplotype. We also noticed a separate fix that resolves your case. Specifically, your case was sensitive to the kmer length used to construct the graph. By default, this is 10, but we noticed that increasing to 15 also resolved your issue. We think this may reflect local repetitiveness. We have exposed this parameter in make_examples as: --dbg_min_k. This is available when running make_examples directly, but not in the Docker image. Since the issue should be resolved in v0.9 without this change, this is mostly for your information if you want to experiment with other tweaks. We would be interested to hear your feedback confirming this case is resolved in v0.9. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/209#issuecomment-553647872
Deployability,release,release,"Hi @anitagh ,; the release won't come out within a week. Sorry for the inconvenience. For now you'll have to run make_examples separately and add the `--sample_name` flag. Another way to fix this is to make sure your BAM file header has one SM tag in it. If your BAM file is not very big, using `samtools reheader` to add a proper SM tag might also be a reasonable solution for now. If you're using GCP, you can try out the Google Cloud version that @samanvp pointed to in earlier comments. (If you have any feedback on that tool, please report to https://github.com/googlegenomics/gcp-deepvariant-runner/issues.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/222#issuecomment-535654563
Usability,feedback,feedback,"Hi @anitagh ,; the release won't come out within a week. Sorry for the inconvenience. For now you'll have to run make_examples separately and add the `--sample_name` flag. Another way to fix this is to make sure your BAM file header has one SM tag in it. If your BAM file is not very big, using `samtools reheader` to add a proper SM tag might also be a reasonable solution for now. If you're using GCP, you can try out the Google Cloud version that @samanvp pointed to in earlier comments. (If you have any feedback on that tool, please report to https://github.com/googlegenomics/gcp-deepvariant-runner/issues.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/222#issuecomment-535654563
Usability,simpl,simply,"Thank you very much for your reply. However I have 2 questions regarding the example visualization. . 1. As I am using multiple cores, the example files are splitted in to ; examples.tfrecord-00000-of-00008.gz to examples.tfrecord-00007-of-00008.gz. can I simply use ""examples.tfrecord-00000-of-00008.gz"" as the source path? or do I have to combine the 8 examples file together first?. 2. I cannot run the program as stated in the notebook as the `label` is not one of the features in the example for deepvariant. Is the label in the notebook the same as `alt_allele_indices/encoded` in Deepvariant? If so, how can I extract the information from the feature? I have tried 'alt_allele_indices/encoded': tf.FixedLenFeature([], tf.string), but it just gives my random symbols.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/229#issuecomment-545946241
Availability,down,downsampling,"Hi @anands-repo . With respect to your second point - The starting training data begins from various coverages, ranging roughly from 27x-60x. From the starting coverage of the BAM files, downsampling is applied in 0.1 increments until the coverage would reach around 20x. As a result, higher coverage ranges are slightly less represented than the 30x-20x range, but not by a substantial amount. There are two main purposes of this - the first is to allow DeepVariant to perform well across many different coverages. The second is to ensure that there are many hard examples to learn from. Our strategy in downsampling is not fixed - for example, as we add more training data in the future, we may decide to have fewer downsample increments to generally keep the same range of examples. The coverage range has also evolved over time, for example in v0.7, the range was roughly 60x-30x. Extending the range to 20x caused a very small decline in accuracy at 50x (and also therefore the case study), but resulted in more substantial gains in the 20x-30x range. I would stress that there isn't one correct downsampling strategy, as long as it represents a diversity of coverages and helps create hard examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/230#issuecomment-546050008
Modifiability,evolve,evolved,"Hi @anands-repo . With respect to your second point - The starting training data begins from various coverages, ranging roughly from 27x-60x. From the starting coverage of the BAM files, downsampling is applied in 0.1 increments until the coverage would reach around 20x. As a result, higher coverage ranges are slightly less represented than the 30x-20x range, but not by a substantial amount. There are two main purposes of this - the first is to allow DeepVariant to perform well across many different coverages. The second is to ensure that there are many hard examples to learn from. Our strategy in downsampling is not fixed - for example, as we add more training data in the future, we may decide to have fewer downsample increments to generally keep the same range of examples. The coverage range has also evolved over time, for example in v0.7, the range was roughly 60x-30x. Extending the range to 20x caused a very small decline in accuracy at 50x (and also therefore the case study), but resulted in more substantial gains in the 20x-30x range. I would stress that there isn't one correct downsampling strategy, as long as it represents a diversity of coverages and helps create hard examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/230#issuecomment-546050008
Performance,perform,perform,"Hi @anands-repo . With respect to your second point - The starting training data begins from various coverages, ranging roughly from 27x-60x. From the starting coverage of the BAM files, downsampling is applied in 0.1 increments until the coverage would reach around 20x. As a result, higher coverage ranges are slightly less represented than the 30x-20x range, but not by a substantial amount. There are two main purposes of this - the first is to allow DeepVariant to perform well across many different coverages. The second is to ensure that there are many hard examples to learn from. Our strategy in downsampling is not fixed - for example, as we add more training data in the future, we may decide to have fewer downsample increments to generally keep the same range of examples. The coverage range has also evolved over time, for example in v0.7, the range was roughly 60x-30x. Extending the range to 20x caused a very small decline in accuracy at 50x (and also therefore the case study), but resulted in more substantial gains in the 20x-30x range. I would stress that there isn't one correct downsampling strategy, as long as it represents a diversity of coverages and helps create hard examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/230#issuecomment-546050008
Usability,learn,learn,"Hi @anands-repo . With respect to your second point - The starting training data begins from various coverages, ranging roughly from 27x-60x. From the starting coverage of the BAM files, downsampling is applied in 0.1 increments until the coverage would reach around 20x. As a result, higher coverage ranges are slightly less represented than the 30x-20x range, but not by a substantial amount. There are two main purposes of this - the first is to allow DeepVariant to perform well across many different coverages. The second is to ensure that there are many hard examples to learn from. Our strategy in downsampling is not fixed - for example, as we add more training data in the future, we may decide to have fewer downsample increments to generally keep the same range of examples. The coverage range has also evolved over time, for example in v0.7, the range was roughly 60x-30x. Extending the range to 20x caused a very small decline in accuracy at 50x (and also therefore the case study), but resulted in more substantial gains in the 20x-30x range. I would stress that there isn't one correct downsampling strategy, as long as it represents a diversity of coverages and helps create hard examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/230#issuecomment-546050008
Availability,error,errors,"Hi @sclan ; to give you an update, I made an internal fix to make sure [run_deepvariant.py](https://github.com/google/deepvariant/blob/r0.9/scripts/run_deepvariant.py) can output more clear errors when they occur.; The new code is not on GitHub yet, but will come out in the next release. Specifically, I changed the main function to be: ; ```; def main(_):; check_or_create_intermediate_results_dir(FLAGS.intermediate_results_dir); check_flags(). commands = create_all_commands(); for command in commands:; print('\n***** Running the command:*****\n{}\n'.format(command)); try:; subprocess.check_call(command, shell=True, executable='/bin/bash'); except subprocess.CalledProcessError as e:; logging.info(e.output); raise; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-596040813
Deployability,update,update,"Hi @sclan ; to give you an update, I made an internal fix to make sure [run_deepvariant.py](https://github.com/google/deepvariant/blob/r0.9/scripts/run_deepvariant.py) can output more clear errors when they occur.; The new code is not on GitHub yet, but will come out in the next release. Specifically, I changed the main function to be: ; ```; def main(_):; check_or_create_intermediate_results_dir(FLAGS.intermediate_results_dir); check_flags(). commands = create_all_commands(); for command in commands:; print('\n***** Running the command:*****\n{}\n'.format(command)); try:; subprocess.check_call(command, shell=True, executable='/bin/bash'); except subprocess.CalledProcessError as e:; logging.info(e.output); raise; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-596040813
Testability,log,logging,"Hi @sclan ; to give you an update, I made an internal fix to make sure [run_deepvariant.py](https://github.com/google/deepvariant/blob/r0.9/scripts/run_deepvariant.py) can output more clear errors when they occur.; The new code is not on GitHub yet, but will come out in the next release. Specifically, I changed the main function to be: ; ```; def main(_):; check_or_create_intermediate_results_dir(FLAGS.intermediate_results_dir); check_flags(). commands = create_all_commands(); for command in commands:; print('\n***** Running the command:*****\n{}\n'.format(command)); try:; subprocess.check_call(command, shell=True, executable='/bin/bash'); except subprocess.CalledProcessError as e:; logging.info(e.output); raise; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-596040813
Usability,clear,clear,"Hi @sclan ; to give you an update, I made an internal fix to make sure [run_deepvariant.py](https://github.com/google/deepvariant/blob/r0.9/scripts/run_deepvariant.py) can output more clear errors when they occur.; The new code is not on GitHub yet, but will come out in the next release. Specifically, I changed the main function to be: ; ```; def main(_):; check_or_create_intermediate_results_dir(FLAGS.intermediate_results_dir); check_flags(). commands = create_all_commands(); for command in commands:; print('\n***** Running the command:*****\n{}\n'.format(command)); try:; subprocess.check_call(command, shell=True, executable='/bin/bash'); except subprocess.CalledProcessError as e:; logging.info(e.output); raise; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-596040813
Deployability,update,updated,"Hi @ydLiu-HIT . Thank you for your question. The answer to this is complicated. It looks like the region that these elements is in is a LINE element - long regions with multiple copies through the genome that have high sequence similarity to each other. Because of the high sequence similarity, reads to line elements can map to other parts of the genome, and they are generally very difficult regions to call correctly. We've seen the behavior in DeepVariant not calling variants that are near other variants and in regions with two (or more) variant-rich haplotypes. We think that one of the reasons for this is that DeepVariant has learned that these regions represent uncaptured segmental duplication and LINE elements, which are often labelled as not variant in the more comprehensive genome in a bottle truth set. . Whether these positions represent true variants at that position, or sequences from a similar LINE element elsewhere is difficult to say. Since this is HG002, if this is within the confident regions, you can see whether Genome in a Bottle indicates them to be true variants. However, Genome in a Bottle has some more recent corrections to variants in/near LINE elements, so it may be better to check the updated (though still beta) [truth set](ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4beta_SmallVariantDraftBenchmark_07192019/). DeepVariant will output every candidate considered, so if you want to find positions that are called in this way, looking for 0/0 or ./. calls with more than 35% ALT support and within 100bp of 2 or more candidate variants may be able to pull out many of these examples. . The other option to pull out examples like this would be to intersect with a LINE element annotation track from UCSC. Please let us know if there is something unclear about this answer. This is a rather complicated concept and explanation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/233#issuecomment-550436768
Usability,learn,learned,"Hi @ydLiu-HIT . Thank you for your question. The answer to this is complicated. It looks like the region that these elements is in is a LINE element - long regions with multiple copies through the genome that have high sequence similarity to each other. Because of the high sequence similarity, reads to line elements can map to other parts of the genome, and they are generally very difficult regions to call correctly. We've seen the behavior in DeepVariant not calling variants that are near other variants and in regions with two (or more) variant-rich haplotypes. We think that one of the reasons for this is that DeepVariant has learned that these regions represent uncaptured segmental duplication and LINE elements, which are often labelled as not variant in the more comprehensive genome in a bottle truth set. . Whether these positions represent true variants at that position, or sequences from a similar LINE element elsewhere is difficult to say. Since this is HG002, if this is within the confident regions, you can see whether Genome in a Bottle indicates them to be true variants. However, Genome in a Bottle has some more recent corrections to variants in/near LINE elements, so it may be better to check the updated (though still beta) [truth set](ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4beta_SmallVariantDraftBenchmark_07192019/). DeepVariant will output every candidate considered, so if you want to find positions that are called in this way, looking for 0/0 or ./. calls with more than 35% ALT support and within 100bp of 2 or more candidate variants may be able to pull out many of these examples. . The other option to pull out examples like this would be to intersect with a LINE element annotation track from UCSC. Please let us know if there is something unclear about this answer. This is a rather complicated concept and explanation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/233#issuecomment-550436768
Deployability,release,release,"Hi @Stikus , ; actually , it seems like simply removing the line; ```; #include <optional>; ```; will build. From the code, we're using ""optional"" from tensorflow::gtl::optional. So we don't really need the #include here. I have confirmed that removing this line builds on Ubuntu14.04. Please give that a try. If it doesn't work, let me know. I will make an internal fix, which will come out in the next release. For now, please make a local edit before you build.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/236#issuecomment-557265475
Usability,simpl,simply,"Hi @Stikus , ; actually , it seems like simply removing the line; ```; #include <optional>; ```; will build. From the code, we're using ""optional"" from tensorflow::gtl::optional. So we don't really need the #include here. I have confirmed that removing this line builds on Ubuntu14.04. Please give that a try. If it doesn't work, let me know. I will make an internal fix, which will come out in the next release. For now, please make a local edit before you build.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/236#issuecomment-557265475
Deployability,release,release,"Hi @segoerge . Thank you for your question. It is a good observation that better support for MNP would be helpful. We have discussed this internally to a limited extent, but have not yet mapped out what would be involved to make the change. I couldn't give a timeframe for this yet (or give an indication as to whether this could be something in the next release). Your feedback is appreciated, as it helps us understand the needs of the user community. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/238#issuecomment-557229093
Usability,feedback,feedback,"Hi @segoerge . Thank you for your question. It is a good observation that better support for MNP would be helpful. We have discussed this internally to a limited extent, but have not yet mapped out what would be involved to make the change. I couldn't give a timeframe for this yet (or give an indication as to whether this could be something in the next release). Your feedback is appreciated, as it helps us understand the needs of the user community. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/238#issuecomment-557229093
Performance,perform,performs,"Hi @mikael-christensen . For somatic calling, we have a version of DeepVariant designed to call somatic variants. This performs well in simulated benchmarks and in benchmarks with COLO829. However, we want to get more feedback from external groups about its performance across real data for more cancer types before we feel comfortable generally releasing it. The time to train DeepVariant internally is a function of the amount and prioritization of training jobs on internal infrastructure. Generally it takes a few days to train the WGS model and is shorter for the WES and PacBio models. Training uses TPUs. I am not sure of the exact resource requirements in the production runs. However, external groups who have successfully trained DeepVariant have generally done so with much less data than what we train for in production, and often warmstart from one of our production models and re-train for their data. External groups have shown improvements for their workflows using GPUs and (I believe) on the order of a day or less of re-training time. The candidates created by the very sensitive caller are not genotyped, so comparing accuracy of the sensitive caller in the same way isn't quite possible. DeepVariant's VCF output will write every position which it generated a candidate for. For a 50x PCR-Free Illumina HG002 run, DeepVariant produced:. 0/0 calls - 934,097 ; ./. calls - 150,238; 0/1 calls - 2,793,521; 1/1 calls - 1,851,566; 1/2 calls - 78,194. This means that for about 19% of the positions which the sensitive caller proposes, DeepVariant does not believe there is sufficient evidence for a variant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/241#issuecomment-559228691
Testability,benchmark,benchmarks,"Hi @mikael-christensen . For somatic calling, we have a version of DeepVariant designed to call somatic variants. This performs well in simulated benchmarks and in benchmarks with COLO829. However, we want to get more feedback from external groups about its performance across real data for more cancer types before we feel comfortable generally releasing it. The time to train DeepVariant internally is a function of the amount and prioritization of training jobs on internal infrastructure. Generally it takes a few days to train the WGS model and is shorter for the WES and PacBio models. Training uses TPUs. I am not sure of the exact resource requirements in the production runs. However, external groups who have successfully trained DeepVariant have generally done so with much less data than what we train for in production, and often warmstart from one of our production models and re-train for their data. External groups have shown improvements for their workflows using GPUs and (I believe) on the order of a day or less of re-training time. The candidates created by the very sensitive caller are not genotyped, so comparing accuracy of the sensitive caller in the same way isn't quite possible. DeepVariant's VCF output will write every position which it generated a candidate for. For a 50x PCR-Free Illumina HG002 run, DeepVariant produced:. 0/0 calls - 934,097 ; ./. calls - 150,238; 0/1 calls - 2,793,521; 1/1 calls - 1,851,566; 1/2 calls - 78,194. This means that for about 19% of the positions which the sensitive caller proposes, DeepVariant does not believe there is sufficient evidence for a variant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/241#issuecomment-559228691
Usability,feedback,feedback,"Hi @mikael-christensen . For somatic calling, we have a version of DeepVariant designed to call somatic variants. This performs well in simulated benchmarks and in benchmarks with COLO829. However, we want to get more feedback from external groups about its performance across real data for more cancer types before we feel comfortable generally releasing it. The time to train DeepVariant internally is a function of the amount and prioritization of training jobs on internal infrastructure. Generally it takes a few days to train the WGS model and is shorter for the WES and PacBio models. Training uses TPUs. I am not sure of the exact resource requirements in the production runs. However, external groups who have successfully trained DeepVariant have generally done so with much less data than what we train for in production, and often warmstart from one of our production models and re-train for their data. External groups have shown improvements for their workflows using GPUs and (I believe) on the order of a day or less of re-training time. The candidates created by the very sensitive caller are not genotyped, so comparing accuracy of the sensitive caller in the same way isn't quite possible. DeepVariant's VCF output will write every position which it generated a candidate for. For a 50x PCR-Free Illumina HG002 run, DeepVariant produced:. 0/0 calls - 934,097 ; ./. calls - 150,238; 0/1 calls - 2,793,521; 1/1 calls - 1,851,566; 1/2 calls - 78,194. This means that for about 19% of the positions which the sensitive caller proposes, DeepVariant does not believe there is sufficient evidence for a variant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/241#issuecomment-559228691
Availability,error,errors,"do docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo docker push localhost:5000/deepvariant:latest; SINGULARITY_NOHTTPS=1 singularity build deepvariant-${VERSION}.simg docker://localhost:5000/deepvariant:latest; ```. To run [Quick Start](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md), instead of using the docker command, you can use this command instead:. ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant-${VERSION}.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; ```. ## GPU image; ```; VERSION=0.9.0-gpu; sudo nvidia-docker pull google/deepvariant:${VERSION}; sudo nvidia-docker tag google/deepvariant:${VERSION} localhost:5000/deepvariant_gpu:latest; sudo nvidia-docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo nvidia-docker push localhost:5000/deepvariant_gpu:latest; SINGULARITY_NOHTTPS=1 singularity build deepvariant-${VERSION}.simg docker://localhost:5000/deepvariant_gpu:latest; ```. Running through Quick Start just to make sure nothing wrong:; ```; singularity -s exec --nv -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant-${VERSION}.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; ```. Both CPU and GPU versions finished on Quick Start data without errors. ## Misc; For installing nvidia-docker and singularity, you can also refer to https://github.com/google/deepvariant/blob/r0.9/scripts/install_nvidia_docker.sh; and; https://github.com/google/deepvariant/blob/r0.9/scripts/install_singularity.sh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243#issuecomment-561996442
Deployability,release,release,"Hi @jguhlin ,; thanks for the feedback, and for letting us know that our users are still interested in Singularity images.; I have been a bit hesitated to make Singularity images part of our formal release process, mostly because the additional quality control burden. ; But, in the future we'll consider building these *.simg files and just distribute them. The steps below I used were documented here:; https://github.com/google/deepvariant/issues/132#issuecomment-482430728. I have the detailed commands that I used for my conversion, and I copied the output *.simg files here:. ```; gs://deepvariant/singularity_images/deepvariant-0.9.0-gpu.simg; gs://deepvariant/singularity_images/deepvariant-0.9.0.simg; ```; Or you can find them in the browser here:; https://console.cloud.google.com/storage/browser/deepvariant/singularity_images/. I was able to test both CPU and GPU version on the Quick Start data (see below). Can you see if if my `deepvariant-0.9.0-gpu.simg` file works for you?. ------. # @pichuan 's notes on building DeepVariant Singularity images for CPU and GPU (v0.9.0). If you don't have singularity on your computer, install it first:; https://sylabs.io/docs/. Once you do, you can pull the DeepVariant Docker image and convert it to a Singularity image. ## CPU image. ```; VERSION=0.9.0; sudo apt -y update && sudo apt-get install -y docker.io; sudo docker pull google/deepvariant:${VERSION}; sudo docker tag google/deepvariant:${VERSION} localhost:5000/deepvariant:latest; sudo docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo docker push localhost:5000/deepvariant:latest; SINGULARITY_NOHTTPS=1 singularity build deepvariant-${VERSION}.simg docker://localhost:5000/deepvariant:latest; ```. To run [Quick Start](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md), instead of using the docker command, you can use this command instead:. ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant-${VERSI",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243#issuecomment-561996442
Testability,test,test,"Hi @jguhlin ,; thanks for the feedback, and for letting us know that our users are still interested in Singularity images.; I have been a bit hesitated to make Singularity images part of our formal release process, mostly because the additional quality control burden. ; But, in the future we'll consider building these *.simg files and just distribute them. The steps below I used were documented here:; https://github.com/google/deepvariant/issues/132#issuecomment-482430728. I have the detailed commands that I used for my conversion, and I copied the output *.simg files here:. ```; gs://deepvariant/singularity_images/deepvariant-0.9.0-gpu.simg; gs://deepvariant/singularity_images/deepvariant-0.9.0.simg; ```; Or you can find them in the browser here:; https://console.cloud.google.com/storage/browser/deepvariant/singularity_images/. I was able to test both CPU and GPU version on the Quick Start data (see below). Can you see if if my `deepvariant-0.9.0-gpu.simg` file works for you?. ------. # @pichuan 's notes on building DeepVariant Singularity images for CPU and GPU (v0.9.0). If you don't have singularity on your computer, install it first:; https://sylabs.io/docs/. Once you do, you can pull the DeepVariant Docker image and convert it to a Singularity image. ## CPU image. ```; VERSION=0.9.0; sudo apt -y update && sudo apt-get install -y docker.io; sudo docker pull google/deepvariant:${VERSION}; sudo docker tag google/deepvariant:${VERSION} localhost:5000/deepvariant:latest; sudo docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo docker push localhost:5000/deepvariant:latest; SINGULARITY_NOHTTPS=1 singularity build deepvariant-${VERSION}.simg docker://localhost:5000/deepvariant:latest; ```. To run [Quick Start](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md), instead of using the docker command, you can use this command instead:. ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant-${VERSI",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243#issuecomment-561996442
Usability,feedback,feedback,"Hi @jguhlin ,; thanks for the feedback, and for letting us know that our users are still interested in Singularity images.; I have been a bit hesitated to make Singularity images part of our formal release process, mostly because the additional quality control burden. ; But, in the future we'll consider building these *.simg files and just distribute them. The steps below I used were documented here:; https://github.com/google/deepvariant/issues/132#issuecomment-482430728. I have the detailed commands that I used for my conversion, and I copied the output *.simg files here:. ```; gs://deepvariant/singularity_images/deepvariant-0.9.0-gpu.simg; gs://deepvariant/singularity_images/deepvariant-0.9.0.simg; ```; Or you can find them in the browser here:; https://console.cloud.google.com/storage/browser/deepvariant/singularity_images/. I was able to test both CPU and GPU version on the Quick Start data (see below). Can you see if if my `deepvariant-0.9.0-gpu.simg` file works for you?. ------. # @pichuan 's notes on building DeepVariant Singularity images for CPU and GPU (v0.9.0). If you don't have singularity on your computer, install it first:; https://sylabs.io/docs/. Once you do, you can pull the DeepVariant Docker image and convert it to a Singularity image. ## CPU image. ```; VERSION=0.9.0; sudo apt -y update && sudo apt-get install -y docker.io; sudo docker pull google/deepvariant:${VERSION}; sudo docker tag google/deepvariant:${VERSION} localhost:5000/deepvariant:latest; sudo docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo docker push localhost:5000/deepvariant:latest; SINGULARITY_NOHTTPS=1 singularity build deepvariant-${VERSION}.simg docker://localhost:5000/deepvariant:latest; ```. To run [Quick Start](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md), instead of using the docker command, you can use this command instead:. ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant-${VERSI",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243#issuecomment-561996442
Availability,error,errors,"I strongly concur. Singularity images would be nice. Using Docker has already given me multiple diseases. It's such a hassle and from what other colleagues tell me, I am not the only one. ; Note that there is nothing wrong with the deepvariant image, it's the Docker process that can cause many problems running, pulling images, restarting, generating errors, etc ... ; Like this issue, unresolved since 2017: ; https://github.com/docker/for-win/issues/813. Dark Souls bosses are easier to take down than pulling a docker image on some systems. ; So to be clear again, nothing wrong at all with deepvariant, which I love more and more btw, but having something else than Docker would be indeed really great.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243#issuecomment-562108502
Usability,clear,clear,"I strongly concur. Singularity images would be nice. Using Docker has already given me multiple diseases. It's such a hassle and from what other colleagues tell me, I am not the only one. ; Note that there is nothing wrong with the deepvariant image, it's the Docker process that can cause many problems running, pulling images, restarting, generating errors, etc ... ; Like this issue, unresolved since 2017: ; https://github.com/docker/for-win/issues/813. Dark Souls bosses are easier to take down than pulling a docker image on some systems. ; So to be clear again, nothing wrong at all with deepvariant, which I love more and more btw, but having something else than Docker would be indeed really great.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243#issuecomment-562108502
Usability,feedback,feedback,"Hi @aderzelle ; thanks for your feedback. If you have a chance to try out the two images I shared:; ```; gs://deepvariant/singularity_images/deepvariant-0.9.0-gpu.simg; gs://deepvariant/singularity_images/deepvariant-0.9.0.simg; ```; Please let me know whether they work for you or not. If you see any issues, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243#issuecomment-562237254
Availability,down,downstream,"I guess the core question for the variant caller is, (whether or not fully normalized as you have written) whether the raw calls can be interpreted without contradictions by a downstream method (notwithstanding the VCF standard). Considering GT=0 to be ""non-ALT"" instead of ""REF allele"" for such call clusters does seem to work in these cases (though I am not aware of a standardization of such an interpretation). The code I have referenced from DeepVariant above seems to try to filter out cases that do not make sense for such clusters of calls, so a downstream method that assumes GT=0 to imply ""non-ALT"" might work smoothly almost all the time (except perhaps when the algorithm aborts). I also didn't have an idea as to whether, as a method developer, you considered these to be corner cases, and your comment above has provided an answer to that. I myself do not have any measurements on how prevalent these types of cases are, but I encountered some of these cases recently. I will make another post here if I learn anything further about this, but I understand the point-of-view behind the original piece of code that I posted. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/247#issuecomment-563285549
Safety,abort,aborts,"I guess the core question for the variant caller is, (whether or not fully normalized as you have written) whether the raw calls can be interpreted without contradictions by a downstream method (notwithstanding the VCF standard). Considering GT=0 to be ""non-ALT"" instead of ""REF allele"" for such call clusters does seem to work in these cases (though I am not aware of a standardization of such an interpretation). The code I have referenced from DeepVariant above seems to try to filter out cases that do not make sense for such clusters of calls, so a downstream method that assumes GT=0 to imply ""non-ALT"" might work smoothly almost all the time (except perhaps when the algorithm aborts). I also didn't have an idea as to whether, as a method developer, you considered these to be corner cases, and your comment above has provided an answer to that. I myself do not have any measurements on how prevalent these types of cases are, but I encountered some of these cases recently. I will make another post here if I learn anything further about this, but I understand the point-of-view behind the original piece of code that I posted. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/247#issuecomment-563285549
Usability,learn,learn,"I guess the core question for the variant caller is, (whether or not fully normalized as you have written) whether the raw calls can be interpreted without contradictions by a downstream method (notwithstanding the VCF standard). Considering GT=0 to be ""non-ALT"" instead of ""REF allele"" for such call clusters does seem to work in these cases (though I am not aware of a standardization of such an interpretation). The code I have referenced from DeepVariant above seems to try to filter out cases that do not make sense for such clusters of calls, so a downstream method that assumes GT=0 to imply ""non-ALT"" might work smoothly almost all the time (except perhaps when the algorithm aborts). I also didn't have an idea as to whether, as a method developer, you considered these to be corner cases, and your comment above has provided an answer to that. I myself do not have any measurements on how prevalent these types of cases are, but I encountered some of these cases recently. I will make another post here if I learn anything further about this, but I understand the point-of-view behind the original piece of code that I posted. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/247#issuecomment-563285549
Deployability,update,updated,"We have updated the documentation to mention that AVX instructions are needed. These changes will come out with the next release. Thank you for the feedback!. Edit: we specifically mention this requirement again in the quickstart documentation, in addition to the page linked below by @pgrosu. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/248#issuecomment-566700041
Usability,feedback,feedback,"We have updated the documentation to mention that AVX instructions are needed. These changes will come out with the next release. Thank you for the feedback!. Edit: we specifically mention this requirement again in the quickstart documentation, in addition to the page linked below by @pgrosu. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/248#issuecomment-566700041
Energy Efficiency,reduce,reduced,"Looks like I was impatient. Now that it's been going for several hours (And reduced learning rate) tensorboard is giving better results. TPs dropped to 0, and everything dropped to 0, but now it's back up. I thought starting with the pre-trained wgs model would let it just improve but I guess it had to re-learn. Planning to scale up to the entire genome next.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251#issuecomment-563508582
Usability,learn,learning,"Looks like I was impatient. Now that it's been going for several hours (And reduced learning rate) tensorboard is giving better results. TPs dropped to 0, and everything dropped to 0, but now it's back up. I thought starting with the pre-trained wgs model would let it just improve but I guess it had to re-learn. Planning to scale up to the entire genome next.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251#issuecomment-563508582
Performance,perform,perform,"@jguhlin glad to hear that training curves look reasonable! I want to mention that DV is currently written to be a diploid variant caller. In case you are retraining with data from polyploid organisms, it is not yet clear how DeepVariant will perform. I'll close this issue for now, but feel free to reopen if you have any other questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251#issuecomment-566180701
Usability,clear,clear,"@jguhlin glad to hear that training curves look reasonable! I want to mention that DV is currently written to be a diploid variant caller. In case you are retraining with data from polyploid organisms, it is not yet clear how DeepVariant will perform. I'll close this issue for now, but feel free to reopen if you have any other questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251#issuecomment-566180701
Deployability,install,installed,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:; ```; conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0; ```; Everything is installed correctly. Is there a guide to follow for locally installed variant caller?; I'm not sure I've been able to find it. . Thank you again for your support,; Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-566566093
Usability,guid,guide,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:; ```; conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0; ```; Everything is installed correctly. Is there a guide to follow for locally installed variant caller?; I'm not sure I've been able to find it. . Thank you again for your support,; Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-566566093
Availability,error,error,"Hi Andrea, I am getting the same error while I am trying to run the deepvariant code through the conda installation. Did you find a solution for it?. > Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:; > ; > ```; > conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0; > ```; > ; > Everything is installed correctly. Is there a guide to follow for locally installed variant caller?; > I'm not sure I've been able to find it.; > ; > Thank you again for your support,; > Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-567477006
Deployability,install,installation,"Hi Andrea, I am getting the same error while I am trying to run the deepvariant code through the conda installation. Did you find a solution for it?. > Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:; > ; > ```; > conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0; > ```; > ; > Everything is installed correctly. Is there a guide to follow for locally installed variant caller?; > I'm not sure I've been able to find it.; > ; > Thank you again for your support,; > Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-567477006
Usability,guid,guide,"Hi Andrea, I am getting the same error while I am trying to run the deepvariant code through the conda installation. Did you find a solution for it?. > Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:; > ; > ```; > conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0; > ```; > ; > Everything is installed correctly. Is there a guide to follow for locally installed variant caller?; > I'm not sure I've been able to find it.; > ; > Thank you again for your support,; > Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-567477006
Deployability,release,release,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256#issuecomment-568660419
Testability,log,logic,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256#issuecomment-568660419
Usability,simpl,simple,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256#issuecomment-568660419
Usability,feedback,feedback,"Thank you! ; My experiment was designed for the reference to be as close as possible to the studied populations so that shouldn't be an issue. Thanks for the always fast feedback.; I read in the blog post. > Of the 94,554 Mendelian violations where the child is HOM_REF, **only 17,475 (18%) of those have the HOM_REF call based just upon reference and non-reference read counts, the remaining 82% had the HOM_REF call produced by the CNN**. This seemed suspicious, so we investigated the allele depth fractions for each of HOM_REF, HETEROZYGOUS, and HOM_ALT calls in all three individuals. That's interesting because I also get quite a non trivial number of HOM_REF calls which, just based on the biology and specifics of my experiment (I prefer to remain vague about that publicly) is highly suspicious. In fact, all new homozygous variants are suspicious in my experiment. . I have highlighted a sentence in boldface, simply: how did you do that? How do you know that deepvariant made the call based on non-reference/reference read ratio or that it made the call based on its CNN interpretation? I thought the CNN was used for all calls? Or I am missing something obvious here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257#issuecomment-569126095
Integrability,message,message,"If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources?. On the GCP DeepVariant tutorial page:; https://cloud.google.com/life-sciences/docs/tutorials/deepvariant; You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not?. Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good!. Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck?. > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later.; > ; > thanks; > ; > Andy; > ; > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue.; > ; > p.p.s. Is",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260#issuecomment-573487475
Testability,log,logs,"put. One component in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines.; Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from?; If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources?. On the GCP DeepVariant tutorial page:; https://cloud.google.com/life-sciences/docs/tutorials/deepvariant; You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not?. Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good!. Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck?. > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven'",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260#issuecomment-573487475
Usability,guid,guidelines,"> 1. Any idea how I might estimate the expected run time?. The run time of `make_examples` step can be affected by many factors, such as the coverage(depth) of the input. One component in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines.; Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from?; If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources?. On the GCP DeepVariant tutorial page:; https://cloud.google.com/life-sciences/docs/tutorials/deepvariant; You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not?. Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good!. Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck?. > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260#issuecomment-573487475
Availability,avail,available,"Hi @aderzelle . Thank you, this is a good question. We have observed this phenomenon as well. The answer is somewhat complicated. . DeepVariant seems to have learned something about the concept of segmental duplication, where positions that appear to be variants are actually due to mismapping of similar regions which may (or may not) be captured in the reference genome. The way this manifests in a genome pileup is as one phased haplotype that is mostly reference and (one or more) phased haplotype that is variant-dense. The signal for this is further enhanced when the VAF is closer to 0.33 or 0.25 (more directly suggesting copy number 3 or 4), but it can also occur close to 0.5 (which can still indicate a copy number of 4). These regions can be variants in thee diploid genome that are incorrectly called as REF, or they could be markers of a copy number variant. In human genomes, this can suggest a user look into that region for either known copy number variants or coverage differences. One question to ask - are these regions at generally higher coverage than you would expect? . In certain variant-dense species, we have observed this phenomenon to complicate calling [in this blog we investigate this for mosquito genomes](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). In this blog, we show the ability to re-train for a variant-dense species using a pedigree. If you have a pedigree available, we could also explore this with you. We are working on ways that will allow DeepVariant to more explicitly indicate when it thinks this is the case, and to provide more information (e.g. average coverage in the sample) to DeepVariant that will allow it to better separate variants from makers of segmental duplication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266#issuecomment-580713806
Modifiability,enhance,enhanced,"Hi @aderzelle . Thank you, this is a good question. We have observed this phenomenon as well. The answer is somewhat complicated. . DeepVariant seems to have learned something about the concept of segmental duplication, where positions that appear to be variants are actually due to mismapping of similar regions which may (or may not) be captured in the reference genome. The way this manifests in a genome pileup is as one phased haplotype that is mostly reference and (one or more) phased haplotype that is variant-dense. The signal for this is further enhanced when the VAF is closer to 0.33 or 0.25 (more directly suggesting copy number 3 or 4), but it can also occur close to 0.5 (which can still indicate a copy number of 4). These regions can be variants in thee diploid genome that are incorrectly called as REF, or they could be markers of a copy number variant. In human genomes, this can suggest a user look into that region for either known copy number variants or coverage differences. One question to ask - are these regions at generally higher coverage than you would expect? . In certain variant-dense species, we have observed this phenomenon to complicate calling [in this blog we investigate this for mosquito genomes](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). In this blog, we show the ability to re-train for a variant-dense species using a pedigree. If you have a pedigree available, we could also explore this with you. We are working on ways that will allow DeepVariant to more explicitly indicate when it thinks this is the case, and to provide more information (e.g. average coverage in the sample) to DeepVariant that will allow it to better separate variants from makers of segmental duplication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266#issuecomment-580713806
Usability,learn,learned,"Hi @aderzelle . Thank you, this is a good question. We have observed this phenomenon as well. The answer is somewhat complicated. . DeepVariant seems to have learned something about the concept of segmental duplication, where positions that appear to be variants are actually due to mismapping of similar regions which may (or may not) be captured in the reference genome. The way this manifests in a genome pileup is as one phased haplotype that is mostly reference and (one or more) phased haplotype that is variant-dense. The signal for this is further enhanced when the VAF is closer to 0.33 or 0.25 (more directly suggesting copy number 3 or 4), but it can also occur close to 0.5 (which can still indicate a copy number of 4). These regions can be variants in thee diploid genome that are incorrectly called as REF, or they could be markers of a copy number variant. In human genomes, this can suggest a user look into that region for either known copy number variants or coverage differences. One question to ask - are these regions at generally higher coverage than you would expect? . In certain variant-dense species, we have observed this phenomenon to complicate calling [in this blog we investigate this for mosquito genomes](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). In this blog, we show the ability to re-train for a variant-dense species using a pedigree. If you have a pedigree available, we could also explore this with you. We are working on ways that will allow DeepVariant to more explicitly indicate when it thinks this is the case, and to provide more information (e.g. average coverage in the sample) to DeepVariant that will allow it to better separate variants from makers of segmental duplication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266#issuecomment-580713806
Availability,error,errors,"Hello, thanks for your answer. Actually my organism is an ancient tetraploid; it's quite old but we are able to find back the old duplicates easily (they just look like alleles with high divergence). The coverage in those regions doesn't deviate from what is expected. . I would bet the rotifers are variant dense, their genome is small (100 Mb) and there doesn't seem to be a lot of repeats. Somehow, it's quite the exact opposite of the human genome (large and full of repeats). ; The thing is, as it is an asexual, I can't replicate the trio strategy, as I only have a mother and a descendant. So I am unsure about what to do here. I was planning to call the variant in the mother and the daughter, assuming there should be all identical. Any new variant in the daughter I would regard as calling errors. (and I see new variants, though most of the variant in the daughter are the same as in the mother). I am unsure if retraining would make sense here. . EDIT: more simply, does the concept of a pedigree make any sense in a clonal organism?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266#issuecomment-580767025
Usability,simpl,simply,"Hello, thanks for your answer. Actually my organism is an ancient tetraploid; it's quite old but we are able to find back the old duplicates easily (they just look like alleles with high divergence). The coverage in those regions doesn't deviate from what is expected. . I would bet the rotifers are variant dense, their genome is small (100 Mb) and there doesn't seem to be a lot of repeats. Somehow, it's quite the exact opposite of the human genome (large and full of repeats). ; The thing is, as it is an asexual, I can't replicate the trio strategy, as I only have a mother and a descendant. So I am unsure about what to do here. I was planning to call the variant in the mother and the daughter, assuming there should be all identical. Any new variant in the daughter I would regard as calling errors. (and I see new variants, though most of the variant in the daughter are the same as in the mother). I am unsure if retraining would make sense here. . EDIT: more simply, does the concept of a pedigree make any sense in a clonal organism?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266#issuecomment-580767025
Usability,simpl,simply,"Hi @situssog ; Is there a specific reason why you have a fastA file and not a fastQ?; PacBio's tool (https://github.com/PacificBiosciences/bam2fastx) can generate either, so we would recommend simply generating the fastQ file to preserve the base quality scores, so then BWA-MEM, DeepVariant, and any other tools can use them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/270#issuecomment-593729219
Usability,simpl,simpler,"I can open a separate issue if it's helpful, but just a couple more things related to this... ; First, while the haplotype stuff like in the images above is mostly gone with `ws_use_window_selector_model=false`, I still see the problem in some false positive calls. Another thing that happens with things that DV calls de novos but obviously are not is that the kid will just meet some threshold and have a number of MQ ~40 reads with the de novo, where as the parent will have a number of reads with the allele that are MQ ~18 or lower. But the VCF reports AD[1] == 0 for many of these in the parent. If the count of low-quality alleles were reported in the sample fields in the VCF, it would be simpler to filter to make sure the allele was absent from the parent.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272#issuecomment-592194090
Deployability,release,released,"Hi @brentp @kokyriakidis ,. We (genomics team in Google Health) have released DeepVariant 0.10 on 3/26, which includes improving consistency and accuracy by turning off `ws_use_window_selector_model` by default, along with the following other changes:; - Updated to Python3 and TensorFlow2; - Improved PacBio model for amplified libraries. For more information, please read our release notes: https://github.com/google/deepvariant/releases/tag/v0.10.0 ; If you have any feedback about your experience with v0.10, please let us know. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272#issuecomment-605207780
Usability,feedback,feedback,"Hi @brentp @kokyriakidis ,. We (genomics team in Google Health) have released DeepVariant 0.10 on 3/26, which includes improving consistency and accuracy by turning off `ws_use_window_selector_model` by default, along with the following other changes:; - Updated to Python3 and TensorFlow2; - Improved PacBio model for amplified libraries. For more information, please read our release notes: https://github.com/google/deepvariant/releases/tag/v0.10.0 ; If you have any feedback about your experience with v0.10, please let us know. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272#issuecomment-605207780
Availability,error,error,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max; > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable; > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max; > Traceback (most recent call last):; > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>; > import numpy as np; > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>; > from . import core; > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>; > raise ImportError(msg); > ImportError:; > ; > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!; > ; > Importing the multiarray numpy extension module failed. Most; > likely you are trying to import a failed build of numpy.; > Here is how to proceed:; > - If you're working with a numpy git repository, try `git clean -xdf`; > (removes all files not under version control) and rebuild numpy.; > - If you are simply trying to use the numpy version that you have installed:; > your installation is broken - please reinstall numpy.; > - If you have already reinstalled and that did not fix the problem, then:; > 1. Check that you are using the Python you expect (you're using /usr/bin/python),; > and that you have no directories in your PATH or PYTHONPATH that can; > interfere with the Python and numpy versions you're trying to use.; > 2. If (1) looks fine, you can open a new issue at; > https://github.com/numpy/numpy/issues. Please include details on:; > - how you installed Python; > - how you installed numpy; > - your operating system; > - whether or not you have multiple versions of Python installed; > - if you built from source, your compiler versio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-598179709
Deployability,install,installed,"eate failed for thread 63 of 64: Resource temporarily unavailable; > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max; > Traceback (most recent call last):; > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>; > import numpy as np; > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>; > from . import core; > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>; > raise ImportError(msg); > ImportError:; > ; > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!; > ; > Importing the multiarray numpy extension module failed. Most; > likely you are trying to import a failed build of numpy.; > Here is how to proceed:; > - If you're working with a numpy git repository, try `git clean -xdf`; > (removes all files not under version control) and rebuild numpy.; > - If you are simply trying to use the numpy version that you have installed:; > your installation is broken - please reinstall numpy.; > - If you have already reinstalled and that did not fix the problem, then:; > 1. Check that you are using the Python you expect (you're using /usr/bin/python),; > and that you have no directories in your PATH or PYTHONPATH that can; > interfere with the Python and numpy versions you're trying to use.; > 2. If (1) looks fine, you can open a new issue at; > https://github.com/numpy/numpy/issues. Please include details on:; > - how you installed Python; > - how you installed numpy; > - your operating system; > - whether or not you have multiple versions of Python installed; > - if you built from source, your compiler versions and ideally a build log; > ; > Note: this error has many possible causes, so please don't comment on; > an existing issue about this - open a new one instead.; > ; > Original error was: PyCapsule_Import could not import module ""datetime""; > ; > Traceback (most recent call last):; > File ""/usr/lib/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-598179709
Performance,load,loader,"; > - If you have already reinstalled and that did not fix the problem, then:; > 1. Check that you are using the Python you expect (you're using /usr/bin/python),; > and that you have no directories in your PATH or PYTHONPATH that can; > interfere with the Python and numpy versions you're trying to use.; > 2. If (1) looks fine, you can open a new issue at; > https://github.com/numpy/numpy/issues. Please include details on:; > - how you installed Python; > - how you installed numpy; > - your operating system; > - whether or not you have multiple versions of Python installed; > - if you built from source, your compiler versions and ideally a build log; > ; > Note: this error has many possible causes, so please don't comment on; > an existing issue about this - open a new one instead.; > ; > Original error was: PyCapsule_Import could not import module ""datetime""; > ; > Traceback (most recent call last):; > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main; > ""__main__"", fname, loader, pkg_name); > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code; > exec code in run_globals; > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>; > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main; > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call; > return Popen(*popenargs, **kwargs).wait(); > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__; > errread, errwrite); > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child; > self.pid = os.fork(); > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'; > ; > real 19m19.271s; > user 1084m5.580s; > sys 17m12.750s; > Traceback (most recent call last):; > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; > app.run(main); > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; > _run_main(main, args); > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"",",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-598179709
Testability,log,log,"rray numpy extension module failed. Most; > likely you are trying to import a failed build of numpy.; > Here is how to proceed:; > - If you're working with a numpy git repository, try `git clean -xdf`; > (removes all files not under version control) and rebuild numpy.; > - If you are simply trying to use the numpy version that you have installed:; > your installation is broken - please reinstall numpy.; > - If you have already reinstalled and that did not fix the problem, then:; > 1. Check that you are using the Python you expect (you're using /usr/bin/python),; > and that you have no directories in your PATH or PYTHONPATH that can; > interfere with the Python and numpy versions you're trying to use.; > 2. If (1) looks fine, you can open a new issue at; > https://github.com/numpy/numpy/issues. Please include details on:; > - how you installed Python; > - how you installed numpy; > - your operating system; > - whether or not you have multiple versions of Python installed; > - if you built from source, your compiler versions and ideally a build log; > ; > Note: this error has many possible causes, so please don't comment on; > an existing issue about this - open a new one instead.; > ; > Original error was: PyCapsule_Import could not import module ""datetime""; > ; > Traceback (most recent call last):; > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main; > ""__main__"", fname, loader, pkg_name); > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code; > exec code in run_globals; > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>; > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main; > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call; > return Popen(*popenargs, **kwargs).wait(); > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__; > errread, errwrite); > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child; > self.pid = os.fork(); > OSError: [Errno 11] Re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-598179709
Usability,simpl,simply,"eate failed for thread 63 of 64: Resource temporarily unavailable; > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max; > Traceback (most recent call last):; > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>; > import numpy as np; > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>; > from . import core; > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>; > raise ImportError(msg); > ImportError:; > ; > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!; > ; > Importing the multiarray numpy extension module failed. Most; > likely you are trying to import a failed build of numpy.; > Here is how to proceed:; > - If you're working with a numpy git repository, try `git clean -xdf`; > (removes all files not under version control) and rebuild numpy.; > - If you are simply trying to use the numpy version that you have installed:; > your installation is broken - please reinstall numpy.; > - If you have already reinstalled and that did not fix the problem, then:; > 1. Check that you are using the Python you expect (you're using /usr/bin/python),; > and that you have no directories in your PATH or PYTHONPATH that can; > interfere with the Python and numpy versions you're trying to use.; > 2. If (1) looks fine, you can open a new issue at; > https://github.com/numpy/numpy/issues. Please include details on:; > - how you installed Python; > - how you installed numpy; > - your operating system; > - whether or not you have multiple versions of Python installed; > - if you built from source, your compiler versions and ideally a build log; > ; > Note: this error has many possible causes, so please don't comment on; > an existing issue about this - open a new one instead.; > ; > Original error was: PyCapsule_Import could not import module ""datetime""; > ; > Traceback (most recent call last):; > File ""/usr/lib/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-598179709
Usability,clear,clearly,".g.vcf.gz|grep -C 3 ""10764356"" . chromosome_1	10764353	.	C	T,<*>	27.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:27:162:100,62,0:0.382716,0:27,0,51,990,990,990; chromosome_1	10764354	.	A	<*>	0	.	END=10764354	GT:GQ:MIN_DP:PL	0/0:50:162:0,300,2999; chromosome_1	10764355	.	C	T,<*>	26.4	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:26:162:100,62,0:0.382716,0:26,0,60,990,990,990; chromosome_1	10764356	.	A	T,<*>	26.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:26:161:98,63,0:0.391304,0:26,0,64,990,990,990; chromosome_1	10764357	.	T	<*>	0	.	END=10764357	GT:GQ:MIN_DP:PL	0/0:50:162:0,300,2999; chromosome_1	10764358	.	T	C,<*>	33.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:33:172:98,63,0:0.366279,0:33,0,69,990,990,990; chromosome_1	10764359	.	T	<*>	0	.	END=10764381	GT:GQ:MIN_DP:PL	0/0:50:169:0,300,2999. ```; To me in the 3 samples it's the same site that is present. . Here another one (where the SNP is RefCall in one sample and PASS in another). ```; zgrep -w ""chromosome_2"" output.g.vcf.gz|grep -C 2 ""9780248""; chromosome_2	9780195	.	T	<*>	0	.	END=9780244	GT:GQ:MIN_DP:PL	0/0:50:294:0,300,2999; chromosome_2	9780245	.	GGT	G,<*>	37.4	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:36:294:161,131,0:0.445578,0:37,0,41,990,990,990; chromosome_2	9780248	.	A	<*>	0	.	END=9780249	GT:GQ:MIN_DP:PL	0/0:50:163:0,270,2939; chromosome_2	9780250	.	T	TTG,<*>	36.8	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:32:298:161,133,0:0.446309,0:36,0,34,990,990,990; chromosome_2	9780251	.	A	<*>	0	.	END=9780281	GT:GQ:MIN_DP:PL	0/0:50:272:0,300,2999; ```; In this second case, I don't understand why DeepVariant did not even consider there might be a variant there, as the bam clearly shows many reads mapping in that position with a variant site (it's the middle T flanked by 2 homozygous T sites). ![example2](https://user-images.githubusercontent.com/23341393/75358443-2ddbef00-58b3-11ea-9170-dd996a53386b.png). Now I know that calling SNP (in the sense of single nucleotide) variation in the vicinity of more complex events is known to be tricky, therefore this might not be an issue with DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278#issuecomment-591476042
Deployability,release,released,"@claudiologiudice although this issue was closed some time ago, we have just released a new RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) for Illumina data. . Please take a look if you are still considering this and let us know if you have any feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/283#issuecomment-1281193477
Usability,feedback,feedback,"@claudiologiudice although this issue was closed some time ago, we have just released a new RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) for Illumina data. . Please take a look if you are still considering this and let us know if you have any feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/283#issuecomment-1281193477
Security,secur,security,"I got it. Looking at the source code:. <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega@5""></script>; <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega-lite@3.4.0""></script>; <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega-embed@4""></script>. unfortunately the ""https://storage.googleapis.com"" is blocked here for ""security reasons"" :( . I open in my mobile using external network and I can see the complete output. Thanks for the feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290#issuecomment-698670947
Usability,feedback,feedback,"I got it. Looking at the source code:. <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega@5""></script>; <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega-lite@3.4.0""></script>; <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega-embed@4""></script>. unfortunately the ""https://storage.googleapis.com"" is blocked here for ""security reasons"" :( . I open in my mobile using external network and I can see the complete output. Thanks for the feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290#issuecomment-698670947
Deployability,configurat,configuration-file-for-each,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU?. * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291#issuecomment-607407000
Modifiability,config,configuration-file-for-each,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU?. * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291#issuecomment-607407000
Usability,simpl,simplest,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU?. * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291#issuecomment-607407000
Availability,error,error,"Hi @AndrewCarroll the output for `samtools view -H maddog_bam_trim_bwaMEM_sort_dedupped.bam | grep @SQ` is. ```; @SQ SN:I LN:15072434; @SQ SN:II LN:15279421; @SQ SN:III LN:13783801; @SQ SN:IV LN:17493829; @SQ SN:V LN:20924180; @SQ SN:X LN:17718942; @SQ SN:MtDNA LN:13794; ```. The alignment was done by a summer student working under the guidance of a post-doc (she recently had a baby so I didn't want to bother her) and it's possible a different reference version was used. In the error above I had used **c_elegans.PRJEB28388.WS274.genomic.fa**. We have another reference genome on our server so I tried it in-case this was the issue. Using **c_elegans.PRJNA13758.WS265.genomic.fa** deepvariant ran for more than 3 hours and then failed on Chromosome V with this error:. ```; ...; I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]; I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]; I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]; I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]; I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]; I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]; [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?); 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020; Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads; File ""/tmp/Bazel.runfiles_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292#issuecomment-608553986
Usability,guid,guidance,"Hi @AndrewCarroll the output for `samtools view -H maddog_bam_trim_bwaMEM_sort_dedupped.bam | grep @SQ` is. ```; @SQ SN:I LN:15072434; @SQ SN:II LN:15279421; @SQ SN:III LN:13783801; @SQ SN:IV LN:17493829; @SQ SN:V LN:20924180; @SQ SN:X LN:17718942; @SQ SN:MtDNA LN:13794; ```. The alignment was done by a summer student working under the guidance of a post-doc (she recently had a baby so I didn't want to bother her) and it's possible a different reference version was used. In the error above I had used **c_elegans.PRJEB28388.WS274.genomic.fa**. We have another reference genome on our server so I tried it in-case this was the issue. Using **c_elegans.PRJNA13758.WS265.genomic.fa** deepvariant ran for more than 3 hours and then failed on Chromosome V with this error:. ```; ...; I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]; I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]; I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]; I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]; I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]; I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]; [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?); 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020; Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads; File ""/tmp/Bazel.runfiles_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292#issuecomment-608553986
Availability,down,downloaded,",cloud-platform"" \; --image-family ""centos-8"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --boot-disk-size ""200G"" \; --zone ""us-west1-b""; ```. ssh into the machine:; ```; gcloud compute ssh ${USER}-centos8; ```. Check OS version:; ```; [pichuan@pichuan-centos8 ~]$ cat /etc/os-release; NAME=""CentOS Linux""; VERSION=""8""; ID=""centos""; ID_LIKE=""rhel fedora""; VERSION_ID=""8""; PLATFORM_ID=""platform:el8""; PRETTY_NAME=""CentOS Linux 8""; ANSI_COLOR=""0;31""; CPE_NAME=""cpe:/o:centos:centos:8""; HOME_URL=""https://centos.org/""; BUG_REPORT_URL=""https://bugs.centos.org/""; CENTOS_MANTISBT_PROJECT=""CentOS-8""; CENTOS_MANTISBT_PROJECT_VERSION=""8""; ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:; ```; [pichuan@pichuan-centos8 ~]$ singularity --version; singularity version 3.7.0-1.el8; ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:; ```; BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```; [pichuan@pichuan-centos8 ~]$ ls -1 ""${OUTPUT_DIR}/intermediate_results_dir""; call_variants_output.tfrecord.gz; gvcf.tfrecord-00000-of-00001.gz; make_examples.tfrecord-00000-of-00001.gz; ```. Next, I want to try running with `--inter",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296#issuecomment-767294612
Deployability,release,release,"mp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:; ```; gcloud compute instances create ""${USER}-centos8"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-8"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --boot-disk-size ""200G"" \; --zone ""us-west1-b""; ```. ssh into the machine:; ```; gcloud compute ssh ${USER}-centos8; ```. Check OS version:; ```; [pichuan@pichuan-centos8 ~]$ cat /etc/os-release; NAME=""CentOS Linux""; VERSION=""8""; ID=""centos""; ID_LIKE=""rhel fedora""; VERSION_ID=""8""; PLATFORM_ID=""platform:el8""; PRETTY_NAME=""CentOS Linux 8""; ANSI_COLOR=""0;31""; CPE_NAME=""cpe:/o:centos:centos:8""; HOME_URL=""https://centos.org/""; BUG_REPORT_URL=""https://bugs.centos.org/""; CENTOS_MANTISBT_PROJECT=""CentOS-8""; CENTOS_MANTISBT_PROJECT_VERSION=""8""; ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:; ```; [pichuan@pichuan-centos8 ~]$ singularity --version; singularity version 3.7.0-1.el8; ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:; ```; BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittes",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296#issuecomment-767294612
Testability,test,tests,"Hi @simoncchu ,; I tried to run a small Quick Start using the same Singularity and OS version as you. ; With the small tests, I'm not able to see the issue that you observed. And, when I ran with `--intermediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:; ```; gcloud compute instances create ""${USER}-centos8"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-8"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --boot-disk-size ""200G"" \; --zone ""us-west1-b""; ```. ssh into the machine:; ```; gcloud compute ssh ${USER}-centos8; ```. Check OS version:; ```; [pichuan@pichuan-centos8 ~]$ cat /etc/os-release; NAME=""CentOS Linux""; VERSION=""8""; ID=""centos""; ID_LIKE=""rhel fedora""; VERSION_ID=""8""; PLATFORM_ID=""platform:el8""; PRETTY_NAME=""CentOS Linux 8""; ANSI_COLOR=""0;31""; CPE_NAME=""cpe:/o:centos:centos:8""; HOME_URL=""https://centos.org/""; BUG_REPORT_URL=""https://bugs.centos.org/""; CENTOS_MANTISBT_PROJECT=""CentOS-8""; CENTOS_MANTISBT_PROJECT_VERSION=""8""; ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:; ```; [pichuan@pichuan-centos8 ~]$ singularity --version; singularity version 3.7.0-1.el8; ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:; ```; BIN_VERSION=1.1.0. singu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296#issuecomment-767294612
Usability,guid,guides,") that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:; ```; gcloud compute instances create ""${USER}-centos8"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-8"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --boot-disk-size ""200G"" \; --zone ""us-west1-b""; ```. ssh into the machine:; ```; gcloud compute ssh ${USER}-centos8; ```. Check OS version:; ```; [pichuan@pichuan-centos8 ~]$ cat /etc/os-release; NAME=""CentOS Linux""; VERSION=""8""; ID=""centos""; ID_LIKE=""rhel fedora""; VERSION_ID=""8""; PLATFORM_ID=""platform:el8""; PRETTY_NAME=""CentOS Linux 8""; ANSI_COLOR=""0;31""; CPE_NAME=""cpe:/o:centos:centos:8""; HOME_URL=""https://centos.org/""; BUG_REPORT_URL=""https://bugs.centos.org/""; CENTOS_MANTISBT_PROJECT=""CentOS-8""; CENTOS_MANTISBT_PROJECT_VERSION=""8""; ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:; ```; [pichuan@pichuan-centos8 ~]$ singularity --version; singularity version 3.7.0-1.el8; ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:; ```; BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296#issuecomment-767294612
Usability,simpl,simply,"Hello again,. actually the evidence bam folder is borderline unusable, there are so many folders that the file explorer tries to commit suicide every time I attempt to navigate it, same for the ""built-in"" explorer of bam viewers, for example Tablet is simply unusable, it systematically crashes. . I don't know how feasible this is computationally but maybe it would be best if DeepVariant produced a single bam per sample. Or even a single bam per sample per chromosome. If the user wants to subsequently split the bam that's not too complicated (otherwise the bam files themselves are neat I am happy with the results ^^). . Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/297#issuecomment-613950795
Usability,learn,learning,"Hi @meghanasp21 . Yes - ""PASS"" variants are the ones that DeepVariant has made a call and indicate that DeepVariant thinks there is a germline variant at the position.; You can also refer to : https://samtools.github.io/hts-specs/VCFv4.2.pdf; ```; FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position. ; ```; Other than the conventional ""PASS"" to indicate that the tool has made a call, you might also see many other values being filled into this field. Especially if you have run somatic callers, you might notice people use it for various reasons why they filter out a variant. Currently, in DeepVariant, you might see another value ""RefCall"" - this means that DeepVariant identifies a position as a potential candidate early on, but the machine learning model decided that it is actually Ref (0/0). . So yes, for DeepVariant, you should just look at the ones that has ""PASS"".",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/300#issuecomment-617897260
Deployability,install,installing,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301#issuecomment-617907163
Integrability,message,message,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301#issuecomment-617907163
Testability,log,logging,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301#issuecomment-617907163
Usability,clear,clear,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301#issuecomment-617907163
Usability,feedback,feedback,"Hi @tetsuro90 [this documentation](https://github.com/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs) has more information about the representation and the ""half-calls"" specifically. It involves some gnarly issues with overlapping variants in VCF for which there isn't a lot of standardization across tools unfortunately. [This issue](https://github.com/dnanexus-rnd/GLnexus/issues/210) also discusses some potential future developments. Any feedback there is welcome. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/302#issuecomment-620808470
Availability,avail,available,"Hi @WeiweiBian . In our discussion over email, I recommended that you use a somatic caller instead of trying to adapt DeepVariant to your needs, and I still think this is the best way to go. To answer your questions:; 1. No, this is a limit within Inception V3 that DeepVariant uses.; 2. We don't have any other training tutorials for other systems.; 3. We have done some exploratory work on a somatic variant caller, but it is not available yet, and it is meant for tumor-normal pairs with much lower coverage and higher allele frequency. It will not be possible to use this for your case. I think you would be much better off using a somatic variant caller for your research.; If you want to take transforming DeepVariant into a somatic caller on as a research project, you are welcome to do so, since it's open source. But we unfortunately don't have the bandwidth to guide you very much, since we have other exciting improvements to DeepVariant that we are already working on. Best of luck with your work!; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308#issuecomment-628304654
Energy Efficiency,adapt,adapt,"Hi @WeiweiBian . In our discussion over email, I recommended that you use a somatic caller instead of trying to adapt DeepVariant to your needs, and I still think this is the best way to go. To answer your questions:; 1. No, this is a limit within Inception V3 that DeepVariant uses.; 2. We don't have any other training tutorials for other systems.; 3. We have done some exploratory work on a somatic variant caller, but it is not available yet, and it is meant for tumor-normal pairs with much lower coverage and higher allele frequency. It will not be possible to use this for your case. I think you would be much better off using a somatic variant caller for your research.; If you want to take transforming DeepVariant into a somatic caller on as a research project, you are welcome to do so, since it's open source. But we unfortunately don't have the bandwidth to guide you very much, since we have other exciting improvements to DeepVariant that we are already working on. Best of luck with your work!; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308#issuecomment-628304654
Modifiability,adapt,adapt,"Hi @WeiweiBian . In our discussion over email, I recommended that you use a somatic caller instead of trying to adapt DeepVariant to your needs, and I still think this is the best way to go. To answer your questions:; 1. No, this is a limit within Inception V3 that DeepVariant uses.; 2. We don't have any other training tutorials for other systems.; 3. We have done some exploratory work on a somatic variant caller, but it is not available yet, and it is meant for tumor-normal pairs with much lower coverage and higher allele frequency. It will not be possible to use this for your case. I think you would be much better off using a somatic variant caller for your research.; If you want to take transforming DeepVariant into a somatic caller on as a research project, you are welcome to do so, since it's open source. But we unfortunately don't have the bandwidth to guide you very much, since we have other exciting improvements to DeepVariant that we are already working on. Best of luck with your work!; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308#issuecomment-628304654
Usability,guid,guide,"Hi @WeiweiBian . In our discussion over email, I recommended that you use a somatic caller instead of trying to adapt DeepVariant to your needs, and I still think this is the best way to go. To answer your questions:; 1. No, this is a limit within Inception V3 that DeepVariant uses.; 2. We don't have any other training tutorials for other systems.; 3. We have done some exploratory work on a somatic variant caller, but it is not available yet, and it is meant for tumor-normal pairs with much lower coverage and higher allele frequency. It will not be possible to use this for your case. I think you would be much better off using a somatic variant caller for your research.; If you want to take transforming DeepVariant into a somatic caller on as a research project, you are welcome to do so, since it's open source. But we unfortunately don't have the bandwidth to guide you very much, since we have other exciting improvements to DeepVariant that we are already working on. Best of luck with your work!; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308#issuecomment-628304654
Usability,clear,clear,"Hi @colsen ; thanks for confirming that the file was there. At this point I don't have a clear next guess on what could have been wrong. But can you check these two things:. 1. Confirm that docker see this file:; Run a similar command:. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" ls /input/hg19.fasta*. This can confirm that docker actually sees the file. (should have both hg19.fasta and hg19.fasta.fai). 2. Run another program to make sure the the fasta file is well-formed. Given that you have tried samtools faidx it without issue, I think this is probably not the issue, but might still be good to check. @Roj4ck if you have more suggestions on this, please feel free to chime in!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310#issuecomment-637679152
Usability,simpl,simple,"I reviewed the anotated vcf.gz files (just utilized a simple search in a text editor for counts and I identified the following counts:. Checking: /home/username/deepvariant-run/output/RBA2s.cohort.vcf.gz; Family: [2114337 + 2114302] -> [2115432]; 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls; 64534 of the values were ./. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz; Family: [2114337 + 2114302] -> [2009617]; 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls; 59210 of the values were ./. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz; Family: [2114337 + 2114302] -> [2009617, 2115432]; 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls; 118840 of the values were ./.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311#issuecomment-637256372
Security,validat,validation,"Thanks so much, that makes the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. Very much looking forward to reading your comments on warmstarting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-637523488
Usability,clear,clearer,"Thanks so much, that makes the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. Very much looking forward to reading your comments on warmstarting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-637523488
Deployability,release,release,"Hi @mpinese ,; I'm commenting here for your question 4. Internally for warmstarting, we use the same flag (`--start_from_checkpoint`) in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md to train our WES and PacBio release models. You refer to: https://github.com/google/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:; For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-638521636
Modifiability,evolve,evolve,"gle/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:; For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you can set `--start_from_checkpoint=""""`. ). I think your setup above looks reasonable to me. If you want to share more later (like what your training looks like, how long it takes to converge, whether your new model works better on your data, etc), feel free to add more in this issue. I will close it for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-638521636
Testability,log,logic,"Hi @mpinese ,; I'm commenting here for your question 4. Internally for warmstarting, we use the same flag (`--start_from_checkpoint`) in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md to train our WES and PacBio release models. You refer to: https://github.com/google/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:; For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-638521636
Usability,intuit,intuition,"gle/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:; For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you can set `--start_from_checkpoint=""""`. ). I think your setup above looks reasonable to me. If you want to share more later (like what your training looks like, how long it takes to converge, whether your new model works better on your data, etc), feel free to add more in this issue. I will close it for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-638521636
Availability,checkpoint,checkpoint,"the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:; Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , ; our **training** set are the labeled examples that our classifier actually learns from. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set; When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release.; Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set; When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-638566733
Deployability,update,update,"the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:; Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , ; our **training** set are the labeled examples that our classifier actually learns from. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set; When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release.; Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set; When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-638566733
Safety,safe,safer,"> Thanks so much, that makes the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:; Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , ; our **training** set are the labeled examples that our classifier actually learns from. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set; When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release.; Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set; When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-638566733
Security,validat,validation,"> Thanks so much, that makes the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:; Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , ; our **training** set are the labeled examples that our classifier actually learns from. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set; When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release.; Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set; When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-638566733
Testability,test,test,"the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:; Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , ; our **training** set are the labeled examples that our classifier actually learns from. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set; When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release.; Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set; When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-638566733
Usability,clear,clearer,"> Thanks so much, that makes the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:; Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , ; our **training** set are the labeled examples that our classifier actually learns from. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set; When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release.; Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set; When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-638566733
Usability,guid,guidance,"hello @akolesnikov,. I was wondering if you might have some additional guidance towards running deepvariant. I have started running deepvariant successfully in another server, but I would like to be run the process in parallel in multiple servers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315#issuecomment-640852141
Performance,perform,performs,"GIAB used a combination of different kinds of sequencing data to get the truth VCF, including long-range technologies that give better results than using short-read sequencing alone. Learning from this truth set with more context is why DeepVariant performs better than purely looking at AD and VAF alone. DeepVariant learns to balance false positives and false negatives, so when it labels some high-AD loci as '0' it is because they look like sites that GIAB labeled as hom-ref. STRs in particular can cause false positives when using simple allele depth approaches. Since DeepVariant sees the base sequence in the pileup image, it effectively already has an STR channel, so it can ""learn"" about STRs and take their presence into account. While the GIAB truth set is not perfect, it has generally performed far better by employing multiple technologies relative to using short-read allele depth alone. I would say it's more likely that these sites were determined by other technologies to be hom-ref than that they were missed by GIAB.; Does that answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317#issuecomment-644965403
Usability,learn,learns,"GIAB used a combination of different kinds of sequencing data to get the truth VCF, including long-range technologies that give better results than using short-read sequencing alone. Learning from this truth set with more context is why DeepVariant performs better than purely looking at AD and VAF alone. DeepVariant learns to balance false positives and false negatives, so when it labels some high-AD loci as '0' it is because they look like sites that GIAB labeled as hom-ref. STRs in particular can cause false positives when using simple allele depth approaches. Since DeepVariant sees the base sequence in the pileup image, it effectively already has an STR channel, so it can ""learn"" about STRs and take their presence into account. While the GIAB truth set is not perfect, it has generally performed far better by employing multiple technologies relative to using short-read allele depth alone. I would say it's more likely that these sites were determined by other technologies to be hom-ref than that they were missed by GIAB.; Does that answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317#issuecomment-644965403
Modifiability,rewrite,rewrite,"> https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md; @MariaNattestad Thank you for the quick response.I guess my question was not clear so, I will try to rewrite my query. 1) I wanted to know the base at all coordinates in a list of interval regions. The VCF file does not output all the positions in that interval region. If VCF file is able to give hom-ref, the. why are multiple locations missed? and out of around 1000bp, I am able to get information of around 200 bp?. Is there an option where we can force the deep variant to give information for all base position? and later filter if they are bad quality of not; Just like GATK has BP resolution option when we run the variant caller. 2) ; #CHROM | POS | ID | REF | ALT | QUAL | FILTER | INFO | FORMAT | DRR015476; -- | -- | -- | -- | -- | -- | -- | -- | -- | --; 6 | 5285 | . | T | G | 1.6 | RefCall | . | GT:GQ:DP:AD:VAF:PL | ./.:5:213:107,106:0.497653:0,3,31; -- | -- | -- | -- | -- | -- | -- | -- | -- | --; 6 | 5288 | . | G | A | 22.1 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:20:220:106,107:0.486364:22,0,25. When we see allele depth(AD) for both rows, we observed that both the row has the almost same number of reads supporting it(107,106 and 106,107). Why is no call(./.) given for first and not for second? Can you please elaborate in why are multiple places given ./. despite reads supporting it",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318#issuecomment-645516372
Usability,clear,clear,"> https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md; @MariaNattestad Thank you for the quick response.I guess my question was not clear so, I will try to rewrite my query. 1) I wanted to know the base at all coordinates in a list of interval regions. The VCF file does not output all the positions in that interval region. If VCF file is able to give hom-ref, the. why are multiple locations missed? and out of around 1000bp, I am able to get information of around 200 bp?. Is there an option where we can force the deep variant to give information for all base position? and later filter if they are bad quality of not; Just like GATK has BP resolution option when we run the variant caller. 2) ; #CHROM | POS | ID | REF | ALT | QUAL | FILTER | INFO | FORMAT | DRR015476; -- | -- | -- | -- | -- | -- | -- | -- | -- | --; 6 | 5285 | . | T | G | 1.6 | RefCall | . | GT:GQ:DP:AD:VAF:PL | ./.:5:213:107,106:0.497653:0,3,31; -- | -- | -- | -- | -- | -- | -- | -- | -- | --; 6 | 5288 | . | G | A | 22.1 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:20:220:106,107:0.486364:22,0,25. When we see allele depth(AD) for both rows, we observed that both the row has the almost same number of reads supporting it(107,106 and 106,107). Why is no call(./.) given for first and not for second? Can you please elaborate in why are multiple places given ./. despite reads supporting it",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318#issuecomment-645516372
Availability,avail,available,"Hi @dhwani2410 . Is there any chance that this is from a sample for which publicly available data is present (e.g. HG001). I could look at pileups in the region for a more informed opinion. To your question about different variants - DeepVariant classifies event on a position-by-position basis, so the classifier does not have information about what output it gave at nearby variants. This can cause you to know a bit more than the classifier when looking at its full output. But it explains why the classifier can have a different result when two variants are putatively phased. One phenomenon that we observe with DeepVariant is that it often calls positions with substantial support as reference when they are nearby to other variants. This seems to be related to regions of segmental duplication, where the apparent variants come from reads that are mismapped to the region. We have a poster which documents this effect: https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096. So DeepVariant seems to learn something of the signature of segmental duplication and is likely not calling these positions as variant for this reason. On average, DeepVariant seems to be correct in these cases (that they are not true variants) more often than it is incorrect, which is why it has learned this pattern. However, it will not be correct in all cases. Either way, when you observe this pattern, it is a good idea to look at the region and consider whether the apparent variants may come from a duplication of related sequence.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318#issuecomment-645810441
Usability,learn,learn,"Hi @dhwani2410 . Is there any chance that this is from a sample for which publicly available data is present (e.g. HG001). I could look at pileups in the region for a more informed opinion. To your question about different variants - DeepVariant classifies event on a position-by-position basis, so the classifier does not have information about what output it gave at nearby variants. This can cause you to know a bit more than the classifier when looking at its full output. But it explains why the classifier can have a different result when two variants are putatively phased. One phenomenon that we observe with DeepVariant is that it often calls positions with substantial support as reference when they are nearby to other variants. This seems to be related to regions of segmental duplication, where the apparent variants come from reads that are mismapped to the region. We have a poster which documents this effect: https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096. So DeepVariant seems to learn something of the signature of segmental duplication and is likely not calling these positions as variant for this reason. On average, DeepVariant seems to be correct in these cases (that they are not true variants) more often than it is incorrect, which is why it has learned this pattern. However, it will not be correct in all cases. Either way, when you observe this pattern, it is a good idea to look at the region and consider whether the apparent variants may come from a duplication of related sequence.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318#issuecomment-645810441
Usability,clear,clear,"@AndrewCarroll Thanks for your response and the VCF example I have shared is my own data and not public data. Could you please refer to which part of my query you have answered? Maybe, I am not clear with your replies and which part does it address?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318#issuecomment-645829089
Usability,guid,guide,"Hi Sebastian, . That output doesn't look right. Did you follow [DeepVariant quick start guide](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md#notes-on-gpu-image) ?. Could you please provide the exact commands that you run?. Thank you; Alexey",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321#issuecomment-655676506
Deployability,install,installed,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321#issuecomment-656000987
Testability,log,logs,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321#issuecomment-656000987
Usability,guid,guide,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321#issuecomment-656000987
Availability,error,error,"Hi Gunjan,; and thanks a lot for your reply. Yes, the directories where all accessible and under ```root``` (I know it can be done better....).; Indeed when I added to the docker command the option ``` --user root``` the error changed and was very clear: ``` disk full```.; It remains to me to free disk or change docker location, add a docker group to allow users run it without ```sudo``` and rerun it. I am pretty confident it should work. With Regards,; -A",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325#issuecomment-659193187
Security,access,accessible,"Hi Gunjan,; and thanks a lot for your reply. Yes, the directories where all accessible and under ```root``` (I know it can be done better....).; Indeed when I added to the docker command the option ``` --user root``` the error changed and was very clear: ``` disk full```.; It remains to me to free disk or change docker location, add a docker group to allow users run it without ```sudo``` and rerun it. I am pretty confident it should work. With Regards,; -A",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325#issuecomment-659193187
Usability,clear,clear,"Hi Gunjan,; and thanks a lot for your reply. Yes, the directories where all accessible and under ```root``` (I know it can be done better....).; Indeed when I added to the docker command the option ``` --user root``` the error changed and was very clear: ``` disk full```.; It remains to me to free disk or change docker location, add a docker group to allow users run it without ```sudo``` and rerun it. I am pretty confident it should work. With Regards,; -A",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325#issuecomment-659193187
Availability,error,error-correction,"Hi @X1angyang . The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:; ```; import tensorflow as tf. !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/; checkpoint_path = '/tmp/model.ckpt'. reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path); shape_map_for_layers = reader.get_variable_to_shape_map(); print(shape_map_for_layers); ```; I just tested that in Colab (https://colab.research.google.com/). However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/.; It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. I hope that helps!; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328#issuecomment-663252998
Modifiability,layers,layers,"Hi @X1angyang . The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:; ```; import tensorflow as tf. !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/; checkpoint_path = '/tmp/model.ckpt'. reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path); shape_map_for_layers = reader.get_variable_to_shape_map(); print(shape_map_for_layers); ```; I just tested that in Colab (https://colab.research.google.com/). However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/.; It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. I hope that helps!; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328#issuecomment-663252998
Testability,test,tested,"Hi @X1angyang . The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:; ```; import tensorflow as tf. !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/; checkpoint_path = '/tmp/model.ckpt'. reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path); shape_map_for_layers = reader.get_variable_to_shape_map(); print(shape_map_for_layers); ```; I just tested that in Colab (https://colab.research.google.com/). However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/.; It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. I hope that helps!; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328#issuecomment-663252998
Usability,learn,learning,"Hi @X1angyang . The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:; ```; import tensorflow as tf. !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/; checkpoint_path = '/tmp/model.ckpt'. reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path); shape_map_for_layers = reader.get_variable_to_shape_map(); print(shape_map_for_layers); ```; I just tested that in Colab (https://colab.research.google.com/). However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/.; It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. I hope that helps!; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328#issuecomment-663252998
Availability,error,error-correction,"> Hi @X1angyang; > ; > The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:; > ; > ```; > import tensorflow as tf; > ; > !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/; > checkpoint_path = '/tmp/model.ckpt'; > ; > reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path); > shape_map_for_layers = reader.get_variable_to_shape_map(); > print(shape_map_for_layers); > ```; > ; > I just tested that in Colab (https://colab.research.google.com/).; > ; > However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/.; > It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus.; > ; > I hope that helps!; > Maria. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328#issuecomment-663306398
Modifiability,layers,layers,"> Hi @X1angyang; > ; > The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:; > ; > ```; > import tensorflow as tf; > ; > !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/; > checkpoint_path = '/tmp/model.ckpt'; > ; > reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path); > shape_map_for_layers = reader.get_variable_to_shape_map(); > print(shape_map_for_layers); > ```; > ; > I just tested that in Colab (https://colab.research.google.com/).; > ; > However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/.; > It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus.; > ; > I hope that helps!; > Maria. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328#issuecomment-663306398
Testability,test,tested,"> Hi @X1angyang; > ; > The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:; > ; > ```; > import tensorflow as tf; > ; > !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/; > checkpoint_path = '/tmp/model.ckpt'; > ; > reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path); > shape_map_for_layers = reader.get_variable_to_shape_map(); > print(shape_map_for_layers); > ```; > ; > I just tested that in Colab (https://colab.research.google.com/).; > ; > However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/.; > It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus.; > ; > I hope that helps!; > Maria. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328#issuecomment-663306398
Usability,learn,learning,"> Hi @X1angyang; > ; > The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:; > ; > ```; > import tensorflow as tf; > ; > !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/; > checkpoint_path = '/tmp/model.ckpt'; > ; > reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path); > shape_map_for_layers = reader.get_variable_to_shape_map(); > print(shape_map_for_layers); > ```; > ; > I just tested that in Colab (https://colab.research.google.com/).; > ; > However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/.; > It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus.; > ; > I hope that helps!; > Maria. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328#issuecomment-663306398
Deployability,release,released,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:; Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:; In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m); * Accuracy (on case study chr20); - Indel F1: 0.983872 —> 0.982728; - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:; 1. Amplicon will likely use the height=100; 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338#issuecomment-681126260
Energy Efficiency,reduce,reduces,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:; Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:; In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m); * Accuracy (on case study chr20); - Indel F1: 0.983872 —> 0.982728; - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:; 1. Amplicon will likely use the height=100; 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338#issuecomment-681126260
Performance,bottleneck,bottleneck,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:; Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:; In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m); * Accuracy (on case study chr20); - Indel F1: 0.983872 —> 0.982728; - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:; 1. Amplicon will likely use the height=100; 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338#issuecomment-681126260
Usability,feedback,feedback,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:; Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:; In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m); * Accuracy (on case study chr20); - Indel F1: 0.983872 —> 0.982728; - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:; 1. Amplicon will likely use the height=100; 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338#issuecomment-681126260
Availability,avail,available,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? ; - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339#issuecomment-681204545
Deployability,configurat,configurations,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? ; - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339#issuecomment-681204545
Modifiability,config,configurations,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? ; - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339#issuecomment-681204545
Testability,test,testdata,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? ; - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339#issuecomment-681204545
Usability,guid,guidance,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? ; - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339#issuecomment-681204545
Testability,test,test,"Hi @JakeHagen ; Currently there isn't a very clean way to do this. You can modify the code and build DeepVariant from source: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md. I'm personally interested in learning more about what you're trying to do - what is the expected input and output. If there's general enough use cases, maybe in the future we can make things easier to import, even though we don't currently have plans for that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/344#issuecomment-689698461
Usability,learn,learning,"Hi @JakeHagen ; Currently there isn't a very clean way to do this. You can modify the code and build DeepVariant from source: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md. I'm personally interested in learning more about what you're trying to do - what is the expected input and output. If there's general enough use cases, maybe in the future we can make things easier to import, even though we don't currently have plans for that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/344#issuecomment-689698461
Availability,error,error,"Hi @sh940202123 , it's not obvious to me why this might be the case. It's strange that no error message comes out at all. Can you try something even simpler, like:; ```; sudo docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/make_examples --help; ```; And see if the information comes out correctly?. I'll also check with my team member tomorrow to see if anyone has other suggestions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-690806128
Integrability,message,message,"Hi @sh940202123 , it's not obvious to me why this might be the case. It's strange that no error message comes out at all. Can you try something even simpler, like:; ```; sudo docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/make_examples --help; ```; And see if the information comes out correctly?. I'll also check with my team member tomorrow to see if anyone has other suggestions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-690806128
Usability,simpl,simpler,"Hi @sh940202123 , it's not obvious to me why this might be the case. It's strange that no error message comes out at all. Can you try something even simpler, like:; ```; sudo docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/make_examples --help; ```; And see if the information comes out correctly?. I'll also check with my team member tomorrow to see if anyone has other suggestions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-690806128
Usability,feedback,feedback,"Hi Ted,; Thanks for your feedback. Just to be sure I've got this right. You confirm that variants with 0/0 genotype and zero DP in the merged VCF are actually positions with no reads in that sample and so can be set to missing?; I have also an additional comment about deepvariant v1.0.0. I've noticed that it is much slower than v.0.9.0. I've used them both on the sample samples (30-60X WGS) using singularity and v0.9.0 takes 10-14h while v1.0.0 takes more than 24h per sample. Is this expected?. Many thanks!; Edoardo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346#issuecomment-694394538
Usability,simpl,simplest,"Hi @anands-repo, glad you were able to get it working! I don't have any other comments on the fix and will defer to the relevant bazel issue. In general, I would recommend running DeepVariant using Docker for the simplest setup. If you are building from source because you want to experiment with changes to the codebase, I'd still recommend Docker. You can clone the DeepVariant repo, modify the source code, and build a Docker image with your changes using [the provided Dockerfile](https://github.com/google/deepvariant/blob/r1.0/Dockerfile).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356#issuecomment-698549305
Performance,perform,perform,"Hi @leorippel, DeepVariant has previously been applied to plant species. In the case of rice, there was good evidence of high accuracy. You can see [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant). However, these rice genomes were diploid and with a similar variant density of humans. DeepVariant is currently written to be a diploid variant caller. So if the plant species you are working with is polyploid, it is not yet clear how DeepVariant will perform. That said, I am also not sure how other variant callers perform on polyploid samples. It would be possible to train DeepVariant models for a specific genome, but this would require a gold set for the training. We have a previous example of this in mosquitos [in this blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357#issuecomment-698486292
Usability,clear,clear,"Hi @leorippel, DeepVariant has previously been applied to plant species. In the case of rice, there was good evidence of high accuracy. You can see [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant). However, these rice genomes were diploid and with a similar variant density of humans. DeepVariant is currently written to be a diploid variant caller. So if the plant species you are working with is polyploid, it is not yet clear how DeepVariant will perform. That said, I am also not sure how other variant callers perform on polyploid samples. It would be possible to train DeepVariant models for a specific genome, but this would require a gold set for the training. We have a previous example of this in mosquitos [in this blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357#issuecomment-698486292
Safety,sanity check,sanity check,"@pichuan . As you know, I am running the shuffle script using Spark. I am wondering how many output files are expected from running the script. When I use DirectRunner, I get a single output file. When I use the SparkRunner I get as many output files as there are input files fitting the pattern (I have noticed this mismatch between spark/direct runner in another situation as well: https://stackoverflow.com/questions/64450391/apache-beam-beam-flatten-doesnt-flatten-files-with-sparkrunner-but-does-so-wi). Is this the expected result when using Dataflow runner as well? Basically, I am simply trying to do a sanity check to make sure that the shuffler isn't simply reading in the data and copying it without shuffling, or simply shuffling within each shard. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360#issuecomment-713149241
Usability,simpl,simply,"@pichuan . As you know, I am running the shuffle script using Spark. I am wondering how many output files are expected from running the script. When I use DirectRunner, I get a single output file. When I use the SparkRunner I get as many output files as there are input files fitting the pattern (I have noticed this mismatch between spark/direct runner in another situation as well: https://stackoverflow.com/questions/64450391/apache-beam-beam-flatten-doesnt-flatten-files-with-sparkrunner-but-does-so-wi). Is this the expected result when using Dataflow runner as well? Basically, I am simply trying to do a sanity check to make sure that the shuffler isn't simply reading in the data and copying it without shuffling, or simply shuffling within each shard. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360#issuecomment-713149241
Usability,clear,clear,"Hi @loipf , ; You can use `--help` with the different binaries, for example:. ```; docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --help; ```. To see the various binaries, you can use:; ```; docker run google/deepvariant:1.0.0 ls /opt/deepvariant/bin/; ```; to list all the binaries. Then, for example, you can run:. ```; docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/show_examples --help; ```. or any other binaries. I understand your point though. It might be easier if we have a more clear help page without knowing the structure. I'll think about this and see if we can improve in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/362#issuecomment-709713660
Deployability,release,release,"Hi @loipf ; I've made changes in internal code. It'll come out in the next release.; After the next release, feel free to let us know if have more feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/362#issuecomment-737653058
Usability,feedback,feedback,"Hi @loipf ; I've made changes in internal code. It'll come out in the next release.; After the next release, feel free to let us know if have more feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/362#issuecomment-737653058
Deployability,release,release,"@AndrewCarroll, many thanks for such quick response!. > We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO rel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-709941019
Energy Efficiency,reduce,reduce,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-709941019
Performance,optimiz,optimizations,"s. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-709941019
Testability,benchmark,benchmark,"@AndrewCarroll, many thanks for such quick response!. > We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO rel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-709941019
Usability,learn,learning,"@AndrewCarroll, many thanks for such quick response!. > We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO rel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-709941019
Availability,down,downside,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735703602
Testability,test,test,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735703602
Usability,simpl,simpler,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735703602
Integrability,depend,depends,"Thanks for your comments @gunjanbaid . I think, since everything else works well on single node systems, having the shuffle script work well on single node systems is also desirable. It is good to be able to shuffle only smaller files like you said. But if we limit ourselves to the original tfrecord outputs, that comes with the limitation that the shuffles are localized and not global as in the current script. However, a way to shuffle globally can be constructed from this idea with an additional step. This additional step will simply partition the input data into random buckets. Then we shuffle each bucket. I believe this is equivalent to a global shuffle with uniform probability for each permutation. This would be something like:; ```; input_data = readers | ""FlattenInputs"" >> beam.Flatten(); partitions = input_data | ""PartitionInputs"" >> beam.Partition(<random_partition_function_name>, <num_partitions>); for i, p in enumerate(partitions):; writing = p | ""WritePartition%d"" % i >> beam.io.WriteTFRecord(...); ```. Then each partition may be shuffled individually using the shuffle script. I have rolled both partitioning and shuffling into the same [script](https://github.com/anands-repo/deepvariant/blob/r1.0/tools/shuffle_tfrecords_beam_for_local.py). I will report back regarding whether this works as expected. This depends on beam.Partition behaving properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365#issuecomment-720871901
Usability,simpl,simply,"Thanks for your comments @gunjanbaid . I think, since everything else works well on single node systems, having the shuffle script work well on single node systems is also desirable. It is good to be able to shuffle only smaller files like you said. But if we limit ourselves to the original tfrecord outputs, that comes with the limitation that the shuffles are localized and not global as in the current script. However, a way to shuffle globally can be constructed from this idea with an additional step. This additional step will simply partition the input data into random buckets. Then we shuffle each bucket. I believe this is equivalent to a global shuffle with uniform probability for each permutation. This would be something like:; ```; input_data = readers | ""FlattenInputs"" >> beam.Flatten(); partitions = input_data | ""PartitionInputs"" >> beam.Partition(<random_partition_function_name>, <num_partitions>); for i, p in enumerate(partitions):; writing = p | ""WritePartition%d"" % i >> beam.io.WriteTFRecord(...); ```. Then each partition may be shuffled individually using the shuffle script. I have rolled both partitioning and shuffling into the same [script](https://github.com/anands-repo/deepvariant/blob/r1.0/tools/shuffle_tfrecords_beam_for_local.py). I will report back regarding whether this works as expected. This depends on beam.Partition behaving properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365#issuecomment-720871901
Testability,test,tested,"Hi @pichuan @gunjanbaid . I apologize for overcomplicating this PR. I should have taken the rest of the discussion elsewhere. Thanks for your patient responses and advise so far. Even though I made many commits here, the only PR here is for `shuffle_tfrecords_beam.py`, and the changes are trivial as @gunjanbaid has reviewed already. The purpose of the PR is to enable `shuffle_tfrecords_beam.py` to work with DirectRunner with multiple workers (`--direct_num_workers=2, --direct_running_mode=""multi_processing""` for example), as well as with SparkRunner, FlinkRunner etc. To answer @gunjanbaid 's question on using beam.DoFn instead of a callable, my personal opinion is that that may not be necessary. The issue I saw is that `lambda` functions cannot be invoked through ParDo for these runners/modes. I attribute it to the same issue that we see with python multiprocessing which uses pickle to dispatch functions across processes and lambda functions are not picklable. So in this context, I think a callable meets the minimum requirements to enable this. I have tested on my end that the original deepvariant script and the modified script give the same output for a testcase. I am not trying to push you into accepting this PR, but just trying to clear any confusions that may have been caused by my multiple commits to the same branch, so that if and when you do revisit this matter, there is a summary about what's going on. I understand that the changes may still be far outside the priority of the team, so please take this at the appropriate pace. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365#issuecomment-723468092
Usability,clear,clear,"Hi @pichuan @gunjanbaid . I apologize for overcomplicating this PR. I should have taken the rest of the discussion elsewhere. Thanks for your patient responses and advise so far. Even though I made many commits here, the only PR here is for `shuffle_tfrecords_beam.py`, and the changes are trivial as @gunjanbaid has reviewed already. The purpose of the PR is to enable `shuffle_tfrecords_beam.py` to work with DirectRunner with multiple workers (`--direct_num_workers=2, --direct_running_mode=""multi_processing""` for example), as well as with SparkRunner, FlinkRunner etc. To answer @gunjanbaid 's question on using beam.DoFn instead of a callable, my personal opinion is that that may not be necessary. The issue I saw is that `lambda` functions cannot be invoked through ParDo for these runners/modes. I attribute it to the same issue that we see with python multiprocessing which uses pickle to dispatch functions across processes and lambda functions are not picklable. So in this context, I think a callable meets the minimum requirements to enable this. I have tested on my end that the original deepvariant script and the modified script give the same output for a testcase. I am not trying to push you into accepting this PR, but just trying to clear any confusions that may have been caused by my multiple commits to the same branch, so that if and when you do revisit this matter, there is a summary about what's going on. I understand that the changes may still be far outside the priority of the team, so please take this at the appropriate pace. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365#issuecomment-723468092
Usability,simpl,simplify,"Hi @anands-repo ; Our codebase still supports TPU like before!; We decided to change our training tutorial to use GPU and CPU because it's a more general setting. Even though our code still supports TPU, we think it's better to simplify our tutorial.; If you encounter any issues when using DeepVariant with TPU, and if you suspect the issue might be in our codebase, please let us know.; (Btw, out of curiosity : have you been training DeepVariant model with TPUs? If you are using that, I would actually really love to hear your feedback. I didn't think we have that many external users with this functionality yet.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/376#issuecomment-720015500
Energy Efficiency,efficient,efficient,"@pichuan Thanks for confirming!. I tried GPU-based training, but since the codebase currently doesn't support multi-GPU runs, it may not be efficient for me to use GPU-based training. Hence I am looking into using TPUs for the same. I will gladly provide feedback once I am able to do it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/376#issuecomment-720216342
Usability,feedback,feedback,"@pichuan Thanks for confirming!. I tried GPU-based training, but since the codebase currently doesn't support multi-GPU runs, it may not be efficient for me to use GPU-based training. Hence I am looking into using TPUs for the same. I will gladly provide feedback once I am able to do it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/376#issuecomment-720216342
Usability,simpl,simple,"Thanks for the comment, @jumpyknight . What you suggest is interesting, but do you think it is plausible even if the insertion is of length 5? If that was the case, I would have expected to have more pixels 'lit up' as snps, or more pixeld 'darkened' as insertions, right after the position at which the insertion took place. However, no such behaviour takes place (referring again to the 6th channel). I'm not sure what infomation is relevant here so I'll post a bunch of stuff:. 1. The variant: as I said it is at chr20-10001435, it is labeled to be a simple SNP, hom-alt 1/1.; 2. The bam-file read I mentioned: . - Starts at: 10001358; - Cigar: 78M, 5I, 18M. That means that we have; 10001358 ... 10001435 X X X X X 10001436 ... 10001453; M ... M I I I I I M ... M; Where M indicated Match and I indicates Insertion. - It is the forward read, with mapping quality 60, ; - Has the following tags: [(RG, NA12878), (XT, U), (NM, 5), (SM, 37), (AM, 37), (X0, 1), (X1, 0), (XM, 0), (XO, 1), (XG, 5), (MD, 96)]",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379#issuecomment-723807366
Testability,benchmark,benchmarks,"Hi @yonatansc97 . In the default WGS and exome models, DeepVariant does not expand the content of insertion sequences. Instead, it represents them with a character marker that indicates that an insertion is present at the position. To a human, this visually looks like a SNP in the pileup, but the machine learning model can easily distinguish this from a SNP event. This representation is necessary in order to keep the pileup at a consistent width. The information of the content of the insertion is also implicitly present in the 5th channel (supports variant) which only gets a value of 255 with the read when the insertion sequence matches the event to be called. The representation is in some ways sub-optimal, because in some cases information is lost in the presentation to the network. We have developed an additional process, which aligns reads to the ALT allele as well, which expands insertion sequences and makes the full sequence of ALT insertion events visible. This improves accuracy for calling variants in PacBio HiFi data and Oxford Nanopore data, and is defaulted to on in those models. . The flag for this option is: --alt_aligned_pileup, which can have the values: --alt_aligned_pileup=diff_channels and --alt_aligned_pileup=rows. . From our benchmarks, the --alt_aligned_pileup flag did increase accuracy in Illumina WGS and exome models, but only marginally. Since this also somewhat increases the compute cost, we do not use this option in Illumina WGS and exome models. We discuss this feature in more detail in our [blog on DeepVariant v1.0](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379#issuecomment-724880533
Usability,learn,learning,"Hi @yonatansc97 . In the default WGS and exome models, DeepVariant does not expand the content of insertion sequences. Instead, it represents them with a character marker that indicates that an insertion is present at the position. To a human, this visually looks like a SNP in the pileup, but the machine learning model can easily distinguish this from a SNP event. This representation is necessary in order to keep the pileup at a consistent width. The information of the content of the insertion is also implicitly present in the 5th channel (supports variant) which only gets a value of 255 with the read when the insertion sequence matches the event to be called. The representation is in some ways sub-optimal, because in some cases information is lost in the presentation to the network. We have developed an additional process, which aligns reads to the ALT allele as well, which expands insertion sequences and makes the full sequence of ALT insertion events visible. This improves accuracy for calling variants in PacBio HiFi data and Oxford Nanopore data, and is defaulted to on in those models. . The flag for this option is: --alt_aligned_pileup, which can have the values: --alt_aligned_pileup=diff_channels and --alt_aligned_pileup=rows. . From our benchmarks, the --alt_aligned_pileup flag did increase accuracy in Illumina WGS and exome models, but only marginally. Since this also somewhat increases the compute cost, we do not use this option in Illumina WGS and exome models. We discuss this feature in more detail in our [blog on DeepVariant v1.0](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379#issuecomment-724880533
Modifiability,variab,variables,"Hi @anands-repo . This is something that we observe in training as well, and has also been reported by other users - [see this GitHub issue](https://github.com/google/deepvariant/issues/185) for deeper discussion. In short, this occurs because not all variables are loaded when warmstarting a model. Retraining does quickly re-learn, but this is the reason for the initial drop. You should still be able to train models to high accuracy, despite this phenomenon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/383#issuecomment-727745570
Performance,load,loaded,"Hi @anands-repo . This is something that we observe in training as well, and has also been reported by other users - [see this GitHub issue](https://github.com/google/deepvariant/issues/185) for deeper discussion. In short, this occurs because not all variables are loaded when warmstarting a model. Retraining does quickly re-learn, but this is the reason for the initial drop. You should still be able to train models to high accuracy, despite this phenomenon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/383#issuecomment-727745570
Usability,learn,learn,"Hi @anands-repo . This is something that we observe in training as well, and has also been reported by other users - [see this GitHub issue](https://github.com/google/deepvariant/issues/185) for deeper discussion. In short, this occurs because not all variables are loaded when warmstarting a model. Retraining does quickly re-learn, but this is the reason for the initial drop. You should still be able to train models to high accuracy, despite this phenomenon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/383#issuecomment-727745570
Usability,simpl,simple,"Hi @gevro . DeepVariant has two steps in calling variants. In the first (**make_examples**) a fairly simple, human-written heuristic identifies positions that are potentially variant and creates pileup examples of them. In the second stage (**call_variants**), a neural network classifies whether those positions are real variants or not and genotypes them. A RefCall entry occurs when a candidate variant is proposed and then is specifically rejected as non-variant. In addition, the model provides an estimate of its confidence (expressed as the QUAL and GQ fields for the entry). In the gVCF, a separate process determines the confidence for regions of the genome where no candidate is proposed and combines this with the information of the positions that have received a RefCall. For GLnexus, the genotyping is able to include the knowledge that these positions have a proposed alternate allele, but received a reference call.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/386#issuecomment-728248806
Usability,feedback,feedback,"Thank you, we're taking a look at the example and expect to have feedback relatively soon (with some delay for holidays in the US).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/388#issuecomment-734139746
Usability,simpl,simpleRepeat,"Thanks so much! I think I understand the issue now, but Deepvariant is still giving a wrong answer.; You can see that this particular location in the genome has a tandem repeat of 6 copies of a 47 bp sequence:; https://genome.ucsc.edu/cgi-bin/hgc?hgsid=960400993_qiSzxsvvUkaPrDYeYJ2KhGLAK2ay&c=chr3&l=76220845&r=76221817&o=76221234&t=76221509&g=simpleRepeat&i=trf. But I actually have long PacBio reads for this sample, with the zmw consensus sequence at the same location of:; TGAGCTTAATCATAGAACATGGTAATACTAGGAGACATCATGAAGGATCCCTGTGTTGTAGATATACTCTTCTTTACTTCCATTGAGAAGTAGTAGTTCAATTTCCCCAGGTAGTCTGAATCAATAACCCCAGGCAATATTGACTGTTTCTGTGGTGAAAGCATTCCTCCATCTAGAACTAAGTCCTCTTGCCCAACAGAAGATAAAGTCATGAGCATGGGAAGCAAAAATTTTGCTAGTGGGTAACTCAGGGTGATGGTGAGTAGTGCCACTCTCATTTTACAAGTGGGTAACTCAGGGTGATGGTGAGCAGTGCCACTCTCATTTTACAAGTGGGTAACTCAGGGTGATGGTGAGTAGTGCCACTCTCATTTTACAAGTGGGTAACTCAGGGTGATGGTGAGCAGTGCCCCTCTCATTTTACAAGTGGGTAACTCAGGGTGACGGTGAGCAGTGCCACTCTCATTTTACAAGTAACTCAGGGTGATGGTGAGCAGTGCCACTCTCATTTTCCACGCTTTGATTCCTGAACCCATTAATTGTGGCTGTTGATGAAACTACTATATGTTGGAAACTGCTTCAGAGAATATACAACCTTCTGCAGAACCTTGGCCCAGCTGTGTAAGGTATTGCGATCTAGCTGGTACTGTAACTGAATTCAAAAGACCCTTTTATCATTTTTATCAAGTTAGCTGCTTCTGGATGATGGGGAACATGGTAAGACCGATGGACTTCATGACCATGAGCCCATTGCCACACTTTTTTGTCTTTGAGGTGAGTTCCTTGATCAGAAGCAATGCTGTATTTAATACTGTGCCTGTGGATAAGACATTTTATAAGTCCACGGATGGTAGTTTCGGTGGAAGCATTGCACCCACGGAAGACAAATCCATAACCTGAGAAGGGTCTATTCCAATAAGCACAAAATGCTGCCACTTCCATAGTGGAAGCAGTCTAATGTAGATAAACTGCCACTAGGTAGCTGGCTGATCACCCTGGGGAATAATGCCAAATGGGATCACAATGTGGTCTCTACTGCTGGCAGATTGTATAATCTGCCAGTGGTGGCCATAGCTAGGTCAGCCTTGGTGAGTGGAAACCTATGTTGCTGAGTGCATGCATAACCTTCATCCCTGCCACCATGTCCACCTGTTACTGGTGGAATGTATCTGAGCCACGTGGCACCAAAACACGTTACCAGTGGCAAATTGGTATGGGTTTGCAGCAACTTCAGTTCTTGCCTCCTCAGAAGAAAGAATCTGACTGAGAGGCATAAGGTAGAAGGAGGGGCTGAGGCAAGTTTTAGAGCAGGAGTGAATGTTTATTTAAAAAGCCTTAGAGCAGGAATGAAAGGAAGGAAAGAAAGTATACTTGGAAGAGGGCCAAGTGGGTGACTTGAAAGACAAGTGTACATGTTGACCTTGTGACTAGGCTTATACGTTGGCATAATTCCAGGGTCTTGTGTCACTTCTCCCAACCCGCCCAACCCTTGAGATCTTATTGGGAAGCTGCTGATAACCAGT",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/388#issuecomment-735306929
Availability,echo,echo,"Thanks for the update, @Stikus . In the code I'm working on, I currently changed that block to:; ```; # Because of an issue with pypi's numpy on Ubuntu 14.04. we need to compile from; # source.; # See https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-279061085; if [[ ""$(lsb_release -d)"" == *Ubuntu*14.04.* ]]; then; echo ""Installing numpy with -no-binary=:all:. This will take a bit longer.""; pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}""; else; pip3 install ""${PIP_ARGS[@]}"" ""numpy==${DV_TF_NUMPY_VERSION}""; fi; ```. But I'm also wondering if I should just remove if/else statement for different Ubuntu versions if we're not internally testing it to make sure everything still runs. So I might end up simplifying this further in the next release. Glad to hear that the fix you mentioned above worked for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394#issuecomment-742700347
Deployability,update,update,"Thanks for the update, @Stikus . In the code I'm working on, I currently changed that block to:; ```; # Because of an issue with pypi's numpy on Ubuntu 14.04. we need to compile from; # source.; # See https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-279061085; if [[ ""$(lsb_release -d)"" == *Ubuntu*14.04.* ]]; then; echo ""Installing numpy with -no-binary=:all:. This will take a bit longer.""; pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}""; else; pip3 install ""${PIP_ARGS[@]}"" ""numpy==${DV_TF_NUMPY_VERSION}""; fi; ```. But I'm also wondering if I should just remove if/else statement for different Ubuntu versions if we're not internally testing it to make sure everything still runs. So I might end up simplifying this further in the next release. Glad to hear that the fix you mentioned above worked for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394#issuecomment-742700347
Testability,test,testing,"Thanks for the update, @Stikus . In the code I'm working on, I currently changed that block to:; ```; # Because of an issue with pypi's numpy on Ubuntu 14.04. we need to compile from; # source.; # See https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-279061085; if [[ ""$(lsb_release -d)"" == *Ubuntu*14.04.* ]]; then; echo ""Installing numpy with -no-binary=:all:. This will take a bit longer.""; pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}""; else; pip3 install ""${PIP_ARGS[@]}"" ""numpy==${DV_TF_NUMPY_VERSION}""; fi; ```. But I'm also wondering if I should just remove if/else statement for different Ubuntu versions if we're not internally testing it to make sure everything still runs. So I might end up simplifying this further in the next release. Glad to hear that the fix you mentioned above worked for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394#issuecomment-742700347
Usability,simpl,simplifying,"Thanks for the update, @Stikus . In the code I'm working on, I currently changed that block to:; ```; # Because of an issue with pypi's numpy on Ubuntu 14.04. we need to compile from; # source.; # See https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-279061085; if [[ ""$(lsb_release -d)"" == *Ubuntu*14.04.* ]]; then; echo ""Installing numpy with -no-binary=:all:. This will take a bit longer.""; pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}""; else; pip3 install ""${PIP_ARGS[@]}"" ""numpy==${DV_TF_NUMPY_VERSION}""; fi; ```. But I'm also wondering if I should just remove if/else statement for different Ubuntu versions if we're not internally testing it to make sure everything still runs. So I might end up simplifying this further in the next release. Glad to hear that the fix you mentioned above worked for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394#issuecomment-742700347
Availability,error,error,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository; 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. ; 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory.; 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances.; 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399#issuecomment-749327121
Deployability,install,installation,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository; 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. ; 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory.; 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances.; 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399#issuecomment-749327121
Modifiability,config,configuration,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository; 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. ; 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory.; 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances.; 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399#issuecomment-749327121
Usability,clear,clear,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository; 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. ; 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory.; 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances.; 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399#issuecomment-749327121
Testability,log,logic,"Hi @GuillaumeHolley . This is a complicated issue, and though I've looked into it, I'm not 100% sure the following is correct, but I am reasonably confident:. DeepVariant is consists of some human-written heuristics which are used to identify positions that are candidate to be variant. Identified candidates are given to the neural network for classification. After this classification, another set of human-written heuristics converts the output probabilities of the network to VCF and gVCF entries. . As part of this process, there are positions that were never proposed as candidates because they do not have enough support to reach the candidate generation threshold (for the PacBio defaults, this is at least 2 reads which support an alternate event with a minimum ALT fraction of 0.12, and where the reads used for the calculation have MAPQ>=5). When this threshold is not met, the site is always considered either a reference or a no-call, and human-written heuristics are used to make a determine the genotype quality of the position for binning in the gVCF. This logic is fairly simple, and can sometimes result in a calculation of HET being the most likely, even if DeepVariant's neural network never made a call. The GQ should be set to 0 in these cases. This occurs because we value the neural network's output much more highly, and would like only it to make calls. . Hopefully this helps clarify what is going on here. The situation and explanation is complicated, so please feel free to ask further questions. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403#issuecomment-759187363
Usability,simpl,simple,"Hi @GuillaumeHolley . This is a complicated issue, and though I've looked into it, I'm not 100% sure the following is correct, but I am reasonably confident:. DeepVariant is consists of some human-written heuristics which are used to identify positions that are candidate to be variant. Identified candidates are given to the neural network for classification. After this classification, another set of human-written heuristics converts the output probabilities of the network to VCF and gVCF entries. . As part of this process, there are positions that were never proposed as candidates because they do not have enough support to reach the candidate generation threshold (for the PacBio defaults, this is at least 2 reads which support an alternate event with a minimum ALT fraction of 0.12, and where the reads used for the calculation have MAPQ>=5). When this threshold is not met, the site is always considered either a reference or a no-call, and human-written heuristics are used to make a determine the genotype quality of the position for binning in the gVCF. This logic is fairly simple, and can sometimes result in a calculation of HET being the most likely, even if DeepVariant's neural network never made a call. The GQ should be set to 0 in these cases. This occurs because we value the neural network's output much more highly, and would like only it to make calls. . Hopefully this helps clarify what is going on here. The situation and explanation is complicated, so please feel free to ask further questions. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403#issuecomment-759187363
Usability,clear,clear,"Hi @AndrewCarroll,. Thank you very much for this detailed explanation, it is clear to me what is happening now. Unfortunately, this is rather problematic for my use case because I am processing a cohort (DeepVariant + GLnexus) and I impute the variants afterwards. The imputation is based on the PL values: If there are sites where some samples have an actual variant but most of the other samples have a no call (hence with a discrepancy between GQ and PLs), those sites will end up with a low imputation score. Since my reads are ONT corrected, I rely very much on the imputation scores as a post-filtering step to guarantee the quality of my set. Guillaume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403#issuecomment-760221333
Availability,down,downstream,"Hi Guillaume,. Thank you for filing the issue and for the detailed explanation of your use case. I'd like to ask you some more questions to better understand the situation and to find a potential solution. 1. If we call the cohort you're processing with DeepVariant+GLnexus ""cohort **X**"", are you (a) using cohort **X** as a reference panel to impute another cohort **Y**, (b) using another cohort **Y** as a reference panel to impute your cohort **X**, or (c) using imputation software (e.g. Beagle) to re-genotype cohort **X** using the PL values in cohort **X** itself?; 2. In the example case you mentioned (""some samples have an actual variant but most of the other samples have a no call""), the no calls in the other samples would imply we don't have enough evidence (in terms of coverage, read quality, mapping quality, etc.) to call the other samples either reference or variant. Would keeping that cohort-level variant desirable for your downstream application? If it is, is there any other type of filters you can use (other than the imputation score) to keep those records (e.g. maximum of GQs in all samples)?; 3. Changing the no-call genotypes `./.` to the reference calls `0/0` is a relatively simple transformation (e.g. `bcftools +missing2ref`) that we use for some specific downstream applications. Would that help in your situation?. Thank you,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403#issuecomment-760500649
Usability,simpl,simple,"Hi Guillaume,. Thank you for filing the issue and for the detailed explanation of your use case. I'd like to ask you some more questions to better understand the situation and to find a potential solution. 1. If we call the cohort you're processing with DeepVariant+GLnexus ""cohort **X**"", are you (a) using cohort **X** as a reference panel to impute another cohort **Y**, (b) using another cohort **Y** as a reference panel to impute your cohort **X**, or (c) using imputation software (e.g. Beagle) to re-genotype cohort **X** using the PL values in cohort **X** itself?; 2. In the example case you mentioned (""some samples have an actual variant but most of the other samples have a no call""), the no calls in the other samples would imply we don't have enough evidence (in terms of coverage, read quality, mapping quality, etc.) to call the other samples either reference or variant. Would keeping that cohort-level variant desirable for your downstream application? If it is, is there any other type of filters you can use (other than the imputation score) to keep those records (e.g. maximum of GQs in all samples)?; 3. Changing the no-call genotypes `./.` to the reference calls `0/0` is a relatively simple transformation (e.g. `bcftools +missing2ref`) that we use for some specific downstream applications. Would that help in your situation?. Thank you,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403#issuecomment-760500649
Availability,down,downstream,"Hi @tedyun,. 1. In this use case, we have phased and accurate data from the same cohort **X** that we use for the imputation.; 2. I was actually thinking about simply deleting the GQ=0 sites from my GVCFs which seem to be the simpler solution. As you said, they don't provide any useful information. I just wanted to point out here that having those records in output might confuse downstream applications (i.e. imputation).; 3. Unfortunately not. The problem is that our imputation system is exclusively based on the PL values and doesn't even read GT or GQ. Thank you for your questions and suggestions. Guillaume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403#issuecomment-760775624
Usability,simpl,simply,"Hi @tedyun,. 1. In this use case, we have phased and accurate data from the same cohort **X** that we use for the imputation.; 2. I was actually thinking about simply deleting the GQ=0 sites from my GVCFs which seem to be the simpler solution. As you said, they don't provide any useful information. I just wanted to point out here that having those records in output might confuse downstream applications (i.e. imputation).; 3. Unfortunately not. The problem is that our imputation system is exclusively based on the PL values and doesn't even read GT or GQ. Thank you for your questions and suggestions. Guillaume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403#issuecomment-760775624
Usability,feedback,feedback,"Sorry for the delay, I appreciate your feedback in the matter. Closing the issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403#issuecomment-768232068
Availability,reliab,reliable,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412#issuecomment-767885510
Modifiability,layers,layers,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412#issuecomment-767885510
Performance,perform,performs,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412#issuecomment-767885510
Usability,clear,clear,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412#issuecomment-767885510
Deployability,pipeline,pipeline,"Thanks @lvclark for the context of why you're doing this. That's very helpful to know. In that case, you'll want to run the whole pipeline separately as well (meaning, make_examples -> call_variants -> postprocess_variants separately), one using `gvcf1_parent1.tfrecord@32.gz` , the other one with `gvcf3_parent1.tfrecord@32.gz`. And not to combine them mid-way. Our team is working on makeing chrX/Y calling a bit better for DeepVariant and DeepTrio. I'll make sure to pass this feedback as well so we'll think about the use case here a bit more later on.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413#issuecomment-1480069130
Usability,feedback,feedback,"Thanks @lvclark for the context of why you're doing this. That's very helpful to know. In that case, you'll want to run the whole pipeline separately as well (meaning, make_examples -> call_variants -> postprocess_variants separately), one using `gvcf1_parent1.tfrecord@32.gz` , the other one with `gvcf3_parent1.tfrecord@32.gz`. And not to combine them mid-way. Our team is working on makeing chrX/Y calling a bit better for DeepVariant and DeepTrio. I'll make sure to pass this feedback as well so we'll think about the use case here a bit more later on.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413#issuecomment-1480069130
Deployability,update,update,"I've filed an internal issue to track. The update should come out in the next release. I'll close this comment for now, but will post an update once it's out.; Thanks for the feedback!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414#issuecomment-771075195
Usability,feedback,feedback,"I've filed an internal issue to track. The update should come out in the next release. I'll close this comment for now, but will post an update once it's out.; Thanks for the feedback!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414#issuecomment-771075195
Performance,bottleneck,bottlenecking,"These mostly run on nodes with Intel Xeon Gold 6140, occasionally on Intel Xeon E5-2697v4, but these are much slower anyway. . I had noticed the same speed improvement in v1.1.0 with openvino as your metrics, but I haven't tested the new version extensively with and without. However, when rerunning identical samples (both with openvino flags), I've noticed that v1.2.0 takes longer wall clock time compared to v1.1.0, but less CPU time. Maybe it is just node variation or other jobs bottlenecking IO, so hopefully will see a clearer result after more samples run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416#issuecomment-889821634
Testability,test,tested,"These mostly run on nodes with Intel Xeon Gold 6140, occasionally on Intel Xeon E5-2697v4, but these are much slower anyway. . I had noticed the same speed improvement in v1.1.0 with openvino as your metrics, but I haven't tested the new version extensively with and without. However, when rerunning identical samples (both with openvino flags), I've noticed that v1.2.0 takes longer wall clock time compared to v1.1.0, but less CPU time. Maybe it is just node variation or other jobs bottlenecking IO, so hopefully will see a clearer result after more samples run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416#issuecomment-889821634
Usability,clear,clearer,"These mostly run on nodes with Intel Xeon Gold 6140, occasionally on Intel Xeon E5-2697v4, but these are much slower anyway. . I had noticed the same speed improvement in v1.1.0 with openvino as your metrics, but I haven't tested the new version extensively with and without. However, when rerunning identical samples (both with openvino flags), I've noticed that v1.2.0 takes longer wall clock time compared to v1.1.0, but less CPU time. Maybe it is just node variation or other jobs bottlenecking IO, so hopefully will see a clearer result after more samples run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416#issuecomment-889821634
Availability,down,downloaded,"Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue.; ; # Get a CentOS 7 machine; ```; gcloud compute instances create ""${USER}-centos"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"" \; --boot-disk-size ""200"" ; ```. # Install Singularity 3.5; Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:; ```; [pichuan@pichuan-centos singularity]$ singularity --version; singularity version 3.5.2; ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:; ```; singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:1.1.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --regions ""chr20:10,000,000-10,010,000"" \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --num_shards 24 -v 2; ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419#issuecomment-774634611
Deployability,install,installation,"Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue.; ; # Get a CentOS 7 machine; ```; gcloud compute instances create ""${USER}-centos"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"" \; --boot-disk-size ""200"" ; ```. # Install Singularity 3.5; Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:; ```; [pichuan@pichuan-centos singularity]$ singularity --version; singularity version 3.5.2; ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:; ```; singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:1.1.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --regions ""chr20:10,000,000-10,010,000"" \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --num_shards 24 -v 2; ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419#issuecomment-774634611
Usability,guid,guides,"Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue.; ; # Get a CentOS 7 machine; ```; gcloud compute instances create ""${USER}-centos"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"" \; --boot-disk-size ""200"" ; ```. # Install Singularity 3.5; Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:; ```; [pichuan@pichuan-centos singularity]$ singularity --version; singularity version 3.5.2; ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:; ```; singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:1.1.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --regions ""chr20:10,000,000-10,010,000"" \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --num_shards 24 -v 2; ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419#issuecomment-774634611
Availability,error,error,"I think the error is simpler than that. If you grep for these specific flags (i.e. `noparse_sam_aux_fields`, `norealign_reads`, and `nosort_by_haplotypes`) they do not exist in `make_examples` -- though it uses them in the command-line -- and thus it does not know how to process them, which is probably why you are seeing the 252 exit status:. ```Bash; $ grep -E 'noparse_sam_aux_fields|norealign_reads|nosort_by_haplotypes' deepvariant/make_examples.py; $; ```; Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419#issuecomment-774693583
Usability,simpl,simpler,"I think the error is simpler than that. If you grep for these specific flags (i.e. `noparse_sam_aux_fields`, `norealign_reads`, and `nosort_by_haplotypes`) they do not exist in `make_examples` -- though it uses them in the command-line -- and thus it does not know how to process them, which is probably why you are seeing the 252 exit status:. ```Bash; $ grep -E 'noparse_sam_aux_fields|norealign_reads|nosort_by_haplotypes' deepvariant/make_examples.py; $; ```; Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419#issuecomment-774693583
Availability,error,error,"Hi Andrew, thank you very much for the feedback. This is something new I have learnt about the BAM files. Using the filtered BAM file, the error message disappears. The number of variants called has also increased considerably (~x20 for variants with PASS tag). Our reads are in fact HiFi. We have been doing the alignment with `minimap2 -ax map-pb` because to our understanding `deepvariant` is designed for read alignments (and not assembly-to-reference alignments as achieved with `minimap2 -ax asm`). Is this a misunderstanding? Could `deepvariant` be safely used with BAMs for assembly-to-reference alignments?. Thank you again,; Eugenio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434#issuecomment-815783472
Integrability,message,message,"Hi Andrew, thank you very much for the feedback. This is something new I have learnt about the BAM files. Using the filtered BAM file, the error message disappears. The number of variants called has also increased considerably (~x20 for variants with PASS tag). Our reads are in fact HiFi. We have been doing the alignment with `minimap2 -ax map-pb` because to our understanding `deepvariant` is designed for read alignments (and not assembly-to-reference alignments as achieved with `minimap2 -ax asm`). Is this a misunderstanding? Could `deepvariant` be safely used with BAMs for assembly-to-reference alignments?. Thank you again,; Eugenio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434#issuecomment-815783472
Safety,safe,safely,"Hi Andrew, thank you very much for the feedback. This is something new I have learnt about the BAM files. Using the filtered BAM file, the error message disappears. The number of variants called has also increased considerably (~x20 for variants with PASS tag). Our reads are in fact HiFi. We have been doing the alignment with `minimap2 -ax map-pb` because to our understanding `deepvariant` is designed for read alignments (and not assembly-to-reference alignments as achieved with `minimap2 -ax asm`). Is this a misunderstanding? Could `deepvariant` be safely used with BAMs for assembly-to-reference alignments?. Thank you again,; Eugenio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434#issuecomment-815783472
Usability,feedback,feedback,"Hi Andrew, thank you very much for the feedback. This is something new I have learnt about the BAM files. Using the filtered BAM file, the error message disappears. The number of variants called has also increased considerably (~x20 for variants with PASS tag). Our reads are in fact HiFi. We have been doing the alignment with `minimap2 -ax map-pb` because to our understanding `deepvariant` is designed for read alignments (and not assembly-to-reference alignments as achieved with `minimap2 -ax asm`). Is this a misunderstanding? Could `deepvariant` be safely used with BAMs for assembly-to-reference alignments?. Thank you again,; Eugenio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434#issuecomment-815783472
Usability,clear,clear,"I see! I am very grateful for the support. Not only the problem is solved, it is everything much more clear now, and I have learned a lot from your feedback. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434#issuecomment-816108659
Usability,learn,learned,"Hi @shadrinams . From your description, it looks like you are running the DeepTrio merge component in the way that I would recommend. . Can I ask a few questions:. 1. Are you taking the values from the QUAL field of the multisample glnexus VCF; 2. How have you determined the TP sites? Are these Genome in a Bottle, or do they come from some other source. Your QUAL values for all calls match expectations for DeepTrio. However, I would not expect that ""true"" variants would have a distribution which departs from all calls. I can think of one reason that this might be the case:. Are these true variants de novos? DeepTrio's quality distribution for de novo variants is very different from its general quality distribution. This occurs because DeepTrio has learned that de novo events are quite rare, and so requires a higher standard of evidence to make a call which is a de novo. In these cases, DeepTrio is not extremely confident in the call, which results in a lower quality value. . The other things it might be good to look at is the genotype quality (GQ) field. This is the most direct measure of DeepTrio's confidence in a call, and maps directly from the probability for the called class. The QUAL value of a multisample VCF comes from GLnexus, and is a bit less direct measure of call confidence (still, I don't expect the distributions to be off in the manner that you see). I don't think I can immediately answer this puzzle, but hopefully with a little more information we can figure it out. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-820157988
Deployability,pipeline,pipeline,"drew,. Thank you for your quick reply!. > 1. Are you taking the values from the QUAL field of the multisample glnexus VCF. Yes, exactly!. > 2. How have you determined the TP sites? Are these Genome in a Bottle, or do they come from some other source. These are PCGC data, TPs were determined by combination of methods and manually curated. We expect an accuracy of the; found TPs to be > 95% (based on PCR for a similar dataset), although we might still miss some TP calls. > Are these true variants de novos? DeepTrio's quality distribution for de novo variants is very different from its general quality distribution. This occurs because DeepTrio has learned that de novo events are quite rare, and so requires a higher standard of evidence to make a call which is a de novo. In these cases, DeepTrio is not extremely confident in the call, which results in a lower quality value. I am sorry, I did not mention it. Yes, we are looking for denovos in trios. We are comparing efficiency of a few methods to create a pipeline for a big dataset. I thought we might use the QUAL score from DeepTrio to filter calls found by GATK4 pipeline.; If we use GQ fields for further filtering what values do you recommend for parents and proband?. Now, I use the following filters to retrieve denovo calls from the multisample glnexus VCF:; - Heterozygous ratio of proband = 0.2-0.8; - Homozygous ratio of parents <= 0.1; - ALT allele depth of proband >= 7; - Genotype quality of proband >= 60; - Read depth >= 7; - Allele count = 1; - Some regional filters were applied to remove noisy regions; - Common variants were removed based on 1000genome and gnomad population frequencies; Also, I had to split multiallelic calls and recalculate genotypes based on AD fields as I had a lot of ./. and 0/1 for Homozygous reference calls in parents. As results, I obtained 909 SNPs and 1,236 indels for my 10 test trios. My list of TPs contains 698 SNPs and 61 indels. So, I still have a lot of false-positives calls. Is the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-820564569
Testability,test,test,"d of the multisample glnexus VCF. Yes, exactly!. > 2. How have you determined the TP sites? Are these Genome in a Bottle, or do they come from some other source. These are PCGC data, TPs were determined by combination of methods and manually curated. We expect an accuracy of the; found TPs to be > 95% (based on PCR for a similar dataset), although we might still miss some TP calls. > Are these true variants de novos? DeepTrio's quality distribution for de novo variants is very different from its general quality distribution. This occurs because DeepTrio has learned that de novo events are quite rare, and so requires a higher standard of evidence to make a call which is a de novo. In these cases, DeepTrio is not extremely confident in the call, which results in a lower quality value. I am sorry, I did not mention it. Yes, we are looking for denovos in trios. We are comparing efficiency of a few methods to create a pipeline for a big dataset. I thought we might use the QUAL score from DeepTrio to filter calls found by GATK4 pipeline.; If we use GQ fields for further filtering what values do you recommend for parents and proband?. Now, I use the following filters to retrieve denovo calls from the multisample glnexus VCF:; - Heterozygous ratio of proband = 0.2-0.8; - Homozygous ratio of parents <= 0.1; - ALT allele depth of proband >= 7; - Genotype quality of proband >= 60; - Read depth >= 7; - Allele count = 1; - Some regional filters were applied to remove noisy regions; - Common variants were removed based on 1000genome and gnomad population frequencies; Also, I had to split multiallelic calls and recalculate genotypes based on AD fields as I had a lot of ./. and 0/1 for Homozygous reference calls in parents. As results, I obtained 909 SNPs and 1,236 indels for my 10 test trios. My list of TPs contains 698 SNPs and 61 indels. So, I still have a lot of false-positives calls. Is there a way to filter my variants from DeepTrio further?. Thank you!. Best regards,; Maria.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-820564569
Usability,learn,learned,"Hello Andrew,. Thank you for your quick reply!. > 1. Are you taking the values from the QUAL field of the multisample glnexus VCF. Yes, exactly!. > 2. How have you determined the TP sites? Are these Genome in a Bottle, or do they come from some other source. These are PCGC data, TPs were determined by combination of methods and manually curated. We expect an accuracy of the; found TPs to be > 95% (based on PCR for a similar dataset), although we might still miss some TP calls. > Are these true variants de novos? DeepTrio's quality distribution for de novo variants is very different from its general quality distribution. This occurs because DeepTrio has learned that de novo events are quite rare, and so requires a higher standard of evidence to make a call which is a de novo. In these cases, DeepTrio is not extremely confident in the call, which results in a lower quality value. I am sorry, I did not mention it. Yes, we are looking for denovos in trios. We are comparing efficiency of a few methods to create a pipeline for a big dataset. I thought we might use the QUAL score from DeepTrio to filter calls found by GATK4 pipeline.; If we use GQ fields for further filtering what values do you recommend for parents and proband?. Now, I use the following filters to retrieve denovo calls from the multisample glnexus VCF:; - Heterozygous ratio of proband = 0.2-0.8; - Homozygous ratio of parents <= 0.1; - ALT allele depth of proband >= 7; - Genotype quality of proband >= 60; - Read depth >= 7; - Allele count = 1; - Some regional filters were applied to remove noisy regions; - Common variants were removed based on 1000genome and gnomad population frequencies; Also, I had to split multiallelic calls and recalculate genotypes based on AD fields as I had a lot of ./. and 0/1 for Homozygous reference calls in parents. As results, I obtained 909 SNPs and 1,236 indels for my 10 test trios. My list of TPs contains 698 SNPs and 61 indels. So, I still have a lot of false-positives calls",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-820564569
Performance,optimiz,optimizing,"Hi @shadrinams . Thank you for the plot. This is expected the the found de novo calls are lower in confidence (because DeepTrio has learned that de novo events are rare). Given that a call is a de novo (0/1-0/0-0/0), the higher GQ values will still indicate higher confidence, so more confident de novo calls should be more likely to be true. For calls which are 1/1-0/0-1/1 (or permutations of this), the parent and child models of DeepTrio do not coordinate, so they aren't optimizing for consistency. There is a property of some regions which look like potential segmental duplications where a call that appears to a human as a 1/1/ or 0/1 is actually some kind of CNV. DeepTrio has learned some parts which are predictive of this (generally a variant-dense haplotype and a reference haplotype with higher depth). There may be cases where the child model gives a call a 1/1 and the parent gives a 0/0 when the site itself is similar, but the context is different. If you are looking for homozygous Mendelian violations, you may want to filter regions of high variant density, as apparent calls will come from this phenomenon. . In addition, if you are looking at the sex chromosomes, it would be good to separately call the non-PAR chrX for male samples. For a male sample, running chrX providing only the mother sample provides best results. (chrY should already only have paternal reads outside of the PAR). Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-821020591
Safety,predict,predictive,"Hi @shadrinams . Thank you for the plot. This is expected the the found de novo calls are lower in confidence (because DeepTrio has learned that de novo events are rare). Given that a call is a de novo (0/1-0/0-0/0), the higher GQ values will still indicate higher confidence, so more confident de novo calls should be more likely to be true. For calls which are 1/1-0/0-1/1 (or permutations of this), the parent and child models of DeepTrio do not coordinate, so they aren't optimizing for consistency. There is a property of some regions which look like potential segmental duplications where a call that appears to a human as a 1/1/ or 0/1 is actually some kind of CNV. DeepTrio has learned some parts which are predictive of this (generally a variant-dense haplotype and a reference haplotype with higher depth). There may be cases where the child model gives a call a 1/1 and the parent gives a 0/0 when the site itself is similar, but the context is different. If you are looking for homozygous Mendelian violations, you may want to filter regions of high variant density, as apparent calls will come from this phenomenon. . In addition, if you are looking at the sex chromosomes, it would be good to separately call the non-PAR chrX for male samples. For a male sample, running chrX providing only the mother sample provides best results. (chrY should already only have paternal reads outside of the PAR). Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-821020591
Usability,learn,learned,"Hi @shadrinams . Thank you for the plot. This is expected the the found de novo calls are lower in confidence (because DeepTrio has learned that de novo events are rare). Given that a call is a de novo (0/1-0/0-0/0), the higher GQ values will still indicate higher confidence, so more confident de novo calls should be more likely to be true. For calls which are 1/1-0/0-1/1 (or permutations of this), the parent and child models of DeepTrio do not coordinate, so they aren't optimizing for consistency. There is a property of some regions which look like potential segmental duplications where a call that appears to a human as a 1/1/ or 0/1 is actually some kind of CNV. DeepTrio has learned some parts which are predictive of this (generally a variant-dense haplotype and a reference haplotype with higher depth). There may be cases where the child model gives a call a 1/1 and the parent gives a 0/0 when the site itself is similar, but the context is different. If you are looking for homozygous Mendelian violations, you may want to filter regions of high variant density, as apparent calls will come from this phenomenon. . In addition, if you are looking at the sex chromosomes, it would be good to separately call the non-PAR chrX for male samples. For a male sample, running chrX providing only the mother sample provides best results. (chrY should already only have paternal reads outside of the PAR). Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-821020591
Modifiability,inherit,inherited,"squished and expanded views) with corresponding output in DeepTrio VCF to show you discrepancies between DeepTrio VCFs and BAMs. I think it could be a part of the GQ issue in DeNovos.; The order of FORMAT fields in multisample VCF is proband, mother, father.; The order of samples in IGV is father, mother, proband (from top to bottom). ### **1) True Denovo, QUAL=3, proband GQ=5. It looks good except very low proband GQ.**. chr5 | 92696737 | chr5_92696737_C_T | C | T | 3 | . | AF=0.166667;AQ=3 | GT:DP:AD:GQ:PL:RNC | 0/1:32:17,15:**5**:3,0,32:.. | 0/0:27:27,0:50:0,108,1079:.. | 0/0:21:21,0:50:0,105,1049:.. ![DT_1_04190_chr5_92696737](https://user-images.githubusercontent.com/22089494/115329914-0902a500-a161-11eb-9ab6-a3dc47a92aaf.png); ![DT_1_04190_chr5_92696737_zoom](https://user-images.githubusercontent.com/22089494/115330134-7d3d4880-a161-11eb-9202-10a392b98c07.png). ### **2) Filtered Denovo-like, QUAL=46, proband GQ=13. Mulitallelic, inherited; when VCF is normalized, it passes DeNovo filter as father's genotype misses ALT allele information ** ###. chr5 | 24093912 | chr5_24093912_AAT_A;chr5_24093912_A_AATAT | AAT | A,AATATAT | 46 | . | AF=0.333333,0.166667;AQ=46,15 | GT:DP:AD:GQ:PL:RNC | 1/2:30:5,12,13:13:44,15,55,15,0,53:.. | 0/1:31:16,15,0:46:46,0,70,990,990,990:.. | ./.:30:**27,1,0**:18:0,18,45,990,990,990:II . chr5 | 24093912 | chr5_24093912_AAT_A;chr5_24093912_A_AATAT | AAT | AATATAT | 46 | . | AF=0.166667;AQ=15 | GT:DP:AD:GQ:PL:RNC | 0/1:30:5,13:13:44,15,53:.. | 0/0:31:16,0:46:46,990,990:.. | ./.:30:**27,0**:18:0,990,990:II. ![DT_1_04190_chr5_24093912](https://user-images.githubusercontent.com/22089494/115330437-09e80680-a162-11eb-96cd-2f2d27d45896.png); ![DT_1_04190_chr5_24093912_zoom](https://user-images.githubusercontent.com/22089494/115330450-11a7ab00-a162-11eb-8f5c-927bd1445056.png). ### **3) Filtered Denovo-like, QUAL=27, proband GQ=28. Inherited; it passes DeNovo filter as father's genotype misses ALT allele information ** ###. chr7 | 54624683 | chr7_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-822947862
Usability,learn,learned,"Hello Andrew,. >Thank you for the plot. This is expected the the found de novo calls are lower in confidence (because DeepTrio has learned that de novo events are rare). Given that a call is a de novo (0/1-0/0-0/0), the higher GQ values will still indicate higher confidence, so more confident de novo calls should be more likely to be true. I have tried to verify the idea and plotted normalized histograms for all DeepTrio calls, filtered DeNovo-like (0/1-0/0-0/0) calls and true DeNovo calls (verified in IGV). Please see an attached picture. True DeNovo calls are still in the middle-low part of proband GQ range for DeNovo-like calls. I have tried to look at several DeNovo-like variants with a higher proband GQ, which were not in my list of true DeNovos, but all of them were false-positives. ![DeepTrio_GQproband2](https://user-images.githubusercontent.com/22089494/115329837-ea041300-a160-11eb-8788-31136171e157.png). I have prepared a few examples of DeepTrio variants in IGV (squished and expanded views) with corresponding output in DeepTrio VCF to show you discrepancies between DeepTrio VCFs and BAMs. I think it could be a part of the GQ issue in DeNovos.; The order of FORMAT fields in multisample VCF is proband, mother, father.; The order of samples in IGV is father, mother, proband (from top to bottom). ### **1) True Denovo, QUAL=3, proband GQ=5. It looks good except very low proband GQ.**. chr5 | 92696737 | chr5_92696737_C_T | C | T | 3 | . | AF=0.166667;AQ=3 | GT:DP:AD:GQ:PL:RNC | 0/1:32:17,15:**5**:3,0,32:.. | 0/0:27:27,0:50:0,108,1079:.. | 0/0:21:21,0:50:0,105,1049:.. ![DT_1_04190_chr5_92696737](https://user-images.githubusercontent.com/22089494/115329914-0902a500-a161-11eb-9ab6-a3dc47a92aaf.png); ![DT_1_04190_chr5_92696737_zoom](https://user-images.githubusercontent.com/22089494/115330134-7d3d4880-a161-11eb-9202-10a392b98c07.png). ### **2) Filtered Denovo-like, QUAL=46, proband GQ=13. Mulitallelic, inherited; when VCF is normalized, it passes DeNovo filter as fat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-822947862
Availability,down,downstream,"hether some harmonization or filtering after calls could make the resulting calls more uniform. With respect to your examples. It is often difficult to be definitive about why DeepVariant does not make certain calls. I cannot give you a reason, but for some cases I can share observations. In all cases, I will list the call followed by observation. ```; chr5 | 92696737 | chr5_92696737_C_T | C | T | 3 | . | AF=0.166667;AQ=3 | GT:DP:AD:GQ:PL:RNC | 0/1:32:17,15:5:3,0,32:.. | 0/0:27:27,0:50:0,108,1079:.. | 0/0:21:21,0:50:0,105,1049:..; ```. This looks clean. DeepTrio's GQ is low probably because it is a clear de novo and it has learned such events are rare. ```; chr5 | 24093912 | chr5_24093912_AAT_A;chr5_24093912_A_AATAT | AAT | A,AATATAT | 46 | . | AF=0.333333,0.166667;AQ=46,15 | GT:DP:AD:GQ:PL:RNC | 1/2:30:5,12,13:13:44,15,55,15,0,53:.. | 0/1:31:16,15,0:46:46,0,70,990,990,990:.. | ./.:30:27,1,0:18:0,18,45,990,990,990:II; ```; One thing I note - it looks to me like there are 3 alleles represented in the reads for the top parent: 1) there is an insertion event in-phase with a downstream HET SNP. 2) There is a reference allele in-phase with REF at that later position. 3) There is evidence for a T SNP that is also in-phase with the downstream HET variant. For the reads that are HET T, it could be interesting to see if they overlap any other variants that would suggest that they come from a copy number variant elsewhere in the genome. It may be the case that DeepTrio does not call a variant in the parent because some of the variant reads may be coming from elsewhere. ```; chr7 | 54624683 | chr7_54624683_A_AATC | A | AATC | 27 | . | AF=0.166667;AQ=27 | GT:DP:AD:GQ:PL:RNC | 0/1:39:22,16:28:27,0,48:.. | 0/0:40:40,0:50:0,120,1199:.. | 0/0:28:28,0:50:0,90,899:..; ```. This is interesting, since the evidence reported by DeepTrio doesn't match the view in the BAM. DeepTrio is saying that both parents don't have evidence for variant reads at this position. It is possible the after r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-823860823
Usability,clear,clear,"Hi @shadrinams . Your observations about some apparent de novos originating from undercalling of a parent is interesting. I think that we'll look into whether some harmonization or filtering after calls could make the resulting calls more uniform. With respect to your examples. It is often difficult to be definitive about why DeepVariant does not make certain calls. I cannot give you a reason, but for some cases I can share observations. In all cases, I will list the call followed by observation. ```; chr5 | 92696737 | chr5_92696737_C_T | C | T | 3 | . | AF=0.166667;AQ=3 | GT:DP:AD:GQ:PL:RNC | 0/1:32:17,15:5:3,0,32:.. | 0/0:27:27,0:50:0,108,1079:.. | 0/0:21:21,0:50:0,105,1049:..; ```. This looks clean. DeepTrio's GQ is low probably because it is a clear de novo and it has learned such events are rare. ```; chr5 | 24093912 | chr5_24093912_AAT_A;chr5_24093912_A_AATAT | AAT | A,AATATAT | 46 | . | AF=0.333333,0.166667;AQ=46,15 | GT:DP:AD:GQ:PL:RNC | 1/2:30:5,12,13:13:44,15,55,15,0,53:.. | 0/1:31:16,15,0:46:46,0,70,990,990,990:.. | ./.:30:27,1,0:18:0,18,45,990,990,990:II; ```; One thing I note - it looks to me like there are 3 alleles represented in the reads for the top parent: 1) there is an insertion event in-phase with a downstream HET SNP. 2) There is a reference allele in-phase with REF at that later position. 3) There is evidence for a T SNP that is also in-phase with the downstream HET variant. For the reads that are HET T, it could be interesting to see if they overlap any other variants that would suggest that they come from a copy number variant elsewhere in the genome. It may be the case that DeepTrio does not call a variant in the parent because some of the variant reads may be coming from elsewhere. ```; chr7 | 54624683 | chr7_54624683_A_AATC | A | AATC | 27 | . | AF=0.166667;AQ=27 | GT:DP:AD:GQ:PL:RNC | 0/1:39:22,16:28:27,0,48:.. | 0/0:40:40,0:50:0,120,1199:.. | 0/0:28:28,0:50:0,90,899:..; ```. This is interesting, since the evidence reported by DeepTrio do",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-823860823
Deployability,update,updated,"Hi @rabiafidan . We have found the following:. 1. The use of GLnexus preset DeepVariant_unfiltered is preferred for retaining True de novo calls, and we have updated the documentation for this.; 2. We also observe a reduction in 0/1 child, 0/0 parent calls when post-processing the final VCF to set a parent to ./. when that parent is 0/0, the child is 0/1 or 1/1, and that parent has either less than 8 reads covering the variant position, or and allele fraction of > 0.15. . The second filter seems to help reduces cases where there is not enough confidence to clearly call a de novo. Does this filtering strategy seem like it might further help refine your calls? We are considering whether to recommend postprocessing of this nature via a script in the future. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-899967692
Energy Efficiency,reduce,reduces,"Hi @rabiafidan . We have found the following:. 1. The use of GLnexus preset DeepVariant_unfiltered is preferred for retaining True de novo calls, and we have updated the documentation for this.; 2. We also observe a reduction in 0/1 child, 0/0 parent calls when post-processing the final VCF to set a parent to ./. when that parent is 0/0, the child is 0/1 or 1/1, and that parent has either less than 8 reads covering the variant position, or and allele fraction of > 0.15. . The second filter seems to help reduces cases where there is not enough confidence to clearly call a de novo. Does this filtering strategy seem like it might further help refine your calls? We are considering whether to recommend postprocessing of this nature via a script in the future. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-899967692
Usability,clear,clearly,"Hi @rabiafidan . We have found the following:. 1. The use of GLnexus preset DeepVariant_unfiltered is preferred for retaining True de novo calls, and we have updated the documentation for this.; 2. We also observe a reduction in 0/1 child, 0/0 parent calls when post-processing the final VCF to set a parent to ./. when that parent is 0/0, the child is 0/1 or 1/1, and that parent has either less than 8 reads covering the variant position, or and allele fraction of > 0.15. . The second filter seems to help reduces cases where there is not enough confidence to clearly call a de novo. Does this filtering strategy seem like it might further help refine your calls? We are considering whether to recommend postprocessing of this nature via a script in the future. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-899967692
Availability,down,downloaded,"@pichuan Hi! When I downloaded all the pre-built binaries and run the run-prereq.sh scripts. How I suppose to run the make_example parallel?; I use the following command: ; seq 0 47 | parallel -q --halt 2 --line-buffer python3 make_examples.zip --mode calling --ref /data/input/human_g1k_v37.fasta --reads /data/input/c6c4c1db-4328-4aa9-b038-074c9a453117.dedup.bam --examples make_examples.tfrecord@12.gz. showing error:. E0430 18:57:45.160124 140015706085184 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9oblbsyi/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '33']"".; E0430 18:57:45.128387 140717112878912 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gd9fj22_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '2']"".; E0430 18:57:45.149224 139802704738112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_2hbfxd92/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '16']"".; E0430 18:57:45.196900 140351706515264 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__ytoy0a6/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '4']"".; E0430 18:57:45.304501 140371551012672 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_w7jwxlqo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '28']"".; E0430 18:57:45.337900 139621533210432 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Baz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
Usability,guid,guidance,"does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_dqd3ut4s/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '32']"".; E0430 18:57:45.247818 140240365713216 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8s9w7qaa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '25']"".; E0430 18:57:45.247906 139736525375296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8kqng5_c/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '1']"".; E0430 18:57:45.354531 139703252227904 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_47rk8xc1/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '22']"".; E0430 18:57:45.318170 140515109386048 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4p5rc3ja/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '35']"".; E0430 18:57:45.306068 140062873229120 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xavizfpc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '42']"".; E0430 18:57:45.268234 140590012761920 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_ovmu_l59/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '8']"".; parallel: This job failed:. could you provide any guidance on how to run the make_example.zip working on parallel (like create a 48 jobs)? Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
Deployability,install,installing,"Hi Brent I will check in on this and get back to you. Just to be clear, are using `FROM google/deepvariant` and then installing bioconda and using that to install bcftools / samtools or are you using bioconda to install deepvariant, samtools, and bcftools?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/445#issuecomment-822571186
Usability,clear,clear,"Hi Brent I will check in on this and get back to you. Just to be clear, are using `FROM google/deepvariant` and then installing bioconda and using that to install bcftools / samtools or are you using bioconda to install deepvariant, samtools, and bcftools?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/445#issuecomment-822571186
Modifiability,sandbox,sandbox,"Hi @danielecook , I was trying various things that would require least amount of effort. I ended up just skipping and using the `google/deepvariant` docker image as-is.; I'm no expert in docker, just trying to get things running. ; Then I also have the issue that singularity can't use/convert the deepvariant docker image:; ```; $ singularity build --sandbox deepvariant_1_1_0 docker://gcr.io/deepvariant-docker/deepvariant:1.1.0; WARNING: Building sandbox as non-root may result in wrong file permissions; Docker image path: gcr.io/deepvariant-docker/deepvariant:1.1.0; ERROR MANIFEST_UNKNOWN: Manifest with tag '1.1.0' has media type 'application/vnd.docker.distribution.manifest.v2+json', but client accepts 'application/json'.; Cleaning up...; ```; This may be my inexperience in these things, but I'm simply having trouble getting them running.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/445#issuecomment-822613451
Testability,sandbox,sandbox,"Hi @danielecook , I was trying various things that would require least amount of effort. I ended up just skipping and using the `google/deepvariant` docker image as-is.; I'm no expert in docker, just trying to get things running. ; Then I also have the issue that singularity can't use/convert the deepvariant docker image:; ```; $ singularity build --sandbox deepvariant_1_1_0 docker://gcr.io/deepvariant-docker/deepvariant:1.1.0; WARNING: Building sandbox as non-root may result in wrong file permissions; Docker image path: gcr.io/deepvariant-docker/deepvariant:1.1.0; ERROR MANIFEST_UNKNOWN: Manifest with tag '1.1.0' has media type 'application/vnd.docker.distribution.manifest.v2+json', but client accepts 'application/json'.; Cleaning up...; ```; This may be my inexperience in these things, but I'm simply having trouble getting them running.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/445#issuecomment-822613451
Usability,simpl,simply,"Hi @danielecook , I was trying various things that would require least amount of effort. I ended up just skipping and using the `google/deepvariant` docker image as-is.; I'm no expert in docker, just trying to get things running. ; Then I also have the issue that singularity can't use/convert the deepvariant docker image:; ```; $ singularity build --sandbox deepvariant_1_1_0 docker://gcr.io/deepvariant-docker/deepvariant:1.1.0; WARNING: Building sandbox as non-root may result in wrong file permissions; Docker image path: gcr.io/deepvariant-docker/deepvariant:1.1.0; ERROR MANIFEST_UNKNOWN: Manifest with tag '1.1.0' has media type 'application/vnd.docker.distribution.manifest.v2+json', but client accepts 'application/json'.; Cleaning up...; ```; This may be my inexperience in these things, but I'm simply having trouble getting them running.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/445#issuecomment-822613451
Usability,simpl,simply,"Yes, it works simply updating singularity. Thanks for figuring this out!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/445#issuecomment-822637152
Availability,error,error,"Hi @ruolin ,; thanks for reporting this issue. I'll try running on your BAM and reference and see if we can reproduce the issue.; We have in the past seen cases where the jobs run out of memory, and our error messages in that situation isn't very clear. So @danielecook 's guess of OOM makes sense. But the memory you're reporting sounds like it should be enough. So let me see if I can reproduce this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/446#issuecomment-826519447
Integrability,message,messages,"Hi @ruolin ,; thanks for reporting this issue. I'll try running on your BAM and reference and see if we can reproduce the issue.; We have in the past seen cases where the jobs run out of memory, and our error messages in that situation isn't very clear. So @danielecook 's guess of OOM makes sense. But the memory you're reporting sounds like it should be enough. So let me see if I can reproduce this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/446#issuecomment-826519447
Usability,clear,clear,"Hi @ruolin ,; thanks for reporting this issue. I'll try running on your BAM and reference and see if we can reproduce the issue.; We have in the past seen cases where the jobs run out of memory, and our error messages in that situation isn't very clear. So @danielecook 's guess of OOM makes sense. But the memory you're reporting sounds like it should be enough. So let me see if I can reproduce this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/446#issuecomment-826519447
Usability,feedback,feedback,"Hi Andrew, sorry for the delay,. I was aiming to get per nucleotide values of read depth, base and mapping qualities. I just come to check that this information can in fact be obtained from `samtools depth` and `samtools mpileup`, so I guess that will work for us. Thank you for the feedback!; Eugenio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/447#issuecomment-830259038
Usability,simpl,simple,"Hi @husamia . I am curious, are you working with human sequence data, or do these come from non-human species?. I will take a look at the ratio of these events in our sample. If we could provide a simple filtering script to postprocess a callset, would this be something that you might use?. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/450#issuecomment-830343312
Usability,feedback,feedback,@AndrewCarroll I am working with human data for clinical diagnosis of rare disease. There is great interest in the clinical community to call de novo variants in the child from trio data. There is data about the rates of those events. . In 1% of the human genome you should find zero de novo in most cases. In less than 50% of the cases there are one or two that are real. I can run it on some curated data and provide feedback.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/450#issuecomment-830492777
Usability,feedback,feedback,"Hi @husamia . Thank you for the extra information. If there is any feedback that you can provide, I would be very grateful for examples. If you would like to follow up by email, you can reach me at awcarroll@google.com. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/450#issuecomment-831068064
Availability,error,errors,"Hi @ASLeonard . This is an interesting question. The ability to sort reads and label them with a phasing tag (specifically HP) is general to DeepVariant (meaning that on the code level it is straightforward to add to DeepVariant). This feature is only used for long reads. We performed experiments in non-trio phasing of short reads, but found there is not enough information for local phasing to add information. . Trio phasing can be much more informative for short reads over long ranges. The main obstacle is that we do not have pre-trained models which have learned how to use this information as we have for long reads. It would, in theory, be possible to train models for this (though it would be a reasonable amount of work). One of the obstacles for us to do this is that we don't know what to recommend as the best practices for the trio binning process. . I don't think that variant calling on an assembly is necessarily a good idea, because the assembly itself will have errors, and the expected distribution of REF, HET, and HOM calls will be quite different from the typical variant calling problem. Assemblies are usually less complete than the reference, especially with short read data, and this is likely to create a lot of mapping artifacts. . I don't have any good recommendations for how to incorporate trio haplotype information at this time, but if you have reasonable suggestions on how to do so, we are happy to consider using them within DeepTrio. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/451#issuecomment-830342295
Performance,perform,performed,"Hi @ASLeonard . This is an interesting question. The ability to sort reads and label them with a phasing tag (specifically HP) is general to DeepVariant (meaning that on the code level it is straightforward to add to DeepVariant). This feature is only used for long reads. We performed experiments in non-trio phasing of short reads, but found there is not enough information for local phasing to add information. . Trio phasing can be much more informative for short reads over long ranges. The main obstacle is that we do not have pre-trained models which have learned how to use this information as we have for long reads. It would, in theory, be possible to train models for this (though it would be a reasonable amount of work). One of the obstacles for us to do this is that we don't know what to recommend as the best practices for the trio binning process. . I don't think that variant calling on an assembly is necessarily a good idea, because the assembly itself will have errors, and the expected distribution of REF, HET, and HOM calls will be quite different from the typical variant calling problem. Assemblies are usually less complete than the reference, especially with short read data, and this is likely to create a lot of mapping artifacts. . I don't have any good recommendations for how to incorporate trio haplotype information at this time, but if you have reasonable suggestions on how to do so, we are happy to consider using them within DeepTrio. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/451#issuecomment-830342295
Usability,learn,learned,"Hi @ASLeonard . This is an interesting question. The ability to sort reads and label them with a phasing tag (specifically HP) is general to DeepVariant (meaning that on the code level it is straightforward to add to DeepVariant). This feature is only used for long reads. We performed experiments in non-trio phasing of short reads, but found there is not enough information for local phasing to add information. . Trio phasing can be much more informative for short reads over long ranges. The main obstacle is that we do not have pre-trained models which have learned how to use this information as we have for long reads. It would, in theory, be possible to train models for this (though it would be a reasonable amount of work). One of the obstacles for us to do this is that we don't know what to recommend as the best practices for the trio binning process. . I don't think that variant calling on an assembly is necessarily a good idea, because the assembly itself will have errors, and the expected distribution of REF, HET, and HOM calls will be quite different from the typical variant calling problem. Assemblies are usually less complete than the reference, especially with short read data, and this is likely to create a lot of mapping artifacts. . I don't have any good recommendations for how to incorporate trio haplotype information at this time, but if you have reasonable suggestions on how to do so, we are happy to consider using them within DeepTrio. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/451#issuecomment-830342295
Deployability,install,installation-guide,"Dear @gunjanbaid . thanks for your reply,. I am using the docker version, and the quick start with gpu does not work. ; I have Ubuntu 20.04.; I used the exact same command as quickstart. the issue is the TF and CUDA version which is not matched with the deep variant current ubuntu version. I am using RTX 3090 and this card needs a higher version of TF. I tried to make a Docker-based on the versions that I need but unfortunately, this failed too,. Would it be possible to have an additional docker for these gpu cards?. I followed the exact libraries mentioned in this link to make the docker; https://www.fatalerrors.org/a/rtx3090-ubuntu-20.04-tensorflow-2.4.0-installation-guide.html. update:. The issue is with the CUDA version, most recent GPU cards need CUDA 11. ; Is there any plan for an update? . Thanks in advance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452#issuecomment-834942601
Usability,guid,guide,"Dear @gunjanbaid . thanks for your reply,. I am using the docker version, and the quick start with gpu does not work. ; I have Ubuntu 20.04.; I used the exact same command as quickstart. the issue is the TF and CUDA version which is not matched with the deep variant current ubuntu version. I am using RTX 3090 and this card needs a higher version of TF. I tried to make a Docker-based on the versions that I need but unfortunately, this failed too,. Would it be possible to have an additional docker for these gpu cards?. I followed the exact libraries mentioned in this link to make the docker; https://www.fatalerrors.org/a/rtx3090-ubuntu-20.04-tensorflow-2.4.0-installation-guide.html. update:. The issue is with the CUDA version, most recent GPU cards need CUDA 11. ; Is there any plan for an update? . Thanks in advance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452#issuecomment-834942601
Safety,predict,predict,"Hi @pichuan, thanks for your response! Yes, ideally I'd like to build a variant classifier that could predict those callset values that I could then use as a proxy for the ""confidence level"" of a variant, i.e. a variant identified in multiple callsets might be more likely to be a ""real"" variant than one predicted in just one or two. . Based on your description of the 3-class system in the codebase, would it theoretically be more feasible to feed the algorithm an edited VCF files that bins the callset values into three categories? (E.g. 0 = 1 callset, 1 = 2-4 callsets, 2 = >5 callsets). Thank you for sharing the blog post and other resources, I'll take a look through those to try to learn more as well!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/454#issuecomment-835842617
Usability,learn,learn,"Hi @pichuan, thanks for your response! Yes, ideally I'd like to build a variant classifier that could predict those callset values that I could then use as a proxy for the ""confidence level"" of a variant, i.e. a variant identified in multiple callsets might be more likely to be a ""real"" variant than one predicted in just one or two. . Based on your description of the 3-class system in the codebase, would it theoretically be more feasible to feed the algorithm an edited VCF files that bins the callset values into three categories? (E.g. 0 = 1 callset, 1 = 2-4 callsets, 2 = >5 callsets). Thank you for sharing the blog post and other resources, I'll take a look through those to try to learn more as well!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/454#issuecomment-835842617
Availability,error,error,"Sorry for my late reply! To be honest, I believe I went with the [quick start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md) and replaced the test data with my own. Then I started to debug on that ValueError, believing that was a potential bug (because of the error saying I was using `--make_examples_extra_args=""sort_by_haplotypes=true,parse_sam_aux_fields=true""`, while I was actually not; I put that first argument to false, not true).; I don't believe there is something wrong with your user flow! Your github is really nice and the docker and dependencies were very easily installed. I wouldn't want to comment more on that without extensively trying your tool, so maybe I can provide with proper feedback later :) I will definitely let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457#issuecomment-844185505
Deployability,install,installed,"Sorry for my late reply! To be honest, I believe I went with the [quick start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md) and replaced the test data with my own. Then I started to debug on that ValueError, believing that was a potential bug (because of the error saying I was using `--make_examples_extra_args=""sort_by_haplotypes=true,parse_sam_aux_fields=true""`, while I was actually not; I put that first argument to false, not true).; I don't believe there is something wrong with your user flow! Your github is really nice and the docker and dependencies were very easily installed. I wouldn't want to comment more on that without extensively trying your tool, so maybe I can provide with proper feedback later :) I will definitely let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457#issuecomment-844185505
Integrability,depend,dependencies,"Sorry for my late reply! To be honest, I believe I went with the [quick start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md) and replaced the test data with my own. Then I started to debug on that ValueError, believing that was a potential bug (because of the error saying I was using `--make_examples_extra_args=""sort_by_haplotypes=true,parse_sam_aux_fields=true""`, while I was actually not; I put that first argument to false, not true).; I don't believe there is something wrong with your user flow! Your github is really nice and the docker and dependencies were very easily installed. I wouldn't want to comment more on that without extensively trying your tool, so maybe I can provide with proper feedback later :) I will definitely let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457#issuecomment-844185505
Testability,test,test,"Sorry for my late reply! To be honest, I believe I went with the [quick start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md) and replaced the test data with my own. Then I started to debug on that ValueError, believing that was a potential bug (because of the error saying I was using `--make_examples_extra_args=""sort_by_haplotypes=true,parse_sam_aux_fields=true""`, while I was actually not; I put that first argument to false, not true).; I don't believe there is something wrong with your user flow! Your github is really nice and the docker and dependencies were very easily installed. I wouldn't want to comment more on that without extensively trying your tool, so maybe I can provide with proper feedback later :) I will definitely let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457#issuecomment-844185505
Usability,feedback,feedback,"Sorry for my late reply! To be honest, I believe I went with the [quick start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md) and replaced the test data with my own. Then I started to debug on that ValueError, believing that was a potential bug (because of the error saying I was using `--make_examples_extra_args=""sort_by_haplotypes=true,parse_sam_aux_fields=true""`, while I was actually not; I put that first argument to false, not true).; I don't believe there is something wrong with your user flow! Your github is really nice and the docker and dependencies were very easily installed. I wouldn't want to comment more on that without extensively trying your tool, so maybe I can provide with proper feedback later :) I will definitely let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457#issuecomment-844185505
Integrability,depend,depending,"Hi @annabeldekker ,; one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:; https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457#issuecomment-844209552
Safety,avoid,avoid,"Hi @annabeldekker ,; one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:; https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457#issuecomment-844209552
Usability,simpl,simplified,"Hi @annabeldekker ,; one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:; https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457#issuecomment-844209552
Deployability,update,update,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own.; You can find the logic here: ; https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457#issuecomment-845522247
Testability,log,logic,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own.; You can find the logic here: ; https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457#issuecomment-845522247
Usability,simpl,simplify,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own.; You can find the logic here: ; https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457#issuecomment-845522247
Deployability,update,update,"Hi @ajsa-nukovic ,; Sorry for the confusion. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. However, from your command, I see that you're running make_examples directly, which is a reasonable use. But you're specifying your own make_examples flag for 1.1.0 code and model, you'll need to add `--add_hp_channel` to make sure the last channel is added. So, to summarize, if you're specifying your own flags for the make_examples step:; - If your BAM is NOT phased: use `--sort_by_haplotypes=false --parse_sam_aux_fields=false --add_hp_channel=true`.; - If your BAM is phased: use `--sort_by_haplotypes=true --parse_sam_aux_fields=true --add_hp_channel=true`.; Basically, in v1.1.0, `add_hp_channel` needs to always be set to true when you're running make_examples for PacBio. . I'll see if I can update the r1.1 documentation to avoid future confusions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458#issuecomment-844317545
Safety,avoid,avoid,"Hi @ajsa-nukovic ,; Sorry for the confusion. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. However, from your command, I see that you're running make_examples directly, which is a reasonable use. But you're specifying your own make_examples flag for 1.1.0 code and model, you'll need to add `--add_hp_channel` to make sure the last channel is added. So, to summarize, if you're specifying your own flags for the make_examples step:; - If your BAM is NOT phased: use `--sort_by_haplotypes=false --parse_sam_aux_fields=false --add_hp_channel=true`.; - If your BAM is phased: use `--sort_by_haplotypes=true --parse_sam_aux_fields=true --add_hp_channel=true`.; Basically, in v1.1.0, `add_hp_channel` needs to always be set to true when you're running make_examples for PacBio. . I'll see if I can update the r1.1 documentation to avoid future confusions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458#issuecomment-844317545
Usability,simpl,simplify,"Hi @ajsa-nukovic ,; Sorry for the confusion. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. However, from your command, I see that you're running make_examples directly, which is a reasonable use. But you're specifying your own make_examples flag for 1.1.0 code and model, you'll need to add `--add_hp_channel` to make sure the last channel is added. So, to summarize, if you're specifying your own flags for the make_examples step:; - If your BAM is NOT phased: use `--sort_by_haplotypes=false --parse_sam_aux_fields=false --add_hp_channel=true`.; - If your BAM is phased: use `--sort_by_haplotypes=true --parse_sam_aux_fields=true --add_hp_channel=true`.; Basically, in v1.1.0, `add_hp_channel` needs to always be set to true when you're running make_examples for PacBio. . I'll see if I can update the r1.1 documentation to avoid future confusions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458#issuecomment-844317545
Usability,learn,learning,"Hi @kyleaoconnell22 . Can I ask a few other questions - first, have you already attempted to use the human model, and, if so, do you have any indication of issues?. Second, do you know some of the rough properties of the genome (does it have a high repeat content? Do you know the approximate variant density and heterozygosity)?. We have been doing some experimentation with silver standard training data. We don't have any conclusive recommendations. We have thought about ising GATK for the silver lablels, but we're worried that this might carry the sort of artifacts that GATK makes into the deep learning model. Another idea we are looking at is to subset the Genome in a Bottle labels to regions which are more similar to the properties of the species to train a model for. I would suggest that this might be more promising as an approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459#issuecomment-858073154
Usability,guid,guidance,"I was not able to get this working @pichuan , if you could provide any guidance that'd still be helpful for me. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462#issuecomment-866335254
Usability,guid,guide,With a brand new t2.medium instance per the quickstart guide and your command I get:. ```; docker: invalid reference format.; See 'docker run --help'. real	0m0.046s; user	0m0.023s; sys	0m0.028s; ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462#issuecomment-867185369
Availability,avail,available,"Hi @Asppagh ,. It can be many different reasons. ; Your machine setup definitely could be one of the factors.; And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:; https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:; Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants?; (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-864304812
Testability,test,testdata,"Hi @Asppagh ,. It can be many different reasons. ; Your machine setup definitely could be one of the factors.; And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:; https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:; Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants?; (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-864304812
Usability,learn,learn,"Hi @Asppagh ,. It can be many different reasons. ; Your machine setup definitely could be one of the factors.; And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:; https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:; Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants?; (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-864304812
Energy Efficiency,power,power,"Thanks @pichuan,; I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo; processor	: 0; vendor_id	: GenuineIntel; cpu family	: 6; model		: 85; model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz; stepping	: 4; microcode	: 0x200004d; cpu MHz		: 2095.078; cache size	: 16896 KB; physical id	: 0; siblings	: 1; core id		: 0; cpu cores	: 1; apicid		: 0; initial apicid	: 0; fpu		: yes; fpu_exception	: yes; cpuid level	: 13; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat; bogomips	: 4190.15; clflush size	: 64; cache_alignment	: 64; address sizes	: 40 bits physical, 48 bits virtual; power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment.; In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:; input file S-001701867.markdup.bam; I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-866226252
Performance,cache,cache,"Thanks @pichuan,; I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo; processor	: 0; vendor_id	: GenuineIntel; cpu family	: 6; model		: 85; model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz; stepping	: 4; microcode	: 0x200004d; cpu MHz		: 2095.078; cache size	: 16896 KB; physical id	: 0; siblings	: 1; core id		: 0; cpu cores	: 1; apicid		: 0; initial apicid	: 0; fpu		: yes; fpu_exception	: yes; cpuid level	: 13; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat; bogomips	: 4190.15; clflush size	: 64; cache_alignment	: 64; address sizes	: 40 bits physical, 48 bits virtual; power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment.; In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:; input file S-001701867.markdup.bam; I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-866226252
Testability,log,log,"elow cpu info:. cat /proc/cpuinfo; processor	: 0; vendor_id	: GenuineIntel; cpu family	: 6; model		: 85; model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz; stepping	: 4; microcode	: 0x200004d; cpu MHz		: 2095.078; cache size	: 16896 KB; physical id	: 0; siblings	: 1; core id		: 0; cpu cores	: 1; apicid		: 0; initial apicid	: 0; fpu		: yes; fpu_exception	: yes; cpuid level	: 13; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat; bogomips	: 4190.15; clflush size	: 64; cache_alignment	: 64; address sizes	: 40 bits physical, 48 bits virtual; power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment.; In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:; input file S-001701867.markdup.bam; I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/inp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-866226252
Usability,clear,clear,"elow cpu info:. cat /proc/cpuinfo; processor	: 0; vendor_id	: GenuineIntel; cpu family	: 6; model		: 85; model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz; stepping	: 4; microcode	: 0x200004d; cpu MHz		: 2095.078; cache size	: 16896 KB; physical id	: 0; siblings	: 1; core id		: 0; cpu cores	: 1; apicid		: 0; initial apicid	: 0; fpu		: yes; fpu_exception	: yes; cpuid level	: 13; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat; bogomips	: 4190.15; clflush size	: 64; cache_alignment	: 64; address sizes	: 40 bits physical, 48 bits virtual; power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment.; In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:; input file S-001701867.markdup.bam; I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/inp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-866226252
Integrability,depend,depend,"Hi @maryawood ,; The default values in make_examples.py are our recommendations.; We do adjust things a bit based on different datatypes. For example, for PacBio, we changed --vsc_min_fraction_indels to 0.12:; https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L238; ```; special_args['vsc_min_fraction_indels'] = 0.12; ```. For retraining, it will depend on your data. For example, using different mappers could end up with different expected distributions of the mapping quality, and you might want to adjust accordingly. Given that training is a more advanced topic and it highly depends on your data, I don't have a simple recipe for that. One general rule is that you want to adjust these thresholds so that the candidate generation step is sensitive enough to propose the true variants, but not over sensitive that it ends up proposing too much noise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/464#issuecomment-867256204
Usability,simpl,simple,"Hi @maryawood ,; The default values in make_examples.py are our recommendations.; We do adjust things a bit based on different datatypes. For example, for PacBio, we changed --vsc_min_fraction_indels to 0.12:; https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L238; ```; special_args['vsc_min_fraction_indels'] = 0.12; ```. For retraining, it will depend on your data. For example, using different mappers could end up with different expected distributions of the mapping quality, and you might want to adjust accordingly. Given that training is a more advanced topic and it highly depends on your data, I don't have a simple recipe for that. One general rule is that you want to adjust these thresholds so that the candidate generation step is sensitive enough to propose the true variants, but not over sensitive that it ends up proposing too much noise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/464#issuecomment-867256204
Availability,error,error,"Hi @Asppagh ; From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs?. This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here.; Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870041849
Deployability,release,release,"Hi @Asppagh ; From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs?. This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here.; Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870041849
Integrability,message,messages,"Hi @Asppagh ; From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs?. This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here.; Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870041849
Testability,log,logs,"Hi @Asppagh ; From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs?. This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here.; Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870041849
Usability,clear,clear,"Hi @Asppagh ; From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs?. This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here.; Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870041849
Availability,error,error,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs.; However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md; to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`; ```; ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; ```; ```; head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam; cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; ```. I ran:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1; ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```; I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
Deployability,install,install,"ng, please report to https://github.com/google/deepvariant/issues; ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear.; But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```; -v,--verbosity: Logging verbosity level. Messages logged at this level or; lower will be included. Set to 1 for debug logging. If the flag was not set; or supplied, the value will be changed from the default of -1 (warning) to 0; (info) after flags are parsed.; (default: '-1'); (an integer); ```; You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity?. I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```; # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \; --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \; --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz""; ```. Here is the log I got from my Singularity run:; ```; INFO: Using cached SIF image; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA128",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
Integrability,message,messages,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs.; However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md; to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`; ```; ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; ```; ```; head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam; cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; ```. I ran:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1; ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```; I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
Modifiability,extend,extend,"1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:08:50.047413 139654431344384 make_examples.py:648] Writing examples to /tmp/tmpj5fx0phm/make_examples.tfrecord-00000-of-00001.gz; I0629 23:08:50.047621 139654431344384 make_examples.py:648] Writing gvcf records to /tmp/tmpj5fx0phm/gvcf.tfrecord-00000-of-00001.gz; I0629 23:08:50.048107 139654431344384 make_examples.py:648] Overhead for preparing inputs: 0 seconds; I0629 23:08:50.064639 139654431344384 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]; I0629 23:10:07.477154 139654431344384 make_examples.py:648] 102 candidates (110 examples) [77.41s elapsed]; I0629 23:10:11.740366 139654431344384 make_examples.py:648] 202 candidates (223 examples) [4.26s elapsed]; [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads; reads.extend(sam_reader.query(region)); File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__; record, not_done = self._raw_next(); File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next; not_done = self._cc_iterable.PythonNext(record); ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
Performance,cache,cached,"er); ```; You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity?. I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```; # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \; --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \; --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz""; ```. Here is the log I got from my Singularity run:; ```; INFO: Using cached SIF image; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']; I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
Testability,log,logs,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs.; However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md; to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`; ```; ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; ```; ```; head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam; cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; ```. I ran:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1; ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```; I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
Usability,clear,clear,"e_examples_runner; candidates, examples, gvcfs, runtimes = region_processor.process(region); File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process; reads = self.region_reads(region); File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads; error_message + '\nFailed to parse BAM/CRAM file. '; ValueError: Data loss: Failed to parse SAM record; Failed to parse BAM/CRAM file. This is often caused by:; (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file.; (2) Your BAM/CRAM file could be corrupted. Please check its md5.; If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues; ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear.; But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```; -v,--verbosity: Logging verbosity level. Messages logged at this level or; lower will be included. Set to 1 for debug logging. If the flag was not set; or supplied, the value will be changed from the default of -1 (warning) to 0; (info) after flags are parsed.; (default: '-1'); (an integer); ```; You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity?. I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```; # Pull the image.; BIN_VERSION=1.1.0; singular",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
Availability,avail,available,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster.; Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474#issuecomment-883613731
Performance,optimiz,optimize,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster.; Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474#issuecomment-883613731
Usability,simpl,simple,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster.; Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474#issuecomment-883613731
Availability,avail,available,"Hi @maca8e . You are correct that DeepVariant_unfiltered is a preferable preset for DeepTrio calling. We had meant to update that in the case study documentation for the most recent release, and failing to do so is an oversight that we will correct. The DeepTrio paper does describe the unfiltered preset as preferable, and it should be reflected here. . With respect to a reference for DeepTrio's workings, have you seen the [DeepTrio preprint](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1)? If so, is there an item you would like described in greater detail. With respect to why NA12891/NA12892 are not used for training the parent model, this is simply because NIST does not have a truth set for these samples, while a truthset from NIST is available for HG001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475#issuecomment-892390206
Deployability,update,update,"Hi @maca8e . You are correct that DeepVariant_unfiltered is a preferable preset for DeepTrio calling. We had meant to update that in the case study documentation for the most recent release, and failing to do so is an oversight that we will correct. The DeepTrio paper does describe the unfiltered preset as preferable, and it should be reflected here. . With respect to a reference for DeepTrio's workings, have you seen the [DeepTrio preprint](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1)? If so, is there an item you would like described in greater detail. With respect to why NA12891/NA12892 are not used for training the parent model, this is simply because NIST does not have a truth set for these samples, while a truthset from NIST is available for HG001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475#issuecomment-892390206
Usability,simpl,simply,"Hi @maca8e . You are correct that DeepVariant_unfiltered is a preferable preset for DeepTrio calling. We had meant to update that in the case study documentation for the most recent release, and failing to do so is an oversight that we will correct. The DeepTrio paper does describe the unfiltered preset as preferable, and it should be reflected here. . With respect to a reference for DeepTrio's workings, have you seen the [DeepTrio preprint](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1)? If so, is there an item you would like described in greater detail. With respect to why NA12891/NA12892 are not used for training the parent model, this is simply because NIST does not have a truth set for these samples, while a truthset from NIST is available for HG001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475#issuecomment-892390206
Deployability,update,updated,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04?. @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > ; > Would also like some clarification on this statement to help me figure out what is going on,; > ; > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-896293199
Integrability,depend,dependencies,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04?. @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > ; > Would also like some clarification on this statement to help me figure out what is going on,; > ; > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-896293199
Testability,test,tested,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04?. @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > ; > Would also like some clarification on this statement to help me figure out what is going on,; > ; > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-896293199
Usability,clear,clear,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04?. @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > ; > Would also like some clarification on this statement to help me figure out what is going on,; > ; > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-896293199
Testability,test,test,"thanks Pi-Chuan, decided to start building from your image with Ubuntu 20.04 to make sure that works before using the Databricks Runtime with Ubuntu 20.04, and got. 18 0.288 ========== [Tue Aug 10 21:03:43 UTC 2021] Stage 'Install bazel' starting; 18 0.297 ./build-prereq.sh: line 50: bazel: command not found; 18 0.298 ~/bazel /opt/deepvariant; 18 0.298 ./build-prereq.sh: line 56: curl: command not found; ------; executor failed running [/bin/sh -c ./build-prereq.sh]: exit code: 127. Assume there's a simple fix to add bazel and curl in, but I have had no time to test further since then, plan to get back on this next month",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-902138384
Usability,simpl,simple,"thanks Pi-Chuan, decided to start building from your image with Ubuntu 20.04 to make sure that works before using the Databricks Runtime with Ubuntu 20.04, and got. 18 0.288 ========== [Tue Aug 10 21:03:43 UTC 2021] Stage 'Install bazel' starting; 18 0.297 ./build-prereq.sh: line 50: bazel: command not found; 18 0.298 ~/bazel /opt/deepvariant; 18 0.298 ./build-prereq.sh: line 56: curl: command not found; ------; executor failed running [/bin/sh -c ./build-prereq.sh]: exit code: 127. Assume there's a simple fix to add bazel and curl in, but I have had no time to test further since then, plan to get back on this next month",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-902138384
Testability,log,log,"@kirti141 from the log, I agree that it isn't quite clear. ; Can you tell us about your machine? How many CPU cores, RAM, what OS, etc.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482#issuecomment-925047566
Usability,clear,clear,"@kirti141 from the log, I agree that it isn't quite clear. ; Can you tell us about your machine? How many CPU cores, RAM, what OS, etc.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482#issuecomment-925047566
Availability,error,error,"Machine has 64 cores, 2TB RAM, Centos is OS. Deep variant docker code works well when input bam file size is less than; 20 Gb file size, but when I increase the file size / coverage, I get the; error. On Wed, Sep 22, 2021 at 9:08 PM Pi-Chuan Chang ***@***.***>; wrote:. > @kirti141 <https://github.com/kirti141> from the log, I agree that it; > isn't quite clear.; > Can you tell us about your machine? How many CPU cores, RAM, what OS, etc.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/482#issuecomment-925047566>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ANQXTKYZH6MM2EGS7CBSZPTUDHZ7FANCNFSM5DR4DILA>; > .; > Triage notifications on the go with GitHub Mobile for iOS; > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>; > or Android; > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.; >; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482#issuecomment-925379521
Testability,log,log,"Machine has 64 cores, 2TB RAM, Centos is OS. Deep variant docker code works well when input bam file size is less than; 20 Gb file size, but when I increase the file size / coverage, I get the; error. On Wed, Sep 22, 2021 at 9:08 PM Pi-Chuan Chang ***@***.***>; wrote:. > @kirti141 <https://github.com/kirti141> from the log, I agree that it; > isn't quite clear.; > Can you tell us about your machine? How many CPU cores, RAM, what OS, etc.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/482#issuecomment-925047566>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ANQXTKYZH6MM2EGS7CBSZPTUDHZ7FANCNFSM5DR4DILA>; > .; > Triage notifications on the go with GitHub Mobile for iOS; > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>; > or Android; > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.; >; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482#issuecomment-925379521
Usability,clear,clear,"Machine has 64 cores, 2TB RAM, Centos is OS. Deep variant docker code works well when input bam file size is less than; 20 Gb file size, but when I increase the file size / coverage, I get the; error. On Wed, Sep 22, 2021 at 9:08 PM Pi-Chuan Chang ***@***.***>; wrote:. > @kirti141 <https://github.com/kirti141> from the log, I agree that it; > isn't quite clear.; > Can you tell us about your machine? How many CPU cores, RAM, what OS, etc.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/482#issuecomment-925047566>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ANQXTKYZH6MM2EGS7CBSZPTUDHZ7FANCNFSM5DR4DILA>; > .; > Triage notifications on the go with GitHub Mobile for iOS; > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>; > or Android; > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.; >; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482#issuecomment-925379521
Availability,error,errors,"Apologies in advance for inviting myself into the conversation. I think it's worth mentioning that while there are merits to doing MNV calling, the use-case given (calling amino acid variation from SNPs) isn't the greatest. There's no guarantee that codons occur as triplets in the genome (though they tend to, they can also get split across exons). which suggests that MNV calling doesn't actually solve the problem in general. Phase-aware consequence predictors (bcftools csq) should, on the other hand, work just fine (neglecting phasing errors of course). FWIW, we use the following pipeline: deepvariant or gatk4 -> whatshap -> shapeit4 -> bcftools csq to predict protein polymorphisms (or really, whole proteomes from whole genomes) and that approach should work in this case too. To be clear, having MNV calling would make an excellent addition to DeepVariant, it just may not be a total solution to the problem posed.; -August",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486#issuecomment-984133927
Deployability,pipeline,pipeline,"Apologies in advance for inviting myself into the conversation. I think it's worth mentioning that while there are merits to doing MNV calling, the use-case given (calling amino acid variation from SNPs) isn't the greatest. There's no guarantee that codons occur as triplets in the genome (though they tend to, they can also get split across exons). which suggests that MNV calling doesn't actually solve the problem in general. Phase-aware consequence predictors (bcftools csq) should, on the other hand, work just fine (neglecting phasing errors of course). FWIW, we use the following pipeline: deepvariant or gatk4 -> whatshap -> shapeit4 -> bcftools csq to predict protein polymorphisms (or really, whole proteomes from whole genomes) and that approach should work in this case too. To be clear, having MNV calling would make an excellent addition to DeepVariant, it just may not be a total solution to the problem posed.; -August",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486#issuecomment-984133927
Modifiability,polymorphi,polymorphisms,"Apologies in advance for inviting myself into the conversation. I think it's worth mentioning that while there are merits to doing MNV calling, the use-case given (calling amino acid variation from SNPs) isn't the greatest. There's no guarantee that codons occur as triplets in the genome (though they tend to, they can also get split across exons). which suggests that MNV calling doesn't actually solve the problem in general. Phase-aware consequence predictors (bcftools csq) should, on the other hand, work just fine (neglecting phasing errors of course). FWIW, we use the following pipeline: deepvariant or gatk4 -> whatshap -> shapeit4 -> bcftools csq to predict protein polymorphisms (or really, whole proteomes from whole genomes) and that approach should work in this case too. To be clear, having MNV calling would make an excellent addition to DeepVariant, it just may not be a total solution to the problem posed.; -August",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486#issuecomment-984133927
Safety,predict,predictors,"Apologies in advance for inviting myself into the conversation. I think it's worth mentioning that while there are merits to doing MNV calling, the use-case given (calling amino acid variation from SNPs) isn't the greatest. There's no guarantee that codons occur as triplets in the genome (though they tend to, they can also get split across exons). which suggests that MNV calling doesn't actually solve the problem in general. Phase-aware consequence predictors (bcftools csq) should, on the other hand, work just fine (neglecting phasing errors of course). FWIW, we use the following pipeline: deepvariant or gatk4 -> whatshap -> shapeit4 -> bcftools csq to predict protein polymorphisms (or really, whole proteomes from whole genomes) and that approach should work in this case too. To be clear, having MNV calling would make an excellent addition to DeepVariant, it just may not be a total solution to the problem posed.; -August",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486#issuecomment-984133927
Usability,clear,clear,"Apologies in advance for inviting myself into the conversation. I think it's worth mentioning that while there are merits to doing MNV calling, the use-case given (calling amino acid variation from SNPs) isn't the greatest. There's no guarantee that codons occur as triplets in the genome (though they tend to, they can also get split across exons). which suggests that MNV calling doesn't actually solve the problem in general. Phase-aware consequence predictors (bcftools csq) should, on the other hand, work just fine (neglecting phasing errors of course). FWIW, we use the following pipeline: deepvariant or gatk4 -> whatshap -> shapeit4 -> bcftools csq to predict protein polymorphisms (or really, whole proteomes from whole genomes) and that approach should work in this case too. To be clear, having MNV calling would make an excellent addition to DeepVariant, it just may not be a total solution to the problem posed.; -August",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486#issuecomment-984133927
Usability,clear,clear,"Hi @Suke-fudan , if your confusion is mainly about this line `The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). `, please ignore it for now. Internally we plan to remove that warning because it is confusing. To be clear:; - Our DeepTrio WGS and PACBIO models are trained with child height=60, and parents height=40. (Therefore 140 total).; - Our DeepTrio WES model was trained with child height and parents height=100, which is 300 total. If you're running PACBIO or WGS, you will see the (incorrect) warning about 140 isn't standard. If that's the case, please feel free to ignore that warning. We will improve in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-940505896
Modifiability,layers,layers,"@SHuang-Broad Glad to hear it worked, and thank you for the nice visualization! Docker has multiple layers and DV expands with its own within them, which is something we've noticed with other folks in terms of resource requirements -- which you could sort of tell from the memory/cpu utilization profiles. You might be able to profile it more granularly, but it might easier to start with some simple input files, and maybe a modified version of DV where you add debug information in the code to get an idea of the points of memory/disk expansion. This way you can trace the code-execution with correlated flow of data-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491#issuecomment-961546511
Usability,simpl,simple,"@SHuang-Broad Glad to hear it worked, and thank you for the nice visualization! Docker has multiple layers and DV expands with its own within them, which is something we've noticed with other folks in terms of resource requirements -- which you could sort of tell from the memory/cpu utilization profiles. You might be able to profile it more granularly, but it might easier to start with some simple input files, and maybe a modified version of DV where you add debug information in the code to get an idea of the points of memory/disk expansion. This way you can trace the code-execution with correlated flow of data-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491#issuecomment-961546511
Safety,safe,safely,"space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Reasons for No Call</h3><p style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-size: medium; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">All entries in the GLnexus pVCF include a Reason for No Call (RNC) field, which is filled in if either GT entry is not called (.). The possible values of RNC include:</p>. Code | Reason | Explanation; -- | -- | --; . | N/A | corresponding allele is called; M | Missing data | input (gVCF) had no data at this genome position; P | Partial data | input only partially covered this genome position; D | Depth | read depth too low to call; – | deletion | sample carries a deletion allele that couldn't be unified into this site; there may be more information in overlapping monoallelic site(s).; L | Lost allele | ^ but other than deletion allele; U | Unphased variants | sample carries multiple non-overlapping variants at this position*, whose phase is not known, so the diploid genotype cannot be called safely. There may be more information in overlapping monoallelic site(s).; O | Overlapping variants | sample carries multiple overlapping variants at this position, so the diploid genotype cannot be called safely. (GLnexus does deal with several common, but not all, cases of overlapping variants output by gVCF callers.) There may be more information in overlapping monoallelic site(s).; 1 | monoallelic | this is a monoallelic site; no assertion about presence/absence of any allele here. <!--EndFragment-->; </body>; </html>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494#issuecomment-1018260954
Testability,assert,assertion,"st probability for the position was as non-variant. Some of the reasons that DeepVariant may suspect a false positive are: strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage.""; So,is it possible for different RNC to correspond to the above reasons(strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage)?; When I met './.' ,I have reason to believe that it is 0/0 with greater probability than 0/1.; However,when the ""RNC"" is II,it means""gVCF input site is non-called"",for example: ./.:137:2,129:5:0,13,4:II ./.:137:86,50:6:0,4,39:II.; In this situation,why doesn't DeepVariant call the mutation(DP=137,alt reads=50)?. ##FORMAT=<ID=RNC,Number=2,Type=Character,Description=""Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele"">; https://github-wiki-see.page/m/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs; One of GLnexus' main functions is to generate a population-wide ""project"" VCF (pVCF) based on the input gVCFs for each individual sample. ; <html>; <body>; <!--StartFragment--><h3 style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Reasons for No Call</h3><p style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-size: medium; font-style: normal; fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494#issuecomment-1018260954
Usability,learn,learned,"Thank you!; You are right.The ""RNC"" comes from GLnexus. ; https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md ""If a candidate is made, but is called as reference (either 0/0 or ./.) it means that the neural network processed the genomic region, but based on all of its learned experience from training data, it decided the highest probability for the position was as non-variant. Some of the reasons that DeepVariant may suspect a false positive are: strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage.""; So,is it possible for different RNC to correspond to the above reasons(strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage)?; When I met './.' ,I have reason to believe that it is 0/0 with greater probability than 0/1.; However,when the ""RNC"" is II,it means""gVCF input site is non-called"",for example: ./.:137:2,129:5:0,13,4:II ./.:137:86,50:6:0,4,39:II.; In this situation,why doesn't DeepVariant call the mutation(DP=137,alt reads=50)?. ##FORMAT=<ID=RNC,Number=2,Type=Character,Description=""Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele"">; https://github-wiki-see.page/m/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs; One of GLnexus' main functions is to generate a population-wide ""project"" VCF (pVCF) based on the input gVCFs for each individual sample. ; <html>; <body>; <!--StartFragment--><h3 style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-tran",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494#issuecomment-1018260954
Usability,learn,learning,"Hi!. DeepVariant is not made to call variants in super high depth datasets, so this is not something we recommend doing.; There are several reasons for this, but one is that the pileup images can't be much taller than around 400 reads total due to limitations in the type of machine learning model we use. The cutoff of 1.5k reads is how many reads are considered when creating candidates, but a random selection of 95 of those are actually shown to the model. Increasing the pileup image height above 95 would require retraining, and we don't find that this helps significantly enough to justify the longer runtime. For high-depth applications we generally recommend that you use a specialized variant caller that is meant to support and make full use of all those reads. . To answer your question of The flags that can be set with call_variants are all the ones listed with ""flags.DEFINE""... in https://github.com/google/deepvariant/blob/r1.2/deepvariant/call_variants.py. The cutoff of 1.5k reads is `max_reads_per_partition` in https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py. This is not a flag we recommend users to change though. Just for my curiosity though, can you share what your application is and why the depth is so high?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/496#issuecomment-990103583
Usability,clear,clear,"Hello Maria,. Thanks for your reply, this is very clear.; I have such a depth because only a small region of my genome is amplified (~15kb) and several DNA are pooled by ""sample"". As the aim is to find if one out of the pooled DNA contains SNPs, I need to use all PacBio sequences instead of a subset.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/496#issuecomment-992270390
Usability,guid,guides,"From the help page of Singularity Container [link](https://sylabs.io/guides/3.1/user-guide/cli/singularity_run.html):; ```; -B, --bind strings a user-bind path specification. spec has the format src[:dest[:opts]], where src and dest are outside and inside paths. If dest is not given, it is set equal to src. Mount options ('opts') may be specified as 'ro' (read-only) or 'rw' (read/write, which is the default). Multiple bind paths can be given by a comma separated list.; ```. Basically, this option binds your local directory to the directory inside a container. Also see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md#notes-on-singularity",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/506#issuecomment-1017088492
Availability,error,error,"Hi @NagaComBio. Sorry for the delay! I don't have a clear solution to this problem just from looking at the error message, but if you can share the data, e.g. with just a small slice of the bam, then I can try to reproduce the issue. If that's possible, you can email me at marianattestad@google.com. For now I can tell you that `--group_variants=false` is only applicable when using `vcf_candidate_importer`, which is the most common way that this error occurs, since the input VCF for that can have multiple candidate variants in the same position, which isn't supposed to be possible when the candidates are generated by make_examples without `vcf_candidate_importer`. Thanks,; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517#issuecomment-1050344301
Integrability,message,message,"Hi @NagaComBio. Sorry for the delay! I don't have a clear solution to this problem just from looking at the error message, but if you can share the data, e.g. with just a small slice of the bam, then I can try to reproduce the issue. If that's possible, you can email me at marianattestad@google.com. For now I can tell you that `--group_variants=false` is only applicable when using `vcf_candidate_importer`, which is the most common way that this error occurs, since the input VCF for that can have multiple candidate variants in the same position, which isn't supposed to be possible when the candidates are generated by make_examples without `vcf_candidate_importer`. Thanks,; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517#issuecomment-1050344301
Usability,clear,clear,"Hi @NagaComBio. Sorry for the delay! I don't have a clear solution to this problem just from looking at the error message, but if you can share the data, e.g. with just a small slice of the bam, then I can try to reproduce the issue. If that's possible, you can email me at marianattestad@google.com. For now I can tell you that `--group_variants=false` is only applicable when using `vcf_candidate_importer`, which is the most common way that this error occurs, since the input VCF for that can have multiple candidate variants in the same position, which isn't supposed to be possible when the candidates are generated by make_examples without `vcf_candidate_importer`. Thanks,; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517#issuecomment-1050344301
Availability,avail,available,"ng, it is likely that DeepTrio would call variants on; > chromosomeX similar to how it would call a duo sample. To assess this, we ran DeepVariant and; > DeepTrio on chromosomeX of the son (HG002) and measured the number of heterozygous; > variant calls in the non-PAR regions of chromosomeX.; > ; > For DeepVariant, 4.45% (455/101866) of calls in non-PAR regions of chromosomeX are; > heterozygous. In DeepTrio, 24.5% (21633/88314) are heterozygous. This substantial difference; > suggests that applying DeepTrio directly to chromosomeX in male samples is problematic.; > Since chromosomeX in males is inherited from the mother, we performed calling on; > chromosomeX with only the mother provided as the parent. This reduced heterozygous calls to; > 3.37% (3518/104427), which is better than in the DeepVariant case. For male samples, this; > recommends that variant calling should be run with both parents on the autosomal and PAR; > regions using a BED file to restrict location, and additional variant calling should be performed; > using only the mother’s file provided as parent for the non-PAR regions of chromosomeX, and; > only the father’s provided for the non-PAR regions of chromosomeY.; > ; > This experiment indicates that allowing the model to infer a hemizygous chromosome through; > coverage and explicitly training for hemizygous variants is an opportunity for improvement,; > both for DeepVariant and DeepTrio. Over the long term:. We anticipate that the T2T consortium will be shortly be making available complete assemblies of ChrX and ChrY for the HG002 sample. This should allow us to make training labels for ChrX and ChrY in the hemizygous case, and we have a few internal ideas around how to use this to make models that will generally take this into account. I would be curious in your feedback about whether the short term solution is acceptable, whether you find it insufficient, or if you would recommend other approaches for us to address the issue. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1045294025
Energy Efficiency,reduce,reduced,"g section (near the end of results):. > The Genome in a Bottle truth sets do not contain chromosomeX or chromosomeY variants in a; > male individual. As a result, DeepTrio has never been trained with hemizygous sites. Because we; > train DeepTrio to perform duo calling, it is likely that DeepTrio would call variants on; > chromosomeX similar to how it would call a duo sample. To assess this, we ran DeepVariant and; > DeepTrio on chromosomeX of the son (HG002) and measured the number of heterozygous; > variant calls in the non-PAR regions of chromosomeX.; > ; > For DeepVariant, 4.45% (455/101866) of calls in non-PAR regions of chromosomeX are; > heterozygous. In DeepTrio, 24.5% (21633/88314) are heterozygous. This substantial difference; > suggests that applying DeepTrio directly to chromosomeX in male samples is problematic.; > Since chromosomeX in males is inherited from the mother, we performed calling on; > chromosomeX with only the mother provided as the parent. This reduced heterozygous calls to; > 3.37% (3518/104427), which is better than in the DeepVariant case. For male samples, this; > recommends that variant calling should be run with both parents on the autosomal and PAR; > regions using a BED file to restrict location, and additional variant calling should be performed; > using only the mother’s file provided as parent for the non-PAR regions of chromosomeX, and; > only the father’s provided for the non-PAR regions of chromosomeY.; > ; > This experiment indicates that allowing the model to infer a hemizygous chromosome through; > coverage and explicitly training for hemizygous variants is an opportunity for improvement,; > both for DeepVariant and DeepTrio. Over the long term:. We anticipate that the T2T consortium will be shortly be making available complete assemblies of ChrX and ChrY for the HG002 sample. This should allow us to make training labels for ChrX and ChrY in the hemizygous case, and we have a few internal ideas around how to use this to mak",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1045294025
Modifiability,inherit,inherited,"and son samples)."". The DeepTrio manuscript has benchmarks for this strategy in the following section (near the end of results):. > The Genome in a Bottle truth sets do not contain chromosomeX or chromosomeY variants in a; > male individual. As a result, DeepTrio has never been trained with hemizygous sites. Because we; > train DeepTrio to perform duo calling, it is likely that DeepTrio would call variants on; > chromosomeX similar to how it would call a duo sample. To assess this, we ran DeepVariant and; > DeepTrio on chromosomeX of the son (HG002) and measured the number of heterozygous; > variant calls in the non-PAR regions of chromosomeX.; > ; > For DeepVariant, 4.45% (455/101866) of calls in non-PAR regions of chromosomeX are; > heterozygous. In DeepTrio, 24.5% (21633/88314) are heterozygous. This substantial difference; > suggests that applying DeepTrio directly to chromosomeX in male samples is problematic.; > Since chromosomeX in males is inherited from the mother, we performed calling on; > chromosomeX with only the mother provided as the parent. This reduced heterozygous calls to; > 3.37% (3518/104427), which is better than in the DeepVariant case. For male samples, this; > recommends that variant calling should be run with both parents on the autosomal and PAR; > regions using a BED file to restrict location, and additional variant calling should be performed; > using only the mother’s file provided as parent for the non-PAR regions of chromosomeX, and; > only the father’s provided for the non-PAR regions of chromosomeY.; > ; > This experiment indicates that allowing the model to infer a hemizygous chromosome through; > coverage and explicitly training for hemizygous variants is an opportunity for improvement,; > both for DeepVariant and DeepTrio. Over the long term:. We anticipate that the T2T consortium will be shortly be making available complete assemblies of ChrX and ChrY for the HG002 sample. This should allow us to make training labels for ChrX an",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1045294025
Performance,perform,perform,"to run separate calling on the non-PAR regions of ChrX and ChrY, where only the mother sample is provided as the parent for calling of the son, and (less importantly as it is unclear whether this is an issue with chrY) only the father sample is provided for calling chrY on the son. In the documentation, this is expressed in the following statement: ""Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we recommend running these providing only the parent who contributed the child's chromosome (e.g. for chromosomeX, only the mother and son samples and for chromosomeY only the father and son samples)."". The DeepTrio manuscript has benchmarks for this strategy in the following section (near the end of results):. > The Genome in a Bottle truth sets do not contain chromosomeX or chromosomeY variants in a; > male individual. As a result, DeepTrio has never been trained with hemizygous sites. Because we; > train DeepTrio to perform duo calling, it is likely that DeepTrio would call variants on; > chromosomeX similar to how it would call a duo sample. To assess this, we ran DeepVariant and; > DeepTrio on chromosomeX of the son (HG002) and measured the number of heterozygous; > variant calls in the non-PAR regions of chromosomeX.; > ; > For DeepVariant, 4.45% (455/101866) of calls in non-PAR regions of chromosomeX are; > heterozygous. In DeepTrio, 24.5% (21633/88314) are heterozygous. This substantial difference; > suggests that applying DeepTrio directly to chromosomeX in male samples is problematic.; > Since chromosomeX in males is inherited from the mother, we performed calling on; > chromosomeX with only the mother provided as the parent. This reduced heterozygous calls to; > 3.37% (3518/104427), which is better than in the DeepVariant case. For male samples, this; > recommends that variant calling should be run with both parents on the autosomal and PAR; > regions using a BED file to restrict location, and additional variant calling should be perfor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1045294025
Testability,benchmark,benchmarked,"Hi @Phillip-a-richmond . Thank you for the report. You are correct, that the behavior out-of-the box for DeepTrio can currently be sub-optimal for the sex chromosomes in male samples. We've benchmarked some strategies for dealing with this. In the short term, our recommendation is to run separate calling on the non-PAR regions of ChrX and ChrY, where only the mother sample is provided as the parent for calling of the son, and (less importantly as it is unclear whether this is an issue with chrY) only the father sample is provided for calling chrY on the son. In the documentation, this is expressed in the following statement: ""Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we recommend running these providing only the parent who contributed the child's chromosome (e.g. for chromosomeX, only the mother and son samples and for chromosomeY only the father and son samples)."". The DeepTrio manuscript has benchmarks for this strategy in the following section (near the end of results):. > The Genome in a Bottle truth sets do not contain chromosomeX or chromosomeY variants in a; > male individual. As a result, DeepTrio has never been trained with hemizygous sites. Because we; > train DeepTrio to perform duo calling, it is likely that DeepTrio would call variants on; > chromosomeX similar to how it would call a duo sample. To assess this, we ran DeepVariant and; > DeepTrio on chromosomeX of the son (HG002) and measured the number of heterozygous; > variant calls in the non-PAR regions of chromosomeX.; > ; > For DeepVariant, 4.45% (455/101866) of calls in non-PAR regions of chromosomeX are; > heterozygous. In DeepTrio, 24.5% (21633/88314) are heterozygous. This substantial difference; > suggests that applying DeepTrio directly to chromosomeX in male samples is problematic.; > Since chromosomeX in males is inherited from the mother, we performed calling on; > chromosomeX with only the mother provided as the parent. This reduced heterozygous calls to;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1045294025
Usability,feedback,feedback,"ng, it is likely that DeepTrio would call variants on; > chromosomeX similar to how it would call a duo sample. To assess this, we ran DeepVariant and; > DeepTrio on chromosomeX of the son (HG002) and measured the number of heterozygous; > variant calls in the non-PAR regions of chromosomeX.; > ; > For DeepVariant, 4.45% (455/101866) of calls in non-PAR regions of chromosomeX are; > heterozygous. In DeepTrio, 24.5% (21633/88314) are heterozygous. This substantial difference; > suggests that applying DeepTrio directly to chromosomeX in male samples is problematic.; > Since chromosomeX in males is inherited from the mother, we performed calling on; > chromosomeX with only the mother provided as the parent. This reduced heterozygous calls to; > 3.37% (3518/104427), which is better than in the DeepVariant case. For male samples, this; > recommends that variant calling should be run with both parents on the autosomal and PAR; > regions using a BED file to restrict location, and additional variant calling should be performed; > using only the mother’s file provided as parent for the non-PAR regions of chromosomeX, and; > only the father’s provided for the non-PAR regions of chromosomeY.; > ; > This experiment indicates that allowing the model to infer a hemizygous chromosome through; > coverage and explicitly training for hemizygous variants is an opportunity for improvement,; > both for DeepVariant and DeepTrio. Over the long term:. We anticipate that the T2T consortium will be shortly be making available complete assemblies of ChrX and ChrY for the HG002 sample. This should allow us to make training labels for ChrX and ChrY in the hemizygous case, and we have a few internal ideas around how to use this to make models that will generally take this into account. I would be curious in your feedback about whether the short term solution is acceptable, whether you find it insufficient, or if you would recommend other approaches for us to address the issue. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1045294025
Deployability,update,update,"Hi @Phillip-a-richmond . I understand agree that this workflow is cumbersome and far from ideal. Here is the course of action that we'll propose to take:. Within the next 2 weeks, we anticipate that we'll likely to have GIAB labels for X and Y. The first course of action that we'll take is to incorporate those and attempt to train a new model with this. Based on whether this looks promising, we may ask if you are interested to test a Docker image with this model and provide feedback on it. I can't guarantee that this will work, but I think it has reasonable odds. If it does not, we do have other ideas for X and Y calling, but they would take a bit more time. I will plan to reach back out in 2 weeks with an update about the labels and a refined estimate for when we might have the model. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1050404831
Testability,test,test,"Hi @Phillip-a-richmond . I understand agree that this workflow is cumbersome and far from ideal. Here is the course of action that we'll propose to take:. Within the next 2 weeks, we anticipate that we'll likely to have GIAB labels for X and Y. The first course of action that we'll take is to incorporate those and attempt to train a new model with this. Based on whether this looks promising, we may ask if you are interested to test a Docker image with this model and provide feedback on it. I can't guarantee that this will work, but I think it has reasonable odds. If it does not, we do have other ideas for X and Y calling, but they would take a bit more time. I will plan to reach back out in 2 weeks with an update about the labels and a refined estimate for when we might have the model. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1050404831
Usability,feedback,feedback,"Hi @Phillip-a-richmond . I understand agree that this workflow is cumbersome and far from ideal. Here is the course of action that we'll propose to take:. Within the next 2 weeks, we anticipate that we'll likely to have GIAB labels for X and Y. The first course of action that we'll take is to incorporate those and attempt to train a new model with this. Based on whether this looks promising, we may ask if you are interested to test a Docker image with this model and provide feedback on it. I can't guarantee that this will work, but I think it has reasonable odds. If it does not, we do have other ideas for X and Y calling, but they would take a bit more time. I will plan to reach back out in 2 weeks with an update about the labels and a refined estimate for when we might have the model. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1050404831
Availability,avail,available,"k we are in perfect agreement. Supporting what the community wants is my preferred approach as well. I was only suggesting it after having performed a reduction upon the functional set of DeepVariant's design. Just in case the community requires more variety of labeled inputs, having a parameter processor might help in mapping between DeepVariant's core operation and different parameter sets. This would create flexibility for both the goals of the design team and community. For example, the design team might want flexibility in playing around with different transformations of read signals and parameter search spaces - which might change over time or with different data sets - without worrying that the parameter list might require updating. Let's take the following simplified model optimization:. ![image](https://github.com/google/deepvariant/assets/6555937/f7c33960-59fc-45c9-9cdd-0f015ebd1aae). The team would let this model optimize itself to determine the parameter set under which it performs ideally. Then metadata about the training would be saved along with the optimized model. Then users can query the model's metadata via some program, which would provide information about the training data and parameters limits for different model types. Such metadata would be automatically populated through the model training step (where I'm including the validation and tuning steps). Based on that, a default set of parameters can be auto-generated for general users, and advanced users can then map their custom parameters however they want. It basically makes the training turn-key and usage automatic. Such training could even be remotely initiated through a browser, and different users can make some of their models available to others if they want, thus helping out the community and growing it at the same time. This would also make adapting new models much faster, including where a large set of specialized ensemble models might be required to check against. Best regards,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1696738838
Energy Efficiency,adapt,adapting,"k we are in perfect agreement. Supporting what the community wants is my preferred approach as well. I was only suggesting it after having performed a reduction upon the functional set of DeepVariant's design. Just in case the community requires more variety of labeled inputs, having a parameter processor might help in mapping between DeepVariant's core operation and different parameter sets. This would create flexibility for both the goals of the design team and community. For example, the design team might want flexibility in playing around with different transformations of read signals and parameter search spaces - which might change over time or with different data sets - without worrying that the parameter list might require updating. Let's take the following simplified model optimization:. ![image](https://github.com/google/deepvariant/assets/6555937/f7c33960-59fc-45c9-9cdd-0f015ebd1aae). The team would let this model optimize itself to determine the parameter set under which it performs ideally. Then metadata about the training would be saved along with the optimized model. Then users can query the model's metadata via some program, which would provide information about the training data and parameters limits for different model types. Such metadata would be automatically populated through the model training step (where I'm including the validation and tuning steps). Based on that, a default set of parameters can be auto-generated for general users, and advanced users can then map their custom parameters however they want. It basically makes the training turn-key and usage automatic. Such training could even be remotely initiated through a browser, and different users can make some of their models available to others if they want, thus helping out the community and growing it at the same time. This would also make adapting new models much faster, including where a large set of specialized ensemble models might be required to check against. Best regards,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1696738838
Modifiability,adapt,adapting,"k we are in perfect agreement. Supporting what the community wants is my preferred approach as well. I was only suggesting it after having performed a reduction upon the functional set of DeepVariant's design. Just in case the community requires more variety of labeled inputs, having a parameter processor might help in mapping between DeepVariant's core operation and different parameter sets. This would create flexibility for both the goals of the design team and community. For example, the design team might want flexibility in playing around with different transformations of read signals and parameter search spaces - which might change over time or with different data sets - without worrying that the parameter list might require updating. Let's take the following simplified model optimization:. ![image](https://github.com/google/deepvariant/assets/6555937/f7c33960-59fc-45c9-9cdd-0f015ebd1aae). The team would let this model optimize itself to determine the parameter set under which it performs ideally. Then metadata about the training would be saved along with the optimized model. Then users can query the model's metadata via some program, which would provide information about the training data and parameters limits for different model types. Such metadata would be automatically populated through the model training step (where I'm including the validation and tuning steps). Based on that, a default set of parameters can be auto-generated for general users, and advanced users can then map their custom parameters however they want. It basically makes the training turn-key and usage automatic. Such training could even be remotely initiated through a browser, and different users can make some of their models available to others if they want, thus helping out the community and growing it at the same time. This would also make adapting new models much faster, including where a large set of specialized ensemble models might be required to check against. Best regards,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1696738838
Performance,perform,performed,"Hi Andrew,. I think we are in perfect agreement. Supporting what the community wants is my preferred approach as well. I was only suggesting it after having performed a reduction upon the functional set of DeepVariant's design. Just in case the community requires more variety of labeled inputs, having a parameter processor might help in mapping between DeepVariant's core operation and different parameter sets. This would create flexibility for both the goals of the design team and community. For example, the design team might want flexibility in playing around with different transformations of read signals and parameter search spaces - which might change over time or with different data sets - without worrying that the parameter list might require updating. Let's take the following simplified model optimization:. ![image](https://github.com/google/deepvariant/assets/6555937/f7c33960-59fc-45c9-9cdd-0f015ebd1aae). The team would let this model optimize itself to determine the parameter set under which it performs ideally. Then metadata about the training would be saved along with the optimized model. Then users can query the model's metadata via some program, which would provide information about the training data and parameters limits for different model types. Such metadata would be automatically populated through the model training step (where I'm including the validation and tuning steps). Based on that, a default set of parameters can be auto-generated for general users, and advanced users can then map their custom parameters however they want. It basically makes the training turn-key and usage automatic. Such training could even be remotely initiated through a browser, and different users can make some of their models available to others if they want, thus helping out the community and growing it at the same time. This would also make adapting new models much faster, including where a large set of specialized ensemble models might be required to check against. Be",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1696738838
Security,validat,validation,"k we are in perfect agreement. Supporting what the community wants is my preferred approach as well. I was only suggesting it after having performed a reduction upon the functional set of DeepVariant's design. Just in case the community requires more variety of labeled inputs, having a parameter processor might help in mapping between DeepVariant's core operation and different parameter sets. This would create flexibility for both the goals of the design team and community. For example, the design team might want flexibility in playing around with different transformations of read signals and parameter search spaces - which might change over time or with different data sets - without worrying that the parameter list might require updating. Let's take the following simplified model optimization:. ![image](https://github.com/google/deepvariant/assets/6555937/f7c33960-59fc-45c9-9cdd-0f015ebd1aae). The team would let this model optimize itself to determine the parameter set under which it performs ideally. Then metadata about the training would be saved along with the optimized model. Then users can query the model's metadata via some program, which would provide information about the training data and parameters limits for different model types. Such metadata would be automatically populated through the model training step (where I'm including the validation and tuning steps). Based on that, a default set of parameters can be auto-generated for general users, and advanced users can then map their custom parameters however they want. It basically makes the training turn-key and usage automatic. Such training could even be remotely initiated through a browser, and different users can make some of their models available to others if they want, thus helping out the community and growing it at the same time. This would also make adapting new models much faster, including where a large set of specialized ensemble models might be required to check against. Best regards,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1696738838
Usability,simpl,simplified,"Hi Andrew,. I think we are in perfect agreement. Supporting what the community wants is my preferred approach as well. I was only suggesting it after having performed a reduction upon the functional set of DeepVariant's design. Just in case the community requires more variety of labeled inputs, having a parameter processor might help in mapping between DeepVariant's core operation and different parameter sets. This would create flexibility for both the goals of the design team and community. For example, the design team might want flexibility in playing around with different transformations of read signals and parameter search spaces - which might change over time or with different data sets - without worrying that the parameter list might require updating. Let's take the following simplified model optimization:. ![image](https://github.com/google/deepvariant/assets/6555937/f7c33960-59fc-45c9-9cdd-0f015ebd1aae). The team would let this model optimize itself to determine the parameter set under which it performs ideally. Then metadata about the training would be saved along with the optimized model. Then users can query the model's metadata via some program, which would provide information about the training data and parameters limits for different model types. Such metadata would be automatically populated through the model training step (where I'm including the validation and tuning steps). Based on that, a default set of parameters can be auto-generated for general users, and advanced users can then map their custom parameters however they want. It basically makes the training turn-key and usage automatic. Such training could even be remotely initiated through a browser, and different users can make some of their models available to others if they want, thus helping out the community and growing it at the same time. This would also make adapting new models much faster, including where a large set of specialized ensemble models might be required to check against. Be",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1696738838
Security,access,access,"Hey, thanks for the response! I may have access to a high-quality dataset in the near future. Also, training multiple models for each level of ploidy makes sense. I've got some experience with TF and programming in general (Python and Rust, lately), so curious if you can give some feedback on specific areas that may need to be adjusted for ploidy? Or, if it would be too much for one person I understand too. I've done re-training of DV models for non-model species and have seen great improvements so far, so I'm familiar with that as well. Thanks again!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/519#issuecomment-1053728841
Usability,feedback,feedback,"Hey, thanks for the response! I may have access to a high-quality dataset in the near future. Also, training multiple models for each level of ploidy makes sense. I've got some experience with TF and programming in general (Python and Rust, lately), so curious if you can give some feedback on specific areas that may need to be adjusted for ploidy? Or, if it would be too much for one person I understand too. I've done re-training of DV models for non-model species and have seen great improvements so far, so I'm familiar with that as well. Thanks again!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/519#issuecomment-1053728841
Usability,guid,guidance,"Ah, understood, and thanks very much for the guidance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/519#issuecomment-1054593204
Usability,learn,learned,"g the union of the Truth Set with any MNP variant called in either GATK or DeepVariant, here is the Hap.py that I see:. <google-sheets-html-origin><style type=""text/css""><!--td {border: 1px solid #cccccc;}br {mso-data-placement:same-cell;}--></style>.   | Filter | TRUTH.TOTAL | TRUTH.TP | TRUTH.FN | QUERY.FP | FP.gt | FP.al | METRIC.Recall | METRIC.Precision | METRIC.F1_Score; -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | --; HG001-DV | PASS | 27307 | 26199 | 1108 | 510 | 95 | 10 | 0.9594 | 0.9809 | 0.9700; HG001-GATK | PASS | 27307 | 26258 | 1049 | 1746 | 153 | 17 | 0.9616 | 0.9377 | 0.9495; HG002-DV | PASS | 28662 | 27505 | 1157 | 613 | 77 | 3 | 0.9596 | 0.9782 | 0.9688; HG002-GATK | PASS | 28662 | 27539 | 1123 | 2073 | 133 | 7 | 0.9608 | 0.9300 | 0.9452; HG003-DV | PASS | 28274 | 27234 | 1040 | 588 | 73 | 7 | 0.9632 | 0.9789 | 0.9710; HG003-GATK | PASS | 28274 | 27188 | 1086 | 2060 | 156 | 8 | 0.9616 | 0.9296 | 0.9453. A few observations from these metrics:. 1. For both GATK and DeepVariant, MNPs are notably harder to call. F1 is 0.945-0.97, while we expect F1 of around 0.9945 for DeepVariant at 30x with Illumina.; 2. DeepVariant and GATK4 have fairly similar recall, with DeepVariant being higher in recall in 1 sample and GATK in the other 2.; 3. DeepVariant has noticeably higher precision (0.98 vs 0.93). From this, I suspect that there is at least a few factors that make MNPs more difficult to call. I suspect that DeepVariant has learned some of those factors and is more conservative to call MNPs, even when they might look like real calls. There are several other possible explanations (e.g. that hap.py has trouble in the comparison process correctly annotating sites). Following this current set of metrics, I plan to inspect several of the correctly and incorrectly called variants in IGV with their support and see if I can better understand what factors are making MNPs more difficult to call, and if it seems like there are any specifically addressable issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520#issuecomment-1558724808
Usability,feedback,feedback,"Hi @kishwarshafin,. Thanks for the feedback, I actually forgot to get back to you about this but I have indeed filtered out the `NoCall` for the small cohort I have after your early feedback and it indeed solved this issue. Good luck with solving this!. Guillaume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/521#issuecomment-1102489422
Security,access,access,"Hi Aman,. Thank you for reaching out to us. As you've read in our [Cohort Best Practices](https://github.com/google/deepvariant/blob/r1.3/docs/trio-merge-case-study.md#best-practices-for-multi-sample-variant-calling-with-deepvariant-wes-trio-demonstration), generating a cohort variant callset using DeepVariant includes two separate steps:. 1. Running DeepVariant on all sample reads to generate gVCF files, one per each sample.; 2. Running GLnexus on the gVCFs using the provided `DeepVariantWGS` or `DeepVariantWES`. For the first step, the best way to parallelize computation would be using cloud compute resources if you have access to them. You can find instructions on how to run DeepVariant on Google Cloud Platform [here](https://cloud.google.com/life-sciences/docs/tutorials/deepvariant), or you can use any other cloud service providers using our DeepVariant docker [images](https://github.com/google/deepvariant#how-to-run-deepvariant), to run DeepVariant on each sample (or a batch of samples) separately in multiple machines. I would not recommend parallelizing DeepVariant runs over samples in a single machine though, since a single run of DeepVariant already uses multiple cores in the `make_examples` step - the number of cores to use is controlled by the `--num_shards` parameter. Once you've generated all ~200 gVCFs, one for each sample, the second step should be pretty simple. A single machine with relatively good CPU/RAM capacity should be able to handle merging 200 gVCFs using GLnexus. I hope this helps. Best,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/526#issuecomment-1067196676
Usability,simpl,simple,"Hi Aman,. Thank you for reaching out to us. As you've read in our [Cohort Best Practices](https://github.com/google/deepvariant/blob/r1.3/docs/trio-merge-case-study.md#best-practices-for-multi-sample-variant-calling-with-deepvariant-wes-trio-demonstration), generating a cohort variant callset using DeepVariant includes two separate steps:. 1. Running DeepVariant on all sample reads to generate gVCF files, one per each sample.; 2. Running GLnexus on the gVCFs using the provided `DeepVariantWGS` or `DeepVariantWES`. For the first step, the best way to parallelize computation would be using cloud compute resources if you have access to them. You can find instructions on how to run DeepVariant on Google Cloud Platform [here](https://cloud.google.com/life-sciences/docs/tutorials/deepvariant), or you can use any other cloud service providers using our DeepVariant docker [images](https://github.com/google/deepvariant#how-to-run-deepvariant), to run DeepVariant on each sample (or a batch of samples) separately in multiple machines. I would not recommend parallelizing DeepVariant runs over samples in a single machine though, since a single run of DeepVariant already uses multiple cores in the `make_examples` step - the number of cores to use is controlled by the `--num_shards` parameter. Once you've generated all ~200 gVCFs, one for each sample, the second step should be pretty simple. A single machine with relatively good CPU/RAM capacity should be able to handle merging 200 gVCFs using GLnexus. I hope this helps. Best,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/526#issuecomment-1067196676
Usability,guid,guide,"Hi sorry for the delay,. I reran my sequences with a new copy of the human reference genome from https://www.ncbi.nlm.nih.gov/genome/guide/human/ up until running deepvariant. . I get the same headers as above, it doesn't seem like the bam, sam or even the reference start with the word 'chr1':. sam file header: @SQ	SN:NC_000001.11	LN:248956422; bam file (sorted picard): @HD	VN:1.6	SO:coordinate; @SQ	SN:NC_000001.11	LN:248956422; Original fasta reference file header: >NC_000001.11 Homo sapiens chromosome 1, GRCh38.p13 Primary Assembly; Bed file: chr1	12080	12251. All the code and steps I've done before are on my page under Exome_Pipeline/PE read analysis. I'm not sure! Ah, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/527#issuecomment-1073019775
Deployability,release,release,"Hi @Qianwangwoo ,; First of all, DeepVariant is a germline variant caller - all our release models are trained for germline variant calling. But if I read your question correctly, your question is more about ""why does DeepVariant call this image as HET rather than HOM-ALT"". To answer that question, it'll be similar to this FAQ here: https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. Once a candidate is identified, DeepVariant uses a classifier on it to generate a probability distribution for the 3 classes (0: HOM-REF, 1: HET, 2: HOM-ALT). ; ; From the `PL` field, it would look like HOM-ALT has lower probability than HET, but not necessarily by much `33,0,1`.; And, the classifier takes into account many factors here, which is why the prediction is not always intuitive (and, not always right). . Let me know if this helps and if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/528#issuecomment-1067120303
Safety,predict,prediction,"Hi @Qianwangwoo ,; First of all, DeepVariant is a germline variant caller - all our release models are trained for germline variant calling. But if I read your question correctly, your question is more about ""why does DeepVariant call this image as HET rather than HOM-ALT"". To answer that question, it'll be similar to this FAQ here: https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. Once a candidate is identified, DeepVariant uses a classifier on it to generate a probability distribution for the 3 classes (0: HOM-REF, 1: HET, 2: HOM-ALT). ; ; From the `PL` field, it would look like HOM-ALT has lower probability than HET, but not necessarily by much `33,0,1`.; And, the classifier takes into account many factors here, which is why the prediction is not always intuitive (and, not always right). . Let me know if this helps and if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/528#issuecomment-1067120303
Usability,intuit,intuitive,"Hi @Qianwangwoo ,; First of all, DeepVariant is a germline variant caller - all our release models are trained for germline variant calling. But if I read your question correctly, your question is more about ""why does DeepVariant call this image as HET rather than HOM-ALT"". To answer that question, it'll be similar to this FAQ here: https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. Once a candidate is identified, DeepVariant uses a classifier on it to generate a probability distribution for the 3 classes (0: HOM-REF, 1: HET, 2: HOM-ALT). ; ; From the `PL` field, it would look like HOM-ALT has lower probability than HET, but not necessarily by much `33,0,1`.; And, the classifier takes into account many factors here, which is why the prediction is not always intuitive (and, not always right). . Let me know if this helps and if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/528#issuecomment-1067120303
Deployability,update,updated,"Hi @japhill ,; Just to make sure I understand this - are you saying that the Docker image has /mnt in it, and as result was causing problem with Singularity?. I do see a /mnt directory:; ```; $ sudo docker run google/deepvariant:1.3.0 ls -lh /; total 48K; lrwxrwxrwx 1 root root 7 Oct 6 16:47 bin -> usr/bin; drwxr-xr-x 2 root root 4.0K Apr 15 2020 boot; drwxr-xr-x 5 root root 340 Mar 23 23:34 dev; drwxr-xr-x 1 root root 4.0K Mar 23 23:34 etc; drwxr-xr-x 2 root root 4.0K Apr 15 2020 home; lrwxrwxrwx 1 root root 7 Oct 6 16:47 lib -> usr/lib; lrwxrwxrwx 1 root root 9 Oct 6 16:47 lib32 -> usr/lib32; lrwxrwxrwx 1 root root 9 Oct 6 16:47 lib64 -> usr/lib64; lrwxrwxrwx 1 root root 10 Oct 6 16:47 libx32 -> usr/libx32; drwxr-xr-x 2 root root 4.0K Oct 6 16:47 media; drwxr-xr-x 2 root root 4.0K Oct 6 16:47 mnt; drwxr-xr-x 1 root root 4.0K Dec 6 23:17 opt; dr-xr-xr-x 525 root root 0 Mar 23 23:34 proc; drwx------ 1 root root 4.0K Dec 6 23:15 root; drwxr-xr-x 5 root root 4.0K Oct 6 16:58 run; lrwxrwxrwx 1 root root 8 Oct 6 16:47 sbin -> usr/sbin; drwxr-xr-x 2 root root 4.0K Oct 6 16:47 srv; dr-xr-xr-x 13 root root 0 Mar 23 23:34 sys; drwxrwxrwt 1 root root 4.0K Dec 6 23:19 tmp; drwxr-xr-x 1 root root 4.0K Oct 6 16:47 usr; drwxr-xr-x 1 root root 4.0K Oct 6 16:58 var; ```; which is empty, so I think your suggestion of something like ""RUN rm -rf /mnt/"" makes sense. I'll also do a quick search to see if there are better approaches here. Thanks for the feedback. I'll track internally and make sure this is updated in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530#issuecomment-1076920454
Usability,feedback,feedback,"Hi @japhill ,; Just to make sure I understand this - are you saying that the Docker image has /mnt in it, and as result was causing problem with Singularity?. I do see a /mnt directory:; ```; $ sudo docker run google/deepvariant:1.3.0 ls -lh /; total 48K; lrwxrwxrwx 1 root root 7 Oct 6 16:47 bin -> usr/bin; drwxr-xr-x 2 root root 4.0K Apr 15 2020 boot; drwxr-xr-x 5 root root 340 Mar 23 23:34 dev; drwxr-xr-x 1 root root 4.0K Mar 23 23:34 etc; drwxr-xr-x 2 root root 4.0K Apr 15 2020 home; lrwxrwxrwx 1 root root 7 Oct 6 16:47 lib -> usr/lib; lrwxrwxrwx 1 root root 9 Oct 6 16:47 lib32 -> usr/lib32; lrwxrwxrwx 1 root root 9 Oct 6 16:47 lib64 -> usr/lib64; lrwxrwxrwx 1 root root 10 Oct 6 16:47 libx32 -> usr/libx32; drwxr-xr-x 2 root root 4.0K Oct 6 16:47 media; drwxr-xr-x 2 root root 4.0K Oct 6 16:47 mnt; drwxr-xr-x 1 root root 4.0K Dec 6 23:17 opt; dr-xr-xr-x 525 root root 0 Mar 23 23:34 proc; drwx------ 1 root root 4.0K Dec 6 23:15 root; drwxr-xr-x 5 root root 4.0K Oct 6 16:58 run; lrwxrwxrwx 1 root root 8 Oct 6 16:47 sbin -> usr/sbin; drwxr-xr-x 2 root root 4.0K Oct 6 16:47 srv; dr-xr-xr-x 13 root root 0 Mar 23 23:34 sys; drwxrwxrwt 1 root root 4.0K Dec 6 23:19 tmp; drwxr-xr-x 1 root root 4.0K Oct 6 16:47 usr; drwxr-xr-x 1 root root 4.0K Oct 6 16:58 var; ```; which is empty, so I think your suggestion of something like ""RUN rm -rf /mnt/"" makes sense. I'll also do a quick search to see if there are better approaches here. Thanks for the feedback. I'll track internally and make sure this is updated in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530#issuecomment-1076920454
Usability,guid,guides,"@japhill But for now, I also wonder if you can use the `-B` option in Singularity?; https://sylabs.io/guides/3.1/user-guide/bind_paths_and_mounts.html#user-defined-bind-paths; Can you try it and let me know if it works for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530#issuecomment-1076923302
Usability,guid,guidelines,"thanks for the great piece of software!; I agree with Jordi and would add that some general guidelines on how to use the numerous INFO fields would be nice, do they live somewhere?; it is a pity to have so many metrics and ignore how to put them to good use",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/531#issuecomment-1081798812
Usability,learn,learning,"Hi @jordimaggi ; For anything that is `RefCall`, that means: even though a candidate variant was proposed, our machine learning classifier decided the most likely class is 0 (which means reference). ; You can read this section to get a bit more background on this: https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. You could potentially take the finer-grained information (like `PL`) and try to adjust your own threshold. This could increase the sensitivity, but will likely hurt the specificity of DeepVariant's results. I'll ask @AndrewCarroll to add his thoughts here as well. Hi @splaisan , for the existing fields we have in our VCF file, we follow the standard definitions you can find on https://en.wikipedia.org/wiki/Variant_Call_Format#Common_FORMAT_fields (and we only fill in a subset of them). Let us know if there's anything specific that is not clear to you. Happy to explain more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/531#issuecomment-1082175599
Availability,avail,available,"eature is not necessarily bad, but both using and training with the T2T reference might help it adjust its priors for how likely this is in the T2T reference (we had a [poster](https://twitter.com/acarroll_ATG/status/1231655024213856256) discussing this phenomenon and how DeepVariant reacts to it). . However, I think the effects of retraining here will be fairly minimal and restricted to either segmental duplication regions or structural variants. In addition, it's unclear to me whether this would occur only for short-reads. Right now, the quality of the truth sets is limited by long-read mappability to the reference. With good coverage of HiFi reads, we expect SNP F1 of more than 0.999. It seems likely that the model already knows enough to accurately call variants if the reference can be resolved, and though T2T may help the mapping resolve the remainder, it's unclear whether there is more to learn in further training. This highlights one key point where T2T may help with training - that the current training is limited by the truth set. Training with v4.2.1 truth sets is still constrained by the confident regions of the genome. If we can get fully complete, 100% accurate truth sets covering the genome, this will provide more training examples of difficult regions in the training process, and I think this could further improve a model (whether it's on the T2T reference or on GRCh38). I think there will be an opportunity for this as complete T2T assemblies become available for more samples. Finally, from a practical perspective, the current v4.2.1 truth sets are relative to GRCh38, so in order to train we'd need to first be able to generate truth variants and confident regions for some sample on T2T. That's certainly doable, but it is tricky to do correctly. Hopefully this has answered more questions than it has opened. If this is an area you have ideas about or are interested in collaborating on, we'd certainly be happy to explore those together. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/534#issuecomment-1102014578
Usability,learn,learned,"oad . This is a good question and the best answer I can give to it is both complicated and not conclusive. TL;DR - for long reads, retraining will (probably) not change things much, but there might be other future opportunities to use T2T truth in training strategies regardless of the reference used. . Generally, I think DeepVariant will give good results on T2T without retraining specifically for it, and given the better completeness of the T2T reference, probably just using this will give generally better results. In the past, we have trained with both GRCh37 and GRCh38, and we don't see the model behaving very differently with either reference. It's possible that re-training with the T2T could lead to marginally better accuracy in some areas, especially in segmental duplications, which are better resolved in T2T. At present, GRCh38 will have some segmental duplications that are collapsed, or where a copy is missing. DeepVariant seems to have learned some of the patterns for this, and is can sometimes reject variants in regions that look like segdups. This feature is not necessarily bad, but both using and training with the T2T reference might help it adjust its priors for how likely this is in the T2T reference (we had a [poster](https://twitter.com/acarroll_ATG/status/1231655024213856256) discussing this phenomenon and how DeepVariant reacts to it). . However, I think the effects of retraining here will be fairly minimal and restricted to either segmental duplication regions or structural variants. In addition, it's unclear to me whether this would occur only for short-reads. Right now, the quality of the truth sets is limited by long-read mappability to the reference. With good coverage of HiFi reads, we expect SNP F1 of more than 0.999. It seems likely that the model already knows enough to accurately call variants if the reference can be resolved, and though T2T may help the mapping resolve the remainder, it's unclear whether there is more to learn in further ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/534#issuecomment-1102014578
Availability,avail,available,"@JosephLalli ,. The current schema of DeepVariant training depends on having GIAB calls that we use as truth available against the reference. The GIAB truth set against T2T is still not available and released so currently we are not using T2T to train our models. Lifting the calls over to the T2T reference would not add too much value as it simply doesn't extend the truth set rather transfers it from one reference to the other. We are connected with the GIAB and T2T team. Once the resources are available, we will add those to our training scheme. Let us know if you have any further questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/534#issuecomment-2263703106
Deployability,release,released,"@JosephLalli ,. The current schema of DeepVariant training depends on having GIAB calls that we use as truth available against the reference. The GIAB truth set against T2T is still not available and released so currently we are not using T2T to train our models. Lifting the calls over to the T2T reference would not add too much value as it simply doesn't extend the truth set rather transfers it from one reference to the other. We are connected with the GIAB and T2T team. Once the resources are available, we will add those to our training scheme. Let us know if you have any further questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/534#issuecomment-2263703106
Integrability,depend,depends,"@JosephLalli ,. The current schema of DeepVariant training depends on having GIAB calls that we use as truth available against the reference. The GIAB truth set against T2T is still not available and released so currently we are not using T2T to train our models. Lifting the calls over to the T2T reference would not add too much value as it simply doesn't extend the truth set rather transfers it from one reference to the other. We are connected with the GIAB and T2T team. Once the resources are available, we will add those to our training scheme. Let us know if you have any further questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/534#issuecomment-2263703106
Modifiability,extend,extend,"@JosephLalli ,. The current schema of DeepVariant training depends on having GIAB calls that we use as truth available against the reference. The GIAB truth set against T2T is still not available and released so currently we are not using T2T to train our models. Lifting the calls over to the T2T reference would not add too much value as it simply doesn't extend the truth set rather transfers it from one reference to the other. We are connected with the GIAB and T2T team. Once the resources are available, we will add those to our training scheme. Let us know if you have any further questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/534#issuecomment-2263703106
Usability,simpl,simply,"@JosephLalli ,. The current schema of DeepVariant training depends on having GIAB calls that we use as truth available against the reference. The GIAB truth set against T2T is still not available and released so currently we are not using T2T to train our models. Lifting the calls over to the T2T reference would not add too much value as it simply doesn't extend the truth set rather transfers it from one reference to the other. We are connected with the GIAB and T2T team. Once the resources are available, we will add those to our training scheme. Let us know if you have any further questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/534#issuecomment-2263703106
Availability,down,downsampling,"The images all have to be the same size for the model, so they are standardized to a height of 100 (5 rows for reference + 95 reads). Standardization like this is common in machine learning. Coverage does actually get that high for many of our training datasets when we use the full coverage without downsampling.; By the way, where did you find that link? It's a pretty old version of the notebook. We have since updated it here: https://github.com/google/deepvariant/blob/r1.3/docs/visualizing_examples.ipynb",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/536#issuecomment-1106635811
Deployability,update,updated,"The images all have to be the same size for the model, so they are standardized to a height of 100 (5 rows for reference + 95 reads). Standardization like this is common in machine learning. Coverage does actually get that high for many of our training datasets when we use the full coverage without downsampling.; By the way, where did you find that link? It's a pretty old version of the notebook. We have since updated it here: https://github.com/google/deepvariant/blob/r1.3/docs/visualizing_examples.ipynb",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/536#issuecomment-1106635811
Usability,learn,learning,"The images all have to be the same size for the model, so they are standardized to a height of 100 (5 rows for reference + 95 reads). Standardization like this is common in machine learning. Coverage does actually get that high for many of our training datasets when we use the full coverage without downsampling.; By the way, where did you find that link? It's a pretty old version of the notebook. We have since updated it here: https://github.com/google/deepvariant/blob/r1.3/docs/visualizing_examples.ipynb",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/536#issuecomment-1106635811
Availability,error,error,"necessary for the TPU Node. I am still having an issue pointing deepvariant to the model hosted in the cloud. . I have tried using a model in the deepvariant bucket with the following command and model: ; gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.data-00000-of-00001; ```bash; docker run \; -v `pwd`:`pwd` -w `pwd` \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --call_variants_extra_args use_tpu=true,tpu_name=""variantcaller-node1"",tpu_zone=""europe-west4-a"" \; --customized_model ""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.data-00000-of-00001"" \; --model_type=WGS \; --ref=""input/data/${REF}"" \; --reads=""input/data/${BAM}"" \; --output_vcf=""output/${OUTPUT_VCF}"" \; --output_gvcf=""output/${OUTPUT_GVCF}"" \; --regions chr20 \; --num_shards=$(nproc) \; --intermediate_results_dir /output/intermediate_results_dir; ```. But I get the following error:; ```bash; I0527 20:42:08.331003 139757477517120 run_deepvariant.py:341] Creating a directory for intermediate results in /output/intermediate_results_dir; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 493, in <module>; app.run(main); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run; _run_main(main, args); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 467, in main; commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 382, in create_all_commands_and_logfiles; check_flags(); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 357, in check_flags; raise RuntimeError('The model files {}* do not exist. Potentially '; RuntimeError: The model files gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_sta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
Integrability,message,message,"gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt; [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore; sess.run(self.saver_def.restore_op_name,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run; result = self._run(None, fetches, feed_dict, options_ptr,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run; results = self._do_run(handle, final_targets, final_fetches,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run; return self._do_call(_run_fn, feeds, fetches, targets, options,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call; raise type(e)(node_def, op, message); tensorflow.python.framework.errors_impl.NotFoundError: From /job:tpu_worker/replica:0/task:0:; Unsuccessful TensorSliceReader constructor: Failed to find any matching files for gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt; [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.ex",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
Safety,predict,prediction,"_standard/model.ckpt; [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
Security,access,accessible,"; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 467, in main; commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 382, in create_all_commands_and_logfiles; check_flags(); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 357, in check_flags; raise RuntimeError('The model files {}* do not exist. Potentially '; RuntimeError: The model files gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.data-00000-of-00001* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open; ```. I also get the same error when hosting the model (renamed model.ckpt) in my personal GS bucket -- I have made the storage bucket read accessible to all users so the TPU should have access:; ```bash; docker run \; -v `pwd`:`pwd` -w `pwd` \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --call_variants_extra_args use_tpu=true,tpu_name=""variantcaller-node1"",tpu_zone=""europe-west4-a"" \; --customized_model ""gs://tpu-bwb/analysis-files/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"" \; --model_type=WGS \; --ref=""input/data/${REF}"" \; --reads=""input/data/${BAM}"" \; --output_vcf=""output/${OUTPUT_VCF}"" \; --output_gvcf=""output/${OUTPUT_GVCF}"" \; --regions chr20 \; --num_shards=$(nproc) \; --intermediate_results_dir /output/intermediate_results_dir. I0527 21:26:03.381308 140127359940416 run_deepvariant.py:341] Creating a directory for intermediate results in /output/intermediate_results_dir; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 493, in <module>; app.run(main); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run; _run_main(main, args); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/opt/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
Usability,simpl,simple,"low/python/training/saver.py"", line 607, in _get_saver_or_default; saver = Saver(sharded=True, allow_empty=True); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__; self.build(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build; self._build(self._filename, build_save=True, build_restore=True); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build; self.saver_def = self._builder._build_internal( # pylint: disable=protected-access; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal; restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps; self._AddRestoreOps(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps; all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore; return io_ops.restore_v2(filename_tensor, names, slices, dtypes); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2; _, _, _op, _outputs = _op_def_library._apply_op_helper(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper; op = g._create_op_internal(op_type_name, inputs, dtypes=None,; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal; ret = Operation(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__; self._traceback = tf_stack.extract_stack_for_node(self._c_op); ```. Is there something simple I am missing here? Thanks for the support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
Usability,feedback,feedback,"Hi @PlatonB ,; In v1.6, we have added documentation on how to run on MGI data. Please see the links ""Complete Genomics data: [T7 case study](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md); [G400 case study](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-g400-case-study.md)"" which was also linked from our GitHub main page. And let us know if you encounter any issues or have any feedback. Hopefully the customized models will give you better results!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/538#issuecomment-1817559748
Usability,clear,clear,"Hello @kishwarshafin, . Thank you for your response and for the useful tips!. It appears that I have several types of flags for my reads presenting no quality score: ; ```; 0; 16; 2048; 2064; 4; ```; All the above flags are also present in my reads having a quality score sequence, except for the flag `4` (= read unmapped), which is absent. Also, it seems that in this case flags should be written without the `0x` prefix, which converts them to hexadecimal when they are written in decimal in the sam file, as I understand. ; `0x16` in https://broadinstitute.github.io/picard/explain-flags.html output a flag that cannot be set when read is not paired, and my read are not paired. However, it now seems clear that something went wrong with the alignment since I have all types of reads with no quality score sequence, not only `0x904` type reads (which are read unmapped `0x4`, not primary alignment `0x100`, and supplementary alignment `0x800`).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539#issuecomment-1141201135
Usability,clear,clearly,"@ziphra ,. You are correct. It does seem like something went wrong with your mapping and you have reads without base-qualities. My suspicion was that the aligner was removing base-qualities from non-primary reads, but that's clearly not the case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539#issuecomment-1144255541
Usability,clear,clearly,"Hi @elcortegano . Just to clarify, are you referring to the QUAL field of the VCF (the 6th column of tab-file itself), or the GQ field (the 10th column of a single sample). If you are referring to QUAL as the 6th column, this observation is expected. QUAL measures the probability that the ALT field has at least one allele with the ALT base. So you can think of it as p(HET) + p(HOM), or alternatively as 1 - p(REF). For homozygous positions, they look more clearly non-reference as in many cases they may not have any reference bases. . Heterozygous positions likely have at least some evidence for the Ref allele, which suggests a higher probability that the position might be Ref. If you are interested in filtering, we often recommend that the GQ field in the samples is preferable, as this is a measure of the genotype call itself being correct. There may be some differences between HET and HOM for this due to differences in difficulty in making those types of calls. However, it should be lower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/547#issuecomment-1194472227
Testability,test,test,"@IndyHouseGuy , . Can you please do a simpler test? According to [this](https://stackoverflow.com/questions/50317119/docker-container-creating-directories-owned-by-root-i-need-them-owned-by-10001), there can be a number of things that might cause this behavior, including implicitly running docker as root in the system. In my local test, I have this behavior; ```; # Command 1; time docker run -it -v /data:/data \; google/deepvariant:0.9.0 \; mkdir /data/kishwar/test_ubuntu_docker. # Command 2; docker run -it -u `id -u`:`id -g` -v /data:/data \; google/deepvariant:0.9.0 \; mkdir /data/kishwar/test_ubuntu_docker_u; ```; Output:; ```; root root 4.0K Aug 5 14:55 test_ubuntu_docker/; shafin primarygroup 4.0K Aug 5 14:55 test_ubuntu_docker_u/; ```; Can you please run this and see if you get the same behavior?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550#issuecomment-1206552255
Usability,simpl,simpler,"@IndyHouseGuy , . Can you please do a simpler test? According to [this](https://stackoverflow.com/questions/50317119/docker-container-creating-directories-owned-by-root-i-need-them-owned-by-10001), there can be a number of things that might cause this behavior, including implicitly running docker as root in the system. In my local test, I have this behavior; ```; # Command 1; time docker run -it -v /data:/data \; google/deepvariant:0.9.0 \; mkdir /data/kishwar/test_ubuntu_docker. # Command 2; docker run -it -u `id -u`:`id -g` -v /data:/data \; google/deepvariant:0.9.0 \; mkdir /data/kishwar/test_ubuntu_docker_u; ```; Output:; ```; root root 4.0K Aug 5 14:55 test_ubuntu_docker/; shafin primarygroup 4.0K Aug 5 14:55 test_ubuntu_docker_u/; ```; Can you please run this and see if you get the same behavior?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550#issuecomment-1206552255
Deployability,release,release,"Unfortunately we don't have any documentation on the method except release notes. The main motivation was to reduce the runtime and simplify the pipeline for PacBio data. The phasing is generated from DeepVariant proposed candidates. Proposed candidates are generated by counting alleles at each position and applying heuristics to reduce the number of proposed candidates. The main differences between one-step phasing and WhatsHap are:. * One-step phasing uses a greedy algorithm that processes intervals of 25000 bases long at a time. Using a greedy algorithm makes it inferior to WhatsHap. Although, experiments showed that final DeepVariant accuracy only slightly suffers. * Another big difference is that one-step phasing uses ""noisy"" proposed candidates when WhatsHap is run on genotyped variants produced by running DeepVariant on unphased data. The code which performs the phasing operation is in https://github.com/google/deepvariant/blob/r1.4/deepvariant/direct_phasing.cc. . Please note that our the DeepVariant model in v1.4 is able to run on candidates phased by WhatsHap, and the model has similar performance. This can be done by adding flags to the step of make_examples if run separately. If it is of interest for you to run DeepVariant v1.4 using the WhatsHap flags instead of the direct phasing, we can provide you with instructions to do so.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/557#issuecomment-1228968407
Energy Efficiency,reduce,reduce,"Unfortunately we don't have any documentation on the method except release notes. The main motivation was to reduce the runtime and simplify the pipeline for PacBio data. The phasing is generated from DeepVariant proposed candidates. Proposed candidates are generated by counting alleles at each position and applying heuristics to reduce the number of proposed candidates. The main differences between one-step phasing and WhatsHap are:. * One-step phasing uses a greedy algorithm that processes intervals of 25000 bases long at a time. Using a greedy algorithm makes it inferior to WhatsHap. Although, experiments showed that final DeepVariant accuracy only slightly suffers. * Another big difference is that one-step phasing uses ""noisy"" proposed candidates when WhatsHap is run on genotyped variants produced by running DeepVariant on unphased data. The code which performs the phasing operation is in https://github.com/google/deepvariant/blob/r1.4/deepvariant/direct_phasing.cc. . Please note that our the DeepVariant model in v1.4 is able to run on candidates phased by WhatsHap, and the model has similar performance. This can be done by adding flags to the step of make_examples if run separately. If it is of interest for you to run DeepVariant v1.4 using the WhatsHap flags instead of the direct phasing, we can provide you with instructions to do so.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/557#issuecomment-1228968407
Performance,perform,performs,"Unfortunately we don't have any documentation on the method except release notes. The main motivation was to reduce the runtime and simplify the pipeline for PacBio data. The phasing is generated from DeepVariant proposed candidates. Proposed candidates are generated by counting alleles at each position and applying heuristics to reduce the number of proposed candidates. The main differences between one-step phasing and WhatsHap are:. * One-step phasing uses a greedy algorithm that processes intervals of 25000 bases long at a time. Using a greedy algorithm makes it inferior to WhatsHap. Although, experiments showed that final DeepVariant accuracy only slightly suffers. * Another big difference is that one-step phasing uses ""noisy"" proposed candidates when WhatsHap is run on genotyped variants produced by running DeepVariant on unphased data. The code which performs the phasing operation is in https://github.com/google/deepvariant/blob/r1.4/deepvariant/direct_phasing.cc. . Please note that our the DeepVariant model in v1.4 is able to run on candidates phased by WhatsHap, and the model has similar performance. This can be done by adding flags to the step of make_examples if run separately. If it is of interest for you to run DeepVariant v1.4 using the WhatsHap flags instead of the direct phasing, we can provide you with instructions to do so.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/557#issuecomment-1228968407
Usability,simpl,simplify,"Unfortunately we don't have any documentation on the method except release notes. The main motivation was to reduce the runtime and simplify the pipeline for PacBio data. The phasing is generated from DeepVariant proposed candidates. Proposed candidates are generated by counting alleles at each position and applying heuristics to reduce the number of proposed candidates. The main differences between one-step phasing and WhatsHap are:. * One-step phasing uses a greedy algorithm that processes intervals of 25000 bases long at a time. Using a greedy algorithm makes it inferior to WhatsHap. Although, experiments showed that final DeepVariant accuracy only slightly suffers. * Another big difference is that one-step phasing uses ""noisy"" proposed candidates when WhatsHap is run on genotyped variants produced by running DeepVariant on unphased data. The code which performs the phasing operation is in https://github.com/google/deepvariant/blob/r1.4/deepvariant/direct_phasing.cc. . Please note that our the DeepVariant model in v1.4 is able to run on candidates phased by WhatsHap, and the model has similar performance. This can be done by adding flags to the step of make_examples if run separately. If it is of interest for you to run DeepVariant v1.4 using the WhatsHap flags instead of the direct phasing, we can provide you with instructions to do so.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/557#issuecomment-1228968407
Usability,simpl,simply,"@aalfi ,. Please follow the instructions here: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md. I see that you have `--dry_run=true` which would not run anything and would simply print out the commands:; ```; --[no]dry_run: Optional. If True, only prints out commands without executing them.; (default: 'false'); ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/561#issuecomment-1234749060
Usability,feedback,feedback,"> I'm not very familiar with SINGULARITY_CACHEDIR. But, in your command, if you're running it 3 times, you should use a different --intermediate_results_dir. Output of `make_examples` will be written to that directory. So, if you use the same intermediate_results_dir, that might explain why your data is corrupted. Thank you. I will try and feedback to you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/564#issuecomment-1253259881
Usability,intuit,intuition,"Hi @SHuang-Broad . By default, DeepVariant only looks at the content of the QUAL field (column 11) in order to populate the quality values. DeepVariant is able to look at and read in arbitrary additional tags (e.g. we have used the HP tag for phasing in the past). We have not previously experimented with BAQ, but with the framework above it would not be hard to look at it if you have an intuition that it might help. If you think it is promising, we could either do this investigation ourselves, or we could try to give you some instructions on how to do an experimental training if you are interested. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/565#issuecomment-1251305150
Usability,feedback,feedback,"Hi @jkalleberg ,; please see See: https://gist.github.com/pichuan/7ad09bf1fa8f519facf6806eca835ea6. I'll close this issue for now. Feel free to open more issues if you have any questions or feedback for us.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/568#issuecomment-1256836258
Usability,learn,learned,"Hi @guandailu . We have investigated the use of the MarkDuplicates step. In WGS and exome samples we looked at, we observe no measurable difference at coverages 30x and higher. For coverages lower than 30x, we observe a very slight advantage for MarkDuplicates. For this reason in our [best practices recommendation](https://github.com/google/deepvariant/blob/r1.4/docs/trio-merge-case-study.md), we indicate duplicate marking as optional. . In terms of other preprocessing steps, we observe that running Base Quality Score Recalibrator (BQSR) consistently results in a slight reduction of accuracy (likely because DeepVariant has already learned how to use the underlying concept to recalibrate qualities as it considers calling). We have a strong recommendation NOT to run Variant Score Quality Recalibration (VQSR). We observe large reductions in accuracy, particularly in recall of rare variants. If you would like to filter for higher precision than the default calls, we recommend using the Genotype Quality (GQ) field, setting a higher threshold based on your desire for specificity.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/569#issuecomment-1264952905
Usability,feedback,feedback,"Hi @FarmOmics ,; I'll close this issue. We would love to hear your feedback about the RNAseq caller. Please don't hesitate to reach out again if you have more questions or feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/572#issuecomment-1284746356
Availability,error,error,"Hi @ankurc17 ; Can you tell us more about what the issues are?; For example, what OS are you using, what command did you run and what error messages you've seen.; It'll be great if we can assist you here, because then other users can learn from our conversation too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/574#issuecomment-1276221161
Integrability,message,messages,"Hi @ankurc17 ; Can you tell us more about what the issues are?; For example, what OS are you using, what command did you run and what error messages you've seen.; It'll be great if we can assist you here, because then other users can learn from our conversation too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/574#issuecomment-1276221161
Usability,learn,learn,"Hi @ankurc17 ; Can you tell us more about what the issues are?; For example, what OS are you using, what command did you run and what error messages you've seen.; It'll be great if we can assist you here, because then other users can learn from our conversation too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/574#issuecomment-1276221161
Deployability,update,update,"Hi @imdanique ,; Thanks for the update.; Can try to get to a point where when you run the docker command, your ls can see the file? Because if the ls command can't list the file, that means deepvariant binary would have no chance to find the file.; Does that make sense?; To diagnose the problem, maybe simplifying the script or script to remove the use of variables could help too, in case they were not set correctly. https://docs.docker.com/storage/volumes/ might also be helpful to read.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/577#issuecomment-1285622624
Modifiability,variab,variables,"Hi @imdanique ,; Thanks for the update.; Can try to get to a point where when you run the docker command, your ls can see the file? Because if the ls command can't list the file, that means deepvariant binary would have no chance to find the file.; Does that make sense?; To diagnose the problem, maybe simplifying the script or script to remove the use of variables could help too, in case they were not set correctly. https://docs.docker.com/storage/volumes/ might also be helpful to read.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/577#issuecomment-1285622624
Usability,simpl,simplifying,"Hi @imdanique ,; Thanks for the update.; Can try to get to a point where when you run the docker command, your ls can see the file? Because if the ls command can't list the file, that means deepvariant binary would have no chance to find the file.; Does that make sense?; To diagnose the problem, maybe simplifying the script or script to remove the use of variables could help too, in case they were not set correctly. https://docs.docker.com/storage/volumes/ might also be helpful to read.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/577#issuecomment-1285622624
Usability,guid,guides,"I am guessing it could be related to the use of the `--nv` flag ([docs](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html)), but I am not familiar with using GPUs with singularity containers. This [guide](https://modinst.lu.lv/wp-content/uploads/2021/03/Singularity_seminars_Aleksandrs_Gutcaits.pdf) suggests unsetting the `LD_LIBRARY_PATH`. Which version of singularity are you using?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/580#issuecomment-1304648870
Usability,feedback,feedback,"Great! Thank you for letting us know!; If you're interested in sharing more feedback to us, that will be great too. :); I'll close this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/581#issuecomment-1317489201
Usability,simpl,simply,"Hi @JakeHagen . I took one of our 50x NovaSeq samples that are 150bp PE reads and trimmed the reads into 4 WGS consisting of a sample each for:. 1. first 100bp of the reads; 2. last 100bp of the reads; 3. first 75bp of the reads; 4. last 75bp of the reads. I wasn't able to replicate the effect that you see in any off the output reports. In your approach, did you trim the reads from the end (simply truncating to the first 75bp)? If so, I wasn't able to replicate the effect you see. There might be something more complicated about your sample. One possible explanation is that you have a run with lower sequencing quality and trimming to the first 75bp reads removes some lower quality parts which look suspicious to DeepVariant. If so, I wonder if your results would differ if you retained only the last 75bp reads. But I am not quite sure how to further diagnose. Here are my plots:. <img width=""1264"" alt=""HG003 novaseq 50x_only_first75bp"" src=""https://user-images.githubusercontent.com/583711/205179365-6a05941b-b6ba-47fc-b778-f970a9850651.png"">. <img width=""1266"" alt=""HG003 novaseq 50x_only_last75bp"" src=""https://user-images.githubusercontent.com/583711/205179387-7814e398-1b48-4476-9d35-6f93fafa8fbd.png"">. <img width=""1267"" alt=""HG003 novaseq 50x_only_first100bp"" src=""https://user-images.githubusercontent.com/583711/205179398-b2841a19-cb63-4ea1-9c7e-dfd79fad774d.png"">. <img width=""1286"" alt=""HG003 novaseq 50x_only_last100bp"" src=""https://user-images.githubusercontent.com/583711/205179415-a723bab5-e011-421b-a6a5-a7a750871159.png"">",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586#issuecomment-1334570619
Energy Efficiency,reduce,reduces,"Hi @JakeHagen . I will take a look at running a similar analysis on our exome samples. I suppose one remaining possibility is that the truncation of the reads reduces how far beyond the capture region the sequencing is getting. The edges of the capture region tend to both have less coverage and it's harder to sample both alleles. That's just a guess, I don't have a clear answer and will still try to collect more data. When you run DeepVariant for the exome, do you restrict to the capture regions only and do you add any padding to those?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586#issuecomment-1341790338
Usability,clear,clear,"Hi @JakeHagen . I will take a look at running a similar analysis on our exome samples. I suppose one remaining possibility is that the truncation of the reads reduces how far beyond the capture region the sequencing is getting. The edges of the capture region tend to both have less coverage and it's harder to sample both alleles. That's just a guess, I don't have a clear answer and will still try to collect more data. When you run DeepVariant for the exome, do you restrict to the capture regions only and do you add any padding to those?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586#issuecomment-1341790338
Availability,down,down,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/587#issuecomment-1319519059
Deployability,pipeline,pipeline,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/587#issuecomment-1319519059
Safety,detect,detect,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/587#issuecomment-1319519059
Usability,resume,resume,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/587#issuecomment-1319519059
Deployability,install,install,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that?; > ; > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. Sorry, I am a beginner of Docker. I've learned something about docker technology in the past two days, figuring out what you mean. Now I can build an iamge that does not depend on operating system of my device. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591#issuecomment-1334703761
Integrability,rout,route,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that?; > ; > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. Sorry, I am a beginner of Docker. I've learned something about docker technology in the past two days, figuring out what you mean. Now I can build an iamge that does not depend on operating system of my device. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591#issuecomment-1334703761
Testability,test,test,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that?; > ; > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. Sorry, I am a beginner of Docker. I've learned something about docker technology in the past two days, figuring out what you mean. Now I can build an iamge that does not depend on operating system of my device. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591#issuecomment-1334703761
Usability,learn,learned,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that?; > ; > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. Sorry, I am a beginner of Docker. I've learned something about docker technology in the past two days, figuring out what you mean. Now I can build an iamge that does not depend on operating system of my device. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591#issuecomment-1334703761
Safety,detect,detect," GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the BAM file here (maybe +/- 500 bp from the position). People in the team would be interested in whether this could detect any sort of edge case DeepVariant isn't handling well. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592#issuecomment-1332875716
Usability,learn,learning," GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the BAM file here (maybe +/- 500 bp from the position). People in the team would be interested in whether this could detect any sort of edge case DeepVariant isn't handling well. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592#issuecomment-1332875716
Usability,learn,learned,"Hi @li1ba . For the question about why does the model have a difficult time calling this 1/1 versus 0/1, we have done further investigation. First, it doesn't look like this is a bug in pre-processing or how the data is represented. It seems to be a property of the model. . Second, the property of the model seems to reflect something learned about exome sequencing as opposed to WGS. To determine that, we ran your snippet through both the WGS and WES models. The WGS model is able to confidently call this site as 1/1:. ```; chr2 24146804 . C T 34.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:34:162:0,162:1:34,45,0; ```. When we run the WES model, we replicate your finding (this is with the most recent DeepVariant v1.4, so there is a small difference in the GQ values. ; ```; chr2 24146804 . C T 29.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:162:0,162:1:26,0,1; ```. This suggests that there is some aspect of exome sequencing that the DeepVariant WES model has learned makes this variant difficult to genotype, possibly because there is some signal that indicates only one allele is present. The reason for this might be some factor which isn't understood (at least by me). This is an interesting observation, but really understanding the reason for a 0/1 call at this position would probably need more investigation (for example, by going into the GIAB training labels for exome sequencing and seeing how often positions that look like this are 0/1 versus 1/1 and trying to understand why). With respect to why your collaborator has different results from you, it's very likely that the difference in mappers has a small effect on which reads are present and how many map discordantly. This small difference pushes the output for the variant on the edge of probabilities between 0/1 an 1/1 to the other side.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592#issuecomment-1335960453
Integrability,message,message,"Hi, it seems like you're using the openvino flag. Please remove that flag and try again.; For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597#issuecomment-1350569462
Usability,clear,clear,"Hi, it seems like you're using the openvino flag. Please remove that flag and try again.; For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597#issuecomment-1350569462
Integrability,message,message,"Thanks for the feedback, I’m using a published docker image 1.4.0 which also latest at the time of this posting.How do I disable openvino?On Dec 13, 2022, at 11:51 PM, Pi-Chuan Chang ***@***.***> wrote:﻿; Hi, it seems like you're using the openvino flag. Please remove that flag and try again.; For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup.; In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag); Please let me know if it works after you remove the openvino flag. —Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you modified the open/close state.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597#issuecomment-1350769545
Usability,feedback,feedback,"Thanks for the feedback, I’m using a published docker image 1.4.0 which also latest at the time of this posting.How do I disable openvino?On Dec 13, 2022, at 11:51 PM, Pi-Chuan Chang ***@***.***> wrote:﻿; Hi, it seems like you're using the openvino flag. Please remove that flag and try again.; For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup.; In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag); Please let me know if it works after you remove the openvino flag. —Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you modified the open/close state.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597#issuecomment-1350769545
Availability,error,error,"Thank you for your reply!; For the following code, I don't have permission to modify the path `/usr/bin/`, and I didn't find `/usr/bin/python3` from `subprocess.py`, so I didn't fix the error either.; As a study student, I am working hard to learn to solve these mistakes. It hasn't been resolved yet.; ```; File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 1364, in _execute_child; raise child_exception_type(errno_num, err_msg, err_filename); FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598#issuecomment-1356305359
Usability,learn,learn,"Thank you for your reply!; For the following code, I don't have permission to modify the path `/usr/bin/`, and I didn't find `/usr/bin/python3` from `subprocess.py`, so I didn't fix the error either.; As a study student, I am working hard to learn to solve these mistakes. It hasn't been resolved yet.; ```; File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 1364, in _execute_child; raise child_exception_type(errno_num, err_msg, err_filename); FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598#issuecomment-1356305359
Usability,clear,clear,Closing this. DeepVariant ran 'successfully' after splitting fastq files - although the output was bizarre. It is pretty clear that using DeepVariant with older PacBio data is not worthwhile -- my impression is that older PacBio data is only useful for finding larger SVs and worth using only if there is nothing better (e.g. PacBio HiFi or ONT).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614#issuecomment-1457774639
Availability,down,downstream,"Hi @fardokhtsadat . I realize in looking at the VCF entry, I was mistaken in saying DeepVariant sees a SNP and an Indel. In fact, it sees the candidates as you wrote. CAGCAGCGCT -> C; CAGCAGCGCT -> T. That's entirely on me, it should be clear from the VCF, and I don't have an excuse to make such a basic mistake. The call looks correct, it does look like a HET deletion of AGCAGCGCT. It looks like the realigner has decided to place a gap at this position and left-align the trailing T at the edge of the deletion so that when it is present it looks like a SNP. It's hard to authoritatively say why the realigner is going something without getting the actual BAM snippet and looking at the realignment. That part is in the heuristics of DeepVariant not the neural net. As a result of the re-alignment, DeepVariant sees the T SNP as a candidate but seems to correctly reject it. I think that's what's going on. In any case, a candidate here for the SNP seems to be a function of the realigner? . The behavior should not have changed from DeepVariant v1.2 to v1.5. In general, uncalled multiallelic events should be a relatively rare phenomenon. . Generally, if you need to, you can prune the alleles that are not called in DeepVariant VCF as long as the result of pruning stays compliant with the VCF spec expected by any downstream tools.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618#issuecomment-1470986150
Usability,clear,clear,"Hi @fardokhtsadat . I realize in looking at the VCF entry, I was mistaken in saying DeepVariant sees a SNP and an Indel. In fact, it sees the candidates as you wrote. CAGCAGCGCT -> C; CAGCAGCGCT -> T. That's entirely on me, it should be clear from the VCF, and I don't have an excuse to make such a basic mistake. The call looks correct, it does look like a HET deletion of AGCAGCGCT. It looks like the realigner has decided to place a gap at this position and left-align the trailing T at the edge of the deletion so that when it is present it looks like a SNP. It's hard to authoritatively say why the realigner is going something without getting the actual BAM snippet and looking at the realignment. That part is in the heuristics of DeepVariant not the neural net. As a result of the re-alignment, DeepVariant sees the T SNP as a candidate but seems to correctly reject it. I think that's what's going on. In any case, a candidate here for the SNP seems to be a function of the realigner? . The behavior should not have changed from DeepVariant v1.2 to v1.5. In general, uncalled multiallelic events should be a relatively rare phenomenon. . Generally, if you need to, you can prune the alleles that are not called in DeepVariant VCF as long as the result of pruning stays compliant with the VCF spec expected by any downstream tools.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618#issuecomment-1470986150
Deployability,install,installed,"Do you have CUDA installed on your machine?. Check whether CUDA is installed on your machine. For example, run:; ```; rpm -qa | grep cuda; ```; or; ```; nvcc --version; ```. If you don't have CUDA installed, please follow the instructions on https://docs.nvidia.com/cuda/cuda-installation-guide-linux/ to make sure you install it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471268664
Usability,guid,guide-linux,"Do you have CUDA installed on your machine?. Check whether CUDA is installed on your machine. For example, run:; ```; rpm -qa | grep cuda; ```; or; ```; nvcc --version; ```. If you don't have CUDA installed, please follow the instructions on https://docs.nvidia.com/cuda/cuda-installation-guide-linux/ to make sure you install it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471268664
Usability,feedback,feedback,Excellent! Please let us know if you have any other feedback. I will close this issue now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624#issuecomment-1496232959
Deployability,update,update,Thanks for the update.The differences could be related to a change in the Tensorflow version. We welcome any additional feedback you have.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624#issuecomment-1499324669
Usability,feedback,feedback,Thanks for the update.The differences could be related to a change in the Tensorflow version. We welcome any additional feedback you have.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624#issuecomment-1499324669
Modifiability,extend,extend,"Hi Saurabh,. Currently the `run_deepvariant_keras` is experimental as we plan to develop this further in the future. The model for `run_deepvariant` is for tf-slim and would not simply extend to keras. For now, you should use `run_deepvariant` as that's the one we officially support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/636#issuecomment-1520586245
Usability,simpl,simply,"Hi Saurabh,. Currently the `run_deepvariant_keras` is experimental as we plan to develop this further in the future. The model for `run_deepvariant` is for tf-slim and would not simply extend to keras. For now, you should use `run_deepvariant` as that's the one we officially support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/636#issuecomment-1520586245
Availability,error,error,"@nvnieuwk from my understanding, I would be surprised if the numpy error is related to the region. I feel that might be a red herring. Unfortunately I don't have a clear answer for you because I can't reproduce your exact setting.; If you think it can still be related to the small region, the next thing I'd suggest you try is: Start from the setting that worked before (CRAM file that contains the whole chromosome 21). First confirm that still works, and just change one thing: restrict to a smaller region and see if that still works or fails with the numpy error again. Sorry that I don't have better suggestions for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640#issuecomment-1550815697
Usability,clear,clear,"@nvnieuwk from my understanding, I would be surprised if the numpy error is related to the region. I feel that might be a red herring. Unfortunately I don't have a clear answer for you because I can't reproduce your exact setting.; If you think it can still be related to the small region, the next thing I'd suggest you try is: Start from the setting that worked before (CRAM file that contains the whole chromosome 21). First confirm that still works, and just change one thing: restrict to a smaller region and see if that still works or fails with the numpy error again. Sorry that I don't have better suggestions for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640#issuecomment-1550815697
Usability,feedback,feedback,Hello @pichuan I will try your suggestion and let you know the feedback. As to the SIF conversion its just simple command using singularity (version 3.7.2) pull docker://google/deepvariant:1.4 (Now I use 1.5).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646#issuecomment-1547543795
Usability,clear,clear,"Hello Pi-Chuan, thanks for answer above. Just to make it clear to me... I obtained a VCF file using version 1.4.0, but I cannot see the phase data (e.g., ""|"") in my sample. Does this mean that I need to call variants again using version 1.5.0 and the ""--phase_reads"" flag? Or does DeepVariant not explicitly provide this data, requiring me to run Whatshap + DeepVariant to visualize it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649#issuecomment-1547108448
Usability,clear,clear,"Sorry for the confusion here. We're phasing the reads internally to help with DeepVariant accuracy. (""Direct"" is referring to that we're doing this now directly in DeepVariant instead of relying on external HP tags from other tools like WhatsHap). However, DeepVariant currently still doesn't generate phased variant calls. Therefore you won't see ""|"" in the VCF files. Hopefully this is more clear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649#issuecomment-1547305204
Deployability,integrat,integrate,"'numpy.random.mtrand.RandomState' objects}; 5759 0.094 0.000 0.937 0.000 <frozen importlib._bootstrap_external>:1498(find_spec); 23926 0.089 0.000 0.109 0.000 /usr/lib/python3.8/enum.py:313(__call__); 3136 0.089 0.000 1.439 0.000 <frozen importlib._bootstrap_external>:1034(get_data); 473 0.083 0.000 0.120 0.000 <frozen importlib._bootstrap_external>:1317(_path_hooks); 1417/288 0.077 0.000 0.151 0.001 /usr/lib/python3.8/sre_parse.py:493(_parse); 4910 0.069 0.000 0.069 0.000 {method 'PythonNext' of 'third_party.nucleus.io.python.sam_reader.SamIterable' objects}; 3614/1 0.069 0.000 11.965 11.965 {built-in method builtins.exec}; 701 0.066 0.000 0.066 0.000 {built-in method posix.listdir}; 15152 0.063 0.000 0.268 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:69(_wrapreduction); 32755 0.062 0.000 0.135 0.000 <frozen importlib._bootstrap_external>:121(_path_join); 171/133 0.060 0.000 0.178 0.001 {built-in method _imp.exec_dynamic}; 1 0.059 0.059 0.059 0.059 {method 'calls_from_allele_counts' of 'deepvariant.python.variant_calling_multisample.VariantCaller' objects}; 23323 0.058 0.000 0.163 0.000 /usr/lib/python3.8/inspect.py:2489(__init__); 15563/15359 0.053 0.000 0.384 0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}; 3601 0.053 0.000 0.344 0.000 /usr/lib/python3.8/inspect.py:1102(getfullargspec); 15152 0.051 0.000 0.319 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2703(amax); 12155 0.049 0.000 0.060 0.000 {built-in method builtins.max}; 40200 0.049 0.000 0.049 0.000 {built-in method third_party.nucleus.util.python.utils.read_overlaps_region}; ```. In any case, the most optimized approach is some sort shared memory model with minimal overhead to keep the code nimble and easy to integrate or extend. The code itself is pleasantly easy to go through given the nice design, which makes it quite amendable to that. In the meantime, pass-by-reference probably could be an easier win. . Thank you,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826
Energy Efficiency,reduce,reduced,"=========; str | 223354 | 35.79 MB; dict | 88941 | 25.94 MB; code | 50107 | 8.54 MB; type | 6121 | 5.65 MB; tuple | 63884 | 3.62 MB; list | 30942 | 3.19 MB; set | 2864 | 1.51 MB; weakref | 14251 | 1002.02 KB; abc.ABCMeta | 784 | 826.05 KB; cell | 20911 | 816.84 KB; int | 25259 | 697.77 KB; builtin_function_or_method | 8801 | 618.82 KB; google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB; frozenset | 1862 | 541.02 KB; function (__init__) | 3439 | 456.74 KB; ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```; Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time; List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function); 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}; 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.client._pywrap_tf_session.TF_CloseSession}; 434721 0.900 0.000 0.913 0.000 {built-in method builtins.hasattr}; 241294 0.868 0.000 3.111 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5866(get_controller); 241296 0.462 0.000 0.462 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:932(_mode); 413316 0.452 0.000 0.541 0.000 /usr/lib/python3.8/contextlib.py:82(__init__); 9 0.409 0.045 0.409 0.045 {built-in method tensorflow.python.client._pywrap_tf_session.ExtendSession}; 413315/164299 0.370 0.000 1.371 0.000 /usr/lib/python3.8/contextlib.py:117(__exit__); 825922/196 0.347 0.000 61.452 0.314 {built-in method builtins.next}; 113550 0.334 0.000 0.646 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826
Integrability,integrat,integrate,"'numpy.random.mtrand.RandomState' objects}; 5759 0.094 0.000 0.937 0.000 <frozen importlib._bootstrap_external>:1498(find_spec); 23926 0.089 0.000 0.109 0.000 /usr/lib/python3.8/enum.py:313(__call__); 3136 0.089 0.000 1.439 0.000 <frozen importlib._bootstrap_external>:1034(get_data); 473 0.083 0.000 0.120 0.000 <frozen importlib._bootstrap_external>:1317(_path_hooks); 1417/288 0.077 0.000 0.151 0.001 /usr/lib/python3.8/sre_parse.py:493(_parse); 4910 0.069 0.000 0.069 0.000 {method 'PythonNext' of 'third_party.nucleus.io.python.sam_reader.SamIterable' objects}; 3614/1 0.069 0.000 11.965 11.965 {built-in method builtins.exec}; 701 0.066 0.000 0.066 0.000 {built-in method posix.listdir}; 15152 0.063 0.000 0.268 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:69(_wrapreduction); 32755 0.062 0.000 0.135 0.000 <frozen importlib._bootstrap_external>:121(_path_join); 171/133 0.060 0.000 0.178 0.001 {built-in method _imp.exec_dynamic}; 1 0.059 0.059 0.059 0.059 {method 'calls_from_allele_counts' of 'deepvariant.python.variant_calling_multisample.VariantCaller' objects}; 23323 0.058 0.000 0.163 0.000 /usr/lib/python3.8/inspect.py:2489(__init__); 15563/15359 0.053 0.000 0.384 0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}; 3601 0.053 0.000 0.344 0.000 /usr/lib/python3.8/inspect.py:1102(getfullargspec); 15152 0.051 0.000 0.319 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2703(amax); 12155 0.049 0.000 0.060 0.000 {built-in method builtins.max}; 40200 0.049 0.000 0.049 0.000 {built-in method third_party.nucleus.util.python.utils.read_overlaps_region}; ```. In any case, the most optimized approach is some sort shared memory model with minimal overhead to keep the code nimble and easy to integrate or extend. The code itself is pleasantly easy to go through given the nice design, which makes it quite amendable to that. In the meantime, pass-by-reference probably could be an easier win. . Thank you,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826
Modifiability,rewrite,rewrite,"thon3.8/dist-packages/tensorflow/python/framework/ops.py:5351(_device_function_stack); 58954 0.117 0.000 0.200 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/dtypes.py:719(as_dtype); 6669 0.113 0.000 0.118 0.000 /usr/local/lib/python3.8/dist-packages/gast/gast.py:15(create_node); 155316 0.108 0.000 0.183 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6314(get_default_graph); 6015 0.105 0.000 0.105 0.000 {built-in method tensorflow.python.util._tf_stack.extract_stack_for_op}; 6015 0.104 0.000 0.149 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1879(_NodeDef); ```. What I'm seeing above is something quite interesting. The execution seems heavy on [eager execution rather than graph execution](https://blog.tensorflow.org/2018/08/code-with-eager-execution-run-with-graphs.html), which could be the culprit. The thing is that there are some quick ways now to make it fast with almost no major code-rewrite, though there are other natural optimizations that are relatively obvious given the analysis. For the `make_examples` analysis, the task is much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes more heavily used):. ```; Wed May 17 17:51:33 2023 make_examples_profile.txt. 2790682 function calls (2732833 primitive calls) in 11.860 seconds. Ordered by: internal time; List reduced from 11503 to 100 due to restriction <100>. ncalls tottime percall cumtime percall filename:lineno(function); 171/168 1.927 0.011 1.940 0.012 {built-in method _imp.create_dynamic}; 3159 0.930 0.000 0.930 0.000 {method 'read' of '_io.BufferedReader' objects}; 19018 0.694 0.000 0.694 0.000 {built-in method posix.stat}; 2 0.586 0.293 0.586 0.293 {method 'counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}; 3136 0.475 0.000 0.47",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826
Performance,perform,performance,"Hi Andrew and Steve,. That's a good idea, and probably shifting to pass-by-reference might minimize the overhead between functions or even through minimizing the number of functions. A shared memory approach operated upon by the different functions could be ideal with maximizing performance, where even utilizing [NVidia's Unified Virtual Memory (UVM)](https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/) could help, but requires some care. Currently the `call_variants` memory profile looks something like this, with minimal changes across the running process:. ```; types | # objects | total size; ================================================================ | =========== | ============; str | 223354 | 35.79 MB; dict | 88941 | 25.94 MB; code | 50107 | 8.54 MB; type | 6121 | 5.65 MB; tuple | 63884 | 3.62 MB; list | 30942 | 3.19 MB; set | 2864 | 1.51 MB; weakref | 14251 | 1002.02 KB; abc.ABCMeta | 784 | 826.05 KB; cell | 20911 | 816.84 KB; int | 25259 | 697.77 KB; builtin_function_or_method | 8801 | 618.82 KB; google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB; frozenset | 1862 | 541.02 KB; function (__init__) | 3439 | 456.74 KB; ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```; Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time; List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function); 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}; 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.clie",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826
Usability,simpl,simpler,"/local/lib/python3.8/dist-packages/gast/gast.py:15(create_node); 155316 0.108 0.000 0.183 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6314(get_default_graph); 6015 0.105 0.000 0.105 0.000 {built-in method tensorflow.python.util._tf_stack.extract_stack_for_op}; 6015 0.104 0.000 0.149 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1879(_NodeDef); ```. What I'm seeing above is something quite interesting. The execution seems heavy on [eager execution rather than graph execution](https://blog.tensorflow.org/2018/08/code-with-eager-execution-run-with-graphs.html), which could be the culprit. The thing is that there are some quick ways now to make it fast with almost no major code-rewrite, though there are other natural optimizations that are relatively obvious given the analysis. For the `make_examples` analysis, the task is much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes more heavily used):. ```; Wed May 17 17:51:33 2023 make_examples_profile.txt. 2790682 function calls (2732833 primitive calls) in 11.860 seconds. Ordered by: internal time; List reduced from 11503 to 100 due to restriction <100>. ncalls tottime percall cumtime percall filename:lineno(function); 171/168 1.927 0.011 1.940 0.012 {built-in method _imp.create_dynamic}; 3159 0.930 0.000 0.930 0.000 {method 'read' of '_io.BufferedReader' objects}; 19018 0.694 0.000 0.694 0.000 {built-in method posix.stat}; 2 0.586 0.293 0.586 0.293 {method 'counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}; 3136 0.475 0.000 0.475 0.000 {built-in method marshal.loads}; 3136 0.422 0.000 0.422 0.000 {built-in method io.open_code}; 15152 0.183 0.000 0.183 0.000 {method 'reduce' of 'numpy.ufunc' objects}; 3136 0.164 0.000 2.265 0.001 <frozen importlib._bootstrap_external>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826
Usability,feedback,feedback,Thanks for all the feedback and discussion above! I'll close this issue now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1559918325
Testability,test,tested,"@crazysummerW my intuition is that it would not make a big difference, but we have not empirically tested this. My guess is that It could make a difference in variants observed in regions with lower coverage. Here is a similar issue (although, in the context of DNA-seq): https://github.com/google/deepvariant/issues/384",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/652#issuecomment-1555394452
Usability,intuit,intuition,"@crazysummerW my intuition is that it would not make a big difference, but we have not empirically tested this. My guess is that It could make a difference in variants observed in regions with lower coverage. Here is a similar issue (although, in the context of DNA-seq): https://github.com/google/deepvariant/issues/384",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/652#issuecomment-1555394452
Testability,test,testing,"Hi @Zjianglin , I took a quick look of the script and I'm not sure I fully understand what you're testing here. I tried a simplified version on my side. (The following steps has nothing to do with DeepVariant anymore. I'm mostly just testing `ls` and `singularity` right now). First I got these files in my /tmp; ```; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; ```. Then I made my deepvariant.sif. ```; singularity build deepvariant.sif docker://google/deepvariant:1.5.0; ```. First, I check that I have the files; ```; REF=/tmp/ucsc.hg19.chr20.unittest.fasta; ls -al ${REF}*; ```; This worked.; (Note, you were doing something like `ls -al ""${ref_idx}*""`. Don't add the double quotes around the *. That didn't work for me. ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.sif \; ls -al ${REF}*; ```. This also worked fine for me. I can see the files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1573188242
Usability,simpl,simplified,"Hi @Zjianglin , I took a quick look of the script and I'm not sure I fully understand what you're testing here. I tried a simplified version on my side. (The following steps has nothing to do with DeepVariant anymore. I'm mostly just testing `ls` and `singularity` right now). First I got these files in my /tmp; ```; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; ```. Then I made my deepvariant.sif. ```; singularity build deepvariant.sif docker://google/deepvariant:1.5.0; ```. First, I check that I have the files; ```; REF=/tmp/ucsc.hg19.chr20.unittest.fasta; ls -al ${REF}*; ```; This worked.; (Note, you were doing something like `ls -al ""${ref_idx}*""`. Don't add the double quotes around the *. That didn't work for me. ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.sif \; ls -al ${REF}*; ```. This also worked fine for me. I can see the files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1573188242
Usability,guid,guides,"@Zjianglin So the last thing you are missing is that you are not binding your `lustre` folder. Basically the `/lustre/Data/toolsDB/HostRefs/Human_hs37d5/` folder is not seen within your Singularity container at runtime, so you will need to bind that folder like this:. ```; singularity run -B /lustre/Data/toolsDB/HostRefs/Human_hs37d5/:/lustre/Data/toolsDB/HostRefs/Human_hs37d5/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/*; ```. For more information on binding paths, below is a link that expands on how it works:. https://docs.sylabs.io/guides/3.0/user-guide/bind_paths_and_mounts.html",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1575943033
Availability,down,downsample," to this variant call, there are a few things that I see: . First, DeepVariant has a very low confidence in this call. The GQ of 4 corresponds to a 40% chance of being incorrect. Interestingly, the second most likely call at this position according to DeepVariant is 0/0 (HomRef). This is in spite of a 0.27 variant allele frequency (something that most callers would probably consider as either 0/0 HomRef or 0/1 HET). . This is an indication that DeepVariant may think that one of the haplotypes (either the Ref one or the Alt one) are unreliable (e.g. that they are reads which map from a different part of the genome), but doesn't know which to consider. Some other lines of evidence DeepVariant might use for that is the higher coverage (1200 is a coverage that would often be seen in duplicated parts of the genome) and the unusual allele frequency ratio (70% Ref, 30% Alt). Another piece of evidence we can't see but DeepVariant may use is the Insert Size channel if this is Illumina data. . A few questions -. 1) What is the sequencing technology used here, and which type of instrument. Is this PacBio or Illumina here? ; 2) Is this some form of panel sequencing targeting the region? . One suggestion to try (especially if this is panel short read sequencing) - downsample the region to ~80-100 coverage and see if the call changes. Especially look for the GQ confidence to go up as 4 is very low. If you want to avoid such situations more, you might want to put a higher GQ threshold for downstream filtering, and for a specific case like this look for cases where the PL values show higher probability for HOMREF and ALT than HET. A GQ threshold of 10 would be a 90% probability the call is correct, a GQ threshold of 20 would be a 99% threshold. . Just from the evidence presented here, this site is going to be difficult to call, as just on the VAF this look more like a HET (though the other factors mentioned here about copy number variants could be the factor in allowing a REF call.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655#issuecomment-1570674832
Safety,avoid,avoid," to this variant call, there are a few things that I see: . First, DeepVariant has a very low confidence in this call. The GQ of 4 corresponds to a 40% chance of being incorrect. Interestingly, the second most likely call at this position according to DeepVariant is 0/0 (HomRef). This is in spite of a 0.27 variant allele frequency (something that most callers would probably consider as either 0/0 HomRef or 0/1 HET). . This is an indication that DeepVariant may think that one of the haplotypes (either the Ref one or the Alt one) are unreliable (e.g. that they are reads which map from a different part of the genome), but doesn't know which to consider. Some other lines of evidence DeepVariant might use for that is the higher coverage (1200 is a coverage that would often be seen in duplicated parts of the genome) and the unusual allele frequency ratio (70% Ref, 30% Alt). Another piece of evidence we can't see but DeepVariant may use is the Insert Size channel if this is Illumina data. . A few questions -. 1) What is the sequencing technology used here, and which type of instrument. Is this PacBio or Illumina here? ; 2) Is this some form of panel sequencing targeting the region? . One suggestion to try (especially if this is panel short read sequencing) - downsample the region to ~80-100 coverage and see if the call changes. Especially look for the GQ confidence to go up as 4 is very low. If you want to avoid such situations more, you might want to put a higher GQ threshold for downstream filtering, and for a specific case like this look for cases where the PL values show higher probability for HOMREF and ALT than HET. A GQ threshold of 10 would be a 90% probability the call is correct, a GQ threshold of 20 would be a 99% threshold. . Just from the evidence presented here, this site is going to be difficult to call, as just on the VAF this look more like a HET (though the other factors mentioned here about copy number variants could be the factor in allowing a REF call.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655#issuecomment-1570674832
Usability,clear,clear,"Hi @crazysummerW . Thank you for the pileup and for the trace. The trace makes it clear there isn't a variant here. . With respect to this variant call, there are a few things that I see: . First, DeepVariant has a very low confidence in this call. The GQ of 4 corresponds to a 40% chance of being incorrect. Interestingly, the second most likely call at this position according to DeepVariant is 0/0 (HomRef). This is in spite of a 0.27 variant allele frequency (something that most callers would probably consider as either 0/0 HomRef or 0/1 HET). . This is an indication that DeepVariant may think that one of the haplotypes (either the Ref one or the Alt one) are unreliable (e.g. that they are reads which map from a different part of the genome), but doesn't know which to consider. Some other lines of evidence DeepVariant might use for that is the higher coverage (1200 is a coverage that would often be seen in duplicated parts of the genome) and the unusual allele frequency ratio (70% Ref, 30% Alt). Another piece of evidence we can't see but DeepVariant may use is the Insert Size channel if this is Illumina data. . A few questions -. 1) What is the sequencing technology used here, and which type of instrument. Is this PacBio or Illumina here? ; 2) Is this some form of panel sequencing targeting the region? . One suggestion to try (especially if this is panel short read sequencing) - downsample the region to ~80-100 coverage and see if the call changes. Especially look for the GQ confidence to go up as 4 is very low. If you want to avoid such situations more, you might want to put a higher GQ threshold for downstream filtering, and for a specific case like this look for cases where the PL values show higher probability for HOMREF and ALT than HET. A GQ threshold of 10 would be a 90% probability the call is correct, a GQ threshold of 20 would be a 99% threshold. . Just from the evidence presented here, this site is going to be difficult to call, as just on the VAF this loo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655#issuecomment-1570674832
Deployability,install,installed,"@pgrosu Thank you for your response.; When you say ""the cloud"" do you mean to run it on a server/a super computer?; I predicted that I will need root access (sudo) which I don't have.; 1. Is there a way to do this? Docker is not even installed there. 2. So are you suggesting that we should not use mac (apple silicon)?. The ubuntu 20.04 that was installed on my mac (using UTM), should this work?. Thank you for your time and guidance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575677888
Safety,predict,predicted,"@pgrosu Thank you for your response.; When you say ""the cloud"" do you mean to run it on a server/a super computer?; I predicted that I will need root access (sudo) which I don't have.; 1. Is there a way to do this? Docker is not even installed there. 2. So are you suggesting that we should not use mac (apple silicon)?. The ubuntu 20.04 that was installed on my mac (using UTM), should this work?. Thank you for your time and guidance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575677888
Security,access,access,"@pgrosu Thank you for your response.; When you say ""the cloud"" do you mean to run it on a server/a super computer?; I predicted that I will need root access (sudo) which I don't have.; 1. Is there a way to do this? Docker is not even installed there. 2. So are you suggesting that we should not use mac (apple silicon)?. The ubuntu 20.04 that was installed on my mac (using UTM), should this work?. Thank you for your time and guidance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575677888
Usability,guid,guidance,"@pgrosu Thank you for your response.; When you say ""the cloud"" do you mean to run it on a server/a super computer?; I predicted that I will need root access (sudo) which I don't have.; 1. Is there a way to do this? Docker is not even installed there. 2. So are you suggesting that we should not use mac (apple silicon)?. The ubuntu 20.04 that was installed on my mac (using UTM), should this work?. Thank you for your time and guidance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575677888
Availability,error,errors,"This is very good! . #### For Singularity . You can take a look at the following two links:. https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-quick-start.md#notes-on-singularity. https://github.com/google/deepvariant/blob/r1.5/scripts/install_singularity.sh. #### For your Ubuntu instance . You are getting very close! To simplify the install in the `run-prereq.sh` file you can comment out (with the `#` symbol) the following sections:. 1) For the ""Install TensorFlow pip package"" keep only the ones with **CPU-only**, and comment out the others. 2) For ""Install CUDA"", comment out everthing. 3) For ""Install TensorRT"", comment out everthing. And then run it again. The rest of the errors in the `run-prereq.sh` are easy to fix, which we can do later individually by removing each one, and installing the minimum required version. Before we fix `clif`, could you tell me what you get for the following:. ```; lsb_release -sc. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - . add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". sudo apt-get update. sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; ```. You might have a mismatch of a previous version of `clif` or its installed configuration files. You can check that via the following commands:. ```; llvm-config-11 --version; ```. The configs might be an older version, which you can check via the following:. ```; cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; ```. Below is what I have:. ```; $ cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; cat: /lib/llvm-11/cmake/LLVMConfig.cmake: No such file or directory; $; ```. If you have a mismatch between the version and config,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575894236
Deployability,install,install,"This is very good! . #### For Singularity . You can take a look at the following two links:. https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-quick-start.md#notes-on-singularity. https://github.com/google/deepvariant/blob/r1.5/scripts/install_singularity.sh. #### For your Ubuntu instance . You are getting very close! To simplify the install in the `run-prereq.sh` file you can comment out (with the `#` symbol) the following sections:. 1) For the ""Install TensorFlow pip package"" keep only the ones with **CPU-only**, and comment out the others. 2) For ""Install CUDA"", comment out everthing. 3) For ""Install TensorRT"", comment out everthing. And then run it again. The rest of the errors in the `run-prereq.sh` are easy to fix, which we can do later individually by removing each one, and installing the minimum required version. Before we fix `clif`, could you tell me what you get for the following:. ```; lsb_release -sc. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - . add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". sudo apt-get update. sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; ```. You might have a mismatch of a previous version of `clif` or its installed configuration files. You can check that via the following commands:. ```; llvm-config-11 --version; ```. The configs might be an older version, which you can check via the following:. ```; cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; ```. Below is what I have:. ```; $ cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; cat: /lib/llvm-11/cmake/LLVMConfig.cmake: No such file or directory; $; ```. If you have a mismatch between the version and config,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575894236
Modifiability,config,configuration,"singularity.sh. #### For your Ubuntu instance . You are getting very close! To simplify the install in the `run-prereq.sh` file you can comment out (with the `#` symbol) the following sections:. 1) For the ""Install TensorFlow pip package"" keep only the ones with **CPU-only**, and comment out the others. 2) For ""Install CUDA"", comment out everthing. 3) For ""Install TensorRT"", comment out everthing. And then run it again. The rest of the errors in the `run-prereq.sh` are easy to fix, which we can do later individually by removing each one, and installing the minimum required version. Before we fix `clif`, could you tell me what you get for the following:. ```; lsb_release -sc. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - . add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". sudo apt-get update. sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; ```. You might have a mismatch of a previous version of `clif` or its installed configuration files. You can check that via the following commands:. ```; llvm-config-11 --version; ```. The configs might be an older version, which you can check via the following:. ```; cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; ```. Below is what I have:. ```; $ cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; cat: /lib/llvm-11/cmake/LLVMConfig.cmake: No such file or directory; $; ```. If you have a mismatch between the version and config, you can make a clean removal of `llvm` via the following:. ```; sudo apt-get remove llvm-11*; ```. The way clif is installed is via the following:. ```; #!/bin/bash; source settings.sh; sudo tools/build_clif.sh; ```. Let me know what you see. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575894236
Usability,simpl,simplify,"This is very good! . #### For Singularity . You can take a look at the following two links:. https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-quick-start.md#notes-on-singularity. https://github.com/google/deepvariant/blob/r1.5/scripts/install_singularity.sh. #### For your Ubuntu instance . You are getting very close! To simplify the install in the `run-prereq.sh` file you can comment out (with the `#` symbol) the following sections:. 1) For the ""Install TensorFlow pip package"" keep only the ones with **CPU-only**, and comment out the others. 2) For ""Install CUDA"", comment out everthing. 3) For ""Install TensorRT"", comment out everthing. And then run it again. The rest of the errors in the `run-prereq.sh` are easy to fix, which we can do later individually by removing each one, and installing the minimum required version. Before we fix `clif`, could you tell me what you get for the following:. ```; lsb_release -sc. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - . add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". sudo apt-get update. sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; ```. You might have a mismatch of a previous version of `clif` or its installed configuration files. You can check that via the following commands:. ```; llvm-config-11 --version; ```. The configs might be an older version, which you can check via the following:. ```; cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; ```. Below is what I have:. ```; $ cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; cat: /lib/llvm-11/cmake/LLVMConfig.cmake: No such file or directory; $; ```. If you have a mismatch between the version and config,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575894236
Availability,echo,echo,"@pgrosu Thank you for your guidance!!; So this is what I did for the sections you mentioned:; ```; # note_build_stage ""Install TensorFlow pip package"". # if [[ ""${DV_USE_PREINSTALLED_TF}"" = ""1"" ]]; then; # echo ""Skipping TensorFlow installation at user request; will use pre-installed TensorFlow.""; # else; # # Also pip install the latest TensorFlow with cpu support. We don't build the; # # full TF from source, but instead using prebuilt version. However, we still; # # need the full source version to build DeepVariant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
Deployability,install,installation,"@pgrosu Thank you for your guidance!!; So this is what I did for the sections you mentioned:; ```; # note_build_stage ""Install TensorFlow pip package"". # if [[ ""${DV_USE_PREINSTALLED_TF}"" = ""1"" ]]; then; # echo ""Skipping TensorFlow installation at user request; will use pre-installed TensorFlow.""; # else; # # Also pip install the latest TensorFlow with cpu support. We don't build the; # # full TF from source, but instead using prebuilt version. However, we still; # # need the full source version to build DeepVariant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
Integrability,depend,dependency,"nt; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5403k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
Modifiability,config,config," echo ""For debugging:""; # pip3 show nvidia-tensorrt; # TENSORRT_PATH=$(python3 -c 'import tensorrt; print(tensorrt.__path__[0])'); # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer.so.8"" ""${TENSORRT_PATH}/libnvinfer.so.7""; # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}/libnvinfer_plugin.so.7""; # export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}""; # sudo ldconfig; # # Just in case this still doesn't work, we link them.; # # This is a workaround that we might want to get rid of, if we can make sure; # # setting LD_LIBRARY_PATH and `sudo ldconfig`` works.; # if [[ ! -e /usr/local/nvidia/lib ]]; then; # sudo mkdir -p /usr/local/nvidia/lib; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7; # fi; # fi; ```. The output for `./run-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 01:42:32 AM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 01:42:33 AM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 01:42:34 AM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 01:42:35 AM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5403k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
Performance,cache,cached," a workaround that we might want to get rid of, if we can make sure; # # setting LD_LIBRARY_PATH and `sudo ldconfig`` works.; # if [[ ! -e /usr/local/nvidia/lib ]]; then; # sudo mkdir -p /usr/local/nvidia/lib; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7; # fi; # fi; ```. The output for `./run-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 01:42:32 AM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 01:42:33 AM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 01:42:34 AM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 01:42:35 AM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5403k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' start",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
Security,secur,security," > wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - ; --2023-06-05 01:38:40-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... 146.75.46.49; Connecting to apt.llvm.org (apt.llvm.org)|146.75.46.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: ‘STDOUT’. - 100%[===================>] 3.07K --.-KB/s in 0s . 2023-06-05 01:38:40 (48.1 MB/s) - written to stdout [3145/3145]. OK. > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; Hit:1 https://download.docker.com/linux/ubuntu focal InRelease; Hit:2 https://download.sublimetext.com apt/stable/ InRelease; Hit:3 http://ports.ubuntu.com/ubuntu-ports focal InRelease; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Hit:6 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease; Reading package lists... Done ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# sudo apt-get update; Hit:2 https://download.docker.com/linux/ubuntu focal InRelease ; Hit:3 https://download.sublimetext.com apt/stable/ InRelease ; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal InRelease ; Hit:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease ; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:6 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Reading package lists... Done. > sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; Reading package lists... Done; Building dependency tree ; Reading state information... Done; clang-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11 is already the newest version ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
Testability,test,tests,"ho ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue here: https://github.com/pallets/markupsafe/issues/286.; # # Specifically:; # # ImportError: cannot import name 'soft_unicode' from 'markupsafe'.; # # So, forcing a downgrade. This isn't the best solution, but we need it to get; # # our tests pass.; pip3 install ""${PIP_ARGS[@]}"" --upgrade 'markupsafe==2.0.1'. # ################################################################################; # # CUDA; # ################################################################################. # note_build_stage ""Install CUDA"". # # See https://www.tensorflow.org/install/source#gpu for versions required.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_INSTALL_GPU_DRIVERS}"" = ""1"" ]]; then; # # This script is only maintained for Ubuntu 20.04.; # UBUNTU_VERSION=""2004""; # # https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_local; # echo ""Checking for CUDA...""; # if ! dpkg-query -W cuda-11-3; then; # echo ""Installing CUDA...""; # UBUNTU_VERSION=""2004""; # curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin; # sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-rep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
Usability,guid,guidance,"@pgrosu Thank you for your guidance!!; So this is what I did for the sections you mentioned:; ```; # note_build_stage ""Install TensorFlow pip package"". # if [[ ""${DV_USE_PREINSTALLED_TF}"" = ""1"" ]]; then; # echo ""Skipping TensorFlow installation at user request; will use pre-installed TensorFlow.""; # else; # # Also pip install the latest TensorFlow with cpu support. We don't build the; # # full TF from source, but instead using prebuilt version. However, we still; # # need the full source version to build DeepVariant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
Availability,error,error,"@pgrosu The `tools/build_clif.sh` was successful!; Thank you!. After that I tried running this:; ```; > sudo docker run google/deepvariant:""${BIN_VERSION}""; WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested; exec /opt/deepvariant/bin/run_deepvariant: exec format error; > sudo docker run --platform linux/amd64 google/deepvariant:""${BIN_VERSION}""; exec /opt/deepvariant/bin/run_deepvariant: exec format error; ```. Sorry, but what should I run before this?; Should I run the `./build-prereq.sh` and `./build_and_test.sh`?; Should I uncomment the `./run-prereq.sh` sections as well?. Actually, I am currently running the `./build-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
Deployability,install,installation," running the `./build-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
Integrability,depend,dependency,"nt; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
Modifiability,config,config,"@pgrosu The `tools/build_clif.sh` was successful!; Thank you!. After that I tried running this:; ```; > sudo docker run google/deepvariant:""${BIN_VERSION}""; WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested; exec /opt/deepvariant/bin/run_deepvariant: exec format error; > sudo docker run --platform linux/amd64 google/deepvariant:""${BIN_VERSION}""; exec /opt/deepvariant/bin/run_deepvariant: exec format error; ```. Sorry, but what should I run before this?; Should I run the `./build-prereq.sh` and `./build_and_test.sh`?; Should I uncomment the `./run-prereq.sh` sections as well?. Actually, I am currently running the `./build-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
Performance,cache,cached,"un_deepvariant: exec format error; ```. Sorry, but what should I run before this?; Should I run the `./build-prereq.sh` and `./build_and_test.sh`?; Should I uncomment the `./run-prereq.sh` sections as well?. Actually, I am currently running the `./build-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' start",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
Safety,detect,detected,"@pgrosu The `tools/build_clif.sh` was successful!; Thank you!. After that I tried running this:; ```; > sudo docker run google/deepvariant:""${BIN_VERSION}""; WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested; exec /opt/deepvariant/bin/run_deepvariant: exec format error; > sudo docker run --platform linux/amd64 google/deepvariant:""${BIN_VERSION}""; exec /opt/deepvariant/bin/run_deepvariant: exec format error; ```. Sorry, but what should I run before this?; Should I run the `./build-prereq.sh` and `./build_and_test.sh`?; Should I uncomment the `./run-prereq.sh` sections as well?. Actually, I am currently running the `./build-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
Testability,test,test, DV_USE_GCP_OPTIMIZED_TF_WHL=0; ++ DV_USE_GCP_OPTIMIZED_TF_WHL=0; ++ export GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-2.11.0.deepvariant_gcp-cp27-none-linux_x86_64.whl; ++ GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-2.11.0.deepvariant_gcp-cp27-none-linux_x86_64.whl; ++ export GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow; ++ GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow; ++ export GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow; ++ GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow; ++ export DV_TF_NUMPY_VERSION=1.19.2; ++ DV_TF_NUMPY_VERSION=1.19.2; ++ export DV_INSTALL_GPU_DRIVERS=0; ++ DV_INSTALL_GPU_DRIVERS=0; ++ export PYTHON_VERSION=3.8; ++ PYTHON_VERSION=3.8; +++ which python3.8; ++ export PYTHON_BIN_PATH=/usr/bin/python3.8; ++ PYTHON_BIN_PATH=/usr/bin/python3.8; ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages; ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api'; ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api'; + bazel; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; + PATH=/root/bin:/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/...; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
Usability,undo,undo,"Stage 'Cloning TensorFlow from github as ../tensorflow doesn't exist' starting; Cloning into 'tensorflow'...; remote: Enumerating objects: 1585302, done.; remote: Counting objects: 100% (346968/346968), done.; remote: Compressing objects: 100% (5367/5367), done.; remote: Total 1585302 (delta 342939), reused 342327 (delta 341589), pack-reused 1238334; Receiving objects: 100% (1585302/1585302), 920.91 MiB | 18.57 MiB/s, done.; Resolving deltas: 100% (1307043/1307043), done.; Updating files: 100% (29800/29800), done.; Updating files: 100% (12761/12761), done.; Note: switching to 'v2.11.0'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; WARNING: current bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
Availability,error,error,Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2); Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall; Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination; Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await; Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img; Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run; Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1); Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipel,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
Deployability,pipeline,pipeline,"Hi @MariaNattestad ,. Thank you for the reply,; 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out.; ```; Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44; Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]; Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]; Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json; Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96; Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false; Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -; Version: 22.10.7 build 5853; Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT); System: Linux 5.4.0-146-generic; Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04; Encoding: UTF-8 (UTF-8); Process: 683315@victor [127.0.1.1]; CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB); Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]; Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin; Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
Energy Efficiency,monitor,monitor,red/clinical/LongRead/Pipeline/work [ext2/ext3]; Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin; Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]; Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory; Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory; Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000; Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start; Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution; Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null; Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m; Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall; Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2); Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall; Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination; Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await; Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img; Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run; Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1);,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
Modifiability,plugin,plugin,"Hi @MariaNattestad ,. Thank you for the reply,; 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out.; ```; Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44; Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]; Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]; Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json; Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96; Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false; Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -; Version: 22.10.7 build 5853; Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT); System: Linux 5.4.0-146-generic; Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04; Encoding: UTF-8 (UTF-8); Process: 683315@victor [127.0.1.1]; CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB); Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]; Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin; Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
Performance,cache,cache, pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false; Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -; Version: 22.10.7 build 5853; Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT); System: Linux 5.4.0-146-generic; Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04; Encoding: UTF-8 (UTF-8); Process: 683315@victor [127.0.1.1]; CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB); Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]; Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin; Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]; Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory; Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory; Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000; Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start; Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution; Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null; Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m; Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall; Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2); Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall; Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunn,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
Safety,abort,aborted,"I FMA; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf; with vcf.VcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__; self._writer = self._native_writer(output_path, **kwargs); File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer; return NativeVcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__; self._writer = vcf_writer.VcfWriter.to_file(output_path, header,; ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s; user 0m8.421s; sys 0m8.363s. Work dir:; /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line; Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1); Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished; Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed; Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]; Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done; Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
Testability,log,log,"Hi @MariaNattestad ,. Thank you for the reply,; 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out.; ```; Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44; Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]; Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]; Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json; Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96; Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false; Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -; Version: 22.10.7 build 5853; Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT); System: Linux 5.4.0-146-generic; Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04; Encoding: UTF-8 (UTF-8); Process: 683315@victor [127.0.1.1]; CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB); Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]; Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin; Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
Usability,resume,resume,"I FMA; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf; with vcf.VcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__; self._writer = self._native_writer(output_path, **kwargs); File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer; return NativeVcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__; self._writer = vcf_writer.VcfWriter.to_file(output_path, header,; ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s; user 0m8.421s; sys 0m8.363s. Work dir:; /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line; Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1); Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished; Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed; Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]; Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done; Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
Usability,simpl,simpler,"Thank you for your prompt and professional response. I have reviewed my files and reconfigured my IGV. As @AndrewCarroll mentioned, this anomaly is that two different representations of a varaint. While both representations are equivalent, I believe that people might prefer a simpler representation here (e.g., homozygous SNV instead of three heterozygous varaints). Do you have any recommendations for post-processing methods to normalize these types of varaints into a single representation? Alternatively, can DeepVariant introduce more advanced models or encoding methods to handle this situation?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660#issuecomment-1590364272
Availability,down,down,"Thank you @AndrewCarroll. If the read is treated as a minimal [first-class object](https://en.wikipedia.org/wiki/First-class_citizen) (just a simple map/dictionary) would suffice, then you should be able to perform realignment after mapping. This way, it can become reactive without slowing down the analysis. Basically the information would not reside in a file, but rather encapsulated in the read itself. @PengJia6 The thing is that DeepVariant is position-focused at a specific base. That is how `make_examples` generates the variants from the allele counter for a region of a sample, which gets updated every time a new read is added to it. For instance, if you explore the allele counts, you'll get something like this for the different positions (the output is 0-based, and used the 1-based to identify each one):. ##### For Position: 89013075:. ```; position {; reference_name: ""chr10""; position: 89013074; }; ref_base: ""T""; ref_supporting_read_count: 35; read_alleles {; key: ""m64154_210327_091530/103023686/ccs/0""; value {; bases: ""TC""; type: INSERTION; count: 1; }; }; read_alleles {; key: ""m64154_210327_091530/128910218/ccs/0""; value {; bases: ""TC""; type: INSERTION; count: 1; }; }; ...; ```. ##### For Position: 89013076: . ```; position {; reference_name: ""chr10""; position: 89013075; }; ref_base: ""C""; ref_supporting_read_count: 35; read_alleles {; key: ""m64154_210327_091530/103023686/ccs/0""; value {; bases: ""CA""; type: DELETION; count: 1; }; }; read_alleles {; key: ""m64154_210327_091530/128910218/ccs/0""; value {; bases: ""CA""; type: DELETION; count: 1; }; }; ...; ```. ##### For Position: 89013077: ; ```; position {; reference_name: ""chr10""; position: 89013076; }; ref_base: ""A""; read_alleles {; key: ""m64154_210327_091530/142213575/ccs/0""; value {; bases: ""C""; type: SUBSTITUTION; count: 1; }; }; read_alleles {; key: ""m64154_210327_091530/4130912/ccs/0""; value {; bases: ""C""; type: SUBSTITUTION; count: 1; }; }; ...; ```. Given that the allele type (indel/substitution) changes ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660#issuecomment-1590833828
Deployability,update,updated,"Thank you @AndrewCarroll. If the read is treated as a minimal [first-class object](https://en.wikipedia.org/wiki/First-class_citizen) (just a simple map/dictionary) would suffice, then you should be able to perform realignment after mapping. This way, it can become reactive without slowing down the analysis. Basically the information would not reside in a file, but rather encapsulated in the read itself. @PengJia6 The thing is that DeepVariant is position-focused at a specific base. That is how `make_examples` generates the variants from the allele counter for a region of a sample, which gets updated every time a new read is added to it. For instance, if you explore the allele counts, you'll get something like this for the different positions (the output is 0-based, and used the 1-based to identify each one):. ##### For Position: 89013075:. ```; position {; reference_name: ""chr10""; position: 89013074; }; ref_base: ""T""; ref_supporting_read_count: 35; read_alleles {; key: ""m64154_210327_091530/103023686/ccs/0""; value {; bases: ""TC""; type: INSERTION; count: 1; }; }; read_alleles {; key: ""m64154_210327_091530/128910218/ccs/0""; value {; bases: ""TC""; type: INSERTION; count: 1; }; }; ...; ```. ##### For Position: 89013076: . ```; position {; reference_name: ""chr10""; position: 89013075; }; ref_base: ""C""; ref_supporting_read_count: 35; read_alleles {; key: ""m64154_210327_091530/103023686/ccs/0""; value {; bases: ""CA""; type: DELETION; count: 1; }; }; read_alleles {; key: ""m64154_210327_091530/128910218/ccs/0""; value {; bases: ""CA""; type: DELETION; count: 1; }; }; ...; ```. ##### For Position: 89013077: ; ```; position {; reference_name: ""chr10""; position: 89013076; }; ref_base: ""A""; read_alleles {; key: ""m64154_210327_091530/142213575/ccs/0""; value {; bases: ""C""; type: SUBSTITUTION; count: 1; }; }; read_alleles {; key: ""m64154_210327_091530/4130912/ccs/0""; value {; bases: ""C""; type: SUBSTITUTION; count: 1; }; }; ...; ```. Given that the allele type (indel/substitution) changes ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660#issuecomment-1590833828
Performance,perform,perform,"Thank you @AndrewCarroll. If the read is treated as a minimal [first-class object](https://en.wikipedia.org/wiki/First-class_citizen) (just a simple map/dictionary) would suffice, then you should be able to perform realignment after mapping. This way, it can become reactive without slowing down the analysis. Basically the information would not reside in a file, but rather encapsulated in the read itself. @PengJia6 The thing is that DeepVariant is position-focused at a specific base. That is how `make_examples` generates the variants from the allele counter for a region of a sample, which gets updated every time a new read is added to it. For instance, if you explore the allele counts, you'll get something like this for the different positions (the output is 0-based, and used the 1-based to identify each one):. ##### For Position: 89013075:. ```; position {; reference_name: ""chr10""; position: 89013074; }; ref_base: ""T""; ref_supporting_read_count: 35; read_alleles {; key: ""m64154_210327_091530/103023686/ccs/0""; value {; bases: ""TC""; type: INSERTION; count: 1; }; }; read_alleles {; key: ""m64154_210327_091530/128910218/ccs/0""; value {; bases: ""TC""; type: INSERTION; count: 1; }; }; ...; ```. ##### For Position: 89013076: . ```; position {; reference_name: ""chr10""; position: 89013075; }; ref_base: ""C""; ref_supporting_read_count: 35; read_alleles {; key: ""m64154_210327_091530/103023686/ccs/0""; value {; bases: ""CA""; type: DELETION; count: 1; }; }; read_alleles {; key: ""m64154_210327_091530/128910218/ccs/0""; value {; bases: ""CA""; type: DELETION; count: 1; }; }; ...; ```. ##### For Position: 89013077: ; ```; position {; reference_name: ""chr10""; position: 89013076; }; ref_base: ""A""; read_alleles {; key: ""m64154_210327_091530/142213575/ccs/0""; value {; bases: ""C""; type: SUBSTITUTION; count: 1; }; }; read_alleles {; key: ""m64154_210327_091530/4130912/ccs/0""; value {; bases: ""C""; type: SUBSTITUTION; count: 1; }; }; ...; ```. Given that the allele type (indel/substitution) changes ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660#issuecomment-1590833828
Usability,simpl,simple,"Thank you @AndrewCarroll. If the read is treated as a minimal [first-class object](https://en.wikipedia.org/wiki/First-class_citizen) (just a simple map/dictionary) would suffice, then you should be able to perform realignment after mapping. This way, it can become reactive without slowing down the analysis. Basically the information would not reside in a file, but rather encapsulated in the read itself. @PengJia6 The thing is that DeepVariant is position-focused at a specific base. That is how `make_examples` generates the variants from the allele counter for a region of a sample, which gets updated every time a new read is added to it. For instance, if you explore the allele counts, you'll get something like this for the different positions (the output is 0-based, and used the 1-based to identify each one):. ##### For Position: 89013075:. ```; position {; reference_name: ""chr10""; position: 89013074; }; ref_base: ""T""; ref_supporting_read_count: 35; read_alleles {; key: ""m64154_210327_091530/103023686/ccs/0""; value {; bases: ""TC""; type: INSERTION; count: 1; }; }; read_alleles {; key: ""m64154_210327_091530/128910218/ccs/0""; value {; bases: ""TC""; type: INSERTION; count: 1; }; }; ...; ```. ##### For Position: 89013076: . ```; position {; reference_name: ""chr10""; position: 89013075; }; ref_base: ""C""; ref_supporting_read_count: 35; read_alleles {; key: ""m64154_210327_091530/103023686/ccs/0""; value {; bases: ""CA""; type: DELETION; count: 1; }; }; read_alleles {; key: ""m64154_210327_091530/128910218/ccs/0""; value {; bases: ""CA""; type: DELETION; count: 1; }; }; ...; ```. ##### For Position: 89013077: ; ```; position {; reference_name: ""chr10""; position: 89013076; }; ref_base: ""A""; read_alleles {; key: ""m64154_210327_091530/142213575/ccs/0""; value {; bases: ""C""; type: SUBSTITUTION; count: 1; }; }; read_alleles {; key: ""m64154_210327_091530/4130912/ccs/0""; value {; bases: ""C""; type: SUBSTITUTION; count: 1; }; }; ...; ```. Given that the allele type (indel/substitution) changes ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660#issuecomment-1590833828
Deployability,upgrade,upgrade,"Hi @AndrewCarroll and @pgrosu. Thank you for your clear explanation. I understand the this case now and I am looking forward to seeing your new methods for handling these cases, as I believe it will be a significant improvement. I will explore using the method @pgrosu provided to temporarily process these varaints and ensure their uniformity. I will upgrade the version of DeepVariant in the next release of our project ([Chinese Quartet](https://github.com/xjtu-omics/ChineseQuartetGenome)). Furthermore, I noticed some information about DeepTrio in deepvariant homepage. Does DeepTrio support joint calling for quartet families (parents and **two** children)?. Best! ; Peng",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660#issuecomment-1591182902
Usability,clear,clear,"Hi @AndrewCarroll and @pgrosu. Thank you for your clear explanation. I understand the this case now and I am looking forward to seeing your new methods for handling these cases, as I believe it will be a significant improvement. I will explore using the method @pgrosu provided to temporarily process these varaints and ensure their uniformity. I will upgrade the version of DeepVariant in the next release of our project ([Chinese Quartet](https://github.com/xjtu-omics/ChineseQuartetGenome)). Furthermore, I noticed some information about DeepTrio in deepvariant homepage. Does DeepTrio support joint calling for quartet families (parents and **two** children)?. Best! ; Peng",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660#issuecomment-1591182902
Deployability,update,update,"Hi @Npaffen,. That's great to hear that it worked! Let me answer each question individually:. `1` DeepVariant for the PACBIO model enables `--phase-reads` as shown below, but it is only used internally for improving the accuracy by [expanding the region of finding candidates](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L1305-L1321):. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L255. This will not show phased reads in the VCF file. You are correct in that you will need to use a tool like [`WhatsHap`](https://whatshap.readthedocs.io/en/latest/guide.html) to update the VCF file with phasing information. `2` The output for the filter column is generated in the following way for most cases (biallelic):. `2.1` `call_variants` generates the genotype probabilities using the PACBIO model (for this case), for the candidates generated by `make_examples`. `2.2` `postprocessing_variants` operates all of this through the [`add_call_to_variant()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L304-L340) function:. `2.2.1` For each variant's predicted genotype probabilities it looks at the highest probability, denoting that being the most likely genotype. It then generates it via the [`most_likely_genotype(predictions, ploidy=2, n_alleles=2)`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L193-L273) function. `2.2.2` It then takes that most likely genotype and uses it [`compute_filter_fields()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L170-L190) function, to generate the appropriate filter. Basically it, strictly looking at the genotype and it if it sees it as [0, 0] (i.e. 0/0) it will label the Filter column as RefCall. `2.2.3` Then the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function will label the genotype ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1601744318
Safety,predict,predicted,"rnally for improving the accuracy by [expanding the region of finding candidates](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L1305-L1321):. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L255. This will not show phased reads in the VCF file. You are correct in that you will need to use a tool like [`WhatsHap`](https://whatshap.readthedocs.io/en/latest/guide.html) to update the VCF file with phasing information. `2` The output for the filter column is generated in the following way for most cases (biallelic):. `2.1` `call_variants` generates the genotype probabilities using the PACBIO model (for this case), for the candidates generated by `make_examples`. `2.2` `postprocessing_variants` operates all of this through the [`add_call_to_variant()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L304-L340) function:. `2.2.1` For each variant's predicted genotype probabilities it looks at the highest probability, denoting that being the most likely genotype. It then generates it via the [`most_likely_genotype(predictions, ploidy=2, n_alleles=2)`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L193-L273) function. `2.2.2` It then takes that most likely genotype and uses it [`compute_filter_fields()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L170-L190) function, to generate the appropriate filter. Basically it, strictly looking at the genotype and it if it sees it as [0, 0] (i.e. 0/0) it will label the Filter column as RefCall. `2.2.3` Then the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function will label the genotype of each variant with a GQ value < 20 (set via the [`--cnn_homref_call_min_gq`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L86-L89) flag) as [-1,-1] which ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1601744318
Usability,guid,guide,"Hi @Npaffen,. That's great to hear that it worked! Let me answer each question individually:. `1` DeepVariant for the PACBIO model enables `--phase-reads` as shown below, but it is only used internally for improving the accuracy by [expanding the region of finding candidates](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L1305-L1321):. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L255. This will not show phased reads in the VCF file. You are correct in that you will need to use a tool like [`WhatsHap`](https://whatshap.readthedocs.io/en/latest/guide.html) to update the VCF file with phasing information. `2` The output for the filter column is generated in the following way for most cases (biallelic):. `2.1` `call_variants` generates the genotype probabilities using the PACBIO model (for this case), for the candidates generated by `make_examples`. `2.2` `postprocessing_variants` operates all of this through the [`add_call_to_variant()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L304-L340) function:. `2.2.1` For each variant's predicted genotype probabilities it looks at the highest probability, denoting that being the most likely genotype. It then generates it via the [`most_likely_genotype(predictions, ploidy=2, n_alleles=2)`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L193-L273) function. `2.2.2` It then takes that most likely genotype and uses it [`compute_filter_fields()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L170-L190) function, to generate the appropriate filter. Basically it, strictly looking at the genotype and it if it sees it as [0, 0] (i.e. 0/0) it will label the Filter column as RefCall. `2.2.3` Then the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function will label the genotype ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1601744318
Availability,reliab,reliable,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1602118672
Testability,log,logic,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1602118672
Usability,learn,learn,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1602118672
Availability,down,downstream,"Hi @Npaffen , I'm late to this thread. If I'm missing some context please feel free to remind me. Regarding your question ""**why the homref variants and the missings are added to the vcf in the first place**"":. DeepVariant starts with a set of candidates. These candidates came from a set of heuristics that propose a bunch of sites that potentially have variants.; You can find some thresholds we use for the heuristics here: https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L194; which basically means - if there is an alt allele that has a certain number of reads supporting it, and a certain % of reads supporting it, it will be proposed as a potential candidate. Then, DeepVariant applies a classifier on these candidates. To learn more about the representation that DeepVariant uses, https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/ is a good explanation. Each of the example (""image"") has a probability distribution for 3 classes. Based on the probability, the GT is assigned. As mentioned in previous answers, when the probability distribution from DeepVariant shows that it's not as confident, the GT is set to `./.`. By default, DeepVariant outputs the candidates even when they're classified as `0/0`, or when they're set to `./.`. ; This won't affect downstream tools like hap.py, though. Because these are not considered when tools like hap.py calculates accuracy. What DeepVariant outputs complies with the VCF spec, which says ""FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position."" If you look at our VCF, you'll notice that all variants (not including `0/0` and `./.`) should have `PASS`. Hopefully this helps. @Npaffen let me know if there's anything else that's unclear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1610105430
Usability,learn,learn,"Hi @Npaffen , I'm late to this thread. If I'm missing some context please feel free to remind me. Regarding your question ""**why the homref variants and the missings are added to the vcf in the first place**"":. DeepVariant starts with a set of candidates. These candidates came from a set of heuristics that propose a bunch of sites that potentially have variants.; You can find some thresholds we use for the heuristics here: https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L194; which basically means - if there is an alt allele that has a certain number of reads supporting it, and a certain % of reads supporting it, it will be proposed as a potential candidate. Then, DeepVariant applies a classifier on these candidates. To learn more about the representation that DeepVariant uses, https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/ is a good explanation. Each of the example (""image"") has a probability distribution for 3 classes. Based on the probability, the GT is assigned. As mentioned in previous answers, when the probability distribution from DeepVariant shows that it's not as confident, the GT is set to `./.`. By default, DeepVariant outputs the candidates even when they're classified as `0/0`, or when they're set to `./.`. ; This won't affect downstream tools like hap.py, though. Because these are not considered when tools like hap.py calculates accuracy. What DeepVariant outputs complies with the VCF spec, which says ""FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position."" If you look at our VCF, you'll notice that all variants (not including `0/0` and `./.`) should have `PASS`. Hopefully this helps. @Npaffen let me know if there's anything else that's unclear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1610105430
Modifiability,variab,variable,"Hi @Npaffen . This is an interesting question. One advantage is that is these sites give a complete accounting of all positions that the neural network acts on. This makes it easier for us to debug issues (either internally or user-presented) as we remove the variable of whether a row in the VCF was not present because a candidate was not generated, or because the neural network decided it was reference. From a developer perspective (whether internal or external), this is much cleaner. . There is another advantage in allowing users to determine the ideal threshold for sensitivity (e.g. if they highly value sensitivity, they may want to consider positions which are ./. calls of low GQ). These are more advanced use cases and it does seem that most users opt for simpler approaches. . I also tend to think that including these variants won't confuse users, and that they would be used to variant callers using things like the FILTER field to indicate non-variant calls. But if you have a strong opinion that this will be broadly confusing, I will take that into account.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1611797347
Usability,simpl,simpler,"Hi @Npaffen . This is an interesting question. One advantage is that is these sites give a complete accounting of all positions that the neural network acts on. This makes it easier for us to debug issues (either internally or user-presented) as we remove the variable of whether a row in the VCF was not present because a candidate was not generated, or because the neural network decided it was reference. From a developer perspective (whether internal or external), this is much cleaner. . There is another advantage in allowing users to determine the ideal threshold for sensitivity (e.g. if they highly value sensitivity, they may want to consider positions which are ./. calls of low GQ). These are more advanced use cases and it does seem that most users opt for simpler approaches. . I also tend to think that including these variants won't confuse users, and that they would be used to variant callers using things like the FILTER field to indicate non-variant calls. But if you have a strong opinion that this will be broadly confusing, I will take that into account.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1611797347
Energy Efficiency,power,power,"at might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088
Modifiability,layers,layers,"iant/assets/6555937/858f235a-79d1-4fdb-bfad-a9ee79ade41a). If I generate a t-SNE plot of this, I see something like the following using colors to denote the different channels (feature maps):. ![image](https://github.com/google/deepvariant/assets/6555937/20c87e1a-e87f-4b7d-b14d-a7f4d6bfbfef). Now comes the fun part! Let's have different matrices (like the ones that generated the new channels above) that will identify interesting features that might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![im",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088
Performance,optimiz,optimize,"s to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to identify hidden patterns in the data -- as it is all linked from the start (read encoding) to finish (genotype identification). This ability is enabled through the preservation of the propagation of information (which is deeply and directly linked) in order to help one optimize upon. That's why you don't want to throw anything away, as @AndrewCarroll and @pichuan mentioned as it helps you inspect and tweak the selection power and criteria as it is all interconnected. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088
Safety,detect,detect,"Hi @Npaffen,. To expand a bit on my previous explanation, regarding the way you can view a variant coming from a neural network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088
Security,expose,expose,"ollapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![image](https://github.com/google/deepvariant/assets/6555937/679fd1db-16bb-4534-8afb-794c978eb955). This helps me understand which channels (feature maps) might be similar, or different. Now if we look at the heatmap for the original data it would look something like this (notice the variant in the middle):. ![image](https://github.com/google/deepvariant/assets/6555937/858f235a-79d1-4fdb-bfad-a9ee79ade41a). If I generate a t-SNE plot of this, I see something like the following using colors to denote the different channels (feature maps):. ![image](https://github.com/google/deepvariant/assets/6555937/20c87e1a-e87f-4b7d-b14d-a7f4d6bfbfef). Now comes the fun part! Let's have different matrices (like the ones that generated the new channels above) that will identify interesting features that might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have se",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088
Testability,test,testing,"these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088
Usability,simpl,simpler,"Hi @Npaffen,. To expand a bit on my previous explanation, regarding the way you can view a variant coming from a neural network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088
Usability,simpl,simply,@AndrewCarroll and @pgrosu thanks for your view on things and @pgrosu especially for your fantastic explanation of the model!. From a data scientist perspective this makes totally sense. I fully agree that one should always gather as much data as possible since one can always remove data that seems to be not useful afterwards. My intention was not to say that one do not need those data or that you should not use the information which those HomRef sites emerged from. I talked to some of my colleagues yesterday and they agreed that they would expect a variant caller to produce variant calls and information/statistics related to those and all further information would be opt-in since they would have no need for this. But my team could just be the minority here and simply an opt-out option for this behavior would be highly recommended!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1612713066
Usability,simpl,simply,"@pgrosu @pichuan thanks for your continues help in various questions I raised in this thread. Really helpful. How likely is it to generate a DeepTrio model in the future to use such a data combination? I guess it is much more than simply training a model on such pedigrees, right?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1615137619
Usability,clear,clear,"Hi @crazysummerW , ; in your first step, you mentioned ""pbmm2 align hs37d5.fasta fa.fofn n1000.subreads_to_ccs_aligned.bam --sort --rg '@rg\tID:test1\tSM:test1'"". Can you explain to me what you're trying to do in this step?. You also mentioned DeepConsensus. Are you planning to start from your own PacBio subreads BAM?; Unless you're starting with subreads, you don't need to apply DeepConsensus. If you are starting with PacBio HiFi reads (which I think is most of the DeepVariant users), you should directly map your FASTQ files using pbmm2, which gives you a BAM file, then you apply DeepVariant on the BAM. However, if you are indeed starting from PacBio subreads, you will need to follow https://github.com/google/deepconsensus/blob/r1.2/docs/quick_start.md to obtain the FASTQ file (which is the output that DeepConsensus gives you). Then, you map the FASTQ file, which gives you a BAM file, then you apply DeepVariant on the BAM. From your original question, it isn't quite clear to me what you're trying to do here. Can you elaborate?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672#issuecomment-1615164671
Performance,perform,performed,"@pichuan @amwenger ; Thank you very much for your response.; Perhaps I didn't express myself clearly. I performed two tests. One is HiFi reads, which is from the PacBio public revio data set. Another one is n1000.subreads.bam, which is from DeepConsensus. ; Both datasets have been tested successfully. Thank you for your help.; Have a great weekend!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672#issuecomment-1621111780
Testability,test,tests,"@pichuan @amwenger ; Thank you very much for your response.; Perhaps I didn't express myself clearly. I performed two tests. One is HiFi reads, which is from the PacBio public revio data set. Another one is n1000.subreads.bam, which is from DeepConsensus. ; Both datasets have been tested successfully. Thank you for your help.; Have a great weekend!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672#issuecomment-1621111780
Usability,clear,clearly,"@pichuan @amwenger ; Thank you very much for your response.; Perhaps I didn't express myself clearly. I performed two tests. One is HiFi reads, which is from the PacBio public revio data set. Another one is n1000.subreads.bam, which is from DeepConsensus. ; Both datasets have been tested successfully. Thank you for your help.; Have a great weekend!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672#issuecomment-1621111780
Usability,simpl,simplex,"@CWYuan08 , for Nanopore R9.4.1, we suggest using [PEPPER](https://github.com/kishwarshafin/pepper). DeepVariant supports R10.4 simplex and duplex modes. Thank you for confirming that it is working for you now. I will close this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673#issuecomment-1638414183
Usability,simpl,simplex,"Thanks @pgrosu. Knowing these would be very helpful. Besides compute, this can also be an issue with the input data. @Taghrid-M ,. Can you please tell a little more about the data in `HG004-hg38.ont.mm2.bam`:. 1) What chemistry is this data R9 or R10?; 2) What is the basecaller version you used for basecalling this data?; 3) What is the average read length of the reads?. Please note, DeepVariant currently supports R10.4 simplex and duplex variant calling for nanopore. If your data is from previous chemistry or basecaller version, please use [PEPPER](https://github.com/kishwarshafin/pepper) to call variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/681#issuecomment-1641030077
Availability,avail,available,"Thanks @pgrosu @kishwarshafin, I appreciate your swift reply!. Yes, I am using a cluster, and the data have been obtained from precisionFDA https://data.nist.gov/od/id/mds2-2336. **What chemistry is this data? Is it R9 or R10?**; This data was generated using R9.4 flow cells. **What is the basecaller version you used for basecalling this data?**; The basecalling process was performed using Guppy Version 3.6. **What is the average read length of the reads?**; 85X. **Have you tried first going through the DeepVariant Quick Start guide to check if a smaller DeepVariant run completes successfully on your system?**; Yes, I have successfully run it. **How much free memory do you have?**; 1.3T . **How much free disk space do you have?**; I have approximately 14T of free disk space. **How many CPU cores do you have, and what is their occupancy level?**; 16 CPU cores. **Do you have any NVIDIA GPUs available on your system?**; No.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/681#issuecomment-1641137274
Performance,perform,performed,"Thanks @pgrosu @kishwarshafin, I appreciate your swift reply!. Yes, I am using a cluster, and the data have been obtained from precisionFDA https://data.nist.gov/od/id/mds2-2336. **What chemistry is this data? Is it R9 or R10?**; This data was generated using R9.4 flow cells. **What is the basecaller version you used for basecalling this data?**; The basecalling process was performed using Guppy Version 3.6. **What is the average read length of the reads?**; 85X. **Have you tried first going through the DeepVariant Quick Start guide to check if a smaller DeepVariant run completes successfully on your system?**; Yes, I have successfully run it. **How much free memory do you have?**; 1.3T . **How much free disk space do you have?**; I have approximately 14T of free disk space. **How many CPU cores do you have, and what is their occupancy level?**; 16 CPU cores. **Do you have any NVIDIA GPUs available on your system?**; No.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/681#issuecomment-1641137274
Usability,guid,guide,"Thanks @pgrosu @kishwarshafin, I appreciate your swift reply!. Yes, I am using a cluster, and the data have been obtained from precisionFDA https://data.nist.gov/od/id/mds2-2336. **What chemistry is this data? Is it R9 or R10?**; This data was generated using R9.4 flow cells. **What is the basecaller version you used for basecalling this data?**; The basecalling process was performed using Guppy Version 3.6. **What is the average read length of the reads?**; 85X. **Have you tried first going through the DeepVariant Quick Start guide to check if a smaller DeepVariant run completes successfully on your system?**; Yes, I have successfully run it. **How much free memory do you have?**; 1.3T . **How much free disk space do you have?**; I have approximately 14T of free disk space. **How many CPU cores do you have, and what is their occupancy level?**; 16 CPU cores. **Do you have any NVIDIA GPUs available on your system?**; No.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/681#issuecomment-1641137274
Usability,learn,learned,"Hello @pgrosu ; Thanks for the bcftools suggestion, exactly what's needed. I will try to digest your comment about how the model was trained. Thank you for explaining! I will report back. ; @AndrewCarroll I am gonna send you the vcf file, from a protonmail address (the proton domain is sometimes blocked by servers). . Thanks everyone for the great disussion, I hav learned a lot.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1649296164
Availability,reliab,reliably,"sible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust the weights and bias terms for produce specific output given the input, and then you modify the inputs significantly and hope to get the same results, it will naturally fail. The same with DeepVariant and Clair3, they are bounded and determined by their weights and biases. These deep learning models were meant to work at scale under relatively normal data conditions (data conditions they were trained and validated for). The way to adjust for variable datasets is to create an ensemble model. What that means is that you have a model for each subtype of your organism. Then your run your data across all models to see which ones respond with high confidence. In your case, you don't have that and have to adjust for the data you have; as training a new model would require a confident variant truth set. So what is the known starting point is the biology of your organism. To control for variability, the first step is probably to ensure you have good isolates of your organism that you can get sequence data from -- you've seen that in your case it was multiallelic so it probably is not a pure isolate. You probably don't need to arrest them if you validate for known markers and sequenced at the same time to ensure they do not modify. These replicates would form your panels. Then there is probably a whole slew of analysis, and experiments you would need to understand these variants. Part of that can be to see how you can confidence-label the variants generated by DeepVariant and Clair3 given their specific parameters adjusted accordingly. If they become reliably predictable under a diverse dataset, then you can probably use them for your downstream analysis. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832
Energy Efficiency,efficient,efficient,"Hi @Axze-rgb,. This clearly tells me that your organism is highly sensitive under selective pressure. So let's think what might happen if you used Clair3. Clair3 has both a full-alignment network (ResNet), and a pileup network (Bi-LSTM). . Bi-LSTM is just a recurrent neural network that is memory efficient to remember past events without gradient instabilities, by going through the input forward and in reverse while getting output information from the previous step. All this means is that it has a bunch of weights and biases that the input (and previous output) progresses through in a time-series-like pattern -- traversing the input data both forward and in reverse -- remembering internally long-term dependencies stored as states. This means it is fairly memory efficient and fairly fast. In terms of sequence it captures the variants given a progression known by past progressions. As an analogy, given sequence of historical stock prices, can you predict the future stock price. Instead of stock price it produces variants. The ResNet is helpful to train faster as it is uses skip (shortcut) connection by utilizing the residual difference of input and output to learn from (residual learning). They use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832
Integrability,depend,dependencies,"Hi @Axze-rgb,. This clearly tells me that your organism is highly sensitive under selective pressure. So let's think what might happen if you used Clair3. Clair3 has both a full-alignment network (ResNet), and a pileup network (Bi-LSTM). . Bi-LSTM is just a recurrent neural network that is memory efficient to remember past events without gradient instabilities, by going through the input forward and in reverse while getting output information from the previous step. All this means is that it has a bunch of weights and biases that the input (and previous output) progresses through in a time-series-like pattern -- traversing the input data both forward and in reverse -- remembering internally long-term dependencies stored as states. This means it is fairly memory efficient and fairly fast. In terms of sequence it captures the variants given a progression known by past progressions. As an analogy, given sequence of historical stock prices, can you predict the future stock price. Instead of stock price it produces variants. The ResNet is helpful to train faster as it is uses skip (shortcut) connection by utilizing the residual difference of input and output to learn from (residual learning). They use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832
Modifiability,variab,variable,"M is just a recurrent neural network that is memory efficient to remember past events without gradient instabilities, by going through the input forward and in reverse while getting output information from the previous step. All this means is that it has a bunch of weights and biases that the input (and previous output) progresses through in a time-series-like pattern -- traversing the input data both forward and in reverse -- remembering internally long-term dependencies stored as states. This means it is fairly memory efficient and fairly fast. In terms of sequence it captures the variants given a progression known by past progressions. As an analogy, given sequence of historical stock prices, can you predict the future stock price. Instead of stock price it produces variants. The ResNet is helpful to train faster as it is uses skip (shortcut) connection by utilizing the residual difference of input and output to learn from (residual learning). They use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832
Safety,predict,predict,"Hi @Axze-rgb,. This clearly tells me that your organism is highly sensitive under selective pressure. So let's think what might happen if you used Clair3. Clair3 has both a full-alignment network (ResNet), and a pileup network (Bi-LSTM). . Bi-LSTM is just a recurrent neural network that is memory efficient to remember past events without gradient instabilities, by going through the input forward and in reverse while getting output information from the previous step. All this means is that it has a bunch of weights and biases that the input (and previous output) progresses through in a time-series-like pattern -- traversing the input data both forward and in reverse -- remembering internally long-term dependencies stored as states. This means it is fairly memory efficient and fairly fast. In terms of sequence it captures the variants given a progression known by past progressions. As an analogy, given sequence of historical stock prices, can you predict the future stock price. Instead of stock price it produces variants. The ResNet is helpful to train faster as it is uses skip (shortcut) connection by utilizing the residual difference of input and output to learn from (residual learning). They use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832
Security,validat,validated," for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust the weights and bias terms for produce specific output given the input, and then you modify the inputs significantly and hope to get the same results, it will naturally fail. The same with DeepVariant and Clair3, they are bounded and determined by their weights and biases. These deep learning models were meant to work at scale under relatively normal data conditions (data conditions they were trained and validated for). The way to adjust for variable datasets is to create an ensemble model. What that means is that you have a model for each subtype of your organism. Then your run your data across all models to see which ones respond with high confidence. In your case, you don't have that and have to adjust for the data you have; as training a new model would require a confident variant truth set. So what is the known starting point is the biology of your organism. To control for variability, the first step is probably to ensure you have good isolates of your organism that you can get sequence data from -- you've seen that in your case it was multiallelic so it probably is not a pure isolate. You probably don't need to arrest them if you validate for known markers and sequenced at the same time to ensure they do not modify. These replicates would form your panels. Then there is probably a whole slew of analysis, and experiments ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832
Usability,clear,clearly,"Hi @Axze-rgb,. This clearly tells me that your organism is highly sensitive under selective pressure. So let's think what might happen if you used Clair3. Clair3 has both a full-alignment network (ResNet), and a pileup network (Bi-LSTM). . Bi-LSTM is just a recurrent neural network that is memory efficient to remember past events without gradient instabilities, by going through the input forward and in reverse while getting output information from the previous step. All this means is that it has a bunch of weights and biases that the input (and previous output) progresses through in a time-series-like pattern -- traversing the input data both forward and in reverse -- remembering internally long-term dependencies stored as states. This means it is fairly memory efficient and fairly fast. In terms of sequence it captures the variants given a progression known by past progressions. As an analogy, given sequence of historical stock prices, can you predict the future stock price. Instead of stock price it produces variants. The ResNet is helpful to train faster as it is uses skip (shortcut) connection by utilizing the residual difference of input and output to learn from (residual learning). They use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832
Availability,down,down,"Hi @Axze-rgb and Andrew,. Good catch on the newline character – it’s hard to slow down at times when things are fun :) . Before having a presentation -- as my bandwidth is a bit tight these days (though I usually love presentations) -- I would be curious to see what you first get from Clair3 and Andrew's candidate region, as well as any additional insight Andrew might get from the VCF. I have this funny feeling of what the outcome might be – and we can have the presentation later among the three of us (and anyone else that's interested) as things emerge more clearly. Thank you,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1652518468
Usability,clear,clearly,"Hi @Axze-rgb and Andrew,. Good catch on the newline character – it’s hard to slow down at times when things are fun :) . Before having a presentation -- as my bandwidth is a bit tight these days (though I usually love presentations) -- I would be curious to see what you first get from Clair3 and Andrew's candidate region, as well as any additional insight Andrew might get from the VCF. I have this funny feeling of what the outcome might be – and we can have the presentation later among the three of us (and anyone else that's interested) as things emerge more clearly. Thank you,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1652518468
Modifiability,variab,variable,"No problem, your variant of interest isn't a genomic region that may be; hyper variable ie a simple sequence repeat (they can occur in coding; regions) or something else that may lead to the variability your seeing?. Joe. On Mon, 31 Jul 2023, 17:30 Axze-rgb, ***@***.***> wrote:. > nothing is amplified no, it's all PCR free; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/682#issuecomment-1658733075>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2X446P2BMPITLE5763XS7MRXANCNFSM6AAAAAA2QKAKXQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658739773
Usability,simpl,simple,"No problem, your variant of interest isn't a genomic region that may be; hyper variable ie a simple sequence repeat (they can occur in coding; regions) or something else that may lead to the variability your seeing?. Joe. On Mon, 31 Jul 2023, 17:30 Axze-rgb, ***@***.***> wrote:. > nothing is amplified no, it's all PCR free; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/682#issuecomment-1658733075>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2X446P2BMPITLE5763XS7MRXANCNFSM6AAAAAA2QKAKXQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658739773
Usability,simpl,simple,"Sorry but simple sequence repeats are mutable in prokaryotes too.... On Mon, 31 Jul 2023, 17:45 Axze-rgb, ***@***.***> wrote:. > that's not a feature of this genome we are not in humans.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/682#issuecomment-1658755258>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2V75T2P2ATODR6Z6QDXS7OLHANCNFSM6AAAAAA2QKAKXQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658757046
Availability,error,errors,"EE?DE??DDDBADEBEFFFDBEFFEBCBC=?BEEEE@=:?::?7?:8-6?7?@??# AS:i:0 XS:i:0 XN:i:0 XM:i:0 XO:i:0 XG:i:0 NM:i:0 MD:Z:65 YT:Z:UU; ```; The columns of this tab-delimited line are usually the following:. QNAME FLAG RNAME POS MAPQ CIGAR RNEXT PNEXT TLEN SEQ QUAL TAGS. 1) Read Name; 2) SAM flag[ --> decode](http://www.google.com/url?q=http%3A%2F%2Fbroadinstitute.github.io%2Fpicard%2Fexplain-flags.html&sa=D&sntz=1&usg=AOvVaw0FjgFbRCGf0ogpCecZqsJX); 3) contig name or * for unmapped; 4) mapped position of base 1 of a read on the reference sequence; 5) MAPQ mapping quality; 6) CIGAR string describing insertions and deletions; 7) Name of mate; 8) Position of mate; 9) Template length; 10) Read Sequence; 11) Read Quality; 12) Additional information in TAG:TYPE:VALUE format. You can read about the SAM/BAM format at the following link:. https://samtools.github.io/hts-specs/SAMv1.pdf. So to answer the last two questions. So having good coverage (read depth) lets you get around sequencing errors -- meaning you might have to repeat the experiment as well for ensuring consistency in the results. Some labs create error models with different known repeated read lengths to generate a sequencer error model -- you don't need to do that, as that cost a lot of money to do properly. You can probably do it with known samples and compare it to golden truth datasets, but again that will cost. The other way to do it is to try other assays to confirm your results. . Understanding all the moving parts takes time. Sure it is very rewarding once you have a complete understanding of a model, but it all depends on how much time is reasonable for you. You can always keep learning after getting the PhD, as that just teaches you how to independently think about big problems and how to systematically break them down (defending your approach). Once you confidently have that thought process down, you're virtually unbeatable and don't need hone that skill as it already is innate. You can always practice that during",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658769491
Integrability,depend,depends,":?::?7?:8-6?7?@??# AS:i:0 XS:i:0 XN:i:0 XM:i:0 XO:i:0 XG:i:0 NM:i:0 MD:Z:65 YT:Z:UU; ```; The columns of this tab-delimited line are usually the following:. QNAME FLAG RNAME POS MAPQ CIGAR RNEXT PNEXT TLEN SEQ QUAL TAGS. 1) Read Name; 2) SAM flag[ --> decode](http://www.google.com/url?q=http%3A%2F%2Fbroadinstitute.github.io%2Fpicard%2Fexplain-flags.html&sa=D&sntz=1&usg=AOvVaw0FjgFbRCGf0ogpCecZqsJX); 3) contig name or * for unmapped; 4) mapped position of base 1 of a read on the reference sequence; 5) MAPQ mapping quality; 6) CIGAR string describing insertions and deletions; 7) Name of mate; 8) Position of mate; 9) Template length; 10) Read Sequence; 11) Read Quality; 12) Additional information in TAG:TYPE:VALUE format. You can read about the SAM/BAM format at the following link:. https://samtools.github.io/hts-specs/SAMv1.pdf. So to answer the last two questions. So having good coverage (read depth) lets you get around sequencing errors -- meaning you might have to repeat the experiment as well for ensuring consistency in the results. Some labs create error models with different known repeated read lengths to generate a sequencer error model -- you don't need to do that, as that cost a lot of money to do properly. You can probably do it with known samples and compare it to golden truth datasets, but again that will cost. The other way to do it is to try other assays to confirm your results. . Understanding all the moving parts takes time. Sure it is very rewarding once you have a complete understanding of a model, but it all depends on how much time is reasonable for you. You can always keep learning after getting the PhD, as that just teaches you how to independently think about big problems and how to systematically break them down (defending your approach). Once you confidently have that thought process down, you're virtually unbeatable and don't need hone that skill as it already is innate. You can always practice that during your postdoc :) . Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658769491
Modifiability,inherit,inherited,"@Axze-rgb This has to do with you sequencer detecting the Q-Score (Phred-scaled) read quality, that gets stored in the FASTQ files that and get inherited in converted BAM file when you align the reads. So a SAM/BAM file have read quality scores per base following the sequence, like this:. ```; CCTGCACGAACGAAATCCGCATGCGTCTGGTCGTTGTACGGAACGGCGGTTGTGTGACGAACGGC EDDEEDEE=EE?DE??DDDBADEBEFFFDBEFFEBCBC=?BEEEE@=:?::?7?:8-6?7?@??# ; ```. The score is a ASCII encrypted score + the value 33. You can read the Phred value here per ASCII value:. https://support.illumina.com/help/BaseSpace_OLH_009008/Content/Source/Informatics/BS/QualityScoreEncoding_swBS.htm. A full line in a SAM/BAM file would look like this:. ```; readID43GYAX15:7:1:1202:19894/1 256 contig87 540849 1 65M * 0 0 CCTGCACGAACGAAATCCGCATGCGTCTGGTCGTTGTACGGAACGGCGGTTGTGTGACGAACGGC EDDEEDEE=EE?DE??DDDBADEBEFFFDBEFFEBCBC=?BEEEE@=:?::?7?:8-6?7?@??# AS:i:0 XS:i:0 XN:i:0 XM:i:0 XO:i:0 XG:i:0 NM:i:0 MD:Z:65 YT:Z:UU; ```; The columns of this tab-delimited line are usually the following:. QNAME FLAG RNAME POS MAPQ CIGAR RNEXT PNEXT TLEN SEQ QUAL TAGS. 1) Read Name; 2) SAM flag[ --> decode](http://www.google.com/url?q=http%3A%2F%2Fbroadinstitute.github.io%2Fpicard%2Fexplain-flags.html&sa=D&sntz=1&usg=AOvVaw0FjgFbRCGf0ogpCecZqsJX); 3) contig name or * for unmapped; 4) mapped position of base 1 of a read on the reference sequence; 5) MAPQ mapping quality; 6) CIGAR string describing insertions and deletions; 7) Name of mate; 8) Position of mate; 9) Template length; 10) Read Sequence; 11) Read Quality; 12) Additional information in TAG:TYPE:VALUE format. You can read about the SAM/BAM format at the following link:. https://samtools.github.io/hts-specs/SAMv1.pdf. So to answer the last two questions. So having good coverage (read depth) lets you get around sequencing errors -- meaning you might have to repeat the experiment as well for ensuring consistency in the results. Some labs create error models with different known repeated ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658769491
Safety,detect,detecting,"@Axze-rgb This has to do with you sequencer detecting the Q-Score (Phred-scaled) read quality, that gets stored in the FASTQ files that and get inherited in converted BAM file when you align the reads. So a SAM/BAM file have read quality scores per base following the sequence, like this:. ```; CCTGCACGAACGAAATCCGCATGCGTCTGGTCGTTGTACGGAACGGCGGTTGTGTGACGAACGGC EDDEEDEE=EE?DE??DDDBADEBEFFFDBEFFEBCBC=?BEEEE@=:?::?7?:8-6?7?@??# ; ```. The score is a ASCII encrypted score + the value 33. You can read the Phred value here per ASCII value:. https://support.illumina.com/help/BaseSpace_OLH_009008/Content/Source/Informatics/BS/QualityScoreEncoding_swBS.htm. A full line in a SAM/BAM file would look like this:. ```; readID43GYAX15:7:1:1202:19894/1 256 contig87 540849 1 65M * 0 0 CCTGCACGAACGAAATCCGCATGCGTCTGGTCGTTGTACGGAACGGCGGTTGTGTGACGAACGGC EDDEEDEE=EE?DE??DDDBADEBEFFFDBEFFEBCBC=?BEEEE@=:?::?7?:8-6?7?@??# AS:i:0 XS:i:0 XN:i:0 XM:i:0 XO:i:0 XG:i:0 NM:i:0 MD:Z:65 YT:Z:UU; ```; The columns of this tab-delimited line are usually the following:. QNAME FLAG RNAME POS MAPQ CIGAR RNEXT PNEXT TLEN SEQ QUAL TAGS. 1) Read Name; 2) SAM flag[ --> decode](http://www.google.com/url?q=http%3A%2F%2Fbroadinstitute.github.io%2Fpicard%2Fexplain-flags.html&sa=D&sntz=1&usg=AOvVaw0FjgFbRCGf0ogpCecZqsJX); 3) contig name or * for unmapped; 4) mapped position of base 1 of a read on the reference sequence; 5) MAPQ mapping quality; 6) CIGAR string describing insertions and deletions; 7) Name of mate; 8) Position of mate; 9) Template length; 10) Read Sequence; 11) Read Quality; 12) Additional information in TAG:TYPE:VALUE format. You can read about the SAM/BAM format at the following link:. https://samtools.github.io/hts-specs/SAMv1.pdf. So to answer the last two questions. So having good coverage (read depth) lets you get around sequencing errors -- meaning you might have to repeat the experiment as well for ensuring consistency in the results. Some labs create error models with different known repeated ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658769491
Security,encrypt,encrypted,"@Axze-rgb This has to do with you sequencer detecting the Q-Score (Phred-scaled) read quality, that gets stored in the FASTQ files that and get inherited in converted BAM file when you align the reads. So a SAM/BAM file have read quality scores per base following the sequence, like this:. ```; CCTGCACGAACGAAATCCGCATGCGTCTGGTCGTTGTACGGAACGGCGGTTGTGTGACGAACGGC EDDEEDEE=EE?DE??DDDBADEBEFFFDBEFFEBCBC=?BEEEE@=:?::?7?:8-6?7?@??# ; ```. The score is a ASCII encrypted score + the value 33. You can read the Phred value here per ASCII value:. https://support.illumina.com/help/BaseSpace_OLH_009008/Content/Source/Informatics/BS/QualityScoreEncoding_swBS.htm. A full line in a SAM/BAM file would look like this:. ```; readID43GYAX15:7:1:1202:19894/1 256 contig87 540849 1 65M * 0 0 CCTGCACGAACGAAATCCGCATGCGTCTGGTCGTTGTACGGAACGGCGGTTGTGTGACGAACGGC EDDEEDEE=EE?DE??DDDBADEBEFFFDBEFFEBCBC=?BEEEE@=:?::?7?:8-6?7?@??# AS:i:0 XS:i:0 XN:i:0 XM:i:0 XO:i:0 XG:i:0 NM:i:0 MD:Z:65 YT:Z:UU; ```; The columns of this tab-delimited line are usually the following:. QNAME FLAG RNAME POS MAPQ CIGAR RNEXT PNEXT TLEN SEQ QUAL TAGS. 1) Read Name; 2) SAM flag[ --> decode](http://www.google.com/url?q=http%3A%2F%2Fbroadinstitute.github.io%2Fpicard%2Fexplain-flags.html&sa=D&sntz=1&usg=AOvVaw0FjgFbRCGf0ogpCecZqsJX); 3) contig name or * for unmapped; 4) mapped position of base 1 of a read on the reference sequence; 5) MAPQ mapping quality; 6) CIGAR string describing insertions and deletions; 7) Name of mate; 8) Position of mate; 9) Template length; 10) Read Sequence; 11) Read Quality; 12) Additional information in TAG:TYPE:VALUE format. You can read about the SAM/BAM format at the following link:. https://samtools.github.io/hts-specs/SAMv1.pdf. So to answer the last two questions. So having good coverage (read depth) lets you get around sequencing errors -- meaning you might have to repeat the experiment as well for ensuring consistency in the results. Some labs create error models with different known repeated ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658769491
Usability,learn,learning,":?::?7?:8-6?7?@??# AS:i:0 XS:i:0 XN:i:0 XM:i:0 XO:i:0 XG:i:0 NM:i:0 MD:Z:65 YT:Z:UU; ```; The columns of this tab-delimited line are usually the following:. QNAME FLAG RNAME POS MAPQ CIGAR RNEXT PNEXT TLEN SEQ QUAL TAGS. 1) Read Name; 2) SAM flag[ --> decode](http://www.google.com/url?q=http%3A%2F%2Fbroadinstitute.github.io%2Fpicard%2Fexplain-flags.html&sa=D&sntz=1&usg=AOvVaw0FjgFbRCGf0ogpCecZqsJX); 3) contig name or * for unmapped; 4) mapped position of base 1 of a read on the reference sequence; 5) MAPQ mapping quality; 6) CIGAR string describing insertions and deletions; 7) Name of mate; 8) Position of mate; 9) Template length; 10) Read Sequence; 11) Read Quality; 12) Additional information in TAG:TYPE:VALUE format. You can read about the SAM/BAM format at the following link:. https://samtools.github.io/hts-specs/SAMv1.pdf. So to answer the last two questions. So having good coverage (read depth) lets you get around sequencing errors -- meaning you might have to repeat the experiment as well for ensuring consistency in the results. Some labs create error models with different known repeated read lengths to generate a sequencer error model -- you don't need to do that, as that cost a lot of money to do properly. You can probably do it with known samples and compare it to golden truth datasets, but again that will cost. The other way to do it is to try other assays to confirm your results. . Understanding all the moving parts takes time. Sure it is very rewarding once you have a complete understanding of a model, but it all depends on how much time is reasonable for you. You can always keep learning after getting the PhD, as that just teaches you how to independently think about big problems and how to systematically break them down (defending your approach). Once you confidently have that thought process down, you're virtually unbeatable and don't need hone that skill as it already is innate. You can always practice that during your postdoc :) . Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658769491
Availability,down,down,"Hi All,. I appreciate that all of you are contributing scientific insights and taking your time to add thoughts on the DeepVariant GitHub issues. as @pichuan said, please remember that we are all scientists trying to work with each other and be respectful of each other. @Axze-rgb I haven't had a chance to look into the data (with what I have I would be looking at the effect in human data just to get a feel for how unusual it is). However, from your results, I'm not sure that this approach is promising and I am sorry I may have led you down a bad path. I was hoping to see sites that look clearly like subclonal SNPs (so look clear on other sources of error). From your pileups posted, it looks like many of the GQ30 VAF 0.15 population are mapping complexities. . It might be possible to further separate out mapping events, for example by increasing the MapQ filter, or excluding sites that are within a short distance of other variants, but I am not sure the approach will be worthwhile. My apologies, I think that in principle this is an interesting concept, but in practice, I am not sure your results indicate a clear win with the approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658889277
Usability,clear,clearly,"Hi All,. I appreciate that all of you are contributing scientific insights and taking your time to add thoughts on the DeepVariant GitHub issues. as @pichuan said, please remember that we are all scientists trying to work with each other and be respectful of each other. @Axze-rgb I haven't had a chance to look into the data (with what I have I would be looking at the effect in human data just to get a feel for how unusual it is). However, from your results, I'm not sure that this approach is promising and I am sorry I may have led you down a bad path. I was hoping to see sites that look clearly like subclonal SNPs (so look clear on other sources of error). From your pileups posted, it looks like many of the GQ30 VAF 0.15 population are mapping complexities. . It might be possible to further separate out mapping events, for example by increasing the MapQ filter, or excluding sites that are within a short distance of other variants, but I am not sure the approach will be worthwhile. My apologies, I think that in principle this is an interesting concept, but in practice, I am not sure your results indicate a clear win with the approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658889277
Availability,down,down,"you want to pursue a deeper the story, what proportion of time do you want to make it science-driven versus engineering-driven?. The first question you don't have to answer, and is more obvious to you than me -- and it would affect how you pursue the second -- so I'll tackle the second one :) The science-driven one is not so much goal-driven, but rather trying to uncover the mechanism that the model the organism operates through. This can be a rabbit hole as you hypothesis-test the model's response through different experiments. So if you want to pursue de-novo assembly, ask yourself why you got good results previously? That's why I mentioned the ""panel of clonals"", which has the same basic idea. You are starting with a closer variant in its molecular evolution, than a reference which might be quite distant and/or mixed in specific loci or contigs as compared to your clone. . Regarding would I trust a low frequency region given the evidence? When there are multiple variables that start to accumulate with an experiment, I usually start breaking down towards the root cause with simpler experiments. What does low frequency region mean in terms of sequencing regarding your organism's behavior? For example, you can validate known regions in your clone you have verified via other assays to confirm that the sequencing results match -- it might not even be low frequency. Next, as you mentioned, you check for heterozygous k-mer pairs, which you can investigate via tools GenomeScope and Smudgeplot as [presented in the following paper](https://www.nature.com/articles/s41467-020-14998-3). For example, if you look at the figure of the k-mer spectra of diploid Arabidopsis thaliana, it has a similar distribution as your original GQ plot (assuming correlation of GQ with coverage, which would need to be validated):. ![image](https://github.com/google/deepvariant/assets/6555937/6e225f54-b836-4c0d-a4af-88142066bace). They mentioned that _*""for a diploid species, increasing heterozygosi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1659358917
Modifiability,variab,variables,"you want to pursue a deeper the story, what proportion of time do you want to make it science-driven versus engineering-driven?. The first question you don't have to answer, and is more obvious to you than me -- and it would affect how you pursue the second -- so I'll tackle the second one :) The science-driven one is not so much goal-driven, but rather trying to uncover the mechanism that the model the organism operates through. This can be a rabbit hole as you hypothesis-test the model's response through different experiments. So if you want to pursue de-novo assembly, ask yourself why you got good results previously? That's why I mentioned the ""panel of clonals"", which has the same basic idea. You are starting with a closer variant in its molecular evolution, than a reference which might be quite distant and/or mixed in specific loci or contigs as compared to your clone. . Regarding would I trust a low frequency region given the evidence? When there are multiple variables that start to accumulate with an experiment, I usually start breaking down towards the root cause with simpler experiments. What does low frequency region mean in terms of sequencing regarding your organism's behavior? For example, you can validate known regions in your clone you have verified via other assays to confirm that the sequencing results match -- it might not even be low frequency. Next, as you mentioned, you check for heterozygous k-mer pairs, which you can investigate via tools GenomeScope and Smudgeplot as [presented in the following paper](https://www.nature.com/articles/s41467-020-14998-3). For example, if you look at the figure of the k-mer spectra of diploid Arabidopsis thaliana, it has a similar distribution as your original GQ plot (assuming correlation of GQ with coverage, which would need to be validated):. ![image](https://github.com/google/deepvariant/assets/6555937/6e225f54-b836-4c0d-a4af-88142066bace). They mentioned that _*""for a diploid species, increasing heterozygosi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1659358917
Security,validat,validate,"e second -- so I'll tackle the second one :) The science-driven one is not so much goal-driven, but rather trying to uncover the mechanism that the model the organism operates through. This can be a rabbit hole as you hypothesis-test the model's response through different experiments. So if you want to pursue de-novo assembly, ask yourself why you got good results previously? That's why I mentioned the ""panel of clonals"", which has the same basic idea. You are starting with a closer variant in its molecular evolution, than a reference which might be quite distant and/or mixed in specific loci or contigs as compared to your clone. . Regarding would I trust a low frequency region given the evidence? When there are multiple variables that start to accumulate with an experiment, I usually start breaking down towards the root cause with simpler experiments. What does low frequency region mean in terms of sequencing regarding your organism's behavior? For example, you can validate known regions in your clone you have verified via other assays to confirm that the sequencing results match -- it might not even be low frequency. Next, as you mentioned, you check for heterozygous k-mer pairs, which you can investigate via tools GenomeScope and Smudgeplot as [presented in the following paper](https://www.nature.com/articles/s41467-020-14998-3). For example, if you look at the figure of the k-mer spectra of diploid Arabidopsis thaliana, it has a similar distribution as your original GQ plot (assuming correlation of GQ with coverage, which would need to be validated):. ![image](https://github.com/google/deepvariant/assets/6555937/6e225f54-b836-4c0d-a4af-88142066bace). They mentioned that _*""for a diploid species, increasing heterozygosity will result in a higher first peak and a lower second peak"",*_ which makes sense. Then from there, you form hypotheses to test what might be the model of your organism operates by. So do I trust the reads, usually yes from HiFi, but maybe a bett",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1659358917
Testability,test,test,"@Axze-rgb A lot of what I'm gonna say I'm sure you already know well. As it feels you might be in a time-crunch, you're a better judge than me on these:. - Do you have a fairly complete story that you and your advisor are happy with?; - If you want to pursue a deeper the story, what proportion of time do you want to make it science-driven versus engineering-driven?. The first question you don't have to answer, and is more obvious to you than me -- and it would affect how you pursue the second -- so I'll tackle the second one :) The science-driven one is not so much goal-driven, but rather trying to uncover the mechanism that the model the organism operates through. This can be a rabbit hole as you hypothesis-test the model's response through different experiments. So if you want to pursue de-novo assembly, ask yourself why you got good results previously? That's why I mentioned the ""panel of clonals"", which has the same basic idea. You are starting with a closer variant in its molecular evolution, than a reference which might be quite distant and/or mixed in specific loci or contigs as compared to your clone. . Regarding would I trust a low frequency region given the evidence? When there are multiple variables that start to accumulate with an experiment, I usually start breaking down towards the root cause with simpler experiments. What does low frequency region mean in terms of sequencing regarding your organism's behavior? For example, you can validate known regions in your clone you have verified via other assays to confirm that the sequencing results match -- it might not even be low frequency. Next, as you mentioned, you check for heterozygous k-mer pairs, which you can investigate via tools GenomeScope and Smudgeplot as [presented in the following paper](https://www.nature.com/articles/s41467-020-14998-3). For example, if you look at the figure of the k-mer spectra of diploid Arabidopsis thaliana, it has a similar distribution as your original GQ plot (assuming",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1659358917
Usability,simpl,simpler,"you want to pursue a deeper the story, what proportion of time do you want to make it science-driven versus engineering-driven?. The first question you don't have to answer, and is more obvious to you than me -- and it would affect how you pursue the second -- so I'll tackle the second one :) The science-driven one is not so much goal-driven, but rather trying to uncover the mechanism that the model the organism operates through. This can be a rabbit hole as you hypothesis-test the model's response through different experiments. So if you want to pursue de-novo assembly, ask yourself why you got good results previously? That's why I mentioned the ""panel of clonals"", which has the same basic idea. You are starting with a closer variant in its molecular evolution, than a reference which might be quite distant and/or mixed in specific loci or contigs as compared to your clone. . Regarding would I trust a low frequency region given the evidence? When there are multiple variables that start to accumulate with an experiment, I usually start breaking down towards the root cause with simpler experiments. What does low frequency region mean in terms of sequencing regarding your organism's behavior? For example, you can validate known regions in your clone you have verified via other assays to confirm that the sequencing results match -- it might not even be low frequency. Next, as you mentioned, you check for heterozygous k-mer pairs, which you can investigate via tools GenomeScope and Smudgeplot as [presented in the following paper](https://www.nature.com/articles/s41467-020-14998-3). For example, if you look at the figure of the k-mer spectra of diploid Arabidopsis thaliana, it has a similar distribution as your original GQ plot (assuming correlation of GQ with coverage, which would need to be validated):. ![image](https://github.com/google/deepvariant/assets/6555937/6e225f54-b836-4c0d-a4af-88142066bace). They mentioned that _*""for a diploid species, increasing heterozygosi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1659358917
Usability,learn,learn,"Hello, . I write here again to say 2 things; - first I apologize for having be somewhat agressive towards the person who came in the discussion, it seems to me it was just a curious person after all and not a troll. So sorry about that. I apologize @Joe-r-code ; - We are going to do high coverage ONT sequencing, which should allow for a better SNP calling. If it works well, would you be interested to train DeepVariant on my data set? That would make it able to manage high heterozygous genomes. You have been very helpful and I really appreciated it. So, I think we could plan to collaborate on training deepvariant on a highly heterozygous genome with many paralogs? I also ask this, because honestly I would love to understand better Neural Network and I learn better by doing. . The sequencing will take some time though.; Cheers. . EDIT: by ""curious"" I mean someone wanting to understand, I don't mean weird. In French curious can mean weird, so I don't know if this is the case in English.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1722449849
Usability,guid,guidance,"@Axze-rgb Great to hear from you! How have you been? I only want to hear good news :) Thank you kindly for the opportunity, but unfortunately my time is a bit dominated by a few things these days. A collaboration is a big commitment in order for me to assist properly, so for now I prefer to just provide guidance as necessary. Before you sequence too much, it's best to practice training a model with some data on your own to get a feel for what's happening. You have a great tutorial (including the data) on how to train a model [at the following link](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). Again it has germline diploid assumptions built into it. If your data varies a lot, it will take some time for the weights to shift to your data's behavior in order to achieve good accuracy (when possible). Regarding paralogs, you might need to align to a pangenome graph via giraffe, like in [the following paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9365333/). Again you are adding complexities that require careful mastery of nuances you need to recognize when issues come up. Start playing with simple things first to get a good grasp of what's going on. So my humble recommendation is to take baby steps. Try to play with training a model first. Regarding learning about machine learning/deep neural networks (DNNs), find a book/youtube videos you can practice from - which matches your learning style and makes you code - to better understand how Deep Neural Networks work. Ideally sit in a class, so you are forced to do the homework, and begin with the fundamentals before eventually reaching DNNs (which have a prerequisite of some fundamentals of machine learning concepts as background). Most importantly, have fun and keep us posted :). Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1722685163
Deployability,release,release,"Hi @linlin-coder ,; Thank you for bringing up this issue. I noticed that you're working on PacBio data. The reason why this is happening is:. In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be:; Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689#issuecomment-1660748817
Usability,clear,clear,"Hi @linlin-coder ,; Thank you for bringing up this issue. I noticed that you're working on PacBio data. The reason why this is happening is:. In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be:; Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689#issuecomment-1660748817
Deployability,release,release,"> Hi @linlin-coder , Thank you for bringing up this issue.; > ; > I noticed that you're working on PacBio data.; > ; > The reason why this is happening is:; > ; > In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy.; > ; > Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence; > ; > > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase.; > ; > So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be: Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag.; > ; > I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release.; > ; > @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!. Thank you very much for your description of all the details of Deeptrio in the mutation detection environment. In the future, I will follow the process you suggested to redo the mutation detection. If there are no accidents, I will reply to you in the next two days. Thank you again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689#issuecomment-1661404840
Safety,detect,detection,"> Hi @linlin-coder , Thank you for bringing up this issue.; > ; > I noticed that you're working on PacBio data.; > ; > The reason why this is happening is:; > ; > In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy.; > ; > Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence; > ; > > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase.; > ; > So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be: Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag.; > ; > I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release.; > ; > @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!. Thank you very much for your description of all the details of Deeptrio in the mutation detection environment. In the future, I will follow the process you suggested to redo the mutation detection. If there are no accidents, I will reply to you in the next two days. Thank you again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689#issuecomment-1661404840
Usability,clear,clear,"> Hi @linlin-coder , Thank you for bringing up this issue.; > ; > I noticed that you're working on PacBio data.; > ; > The reason why this is happening is:; > ; > In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy.; > ; > Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence; > ; > > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase.; > ; > So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be: Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag.; > ; > I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release.; > ; > @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!. Thank you very much for your description of all the details of Deeptrio in the mutation detection environment. In the future, I will follow the process you suggested to redo the mutation detection. If there are no accidents, I will reply to you in the next two days. Thank you again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689#issuecomment-1661404840
Safety,detect,detected,"e exome](docs/deepvariant-exome-case-study.md).; * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina; RNA-seq.; * PacBio HiFi data, see the; [PacBio case study](docs/deepvariant-pacbio-model-case-study.md).; * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the; [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md); and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md).; * Hybrid PacBio HiFi + Illumina WGS, see the; [hybrid case study](docs/deepvariant-hybrid-case-study.md).; * Oxford Nanopore R9.4.1 data by using; [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```; INPUT_DIR=""${PWD}/YOUR_INPUT_PATH""; OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \; --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \; --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \; --output=/output/pileup --num_records=20. # And then your images are here:; ls ""${OUTPUT_DIR}""/pileup*.png; ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file.; 2. The reference file used to generate the BAM is different than the one used with DeepVariant.; 3. There might not be many SNPs left supported by informative reads.; 4. Besides requiring that you have a model for your technology, how many variants do you see in IGV that are based on high-quality reads?. The reason for RefCall is because the genotype for those calls is 0/0, as per the postprocessing that happens as the last step in DeepVariant. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690#issuecomment-1660589629
Usability,simpl,simplex-case-study,"Hi Sophie,. DeepVariant calls variants based on a trained model for different technologies. I'm not sure it has a model for Horizon Tru-Q 1, and only has models for the following technologies (`WGS`, `WES`, `PACBIO`, `ONT_R104`, `HYBRID_PACBIO_ILLUMINA`):. * NGS (Illumina) data for either a; [whole genome](docs/deepvariant-case-study.md) or; [whole exome](docs/deepvariant-exome-case-study.md).; * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina; RNA-seq.; * PacBio HiFi data, see the; [PacBio case study](docs/deepvariant-pacbio-model-case-study.md).; * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the; [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md); and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md).; * Hybrid PacBio HiFi + Illumina WGS, see the; [hybrid case study](docs/deepvariant-hybrid-case-study.md).; * Oxford Nanopore R9.4.1 data by using; [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```; INPUT_DIR=""${PWD}/YOUR_INPUT_PATH""; OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \; --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \; --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \; --output=/output/pileup --num_records=20. # And then your images are here:; ls ""${OUTPUT_DIR}""/pileup*.png; ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file.; 2. The reference file used to generate the BAM is different than the one used with DeepVariant.; 3. There might not be ma",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690#issuecomment-1660589629
Availability,error,error,"Hi Paul, . I got an error when running on my HPC so I'm just waiting for the IT people to get back to me, I assume its a simple fix since I have managed to get the deepvariant container to work a few months back: . ```; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; ```. I'll let you know when they reply!; Amy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1667418894
Deployability,install,installed,"Hi Paul, . I got an error when running on my HPC so I'm just waiting for the IT people to get back to me, I assume its a simple fix since I have managed to get the deepvariant container to work a few months back: . ```; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; ```. I'll let you know when they reply!; Amy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1667418894
Usability,simpl,simple,"Hi Paul, . I got an error when running on my HPC so I'm just waiting for the IT people to get back to me, I assume its a simple fix since I have managed to get the deepvariant container to work a few months back: . ```; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; ```. I'll let you know when they reply!; Amy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1667418894
Deployability,update,update,"This has a simple fix. Basically you need to replace `/output/realigned_reads` with a location you have write-access to. For example, you can do the following commands:. Assuming you have write-access to this folder `/scratch/c.c21087028/checking_variant_deepvariant`, you can create a sub-directory called `realigned_reads` like this:. ```; mkdir /scratch/c.c21087028/checking_variant_deepvariant/realigned_reads; ```. Then in your Singularity script you have two options to pick from:. #### Option 1 - update only the following line:. ``` ; --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/scratch/c.c21087028/checking_variant_deepvariant/realigned_reads"" \; ```. All other lines for Option 1 remain the same. #### Option 2 - update the following lines (I used -B to make /output accessible inside the container):. ```; sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /scratch/c.c21087028/checking_variant_deepvariant/:/output/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. ```. The rest of the lines can stay the same for Option 2. Regarding a BED file, you don't need one as they are the same thing as regions -- which you already provide:. https://en.wikipedia.org/wiki/BED_(file_format). Let me know how this runs, and if your run into any other issues. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1667728104
Security,access,access,"This has a simple fix. Basically you need to replace `/output/realigned_reads` with a location you have write-access to. For example, you can do the following commands:. Assuming you have write-access to this folder `/scratch/c.c21087028/checking_variant_deepvariant`, you can create a sub-directory called `realigned_reads` like this:. ```; mkdir /scratch/c.c21087028/checking_variant_deepvariant/realigned_reads; ```. Then in your Singularity script you have two options to pick from:. #### Option 1 - update only the following line:. ``` ; --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/scratch/c.c21087028/checking_variant_deepvariant/realigned_reads"" \; ```. All other lines for Option 1 remain the same. #### Option 2 - update the following lines (I used -B to make /output accessible inside the container):. ```; sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /scratch/c.c21087028/checking_variant_deepvariant/:/output/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. ```. The rest of the lines can stay the same for Option 2. Regarding a BED file, you don't need one as they are the same thing as regions -- which you already provide:. https://en.wikipedia.org/wiki/BED_(file_format). Let me know how this runs, and if your run into any other issues. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1667728104
Usability,simpl,simple,"This has a simple fix. Basically you need to replace `/output/realigned_reads` with a location you have write-access to. For example, you can do the following commands:. Assuming you have write-access to this folder `/scratch/c.c21087028/checking_variant_deepvariant`, you can create a sub-directory called `realigned_reads` like this:. ```; mkdir /scratch/c.c21087028/checking_variant_deepvariant/realigned_reads; ```. Then in your Singularity script you have two options to pick from:. #### Option 1 - update only the following line:. ``` ; --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/scratch/c.c21087028/checking_variant_deepvariant/realigned_reads"" \; ```. All other lines for Option 1 remain the same. #### Option 2 - update the following lines (I used -B to make /output accessible inside the container):. ```; sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /scratch/c.c21087028/checking_variant_deepvariant/:/output/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. ```. The rest of the lines can stay the same for Option 2. Regarding a BED file, you don't need one as they are the same thing as regions -- which you already provide:. https://en.wikipedia.org/wiki/BED_(file_format). Let me know how this runs, and if your run into any other issues. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1667728104
Performance,perform,performance,"Thanks Paul for your answer,. That's clear now. That means I need to choose EC2 instance type with 1 GPU because instance with more than 1 GPU does not have any better impact on DeepVariant's performance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696#issuecomment-1679371052
Usability,clear,clear,"Thanks Paul for your answer,. That's clear now. That means I need to choose EC2 instance type with 1 GPU because instance with more than 1 GPU does not have any better impact on DeepVariant's performance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696#issuecomment-1679371052
Deployability,continuous,continuous,"Hi,. I'm sorry for the late reply. I didn't have work yesterday and couldn't conduct the experiment. Thank you very much for the detailed suggestions you provided. I have observed that Paul provided very detailed suggestions regarding the steps he mentioned. I will try them out today and provide feedback here. My research top is about short tandem repeat in DNA, for example, in human reference gene hg 19 chr 2:47641559-47641586,; it has following base sequence:. chr 2:47641559-47641586 CAGGT AAAAAAAAAAAAAAAAAAAAAAAAAAA GGGTT. After search articles about next-generation sequencing, I found that their have so many factors that influence sequencing outout,such as during library construction step, PCR process will cause different repeat times because of polymerase slip. Besides that, during sequencing step, taking the Illumina sequencer as an example, during each round of cleaning, the base may not cleaned thoroughly or successfully combined with the next round of base; or other wise, a continuous series of repeated bases will influence illumination recognition signal in sequencer, and the weakened light intensity will affect the efficiency of the sequencer in identifying each binding base. So the above is the background of my use of Deepvariant. In my research topic, remove noise is very important process, because in somatic cells or cancer cells it's have different repeat times in tandem repeat regions, and I want to call every indel from human cells, but their are so many influence factor that change NGS output data so I can't find truth data in human's gene. Thank you for reading the questions I encountered in my research, I would greatly appreciate it if you could help me,; and wish you a pleasant work and life. Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697#issuecomment-1683209021
Usability,feedback,feedback,"Hi,. I'm sorry for the late reply. I didn't have work yesterday and couldn't conduct the experiment. Thank you very much for the detailed suggestions you provided. I have observed that Paul provided very detailed suggestions regarding the steps he mentioned. I will try them out today and provide feedback here. My research top is about short tandem repeat in DNA, for example, in human reference gene hg 19 chr 2:47641559-47641586,; it has following base sequence:. chr 2:47641559-47641586 CAGGT AAAAAAAAAAAAAAAAAAAAAAAAAAA GGGTT. After search articles about next-generation sequencing, I found that their have so many factors that influence sequencing outout,such as during library construction step, PCR process will cause different repeat times because of polymerase slip. Besides that, during sequencing step, taking the Illumina sequencer as an example, during each round of cleaning, the base may not cleaned thoroughly or successfully combined with the next round of base; or other wise, a continuous series of repeated bases will influence illumination recognition signal in sequencer, and the weakened light intensity will affect the efficiency of the sequencer in identifying each binding base. So the above is the background of my use of Deepvariant. In my research topic, remove noise is very important process, because in somatic cells or cancer cells it's have different repeat times in tandem repeat regions, and I want to call every indel from human cells, but their are so many influence factor that change NGS output data so I can't find truth data in human's gene. Thank you for reading the questions I encountered in my research, I would greatly appreciate it if you could help me,; and wish you a pleasant work and life. Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697#issuecomment-1683209021
Testability,log,logically,"Dear Paul,. Thank you for you suggestions about STR analysis! . Please do not apologize, as you have indeed helped me a lot and saved me a lot of time. Thank you for providing very professional guidance on Deepvariant. I am still unable to reach a conclusion on whether deepvariant can be used in some way on logically paired samples, so I will take a look at the HipSTR and deep repeat you provided, which may be helpful for my research. Thank you very much!. Wich you have a nice day!. Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697#issuecomment-1683275474
Usability,guid,guidance,"Dear Paul,. Thank you for you suggestions about STR analysis! . Please do not apologize, as you have indeed helped me a lot and saved me a lot of time. Thank you for providing very professional guidance on Deepvariant. I am still unable to reach a conclusion on whether deepvariant can be used in some way on logically paired samples, so I will take a look at the HipSTR and deep repeat you provided, which may be helpful for my research. Thank you very much!. Wich you have a nice day!. Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697#issuecomment-1683275474
Security,validat,validation,"Hi Sophie,. Ambitious endeavor, but some things to be aware of. So a low VAF would probably confuse the model training, as that is usually a homozygous reference call. With a low VAF, the other alleles would probably be automatically be selected as supporting information for training in the pileup to confirm a call, as they would be in the majority. Thus when you run your trained model in the future against new BAM files, it would not look so much for the rare variants, but rather select for the more prevalent ones in your data to call a genotype (GT) and subsequently a GQ score of supporting it. . Regarding training and validation, they always usually must be different. For example, in the original DeepVariant paper under the [Supplementary Text and Figures](https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.4235/MediaObjects/41587_2018_BFnbt4235_MOESM81_ESM.pdf), they trained on chromosomes 1-19, and validated on 20-22:. ![image](https://github.com/google/deepvariant/assets/6555937/83cdb7f3-44e9-4185-99cf-81362489dfcf). If you want to train chromosome-by-chromosome, it is possible to take a previously trained model and continue training it as noted in the [tutorial specified by `GCS_PRETRAINED_WGS_MODEL`](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval), but that could create batch effects (meaning it will shift the model one way with one chromosome, and then back with another even under shuffling conditions). So it might be better to start with a complete set of chromosomes you select for training. Not trying to discourage you, so give it a try and let's see what happens -- in any case it will be fun learning experience. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698#issuecomment-1681102936
Usability,learn,learning,"Hi Sophie,. Ambitious endeavor, but some things to be aware of. So a low VAF would probably confuse the model training, as that is usually a homozygous reference call. With a low VAF, the other alleles would probably be automatically be selected as supporting information for training in the pileup to confirm a call, as they would be in the majority. Thus when you run your trained model in the future against new BAM files, it would not look so much for the rare variants, but rather select for the more prevalent ones in your data to call a genotype (GT) and subsequently a GQ score of supporting it. . Regarding training and validation, they always usually must be different. For example, in the original DeepVariant paper under the [Supplementary Text and Figures](https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.4235/MediaObjects/41587_2018_BFnbt4235_MOESM81_ESM.pdf), they trained on chromosomes 1-19, and validated on 20-22:. ![image](https://github.com/google/deepvariant/assets/6555937/83cdb7f3-44e9-4185-99cf-81362489dfcf). If you want to train chromosome-by-chromosome, it is possible to take a previously trained model and continue training it as noted in the [tutorial specified by `GCS_PRETRAINED_WGS_MODEL`](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval), but that could create batch effects (meaning it will shift the model one way with one chromosome, and then back with another even under shuffling conditions). So it might be better to start with a complete set of chromosomes you select for training. Not trying to discourage you, so give it a try and let's see what happens -- in any case it will be fun learning experience. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698#issuecomment-1681102936
Availability,down,downsampling,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698#issuecomment-1711046219
Energy Efficiency,power,power,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698#issuecomment-1711046219
Modifiability,variab,variable,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698#issuecomment-1711046219
Security,validat,validation,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698#issuecomment-1711046219
Testability,test,test,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698#issuecomment-1711046219
Usability,simpl,simple,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698#issuecomment-1711046219
Deployability,update,updated,"Hi Sophie,. So as you know, besides the genetic information passed on from the parents, each of us is born with an additionally small number of novel genetic changes called _de novo_ mutations (i.e. from environmental effects, etc). These traits are thus not passed from the parents, thus violating Mendelian inheritance. So when you use `rtg-tools mendelian` with the `--output` flag, it will save an updated VCF file annotated with calls violating Mendelian inheritance, thus highlighting the _de novo_ mutations. The information (header) fields in these updated annotated VCF files will have the following:. ```; ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency"">; ##INFO=<ID=MCV,Number=.,Type=String,Description=""Variant violates mendelian inheritance constraints"">; ##INFO=<ID=MCU,Number=.,Type=String,Description=""Mendelian consistency status can not be determined"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=DN,Number=1,Type=String,Description=""De novo allele"">; ##FORMAT=<ID=MCP,Number=.,Type=String,Description=""Describes the expected genotype ploidy in cases where the given genotype does not match the expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2; Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y; Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y; Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y; Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y; Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y; Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y; ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699#issuecomment-1716447969
Modifiability,inherit,inheritance,"Hi Sophie,. So as you know, besides the genetic information passed on from the parents, each of us is born with an additionally small number of novel genetic changes called _de novo_ mutations (i.e. from environmental effects, etc). These traits are thus not passed from the parents, thus violating Mendelian inheritance. So when you use `rtg-tools mendelian` with the `--output` flag, it will save an updated VCF file annotated with calls violating Mendelian inheritance, thus highlighting the _de novo_ mutations. The information (header) fields in these updated annotated VCF files will have the following:. ```; ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency"">; ##INFO=<ID=MCV,Number=.,Type=String,Description=""Variant violates mendelian inheritance constraints"">; ##INFO=<ID=MCU,Number=.,Type=String,Description=""Mendelian consistency status can not be determined"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=DN,Number=1,Type=String,Description=""De novo allele"">; ##FORMAT=<ID=MCP,Number=.,Type=String,Description=""Describes the expected genotype ploidy in cases where the given genotype does not match the expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2; Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y; Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y; Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y; Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y; Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y; Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y; ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699#issuecomment-1716447969
Performance,perform,perform,"mber=1,Type=String,Description=""De novo allele"">; ##FORMAT=<ID=MCP,Number=.,Type=String,Description=""Describes the expected genotype ploidy in cases where the given genotype does not match the expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2; Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y; Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y; Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y; Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y; Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y; Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y; ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF refinement based on pedigree information:. * [dv-trio](https://github.com/VCCRI/dv-trio) with [FamSeq](https://github.com/wwylab/FamSeq) (This is an earlier version approach of DeepVariant before DeepTrio); * [Octopus](https://github.com/luntergroup/octopus) ([doc example](https://luntergroup.github.io/octopus/docs/guides/models/trio/)); * [GATK HaplotypeCaller + GenotypeGVCFs + CalculateGenotypePosteriors refinement](https://gatk.broadinstitute.org/hc/en-us/articles/360037226592-CalculateGenotypePosteriors), and [an additional informational link](https://hpc.nih.gov/training/gatk_tutorial/workflow-overview.html); * [DeNovoGear](https://github.com/ultimatesource/denovogear). The key point to take away from this is not that there are options, but how these options internally work to infer the genotype and its probability given the data. Some work better with longer reads, and some with shorter reads. You want to play with them to get a feel of what is hap",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699#issuecomment-1716447969
Security,validat,validation,"umns (father/mother/child) in your joint VCF. To determine a _de novo_ call, you just look for genotypes that would not follow Mendelian inheritance, such as `0/0 0/0 0/1`, such as:. ```; chr7 54624683 chr7_54624683_A_AATC A AATC 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/0:39:22,16:28:27,0,48:.. 0/0:40:40,0:50:0,120,1199:.. 0/1:28:28,0:50:0,90,899:..; ```; Though keep in mind DeepTrio/GLnexus might produce [false positives](https://www.technologynetworks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,; Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf); [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false); [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699#issuecomment-1716447969
Testability,log,login,"orks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,; Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf); [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false); [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003880); [4] [DeepTrio: Variant Calling in Families Using Deep Learning](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1); [5] [A unified haplotype-based method for accurate and comprehensive variant calling](https://pubmed.ncbi.nlm.nih.gov/33782612/) _(This is the Octopus paper.)_; [6] [DeNovoGear: de novo indel and point mutation discovery and phasing](https://pubmed.ncbi.nlm.nih.gov/23975140/)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699#issuecomment-1716447969
Usability,guid,guides,"al daughter2; Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y; Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y; Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y; Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y; Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y; Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y; ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF refinement based on pedigree information:. * [dv-trio](https://github.com/VCCRI/dv-trio) with [FamSeq](https://github.com/wwylab/FamSeq) (This is an earlier version approach of DeepVariant before DeepTrio); * [Octopus](https://github.com/luntergroup/octopus) ([doc example](https://luntergroup.github.io/octopus/docs/guides/models/trio/)); * [GATK HaplotypeCaller + GenotypeGVCFs + CalculateGenotypePosteriors refinement](https://gatk.broadinstitute.org/hc/en-us/articles/360037226592-CalculateGenotypePosteriors), and [an additional informational link](https://hpc.nih.gov/training/gatk_tutorial/workflow-overview.html); * [DeNovoGear](https://github.com/ultimatesource/denovogear). The key point to take away from this is not that there are options, but how these options internally work to infer the genotype and its probability given the data. Some work better with longer reads, and some with shorter reads. You want to play with them to get a feel of what is happening given different data. If you are curious, you can read the papers and mathematics behind each approach, and you'll be surprised by their similarity in approaches of inferring the call and its probability (quality). I have included a list of papers with links in the reference section below. Now if the above is too easy, and you want to make _de novo_ variant calling more exciting, you can us",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699#issuecomment-1716447969
Usability,guid,guidance,"Successfully run, once again sincerely thank you for your guidance！",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700#issuecomment-1687514662
Availability,avail,available,"Hi @raphaelbetschart,. Just wondering on a few things:. 1) What are your QUAL and GQ scores for that call site?; 2) Is that SNP at an exon boundary?; 3) What tissue is this from? (Different tissue types can have an impact with DeepVariant RNA-Seq.); 4) Is this site in the [REDIportal](http://srv00.recas.ba.infn.it/atlas/search.html)? . Based on this curve, at 5x coverage you should still get a good majority of the CDS label variants that you saw at 3x coverage (this figure is from the Supplementary data available in [the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false) listed in the Reference section):. ![image](https://github.com/google/deepvariant/assets/6555937/2f6f8745-89ee-455e-8d20-950ef983875b). Thanks,; Paul. #### References; [1] [A deep-learning-based RNA-seq germline variant caller](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701#issuecomment-1695811605
Testability,log,login,"Hi @raphaelbetschart,. Just wondering on a few things:. 1) What are your QUAL and GQ scores for that call site?; 2) Is that SNP at an exon boundary?; 3) What tissue is this from? (Different tissue types can have an impact with DeepVariant RNA-Seq.); 4) Is this site in the [REDIportal](http://srv00.recas.ba.infn.it/atlas/search.html)? . Based on this curve, at 5x coverage you should still get a good majority of the CDS label variants that you saw at 3x coverage (this figure is from the Supplementary data available in [the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false) listed in the Reference section):. ![image](https://github.com/google/deepvariant/assets/6555937/2f6f8745-89ee-455e-8d20-950ef983875b). Thanks,; Paul. #### References; [1] [A deep-learning-based RNA-seq germline variant caller](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701#issuecomment-1695811605
Usability,learn,learning-based,"Hi @raphaelbetschart,. Just wondering on a few things:. 1) What are your QUAL and GQ scores for that call site?; 2) Is that SNP at an exon boundary?; 3) What tissue is this from? (Different tissue types can have an impact with DeepVariant RNA-Seq.); 4) Is this site in the [REDIportal](http://srv00.recas.ba.infn.it/atlas/search.html)? . Based on this curve, at 5x coverage you should still get a good majority of the CDS label variants that you saw at 3x coverage (this figure is from the Supplementary data available in [the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false) listed in the Reference section):. ![image](https://github.com/google/deepvariant/assets/6555937/2f6f8745-89ee-455e-8d20-950ef983875b). Thanks,; Paul. #### References; [1] [A deep-learning-based RNA-seq germline variant caller](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701#issuecomment-1695811605
Usability,clear,clear,"Hi @raphaelbetschart . In addition to what @pgrosu said. We do (rarely) observe genotype calls that differ substantially from the standard observations about HET/HOM/REF. We have made some prior observations about this for Germline data as a [section in our FAQ ](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data) and you can see a [poster](https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096) describing this observation as well and relating it to situations with a segmental duplication. . A few other questions/observations -. Have you looked at the site in IGV (as @pgrosu mentions)?. What are the MAPQ values for reads? DeepVariant with default parameters won't see reads with MAPQ<5. If there is a clear bias in MAPQ where the REF allele has a much lower MAPQ, DeepVariant may think the REF alleles are mismapped or might not see them in the pileup. The coverage of this region is pretty high (e.g. as an abundant transcript or as a gene family with many isoforms). Do you know which of these two is the case?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701#issuecomment-1696419700
Performance,perform,performance,"Hi @Fred-07 . When we investigated Element sequencing for exomes with the out-of-the-box Illumina models, we generally observed accuracy that was as good or better as that observed with Illumina without any retraining. With v1.5 the whole genome models do include joint training with Element data, and including element in WGS training does make Element accuracy somewhat further improved. I expect we'll try to get Element exomes and add them into the exome model training in the future, but for now I think you will see good performance with the existing exome model. If you do have any feedback on performance with Element exomes, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703#issuecomment-1705737423
Usability,feedback,feedback,"Hi @Fred-07 . When we investigated Element sequencing for exomes with the out-of-the-box Illumina models, we generally observed accuracy that was as good or better as that observed with Illumina without any retraining. With v1.5 the whole genome models do include joint training with Element data, and including element in WGS training does make Element accuracy somewhat further improved. I expect we'll try to get Element exomes and add them into the exome model training in the future, but for now I think you will see good performance with the existing exome model. If you do have any feedback on performance with Element exomes, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703#issuecomment-1705737423
Testability,test,testing,Thanks for the feedback and the good news about the model. I plan to get some WES data from Element for testing.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703#issuecomment-1708558330
Usability,feedback,feedback,Thanks for the feedback and the good news about the model. I plan to get some WES data from Element for testing.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703#issuecomment-1708558330
Deployability,pipeline,pipeline,"Hi Paul; Thanks for all the informations. As learnt from above, I should use the ""DeepVariant -> WhatsHap -> DeepTrio"" pipeline for better accuracy of the Deeptrio, as it do not support the functionality of reading haplotagging. But the DeepVariant can do it since 1.4.0, which means in case of trio analysis, ""DeepVariant + GLnexus"" will already be most accurate way currently, am I get it right?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704#issuecomment-1705891486
Usability,learn,learnt,"Hi Paul; Thanks for all the informations. As learnt from above, I should use the ""DeepVariant -> WhatsHap -> DeepTrio"" pipeline for better accuracy of the Deeptrio, as it do not support the functionality of reading haplotagging. But the DeepVariant can do it since 1.4.0, which means in case of trio analysis, ""DeepVariant + GLnexus"" will already be most accurate way currently, am I get it right?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704#issuecomment-1705891486
Deployability,update,update,"rself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training and v",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706#issuecomment-1720360294
Performance,optimiz,optimizing,"Hi Sophie,. So tuning a model is both an art and a science. Yes, accuracy can be lower initially, as in the following result described [by Pi-Chuan in another post](https://github.com/google/deepvariant/issues/185#issuecomment-494919509):. ![image](https://github.com/google/deepvariant/assets/6555937/bf862317-f323-47eb-bd69-6fe255e3223e). Thus no training parameters are the same for any model, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning; * Random Search; * Grid Search; * Bayesian Optimization; * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706#issuecomment-1720360294
Security,validat,validation,"ough there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training and validation data. . Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706#issuecomment-1720360294
Usability,guid,guide,"Hi Sophie,. So tuning a model is both an art and a science. Yes, accuracy can be lower initially, as in the following result described [by Pi-Chuan in another post](https://github.com/google/deepvariant/issues/185#issuecomment-494919509):. ![image](https://github.com/google/deepvariant/assets/6555937/bf862317-f323-47eb-bd69-6fe255e3223e). Thus no training parameters are the same for any model, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning; * Random Search; * Grid Search; * Bayesian Optimization; * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706#issuecomment-1720360294
Availability,checkpoint,checkpoint,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4; ```Json; {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}; ```. ##### PacBio; ```Json; {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706#issuecomment-1732869051
Performance,perform,performing,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4; ```Json; {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}; ```. ##### PacBio; ```Json; {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706#issuecomment-1732869051
Security,validat,validate,"l`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4; ```Json; {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}; ```. ##### PacBio; ```Json; {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}; ```. ##### WES; ```Json; {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}; ```. ##### WGS; ```Json; {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}; ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706#issuecomment-1732869051
Usability,guid,guide,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4; ```Json; {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}; ```. ##### PacBio; ```Json; {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706#issuecomment-1732869051
Availability,down,downside,"Hi @genieusbio ,. One thing to note:. In https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38 , when you run `run_deepvariant`, this flag is specified: `--regions /input/idt_capture_novogene.grch38.bed`. Which means only the variants within the regions specified in the BED is used. If you look at the BED file:. ```; $ head -5 ./input/idt_capture_novogene.grch38.bed; chr1 69090 70008; chr1 450739 451678; chr1 685715 686654; chr1 925941 926013; chr1 930154 930336; ```. which does not include the region you're looking for. If you want to force call everything in that BAM file, you can simply remove the flag `--regions /input/idt_capture_novogene.grch38.bed` from your run. The downside is that will take longer. Or, if you just want to make sure that particular region is covered, then you can use @pgrosu 's suggestion and specify a small region that covers that range. Hopefully this is clear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708#issuecomment-1720730115
Usability,simpl,simply,"Hi @genieusbio ,. One thing to note:. In https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38 , when you run `run_deepvariant`, this flag is specified: `--regions /input/idt_capture_novogene.grch38.bed`. Which means only the variants within the regions specified in the BED is used. If you look at the BED file:. ```; $ head -5 ./input/idt_capture_novogene.grch38.bed; chr1 69090 70008; chr1 450739 451678; chr1 685715 686654; chr1 925941 926013; chr1 930154 930336; ```. which does not include the region you're looking for. If you want to force call everything in that BAM file, you can simply remove the flag `--regions /input/idt_capture_novogene.grch38.bed` from your run. The downside is that will take longer. Or, if you just want to make sure that particular region is covered, then you can use @pgrosu 's suggestion and specify a small region that covers that range. Hopefully this is clear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708#issuecomment-1720730115
Integrability,message,message,"Hi @pgrosu,. Thank you very much for looking into the paper. We always try to do our best to present the algorithm, code and experimental design to deliver the message most clearly. Any feedback on how to make it better and more understandable is always highly appreciated. However, as this is not a technical issue about DeepVariant, it would be helpful to do this over email. Please send an email to shafin@google.com at your convenience so we can discuss how to improve the manuscript.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/709#issuecomment-1724108680
Usability,clear,clearly,"Hi @pgrosu,. Thank you very much for looking into the paper. We always try to do our best to present the algorithm, code and experimental design to deliver the message most clearly. Any feedback on how to make it better and more understandable is always highly appreciated. However, as this is not a technical issue about DeepVariant, it would be helpful to do this over email. Please send an email to shafin@google.com at your convenience so we can discuss how to improve the manuscript.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/709#issuecomment-1724108680
Energy Efficiency,schedul,schedule,"Thank you for clearing my doubts despite your extremely busy schedule, I hereby present to you my most sincere appreciation",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/713#issuecomment-1746323445
Usability,clear,clearing,"Thank you for clearing my doubts despite your extremely busy schedule, I hereby present to you my most sincere appreciation",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/713#issuecomment-1746323445
Modifiability,evolve,evolved,"Thanks for your reply. Hopefully reads seem to map with a MAPQ of 60 so Deepvariant should see them. We believe the issue is high SNP density making some standing variant hard to call. It's also very possible there is no signal, i.e. the bdelloids have evolved a very low mutation rate and we are chasing ghosts. Since they reproduce asexually (at least in the lab) by automixis, it is a possibility. We developed a simple script, for each SNP that seems to be a de novo one, we look in the ancestral pileup, and we always find the SNP there but not called. I think the high density messes up callers internal maths, and some SNP get a low chance of being called. Anyway, the ONT sequencing is ongoing. We will see with the pileup there. There are also solutions with comparing assembly graphs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716#issuecomment-1761318730
Usability,simpl,simple,"Thanks for your reply. Hopefully reads seem to map with a MAPQ of 60 so Deepvariant should see them. We believe the issue is high SNP density making some standing variant hard to call. It's also very possible there is no signal, i.e. the bdelloids have evolved a very low mutation rate and we are chasing ghosts. Since they reproduce asexually (at least in the lab) by automixis, it is a possibility. We developed a simple script, for each SNP that seems to be a de novo one, we look in the ancestral pileup, and we always find the SNP there but not called. I think the high density messes up callers internal maths, and some SNP get a low chance of being called. Anyway, the ONT sequencing is ongoing. We will see with the pileup there. There are also solutions with comparing assembly graphs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716#issuecomment-1761318730
Availability,error,error,"Hi @amy-houseman . The strand of the read is one of the input channels in DeepVariant, so it is able to see that information and to learn the effects of strand bias on variant calling during training. For more information on what data is seen by DeepVariant, you can see the blog [Looking through DeepVariant's eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). We don't write the strand information itself in the variant calls. For filtering, we instead recommend using the GQ field, which we find to be well calibrated with the probability of genotype error. Because DeepVariant sees the strand information, it will incorporate this into its confidence about the variant. If you have some specific aspect of your problem which you think the strand of reads will behave different from the genomes and exomes DeepVariant is trained on, and you want to do independent filtering, you would have to find some other method to annotate the strand information as DeepVariant does not write this directly to the VCF.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/719#issuecomment-1767795328
Usability,learn,learn,"Hi @amy-houseman . The strand of the read is one of the input channels in DeepVariant, so it is able to see that information and to learn the effects of strand bias on variant calling during training. For more information on what data is seen by DeepVariant, you can see the blog [Looking through DeepVariant's eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). We don't write the strand information itself in the variant calls. For filtering, we instead recommend using the GQ field, which we find to be well calibrated with the probability of genotype error. Because DeepVariant sees the strand information, it will incorporate this into its confidence about the variant. If you have some specific aspect of your problem which you think the strand of reads will behave different from the genomes and exomes DeepVariant is trained on, and you want to do independent filtering, you would have to find some other method to annotate the strand information as DeepVariant does not write this directly to the VCF.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/719#issuecomment-1767795328
Availability,error,error,"@dennishendriksen ,. 1) Totally understandable, please give it a try when you have time and let us know if you are still facing issues. ; 2) Those are R9.4 data. The ONT model only supports R10.4 simplex or duplex. R9.4 has elevated error rate that causes a ton of candidates. DeepTio wouldn't work with R9 data. I am closing this issue. Feel free to reopen in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724#issuecomment-1819466106
Usability,simpl,simplex,"@dennishendriksen ,. 1) Totally understandable, please give it a try when you have time and let us know if you are still facing issues. ; 2) Those are R9.4 data. The ONT model only supports R10.4 simplex or duplex. R9.4 has elevated error rate that causes a ton of candidates. DeepTio wouldn't work with R9 data. I am closing this issue. Feel free to reopen in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724#issuecomment-1819466106
Availability,error,error,"@crazysummerW ; I had the same issue here. It turned out to be the problem of the h5 file in the tmp dir.; If multiple programs open the h5 simultaneously, the error would occur. So I avoided this by creating a unique tmp dir for each sample, which used a lot of file handles. @pichuan @kishwarshafin Could you please take a look at this? Probably renaming the h5 file to keep it unique would be a simple and easy solution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725#issuecomment-1820853733
Safety,avoid,avoided,"@crazysummerW ; I had the same issue here. It turned out to be the problem of the h5 file in the tmp dir.; If multiple programs open the h5 simultaneously, the error would occur. So I avoided this by creating a unique tmp dir for each sample, which used a lot of file handles. @pichuan @kishwarshafin Could you please take a look at this? Probably renaming the h5 file to keep it unique would be a simple and easy solution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725#issuecomment-1820853733
Usability,simpl,simple,"@crazysummerW ; I had the same issue here. It turned out to be the problem of the h5 file in the tmp dir.; If multiple programs open the h5 simultaneously, the error would occur. So I avoided this by creating a unique tmp dir for each sample, which used a lot of file handles. @pichuan @kishwarshafin Could you please take a look at this? Probably renaming the h5 file to keep it unique would be a simple and easy solution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725#issuecomment-1820853733
Availability,robust,robust,"Hi @ZuyaoLiu ,; To confirm what you said -- do you mean that you train your own model (on your own data) and was able to get a much lower Mendelian violation rate? If so, that's great!!. Given that this is a non-human sample (not what we trained on), it seems like the right way to proceed is either to use our DeepTrio model, or like you said, to use your own customized model. So far both approaches seem like they would produce quite decent Mendelian violation rate. In the future, our team is interested in thinking more about making our model more generally robust to all non-human species as well. So thank you for your feedback. Hopefully the two options (either DeepTrio or your own model) will work for you for now. I'll close this issue now, but please feel free to share more thoughts.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726#issuecomment-1852999418
Usability,feedback,feedback,"Hi @ZuyaoLiu ,; To confirm what you said -- do you mean that you train your own model (on your own data) and was able to get a much lower Mendelian violation rate? If so, that's great!!. Given that this is a non-human sample (not what we trained on), it seems like the right way to proceed is either to use our DeepTrio model, or like you said, to use your own customized model. So far both approaches seem like they would produce quite decent Mendelian violation rate. In the future, our team is interested in thinking more about making our model more generally robust to all non-human species as well. So thank you for your feedback. Hopefully the two options (either DeepTrio or your own model) will work for you for now. I'll close this issue now, but please feel free to share more thoughts.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726#issuecomment-1852999418
Usability,feedback,feedback,"@sophienguyen01 it looks like you are trying to call somatic mutations here. If this is the case, please have a look at https://github.com/google/deepsomatic. DeepSomatic is still experimental, and we don't currently support retraining. However, it is designed to call somatic variants. . We would be very interested in the results you observe with this dataset if you are able to provide feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/728#issuecomment-1802319218
Availability,error,error,The error comes from the line `output_queue = multiprocessing.Queue()`; Could you try a simple test? ; Run docker in CLI model: `docker run -it <DeepVariant image> bash`; Inside docker start Python3 and execute:; ```; import multiprocessing; q = multiprocessing.Queue(); ```; Please let us know if that works.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733#issuecomment-1816864777
Testability,test,test,The error comes from the line `output_queue = multiprocessing.Queue()`; Could you try a simple test? ; Run docker in CLI model: `docker run -it <DeepVariant image> bash`; Inside docker start Python3 and execute:; ```; import multiprocessing; q = multiprocessing.Queue(); ```; Please let us know if that works.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733#issuecomment-1816864777
Usability,simpl,simple,The error comes from the line `output_queue = multiprocessing.Queue()`; Could you try a simple test? ; Run docker in CLI model: `docker run -it <DeepVariant image> bash`; Inside docker start Python3 and execute:; ```; import multiprocessing; q = multiprocessing.Queue(); ```; Please let us know if that works.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733#issuecomment-1816864777
Availability,error,error,"Hi, ; thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:; > ; > ```; > import multiprocessing; > q = multiprocessing.Queue(); > ```; > ; > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):; ```; Python 3.8.10 (default, May 26 2023, 14:05:08); [GCC 9.4.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import multiprocessing; >>> q = multiprocessing.Queue(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue; return Queue(maxsize, ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__; self._rlock = ctx.Lock(); File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock; return Lock(ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__; SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__; sl = self._semlock = _multiprocessing.SemLock(; FileNotFoundError: [Errno 2] No such file or directory; ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! ; Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733#issuecomment-1818694916
Deployability,release,release,"Hi, ; thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:; > ; > ```; > import multiprocessing; > q = multiprocessing.Queue(); > ```; > ; > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):; ```; Python 3.8.10 (default, May 26 2023, 14:05:08); [GCC 9.4.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import multiprocessing; >>> q = multiprocessing.Queue(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue; return Queue(maxsize, ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__; self._rlock = ctx.Lock(); File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock; return Lock(ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__; SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__; sl = self._semlock = _multiprocessing.SemLock(; FileNotFoundError: [Errno 2] No such file or directory; ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! ; Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733#issuecomment-1818694916
Integrability,synchroniz,synchronize,"Hi, ; thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:; > ; > ```; > import multiprocessing; > q = multiprocessing.Queue(); > ```; > ; > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):; ```; Python 3.8.10 (default, May 26 2023, 14:05:08); [GCC 9.4.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import multiprocessing; >>> q = multiprocessing.Queue(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue; return Queue(maxsize, ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__; self._rlock = ctx.Lock(); File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock; return Lock(ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__; SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__; sl = self._semlock = _multiprocessing.SemLock(; FileNotFoundError: [Errno 2] No such file or directory; ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! ; Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733#issuecomment-1818694916
Performance,queue,queues,"Hi, ; thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:; > ; > ```; > import multiprocessing; > q = multiprocessing.Queue(); > ```; > ; > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):; ```; Python 3.8.10 (default, May 26 2023, 14:05:08); [GCC 9.4.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import multiprocessing; >>> q = multiprocessing.Queue(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue; return Queue(maxsize, ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__; self._rlock = ctx.Lock(); File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock; return Lock(ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__; SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__; sl = self._semlock = _multiprocessing.SemLock(; FileNotFoundError: [Errno 2] No such file or directory; ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! ; Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733#issuecomment-1818694916
Testability,test,test,"Hi, ; thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:; > ; > ```; > import multiprocessing; > q = multiprocessing.Queue(); > ```; > ; > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):; ```; Python 3.8.10 (default, May 26 2023, 14:05:08); [GCC 9.4.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import multiprocessing; >>> q = multiprocessing.Queue(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue; return Queue(maxsize, ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__; self._rlock = ctx.Lock(); File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock; return Lock(ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__; SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__; sl = self._semlock = _multiprocessing.SemLock(; FileNotFoundError: [Errno 2] No such file or directory; ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! ; Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733#issuecomment-1818694916
Usability,simpl,simple,"Hi, ; thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:; > ; > ```; > import multiprocessing; > q = multiprocessing.Queue(); > ```; > ; > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):; ```; Python 3.8.10 (default, May 26 2023, 14:05:08); [GCC 9.4.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import multiprocessing; >>> q = multiprocessing.Queue(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue; return Queue(maxsize, ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__; self._rlock = ctx.Lock(); File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock; return Lock(ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__; SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__; sl = self._semlock = _multiprocessing.SemLock(; FileNotFoundError: [Errno 2] No such file or directory; ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! ; Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733#issuecomment-1818694916
Availability,avail,available,"> Hi @pioneer-pi ,; > ; > Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:; > ; > ```shell; > ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log; > ```; > ; > Then upload the `dv_build.log` here so a detailed log is available. I pull the docker image of deepvariant:1.5.0, and then I start a container through `docker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```; + DV_PLATFORM=ubuntu-20.04; + ln -sf /usr/bin/python3.8 /usr/local/bin/python3; + cd; + rm -rf clif; + git clone https://github.com/google/clif.git; ProxyChains-3.1 (http://proxychains.sf.net); Cloning into 'clif'...; + cd clif; + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]; + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7; Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`; + git init; Reinitialized existing Git repository in /root/clif/.git/; + ./INSTALL.sh; +++ dirname ./INSTALL.sh; ++ cd .; ++ pwd; + CLIFSRC_DIR=/root/clif; + BUILD_DIR=/root/clif/build; + declare -a CMAKE_G_FLAG; + declare -a MAKE_PA",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737#issuecomment-1818093820
Integrability,message,message,"Reinitialized existing Git repository in /root/clif/.git/; + ./INSTALL.sh; +++ dirname ./INSTALL.sh; ++ cd .; ++ pwd; + CLIFSRC_DIR=/root/clif; + BUILD_DIR=/root/clif/build; + declare -a CMAKE_G_FLAG; + declare -a MAKE_PARALLELISM; + which ninja; + CMAKE_G_FLAGS=(); + MAKE_OR_NINJA=make; + MAKE_PARALLELISM=(-j 2); + [[ -r /proc/cpuinfo ]]; ++ cat /proc/cpuinfo; ++ grep -c '^processor'; + N_CPUS=32; + [[ 32 -gt 0 ]]; + MAKE_PARALLELISM=(-j $N_CPUS); + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}); + echo 'Using make for the clif backend build.'; Using make for the clif backend build.; + [[ '' =~ ^-?-h ]]; + [[ -n '' ]]; ++ which python3; + PYTHON=/usr/local/bin/python3; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""); -- Checking for module 'protobuf'; -- No package 'protobuf' found; CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):; A required package was not found; Call Stack (most recent call first):; /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal); clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules); clif/CMakeLists.txt:22 (include); ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737#issuecomment-1818093820
Modifiability,config,config," source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```; + DV_PLATFORM=ubuntu-20.04; + ln -sf /usr/bin/python3.8 /usr/local/bin/python3; + cd; + rm -rf clif; + git clone https://github.com/google/clif.git; ProxyChains-3.1 (http://proxychains.sf.net); Cloning into 'clif'...; + cd clif; + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]; + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7; Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`; + git init; Reinitialized existing Git repository in /root/clif/.git/; + ./INSTALL.sh; +++ dirname ./INSTALL.sh; ++ cd .; ++ pwd; + CLIFSRC_DIR=/root/clif; + BUILD_DIR=/root/clif/build; + declare -a CMAKE_G_FLAG; + declare -a MAKE_PARALLELISM; + which ninja; + CMAKE_G_FLAGS=(); + MAKE_OR_NINJA=make; + MAKE_PARALLELISM=(-j 2); + [[ -r /proc/cpuinfo ]]; ++ cat /proc/cpuinfo; ++ grep -c '^processor'; + N_CPUS=32; + [[ 32 -gt 0 ]]; + MAKE_PARALLELISM=(-j $N_CPUS); + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}); + echo 'Using make for the clif backend build.'; Using make for the clif backend build.; + [[ '' =~ ^-?-h ]]; + [[ -n '' ]]; ++ which python3; + PYTHON=/usr/local/bin/python3; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; +",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737#issuecomment-1818093820
Testability,test,test,"> Hi @pioneer-pi ,; > ; > Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:; > ; > ```shell; > ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log; > ```; > ; > Then upload the `dv_build.log` here so a detailed log is available. I pull the docker image of deepvariant:1.5.0, and then I start a container through `docker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```; + DV_PLATFORM=ubuntu-20.04; + ln -sf /usr/bin/python3.8 /usr/local/bin/python3; + cd; + rm -rf clif; + git clone https://github.com/google/clif.git; ProxyChains-3.1 (http://proxychains.sf.net); Cloning into 'clif'...; + cd clif; + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]; + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7; Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`; + git init; Reinitialized existing Git repository in /root/clif/.git/; + ./INSTALL.sh; +++ dirname ./INSTALL.sh; ++ cd .; ++ pwd; + CLIFSRC_DIR=/root/clif; + BUILD_DIR=/root/clif/build; + declare -a CMAKE_G_FLAG; + declare -a MAKE_PA",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737#issuecomment-1818093820
Usability,undo,undo,"ot), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```; + DV_PLATFORM=ubuntu-20.04; + ln -sf /usr/bin/python3.8 /usr/local/bin/python3; + cd; + rm -rf clif; + git clone https://github.com/google/clif.git; ProxyChains-3.1 (http://proxychains.sf.net); Cloning into 'clif'...; + cd clif; + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]; + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7; Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`; + git init; Reinitialized existing Git repository in /root/clif/.git/; + ./INSTALL.sh; +++ dirname ./INSTALL.sh; ++ cd .; ++ pwd; + CLIFSRC_DIR=/root/clif; + BUILD_DIR=/root/clif/build; + declare -a CMAKE_G_FLAG; + declare -a MAKE_PARALLELISM; + which ninja; + CMAKE_G_FLAGS=(); + MAKE_OR_NINJA=make; + MAKE_PARALLELISM=(-j 2); + [[ -r /proc/cpuinfo ]]; ++ cat /proc/cpuinfo; ++ grep -c '^processor'; + N_CPUS=32; + [[ 32 -gt 0 ]]; + MAKE_PARALLELISM=(-j $N_CPUS); + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}); + echo 'Using make for the clif backend build.'; Using make for the clif backend build.; + [[ '' =~ ^-?-h ]]; + [[ -n '' ]]; ++ which python3; + PYTHON=/usr/local/bin/python3; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737#issuecomment-1818093820
Availability,checkpoint,checkpoint,"Hi @themkdemiiir,. Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @<num of shards> to the file name and add `--task` flag that specifies the task number for each shard. ; * call_variants will be run with the same number of shards.; * postprocess_variants has to be run in a single process. Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. ```; bin/make_examples \; --examples /tmn/your_examples.tfrecord@200.gz \; --mode calling \; --reads /tmp/your_input_bam.bam \; --realign_reads \; --ref=/tmp/your_reference.fna \; --task=11. # Input for each instance of call_variants is the output of one instance of make_examples:; bin/call_variants.par \; --batch_size=32 \; --checkpoint <Path to the model checkpoint or saved model>.ckpt \; --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \; --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. # Input for for postprocess would be the output of all instances of call_variants:; /tmp/your_call_variants_output.cvo.tfrecord@200.gz; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744#issuecomment-1836586525
Deployability,pipeline,pipeline,"Hi @themkdemiiir,. Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @<num of shards> to the file name and add `--task` flag that specifies the task number for each shard. ; * call_variants will be run with the same number of shards.; * postprocess_variants has to be run in a single process. Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. ```; bin/make_examples \; --examples /tmn/your_examples.tfrecord@200.gz \; --mode calling \; --reads /tmp/your_input_bam.bam \; --realign_reads \; --ref=/tmp/your_reference.fna \; --task=11. # Input for each instance of call_variants is the output of one instance of make_examples:; bin/call_variants.par \; --batch_size=32 \; --checkpoint <Path to the model checkpoint or saved model>.ckpt \; --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \; --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. # Input for for postprocess would be the output of all instances of call_variants:; /tmp/your_call_variants_output.cvo.tfrecord@200.gz; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744#issuecomment-1836586525
Usability,simpl,simply,"Hi @themkdemiiir,. Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @<num of shards> to the file name and add `--task` flag that specifies the task number for each shard. ; * call_variants will be run with the same number of shards.; * postprocess_variants has to be run in a single process. Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. ```; bin/make_examples \; --examples /tmn/your_examples.tfrecord@200.gz \; --mode calling \; --reads /tmp/your_input_bam.bam \; --realign_reads \; --ref=/tmp/your_reference.fna \; --task=11. # Input for each instance of call_variants is the output of one instance of make_examples:; bin/call_variants.par \; --batch_size=32 \; --checkpoint <Path to the model checkpoint or saved model>.ckpt \; --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \; --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. # Input for for postprocess would be the output of all instances of call_variants:; /tmp/your_call_variants_output.cvo.tfrecord@200.gz; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744#issuecomment-1836586525
Usability,clear,clear,"Hi @themkdemiiir ,; @akolesnikov already mentioned this in his previous answer -- it's totally possible to split by chromosomes first. Especially if that works better for your workflow.; (I know for many users, their workflow already split by chromosomes, so it makes sense to go from these BAMs that are per chromosome.). @akolesnikov 's answer above further explains that DeepVariant does its own sharding for any BAM inputs. You can run DeepVariant on a BAM that includes all chromosomes, but you can also do that on a BAM that include one chromosome (and then combine the VCFs later if you like). Please feel free to follow up if it's not clear. I'll close this issue for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744#issuecomment-1839946718
Availability,checkpoint,checkpoint,"> Hi @themkdemiiir,; > ; > Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker.; > ; > * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @ to the file name and add `--task` flag that specifies the task number for each shard.; > * call_variants will be run with the same number of shards.; > * postprocess_variants has to be run in a single process.; > ; > Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:; > ; > ```; > bin/make_examples \; > --examples /tmn/your_examples.tfrecord@200.gz \; > --mode calling \; > --reads /tmp/your_input_bam.bam \; > --realign_reads \; > --ref=/tmp/your_reference.fna \; > --task=11; > ; > # Input for each instance of call_variants is the output of one instance of make_examples:; > bin/call_variants.par \; > --batch_size=32 \; > --checkpoint <Path to the model checkpoint or saved model>.ckpt \; > --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \; > --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz; > ; > # Input for for postprocess would be the output of all instances of call_variants:; > /tmp/your_call_variants_output.cvo.tfrecord@200.gz; > ```. So how could I merge the output of the call_variants step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744#issuecomment-1855569624
Deployability,pipeline,pipeline,"> Hi @themkdemiiir,; > ; > Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker.; > ; > * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @ to the file name and add `--task` flag that specifies the task number for each shard.; > * call_variants will be run with the same number of shards.; > * postprocess_variants has to be run in a single process.; > ; > Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:; > ; > ```; > bin/make_examples \; > --examples /tmn/your_examples.tfrecord@200.gz \; > --mode calling \; > --reads /tmp/your_input_bam.bam \; > --realign_reads \; > --ref=/tmp/your_reference.fna \; > --task=11; > ; > # Input for each instance of call_variants is the output of one instance of make_examples:; > bin/call_variants.par \; > --batch_size=32 \; > --checkpoint <Path to the model checkpoint or saved model>.ckpt \; > --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \; > --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz; > ; > # Input for for postprocess would be the output of all instances of call_variants:; > /tmp/your_call_variants_output.cvo.tfrecord@200.gz; > ```. So how could I merge the output of the call_variants step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744#issuecomment-1855569624
Usability,simpl,simply,"> Hi @themkdemiiir,; > ; > Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker.; > ; > * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @ to the file name and add `--task` flag that specifies the task number for each shard.; > * call_variants will be run with the same number of shards.; > * postprocess_variants has to be run in a single process.; > ; > Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:; > ; > ```; > bin/make_examples \; > --examples /tmn/your_examples.tfrecord@200.gz \; > --mode calling \; > --reads /tmp/your_input_bam.bam \; > --realign_reads \; > --ref=/tmp/your_reference.fna \; > --task=11; > ; > # Input for each instance of call_variants is the output of one instance of make_examples:; > bin/call_variants.par \; > --batch_size=32 \; > --checkpoint <Path to the model checkpoint or saved model>.ckpt \; > --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \; > --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz; > ; > # Input for for postprocess would be the output of all instances of call_variants:; > /tmp/your_call_variants_output.cvo.tfrecord@200.gz; > ```. So how could I merge the output of the call_variants step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744#issuecomment-1855569624
Availability,error,error,"HI @pichuan,. Please check this screenshot out and see if it is more clear. ![image](https://github.com/google/deepvariant/assets/34832128/82ed1379-29b4-403f-aa07-75002c1d831e). What I did was I first shelled into the container. Then I checked the files in `/opt/deepvariant/bin/` in the container because I tried to look for the `run_deeptrio` file, which could not be found when I followed steps in https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity and gave the error message as in the title of this issue. Was it possible that I missed anything when running Deeptrio?. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745#issuecomment-1840119054
Integrability,message,message,"HI @pichuan,. Please check this screenshot out and see if it is more clear. ![image](https://github.com/google/deepvariant/assets/34832128/82ed1379-29b4-403f-aa07-75002c1d831e). What I did was I first shelled into the container. Then I checked the files in `/opt/deepvariant/bin/` in the container because I tried to look for the `run_deeptrio` file, which could not be found when I followed steps in https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity and gave the error message as in the title of this issue. Was it possible that I missed anything when running Deeptrio?. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745#issuecomment-1840119054
Usability,clear,clear,"HI @pichuan,. Please check this screenshot out and see if it is more clear. ![image](https://github.com/google/deepvariant/assets/34832128/82ed1379-29b4-403f-aa07-75002c1d831e). What I did was I first shelled into the container. Then I checked the files in `/opt/deepvariant/bin/` in the container because I tried to look for the `run_deeptrio` file, which could not be found when I followed steps in https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity and gave the error message as in the title of this issue. Was it possible that I missed anything when running Deeptrio?. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745#issuecomment-1840119054
Availability,mainten,maintenance-policy,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version?. I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash; gcloud compute instances create ""${USER}-gpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash; wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh; sudo bash -x install_nvidia_docker.sh ; ```; (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash; wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh; sudo bash -x install_singularity.sh ; ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash; BIN_VERSION=1.6.0; singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu""; ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash; pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif ; -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif; ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745#issuecomment-1840177389
Deployability,install,installed,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version?. I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash; gcloud compute instances create ""${USER}-gpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash; wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh; sudo bash -x install_nvidia_docker.sh ; ```; (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash; wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh; sudo bash -x install_singularity.sh ; ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash; BIN_VERSION=1.6.0; singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu""; ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash; pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif ; -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif; ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745#issuecomment-1840177389
Integrability,message,message,"eloper.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; --model_type is required.; Pass --helpshort or --helpfull to see help on flags.; ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```; --model_type is required.; Pass --helpshort or --helpfull to see help on flags.; ```. From here, I should be able to pass in the arguments from Quick Start and have it working. (If you want, I can continue the test). ---. @alanlamsiu ,; Now I got here, please check these two things on your side:. 1. Can you confirm that your .sif is indeed made from `1.6.0`? (Which is our latest version. Please don't use the rc2 one).; 2. Can you try `/opt/deepvariant/bin/deeptrio/run_deeptrio`? You seem to be using a different path. I'm also not sure where that came from -- if we have any GitHub documentation that's inconsistent, please let me know and I can update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745#issuecomment-1840177389
Performance,optimiz,optimized,"msiu used the .sif file, I'll also do something similar:; So, then I ran:. ```bash; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant_deeptrio-1.6.0-gpu.sif \; /opt/deepvariant/bin/deeptrio/run_deeptrio; ```. The command above gave me:; ```. ==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; --model_type is required.; Pass --helpshort or --helpfull to see help on flags.; ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```; --model_type is required.; Pass --helpsho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745#issuecomment-1840177389
Testability,test,tested,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version?. I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash; gcloud compute instances create ""${USER}-gpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash; wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh; sudo bash -x install_nvidia_docker.sh ; ```; (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash; wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh; sudo bash -x install_singularity.sh ; ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash; BIN_VERSION=1.6.0; singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu""; ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash; pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif ; -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif; ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745#issuecomment-1840177389
Usability,learn,learning-container-license,"s one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:; So, then I ran:. ```bash; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant_deeptrio-1.6.0-gpu.sif \; /opt/deepvariant/bin/deeptrio/run_deeptrio; ```. The command above gave me:; ```. ==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with Ten",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745#issuecomment-1840177389
Availability,down,downstream,"Hi @dmarkie . Thank you for your feedback. We did struggle and discuss internally on the representation that we should use for the hemizygous calls to make. Ultimately, we decided to use 0/0 and 1/1 for our presumption that this would break fewer downstream methods. However, some of that is a subjective judgement. The compromise of having the option in postprocess to handle this is an interesting one. We'll talk internally about the amount of effort and maintenance to support this. I can't make a commitment to anything now, but it is a very reasonable proposal.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/751#issuecomment-1854763028
Usability,feedback,feedback,"Hi @dmarkie . Thank you for your feedback. We did struggle and discuss internally on the representation that we should use for the hemizygous calls to make. Ultimately, we decided to use 0/0 and 1/1 for our presumption that this would break fewer downstream methods. However, some of that is a subjective judgement. The compromise of having the option in postprocess to handle this is an interesting one. We'll talk internally about the amount of effort and maintenance to support this. I can't make a commitment to anything now, but it is a very reasonable proposal.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/751#issuecomment-1854763028
Availability,down,down,"Hi @one-matrix ,. From your original post, you mentioned you ran `python deepvariant/call_variants.py`. That won't work in DeepVariant setup. For DeepVariant, all binaries needs to be built with bazel. Unlike other pure Python setup, simply `python` a .py file won't execute it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash; gcloud compute instances create ""${USER}-cpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-64"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash; gcloud compute ssh ${USER}-cpu --zone us-west1-b; ```. Then I get the repo:. ```bash; git clone https://github.com/google/deepvariant.git; cd deepvariant; ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash; sudo su; ```. and then:. ```bash; ./build-prereq.sh; ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash; ./build_and_test.sh; ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756#issuecomment-1865388872
Testability,test,test,"Hi @one-matrix ,. From your original post, you mentioned you ran `python deepvariant/call_variants.py`. That won't work in DeepVariant setup. For DeepVariant, all binaries needs to be built with bazel. Unlike other pure Python setup, simply `python` a .py file won't execute it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash; gcloud compute instances create ""${USER}-cpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-64"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash; gcloud compute ssh ${USER}-cpu --zone us-west1-b; ```. Then I get the repo:. ```bash; git clone https://github.com/google/deepvariant.git; cd deepvariant; ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash; sudo su; ```. and then:. ```bash; ./build-prereq.sh; ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash; ./build_and_test.sh; ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756#issuecomment-1865388872
Usability,simpl,simply,"Hi @one-matrix ,. From your original post, you mentioned you ran `python deepvariant/call_variants.py`. That won't work in DeepVariant setup. For DeepVariant, all binaries needs to be built with bazel. Unlike other pure Python setup, simply `python` a .py file won't execute it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash; gcloud compute instances create ""${USER}-cpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-64"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash; gcloud compute ssh ${USER}-cpu --zone us-west1-b; ```. Then I get the repo:. ```bash; git clone https://github.com/google/deepvariant.git; cd deepvariant; ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash; sudo su; ```. and then:. ```bash; ./build-prereq.sh; ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash; ./build_and_test.sh; ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756#issuecomment-1865388872
Availability,down,downsample,"I guess the naming pattern is related to second question.; Say I have a 50x depth BAM file that I want to downsample to 20%. I can squeeze about 5 downsampled BAMs out of 50x.; Given I assume i will perform make_examples with ""--training"" within a loop of say 5 iterations, what would the naming scheme look like? Hence the importance of the seed parameter if downsampling same BAM mulitiple times within a loop; i would change seed each time... I hope this clears up the questions and motivations behind them....",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765#issuecomment-1909090188
Performance,perform,perform,"I guess the naming pattern is related to second question.; Say I have a 50x depth BAM file that I want to downsample to 20%. I can squeeze about 5 downsampled BAMs out of 50x.; Given I assume i will perform make_examples with ""--training"" within a loop of say 5 iterations, what would the naming scheme look like? Hence the importance of the seed parameter if downsampling same BAM mulitiple times within a loop; i would change seed each time... I hope this clears up the questions and motivations behind them....",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765#issuecomment-1909090188
Usability,clear,clears,"I guess the naming pattern is related to second question.; Say I have a 50x depth BAM file that I want to downsample to 20%. I can squeeze about 5 downsampled BAMs out of 50x.; Given I assume i will perform make_examples with ""--training"" within a loop of say 5 iterations, what would the naming scheme look like? Hence the importance of the seed parameter if downsampling same BAM mulitiple times within a loop; i would change seed each time... I hope this clears up the questions and motivations behind them....",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765#issuecomment-1909090188
Deployability,install,installed,"@Ge-Lab if possible, please structure your issue using code fences. See [this guide](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) for details. It will make it easier to read and understand if you place logs inside code blocks, for example. [This guide](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html) suggests a few things to try. Interestingly, it apperas you should set `CUDA_VISIBLE_DEVICES=0` within the container itself, but `SINGULARITYENV_CUDA_VISIBLE_DEVICES=0` outside of the container. Were you setting the variable appropriately, within the container or outside of it?. Additionally, since you have two GPUs you will want to set this variable to 0,1. Do you have the `nvidia-container-cli` installed as suggested on the support page?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774#issuecomment-1954774836
Modifiability,variab,variable,"@Ge-Lab if possible, please structure your issue using code fences. See [this guide](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) for details. It will make it easier to read and understand if you place logs inside code blocks, for example. [This guide](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html) suggests a few things to try. Interestingly, it apperas you should set `CUDA_VISIBLE_DEVICES=0` within the container itself, but `SINGULARITYENV_CUDA_VISIBLE_DEVICES=0` outside of the container. Were you setting the variable appropriately, within the container or outside of it?. Additionally, since you have two GPUs you will want to set this variable to 0,1. Do you have the `nvidia-container-cli` installed as suggested on the support page?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774#issuecomment-1954774836
Testability,log,logs,"@Ge-Lab if possible, please structure your issue using code fences. See [this guide](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) for details. It will make it easier to read and understand if you place logs inside code blocks, for example. [This guide](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html) suggests a few things to try. Interestingly, it apperas you should set `CUDA_VISIBLE_DEVICES=0` within the container itself, but `SINGULARITYENV_CUDA_VISIBLE_DEVICES=0` outside of the container. Were you setting the variable appropriately, within the container or outside of it?. Additionally, since you have two GPUs you will want to set this variable to 0,1. Do you have the `nvidia-container-cli` installed as suggested on the support page?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774#issuecomment-1954774836
Usability,guid,guide,"@Ge-Lab if possible, please structure your issue using code fences. See [this guide](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) for details. It will make it easier to read and understand if you place logs inside code blocks, for example. [This guide](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html) suggests a few things to try. Interestingly, it apperas you should set `CUDA_VISIBLE_DEVICES=0` within the container itself, but `SINGULARITYENV_CUDA_VISIBLE_DEVICES=0` outside of the container. Were you setting the variable appropriately, within the container or outside of it?. Additionally, since you have two GPUs you will want to set this variable to 0,1. Do you have the `nvidia-container-cli` installed as suggested on the support page?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774#issuecomment-1954774836
Usability,learn,learned,"Hi @NIBIL401 . I don't see any other specific issues in your command. Without knowing more about the specific types of differences, it's difficult to give advice on what might be missing. One observation that we do have is that DeepVariant has learned not to call RNA editing events as variants. These are post-transcription changes to the RNA sequence. Those edits appear as A->G and T->C in sequencing data. To give more advice beyond this, I think I would need to know more about the sequencing (the most ideal would be to have some a BAM file or snippet with a variant call not being made that we can diagnose why). Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775#issuecomment-1962219387
Testability,test,tested,"Thank you for the feedback! @danielecook I can confirm that the files in the tmp directory do look to be normal as you described above. @kishwarshafin, I tested the docker that you suggested and now it seems that Deepvariant did not run at all (vcfs and gvcfs are empty); [deepvarrun_b37_MND_G33.1kei.log](https://github.com/google/deepvariant/files/14458499/deepvarrun_b37_MND_G33.1kei.log); ; I have attached a log file for one of the samples, so you can see what happened.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776#issuecomment-1972749674
Usability,feedback,feedback,"Thank you for the feedback! @danielecook I can confirm that the files in the tmp directory do look to be normal as you described above. @kishwarshafin, I tested the docker that you suggested and now it seems that Deepvariant did not run at all (vcfs and gvcfs are empty); [deepvarrun_b37_MND_G33.1kei.log](https://github.com/google/deepvariant/files/14458499/deepvarrun_b37_MND_G33.1kei.log); ; I have attached a log file for one of the samples, so you can see what happened.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776#issuecomment-1972749674
Usability,simpl,simple,"There is a simple typo in the `rtg vcfmerge` step above, it should `=` after `Sex`. `--add-header ""##SAMPLE<ID=HG002,Sex=MALE>"" \`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779#issuecomment-2118639400
Modifiability,refactor,refactoring,"Hi @rickymagner ,; If you want to learn more about channels, you can take look at this blog post:; https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/. Two things to mention:; 1. This would be an advanced use of the DeepVariant codebase, both from the research and engineering aspects. We can try to answer your questions, but can't guarantee that we'll have bandwidth or knowledge to support your use case all the way.; 2. Since r1.6, we've made quite a lot of refactoring regarding the channels internal implementation. These new code will come out (with documentation) in a future r1.7, but right now it's not quite ready yet. That said, if you have a specific use case now, it's worth looking at the blog post and the r1.6 to try to implement your own channel! If you have any findings or questions, feel free to share or ask here. We can try our best to answer. I've always enjoyed seeing our users building on top of DeepVariant!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790#issuecomment-1996353013
Usability,learn,learn,"Hi @rickymagner ,; If you want to learn more about channels, you can take look at this blog post:; https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/. Two things to mention:; 1. This would be an advanced use of the DeepVariant codebase, both from the research and engineering aspects. We can try to answer your questions, but can't guarantee that we'll have bandwidth or knowledge to support your use case all the way.; 2. Since r1.6, we've made quite a lot of refactoring regarding the channels internal implementation. These new code will come out (with documentation) in a future r1.7, but right now it's not quite ready yet. That said, if you have a specific use case now, it's worth looking at the blog post and the r1.6 to try to implement your own channel! If you have any findings or questions, feel free to share or ask here. We can try our best to answer. I've always enjoyed seeing our users building on top of DeepVariant!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790#issuecomment-1996353013
Availability,down,downloaded,"> Hello,; > ; > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script.; > ; > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated.; > ; > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:; ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docum",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793#issuecomment-2008095137
Deployability,install,install,"> Hello,; > ; > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script.; > ; > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated.; > ; > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:; ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docum",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793#issuecomment-2008095137
Integrability,depend,dependency,"me chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:; ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?""; If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2.; Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels.; And you will need truth labels for your training set and validation set, . > ; > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there.; > ; > Best, Haley; > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793#issuecomment-2008095137
Security,validat,validation,"ve reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script.; > ; > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated.; > ; > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:; ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793#issuecomment-2008095137
Usability,guid,guidance,"o,; > ; > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script.; > ; > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated.; > ; > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:; ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793#issuecomment-2008095137
Security,validat,validation,"@helizabeth1103 For training / validation sets -- the main point here is to keep them separate. ; And then, for all data that goes into training set, they will need to be shuffle into one set of shards. So that you can get the num_examples, and a consistent path. For example, in our documentation you see something like:. ```; name: ""HG001""; tfrecord_path: ""OUTPUT_GCS_BUCKET/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 342758; ```. For training, you need one `tfrecord_path` that refer to all the files (output of shulffling), and a num_examples. For validation, you need a separate file with similar format. Hope that's clear! I'll close this issue now that you're able to run shuffling!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793#issuecomment-2013124668
Usability,clear,clear,"@helizabeth1103 For training / validation sets -- the main point here is to keep them separate. ; And then, for all data that goes into training set, they will need to be shuffle into one set of shards. So that you can get the num_examples, and a consistent path. For example, in our documentation you see something like:. ```; name: ""HG001""; tfrecord_path: ""OUTPUT_GCS_BUCKET/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 342758; ```. For training, you need one `tfrecord_path` that refer to all the files (output of shulffling), and a num_examples. For validation, you need a separate file with similar format. Hope that's clear! I'll close this issue now that you're able to run shuffling!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793#issuecomment-2013124668
Availability,checkpoint,checkpoints,"Thank you! I re-ran the training and validation sets with that flag, and re-shuffled them. Now, however, when I go to train the model (using the same parameters as the example case study--I just want to test out the process) I'm not getting any checkpoints in the output training directory, just the event log and the json file. What does this mean? Is the training step failing, or do I simply need to adjust my parameters? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797#issuecomment-2029425877
Security,validat,validation,"Thank you! I re-ran the training and validation sets with that flag, and re-shuffled them. Now, however, when I go to train the model (using the same parameters as the example case study--I just want to test out the process) I'm not getting any checkpoints in the output training directory, just the event log and the json file. What does this mean? Is the training step failing, or do I simply need to adjust my parameters? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797#issuecomment-2029425877
Testability,test,test,"Thank you! I re-ran the training and validation sets with that flag, and re-shuffled them. Now, however, when I go to train the model (using the same parameters as the example case study--I just want to test out the process) I'm not getting any checkpoints in the output training directory, just the event log and the json file. What does this mean? Is the training step failing, or do I simply need to adjust my parameters? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797#issuecomment-2029425877
Usability,simpl,simply,"Thank you! I re-ran the training and validation sets with that flag, and re-shuffled them. Now, however, when I go to train the model (using the same parameters as the example case study--I just want to test out the process) I'm not getting any checkpoints in the output training directory, just the event log and the json file. What does this mean? Is the training step failing, or do I simply need to adjust my parameters? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797#issuecomment-2029425877
Safety,timeout,timeout,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:; ```; # Generated by shuffle_tfrecords_beam.py; #; # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz; # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled; #. name: ""Chromosome3""; tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 35759; # class1: 27257; # class0: 1777; # class2: 6725; ```; And here are the log files from the attempted model training: ; [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt); [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help!. Best, ; Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797#issuecomment-2030499725
Security,validat,validation,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:; ```; # Generated by shuffle_tfrecords_beam.py; #; # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz; # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled; #. name: ""Chromosome3""; tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 35759; # class1: 27257; # class0: 1777; # class2: 6725; ```; And here are the log files from the attempted model training: ; [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt); [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help!. Best, ; Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797#issuecomment-2030499725
Testability,log,log,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:; ```; # Generated by shuffle_tfrecords_beam.py; #; # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz; # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled; #. name: ""Chromosome3""; tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 35759; # class1: 27257; # class0: 1777; # class2: 6725; ```; And here are the log files from the attempted model training: ; [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt); [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help!. Best, ; Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797#issuecomment-2030499725
Usability,simpl,simply,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:; ```; # Generated by shuffle_tfrecords_beam.py; #; # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz; # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled; #. name: ""Chromosome3""; tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 35759; # class1: 27257; # class0: 1777; # class2: 6725; ```; And here are the log files from the attempted model training: ; [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt); [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help!. Best, ; Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797#issuecomment-2030499725
Availability,checkpoint,checkpoints,"Great! The logs you posted confirmed that the checkpoints were not being written, but it's not clear _why_ that was the case. I will close this issue for now, but please don't hesitate to reopen if you encounter it again!. To your second question, that's correct! In 1.6, we migrated our training and inference platform from Slim to Keras, and as part of this effort we combined `model_train` and `model_eval` with a single executable `train` to make training easier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797#issuecomment-2033038202
Testability,log,logs,"Great! The logs you posted confirmed that the checkpoints were not being written, but it's not clear _why_ that was the case. I will close this issue for now, but please don't hesitate to reopen if you encounter it again!. To your second question, that's correct! In 1.6, we migrated our training and inference platform from Slim to Keras, and as part of this effort we combined `model_train` and `model_eval` with a single executable `train` to make training easier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797#issuecomment-2033038202
Usability,clear,clear,"Great! The logs you posted confirmed that the checkpoints were not being written, but it's not clear _why_ that was the case. I will close this issue for now, but please don't hesitate to reopen if you encounter it again!. To your second question, that's correct! In 1.6, we migrated our training and inference platform from Slim to Keras, and as part of this effort we combined `model_train` and `model_eval` with a single executable `train` to make training easier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797#issuecomment-2033038202
Availability,checkpoint,checkpoints,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```; I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078; ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797#issuecomment-2033443746
Deployability,update,updated,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```; I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078; ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797#issuecomment-2033443746
Performance,tune,tune,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```; I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078; ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797#issuecomment-2033443746
Testability,log,log,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```; I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078; ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797#issuecomment-2033443746
Usability,clear,clear,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```; I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078; ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797#issuecomment-2033443746
Performance,perform,performed,"Thanks for clearing that up! I appreciate it. I did use hap.py to compare the customized model to the WGS model and it appears to have performed slightly worse, so I'll keep this in mind for future tests.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797#issuecomment-2038129909
Testability,test,tests,"Thanks for clearing that up! I appreciate it. I did use hap.py to compare the customized model to the WGS model and it appears to have performed slightly worse, so I'll keep this in mind for future tests.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797#issuecomment-2038129909
Usability,clear,clearing,"Thanks for clearing that up! I appreciate it. I did use hap.py to compare the customized model to the WGS model and it appears to have performed slightly worse, so I'll keep this in mind for future tests.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797#issuecomment-2038129909
Availability,checkpoint,checkpoints,"Sure! You can find the checkpoints for each sequencing technology at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/`. For example, the model for Illumina data can be found at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt`. These models are mounted in our [Dockerfile](https://github.com/google/deepvariant/blob/r1.6.1/Dockerfile#L156-L200). Take a look at our [custom training case study](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#advanced-case-study-train-a-customized-snp-and-small-indel-variant-caller-for-bgiseq-500-data) if you want to learn more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801#issuecomment-2040306496
Usability,learn,learn,"Sure! You can find the checkpoints for each sequencing technology at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/`. For example, the model for Illumina data can be found at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt`. These models are mounted in our [Dockerfile](https://github.com/google/deepvariant/blob/r1.6.1/Dockerfile#L156-L200). Take a look at our [custom training case study](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#advanced-case-study-train-a-customized-snp-and-small-indel-variant-caller-for-bgiseq-500-data) if you want to learn more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801#issuecomment-2040306496
Availability,checkpoint,checkpoints,"@sophienguyen01 - from the log file it looks like everything worked. Here are all the tune/categorical accuracies from your training data. ```; tune/categorical_accuracy=0.9944317936897278; tune/categorical_accuracy=0.9909400343894958; tune/categorical_accuracy=0.9915463924407959; tune/categorical_accuracy=0.9925118088722229; tune/categorical_accuracy=0.9921825528144836; tune/categorical_accuracy=0.9924613237380981; tune/categorical_accuracy=0.9926846623420715; tune/categorical_accuracy=0.9929667711257935; tune/categorical_accuracy=0.9925829172134399; tune/categorical_accuracy=0.9926416277885437; tune/categorical_accuracy=0.9923893213272095; tune/categorical_accuracy=0.9925225377082825; ```. The first number represents accuracy direct from the pretrained model. Since none of the subsequent tuning evaluations outperformed the original, no checkpoints were created. One thing you could try: reduce the learning rate, and see if that helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802#issuecomment-2042819603
Energy Efficiency,reduce,reduce,"@sophienguyen01 - from the log file it looks like everything worked. Here are all the tune/categorical accuracies from your training data. ```; tune/categorical_accuracy=0.9944317936897278; tune/categorical_accuracy=0.9909400343894958; tune/categorical_accuracy=0.9915463924407959; tune/categorical_accuracy=0.9925118088722229; tune/categorical_accuracy=0.9921825528144836; tune/categorical_accuracy=0.9924613237380981; tune/categorical_accuracy=0.9926846623420715; tune/categorical_accuracy=0.9929667711257935; tune/categorical_accuracy=0.9925829172134399; tune/categorical_accuracy=0.9926416277885437; tune/categorical_accuracy=0.9923893213272095; tune/categorical_accuracy=0.9925225377082825; ```. The first number represents accuracy direct from the pretrained model. Since none of the subsequent tuning evaluations outperformed the original, no checkpoints were created. One thing you could try: reduce the learning rate, and see if that helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802#issuecomment-2042819603
Performance,tune,tune,"@sophienguyen01 - from the log file it looks like everything worked. Here are all the tune/categorical accuracies from your training data. ```; tune/categorical_accuracy=0.9944317936897278; tune/categorical_accuracy=0.9909400343894958; tune/categorical_accuracy=0.9915463924407959; tune/categorical_accuracy=0.9925118088722229; tune/categorical_accuracy=0.9921825528144836; tune/categorical_accuracy=0.9924613237380981; tune/categorical_accuracy=0.9926846623420715; tune/categorical_accuracy=0.9929667711257935; tune/categorical_accuracy=0.9925829172134399; tune/categorical_accuracy=0.9926416277885437; tune/categorical_accuracy=0.9923893213272095; tune/categorical_accuracy=0.9925225377082825; ```. The first number represents accuracy direct from the pretrained model. Since none of the subsequent tuning evaluations outperformed the original, no checkpoints were created. One thing you could try: reduce the learning rate, and see if that helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802#issuecomment-2042819603
Testability,log,log,"@sophienguyen01 - from the log file it looks like everything worked. Here are all the tune/categorical accuracies from your training data. ```; tune/categorical_accuracy=0.9944317936897278; tune/categorical_accuracy=0.9909400343894958; tune/categorical_accuracy=0.9915463924407959; tune/categorical_accuracy=0.9925118088722229; tune/categorical_accuracy=0.9921825528144836; tune/categorical_accuracy=0.9924613237380981; tune/categorical_accuracy=0.9926846623420715; tune/categorical_accuracy=0.9929667711257935; tune/categorical_accuracy=0.9925829172134399; tune/categorical_accuracy=0.9926416277885437; tune/categorical_accuracy=0.9923893213272095; tune/categorical_accuracy=0.9925225377082825; ```. The first number represents accuracy direct from the pretrained model. Since none of the subsequent tuning evaluations outperformed the original, no checkpoints were created. One thing you could try: reduce the learning rate, and see if that helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802#issuecomment-2042819603
Usability,learn,learning,"@sophienguyen01 - from the log file it looks like everything worked. Here are all the tune/categorical accuracies from your training data. ```; tune/categorical_accuracy=0.9944317936897278; tune/categorical_accuracy=0.9909400343894958; tune/categorical_accuracy=0.9915463924407959; tune/categorical_accuracy=0.9925118088722229; tune/categorical_accuracy=0.9921825528144836; tune/categorical_accuracy=0.9924613237380981; tune/categorical_accuracy=0.9926846623420715; tune/categorical_accuracy=0.9929667711257935; tune/categorical_accuracy=0.9925829172134399; tune/categorical_accuracy=0.9926416277885437; tune/categorical_accuracy=0.9923893213272095; tune/categorical_accuracy=0.9925225377082825; ```. The first number represents accuracy direct from the pretrained model. Since none of the subsequent tuning evaluations outperformed the original, no checkpoints were created. One thing you could try: reduce the learning rate, and see if that helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802#issuecomment-2042819603
Performance,perform,performance,"Hi Pichuan,. You can close this issue now. I will try with different samples. I tried to lower the learning rate but it still does not exceed the performance of default model. . I will have to train on different samples. Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802#issuecomment-2057623016
Usability,learn,learning,"Hi Pichuan,. You can close this issue now. I will try with different samples. I tried to lower the learning rate but it still does not exceed the performance of default model. . I will have to train on different samples. Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802#issuecomment-2057623016
Availability,error,error,"Hi @Wenfei-Xian,. A max MAPQ score of 42 will likely have some effect, but I expect not an enormous one. I suspect that MAPQ at the lower end of the ranges would be more important, since if well-calibrated a difference between PHRED=42 and PHRED=60 is a very low additional absolute error probability. I have some bowtie mapped reads handy for a GIAB sample. I think I can conduct a quick experiment to see if that intuition is right.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/809#issuecomment-2067533385
Usability,intuit,intuition,"Hi @Wenfei-Xian,. A max MAPQ score of 42 will likely have some effect, but I expect not an enormous one. I suspect that MAPQ at the lower end of the ranges would be more important, since if well-calibrated a difference between PHRED=42 and PHRED=60 is a very low additional absolute error probability. I have some bowtie mapped reads handy for a GIAB sample. I think I can conduct a quick experiment to see if that intuition is right.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/809#issuecomment-2067533385
Testability,log,logs,"Hi @githubtefo ,; When you run the command, you should have logs in the terminal.; Can you provide those logs?. And, can you try something simple like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md first? That should certainly have logs when you run it. If not, there's something else wrong in your environment that we need to understand first.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810#issuecomment-2068119641
Usability,simpl,simple,"Hi @githubtefo ,; When you run the command, you should have logs in the terminal.; Can you provide those logs?. And, can you try something simple like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md first? That should certainly have logs when you run it. If not, there's something else wrong in your environment that we need to understand first.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810#issuecomment-2068119641
Testability,log,logs,"Sure! please find attached the logs in the terminal.; In the meantime, I will run the simple case study.; Thank you!; [output.log](https://github.com/google/deepvariant/files/15052710/output.log)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810#issuecomment-2068128270
Usability,simpl,simple,"Sure! please find attached the logs in the terminal.; In the meantime, I will run the simple case study.; Thank you!; [output.log](https://github.com/google/deepvariant/files/15052710/output.log)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810#issuecomment-2068128270
Deployability,release,releases,"@githubtefo It might be worth trying a smaller chromosome (like chr20, or try Quick Start) to make sure that things work end-to-end on your machine first.; If you use `--postprocess_variants_extra_args=""cpus=0""` , it's only the last step (`postprocess_variants`) that will be without multiprocessing. That step takes < 1hr for our PacBio BAM - you can see in https://github.com/google/deepvariant/blob/r1.5/docs/metrics.md#runtime-2 (this is before multiprocessing was used in postprocess_variants. @githubtefo We understand that speeding up DeepVariant is very important. We're actively making improvements! Thank you for reporting the issue. Our future releases will be better because of your feedback!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810#issuecomment-2069997953
Usability,feedback,feedback,"@githubtefo It might be worth trying a smaller chromosome (like chr20, or try Quick Start) to make sure that things work end-to-end on your machine first.; If you use `--postprocess_variants_extra_args=""cpus=0""` , it's only the last step (`postprocess_variants`) that will be without multiprocessing. That step takes < 1hr for our PacBio BAM - you can see in https://github.com/google/deepvariant/blob/r1.5/docs/metrics.md#runtime-2 (this is before multiprocessing was used in postprocess_variants. @githubtefo We understand that speeding up DeepVariant is very important. We're actively making improvements! Thank you for reporting the issue. Our future releases will be better because of your feedback!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810#issuecomment-2069997953
Usability,resume,resume,"Update: Internally our team has been making other improvements to postprocess_variants, so I've not been actively looking into this issue. I'll plan to resume in the next few weeks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/811#issuecomment-2235560333
Availability,echo,echo,"tall Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools; ```. ```; sudo yum groupinstall -y 'Development Tools'; # Install RPM packages for dependencies; sudo yum install -y \; autoconf \; automake \; cryptsetup \; fuse3-devel \; git \; glib2-devel \; libseccomp-devel \; libtool \; runc \; squashfs-tools \; wget \; zlib-devel; ```. ```bash; sudo dnf install dnf-plugins-core; sudo dnf copr enable dctrud/squashfs-tools-ng; sudo dnf install squashfs-tools-ng; ```. ```bash; export VERSION=1.21.0 OS=linux ARCH=amd64 && \; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \; rm go$VERSION.$OS-$ARCH.tar.gz; ```. ```bash; echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \; source ~/.bashrc; ```. ```bash; export VERSION=4.1.0 && \; wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \; tar -xzf singularity-ce-${VERSION}.tar.gz && \; cd singularity-ce-${VERSION}; ```. ```bash; ./mconfig && \; make -C builddir && \; sudo make -C builddir install; ```. At this point, I have singularity installed. ```bash; $ singularity --version; singularity-ce version 4.1.0; ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash; singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1; ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716
Deployability,release,release,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash; gcloud compute ssh pichuan-test --zone ""us-west1-b""; ```. Check the Linux version:. ```; $ uname -a; Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux; ```. And I ran this too:. ```; $ cat /etc/os-release; NAME=""AlmaLinux""; VERSION=""9.3 (Shamrock Pampas Cat)""; ID=""almalinux""; ID_LIKE=""rhel centos fedora""; VERSION_ID=""9.3""; PLATFORM_ID=""platform:el9""; PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)""; ANSI_COLOR=""0;34""; LOGO=""fedora-logo-icon""; CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos""; HOME_URL=""https://almalinux.org/""; DOCUMENTATION_URL=""https://wiki.almalinux.org/""; BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9""; ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3""; REDHAT_SUPPORT_PRODUCT=""AlmaLinux""; REDHAT_SUPPORT_PRODUCT_VERSION=""9.3""; ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools; ```. ```; sudo yum groupinstall -y 'Development Tools'; # Install RPM packages for dependencies; sudo yum install -y \; autoconf \; automake \; cryptsetup \; fuse3-devel \; git \; glib2-devel \; libseccomp-devel \; libtool \; runc \; squashfs-tools \; wget \; zlib-devel; ```. ```bash; sudo dnf install dnf-plugins-core; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716
Integrability,depend,dependencies,"D_LIKE=""rhel centos fedora""; VERSION_ID=""9.3""; PLATFORM_ID=""platform:el9""; PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)""; ANSI_COLOR=""0;34""; LOGO=""fedora-logo-icon""; CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos""; HOME_URL=""https://almalinux.org/""; DOCUMENTATION_URL=""https://wiki.almalinux.org/""; BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9""; ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3""; REDHAT_SUPPORT_PRODUCT=""AlmaLinux""; REDHAT_SUPPORT_PRODUCT_VERSION=""9.3""; ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools; ```. ```; sudo yum groupinstall -y 'Development Tools'; # Install RPM packages for dependencies; sudo yum install -y \; autoconf \; automake \; cryptsetup \; fuse3-devel \; git \; glib2-devel \; libseccomp-devel \; libtool \; runc \; squashfs-tools \; wget \; zlib-devel; ```. ```bash; sudo dnf install dnf-plugins-core; sudo dnf copr enable dctrud/squashfs-tools-ng; sudo dnf install squashfs-tools-ng; ```. ```bash; export VERSION=1.21.0 OS=linux ARCH=amd64 && \; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \; rm go$VERSION.$OS-$ARCH.tar.gz; ```. ```bash; echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \; source ~/.bashrc; ```. ```bash; export VERSION=4.1.0 && \; wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \; tar -xzf singularity-ce-${VERSION}.tar.gz && \; cd singularity-ce-${VERSION}; ```. ```bash; ./mconfig && \; make -C builddir && \; sudo make -C builddir install; ```. At this point, I have singularity installed. ```bash; $ singularity --version; singularity-ce version",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716
Modifiability,plugin,plugins-core,"inux:9::baseos""; HOME_URL=""https://almalinux.org/""; DOCUMENTATION_URL=""https://wiki.almalinux.org/""; BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9""; ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3""; REDHAT_SUPPORT_PRODUCT=""AlmaLinux""; REDHAT_SUPPORT_PRODUCT_VERSION=""9.3""; ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools; ```. ```; sudo yum groupinstall -y 'Development Tools'; # Install RPM packages for dependencies; sudo yum install -y \; autoconf \; automake \; cryptsetup \; fuse3-devel \; git \; glib2-devel \; libseccomp-devel \; libtool \; runc \; squashfs-tools \; wget \; zlib-devel; ```. ```bash; sudo dnf install dnf-plugins-core; sudo dnf copr enable dctrud/squashfs-tools-ng; sudo dnf install squashfs-tools-ng; ```. ```bash; export VERSION=1.21.0 OS=linux ARCH=amd64 && \; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \; rm go$VERSION.$OS-$ARCH.tar.gz; ```. ```bash; echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \; source ~/.bashrc; ```. ```bash; export VERSION=4.1.0 && \; wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \; tar -xzf singularity-ce-${VERSION}.tar.gz && \; cd singularity-ce-${VERSION}; ```. ```bash; ./mconfig && \; make -C builddir && \; sudo make -C builddir install; ```. At this point, I have singularity installed. ```bash; $ singularity --version; singularity-ce version 4.1.0; ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash; singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1; ```. From here, I used https:/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716
Testability,test,test,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash; gcloud compute ssh pichuan-test --zone ""us-west1-b""; ```. Check the Linux version:. ```; $ uname -a; Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux; ```. And I ran this too:. ```; $ cat /etc/os-release; NAME=""AlmaLinux""; VERSION=""9.3 (Shamrock Pampas Cat)""; ID=""almalinux""; ID_LIKE=""rhel centos fedora""; VERSION_ID=""9.3""; PLATFORM_ID=""platform:el9""; PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)""; ANSI_COLOR=""0;34""; LOGO=""fedora-logo-icon""; CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos""; HOME_URL=""https://almalinux.org/""; DOCUMENTATION_URL=""https://wiki.almalinux.org/""; BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9""; ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3""; REDHAT_SUPPORT_PRODUCT=""AlmaLinux""; REDHAT_SUPPORT_PRODUCT_VERSION=""9.3""; ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools; ```. ```; sudo yum groupinstall -y 'Development Tools'; # Install RPM packages for dependencies; sudo yum install -y \; autoconf \; automake \; cryptsetup \; fuse3-devel \; git \; glib2-devel \; libseccomp-devel \; libtool \; runc \; squashfs-tools \; wget \; zlib-devel; ```. ```bash; sudo dnf install dnf-plugins-core; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716
Usability,guid,guides,"pichuan-test --zone ""us-west1-b""; ```. Check the Linux version:. ```; $ uname -a; Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux; ```. And I ran this too:. ```; $ cat /etc/os-release; NAME=""AlmaLinux""; VERSION=""9.3 (Shamrock Pampas Cat)""; ID=""almalinux""; ID_LIKE=""rhel centos fedora""; VERSION_ID=""9.3""; PLATFORM_ID=""platform:el9""; PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)""; ANSI_COLOR=""0;34""; LOGO=""fedora-logo-icon""; CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos""; HOME_URL=""https://almalinux.org/""; DOCUMENTATION_URL=""https://wiki.almalinux.org/""; BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9""; ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3""; REDHAT_SUPPORT_PRODUCT=""AlmaLinux""; REDHAT_SUPPORT_PRODUCT_VERSION=""9.3""; ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools; ```. ```; sudo yum groupinstall -y 'Development Tools'; # Install RPM packages for dependencies; sudo yum install -y \; autoconf \; automake \; cryptsetup \; fuse3-devel \; git \; glib2-devel \; libseccomp-devel \; libtool \; runc \; squashfs-tools \; wget \; zlib-devel; ```. ```bash; sudo dnf install dnf-plugins-core; sudo dnf copr enable dctrud/squashfs-tools-ng; sudo dnf install squashfs-tools-ng; ```. ```bash; export VERSION=1.21.0 OS=linux ARCH=amd64 && \; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \; rm go$VERSION.$OS-$ARCH.tar.gz; ```. ```bash; echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \; source ~/.bashrc; ```. ```bash; export VERSION=4.1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716
Usability,simpl,simplex,"@bkurtoglu , can you please let us know what datatype you are using? The current model only works with ONT R10.4 simplex and duplex data. Can you provide some more details about your data and run:. 1) What chemistry is your ONT data?; 2) What basecaller did you use?; 3) How many cpus are you using to run make_examples?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814#issuecomment-2089293222
Deployability,update,update,"1. The type of the data is ONT_R9 simplex.; 2. The basecaller is guppy. ; 3. Threads are set to 48.; I also provide the code as below. Thank you for your help. ; `BIN_VERSION=""1.6.1""; sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; INPUT_DIR=""/home/user/Masaüstü/BEYZA/input"" OUTPUT_DIR=""/home/user/Masaüstü/BEYZA/output"" THREADS=48 MODEL_NAME=""ONT_R104""; sudo docker run -v ""/home/massive/Desktop/beyza/input"":""/input"" -v ""/home/massive/Desktop/beyza/OUTPUTDEEP"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=ONT_R104 --ref=""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads=""/input/NA12878-minion-ul_GRCh38.bam"" –output_vcf=""/output/deep.vcf"" --threads=48; `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814#issuecomment-2089912166
Usability,simpl,simplex,"1. The type of the data is ONT_R9 simplex.; 2. The basecaller is guppy. ; 3. Threads are set to 48.; I also provide the code as below. Thank you for your help. ; `BIN_VERSION=""1.6.1""; sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; INPUT_DIR=""/home/user/Masaüstü/BEYZA/input"" OUTPUT_DIR=""/home/user/Masaüstü/BEYZA/output"" THREADS=48 MODEL_NAME=""ONT_R104""; sudo docker run -v ""/home/massive/Desktop/beyza/input"":""/input"" -v ""/home/massive/Desktop/beyza/OUTPUTDEEP"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=ONT_R104 --ref=""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads=""/input/NA12878-minion-ul_GRCh38.bam"" –output_vcf=""/output/deep.vcf"" --threads=48; `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/814#issuecomment-2089912166
Modifiability,config,config,"Hi @prasundutta87 . Both DeepVariant and DeepTrio can produce gVCFs, so you can joint call in a similar manner. In both cases you should use the DeepVariant_unfiltered preset for GLnexus, because there is family structure present which the other filtering presets wouldn't know about. Because you can joint genotype multiple trio gVCFs from either DeepVariant or DeepTrio in the same way, I would use DeepTrio to produce the gVCFs, take all of the gVCFs and run them together through glnexus, and then you can still use allele frequency information. To be clear, the unfiltered preset looks like this:. ```; sudo docker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/child_trio_1.g.vcf.gz\; /output/parent1_trio_1.g.vcf.gz \; /output/parent2_trio_1.g.vcf.gz \; /output/child_trio_2g.vcf.gz\; /output/parent1_trio_2.g.vcf.gz \; /output/parent2_trio_2.g.vcf.gz \; | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bcftools view - \; | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bgzip -c > output/trio_cohort_merged.vcf.gz; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816#issuecomment-2101970308
Usability,clear,clear,"Hi @prasundutta87 . Both DeepVariant and DeepTrio can produce gVCFs, so you can joint call in a similar manner. In both cases you should use the DeepVariant_unfiltered preset for GLnexus, because there is family structure present which the other filtering presets wouldn't know about. Because you can joint genotype multiple trio gVCFs from either DeepVariant or DeepTrio in the same way, I would use DeepTrio to produce the gVCFs, take all of the gVCFs and run them together through glnexus, and then you can still use allele frequency information. To be clear, the unfiltered preset looks like this:. ```; sudo docker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/child_trio_1.g.vcf.gz\; /output/parent1_trio_1.g.vcf.gz \; /output/parent2_trio_1.g.vcf.gz \; /output/child_trio_2g.vcf.gz\; /output/parent1_trio_2.g.vcf.gz \; /output/parent2_trio_2.g.vcf.gz \; | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bcftools view - \; | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bgzip -c > output/trio_cohort_merged.vcf.gz; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/816#issuecomment-2101970308
Usability,resume,resume,"@NourMarzouka ,. Closing this issue due to inactivity. Please feel free to reopen. . The conclusion is that if you have the intermediate files, then you can resume. Otherwise, you have to restart the run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/825#issuecomment-2158916014
Usability,simpl,simplex-case-study,"Yes i do have a faidx index file too, actually i am doing it with; mitochondrial genome for hg38 version. Again i modified the command kept both input sorted bam and fasta file; along with faidx index in the same directory, By following the given format; https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md; BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type ONT_R104 \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}"". => Here is my recent command used for docker run. It would be great help if; I am able to resolve this issue. Thank you in advance. sudo docker run -v; */media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3*; -v; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result; google/deepvariant:{BIN_VERSION=""1.6.1""} python; /media/manish/Data/Jyoti_Mridha_AIC/Program/deepvariant-1.6.1/scripts/run_deepvariant.py; *--reads; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam; --ref; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829#issuecomment-2162176427
Modifiability,variab,variable,"And, just in case the documentation isn't clear:. This part:. ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; ...; ```. The variable BIN_VERSION was specified in earlier in the steps:. ```; BIN_VERSION=""1.6.1""; ```. So, in Unix command it's equivalent to:. ```; google/deepvariant:""1.6.1"" \; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829#issuecomment-2162210763
Usability,clear,clear,"And, just in case the documentation isn't clear:. This part:. ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; ...; ```. The variable BIN_VERSION was specified in earlier in the steps:. ```; BIN_VERSION=""1.6.1""; ```. So, in Unix command it's equivalent to:. ```; google/deepvariant:""1.6.1"" \; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829#issuecomment-2162210763
Availability,error,error,"Hi,; Thanks a lot for your immediate response, i have followed the above; instructions given by you, now the docker command is running fine, but I; have come across a new error. *[E::hts_open_format] Failed to open file; ""/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result""; : Is a directory*. *ValueError: UNKNOWN: Could not open variants_path:; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result*. here i have used the same path for docker run (-v; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result); and also same path for run_deepvariant (--output_vcf; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result; ) , Here i am not able to rectify what minor error in my command, i am; following the same pattern mentioned in the link shared, i might be; missing out something minute. Here is my command which is used. sudo docker run -v; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3; -v; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result; google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads; /media/manish/Dat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829#issuecomment-2162307749
Modifiability,variab,variable,"p2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3; -v; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result; google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam; --ref; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa; --report_title MITO60_Stats --sample_name MITO60 --output_vcf; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result; --model_type ONT_R104. On Wed, Jun 12, 2024 at 12:03 PM Pi-Chuan Chang ***@***.***>; wrote:. > And, just in case the documentation isn't clear:; >; > This part:; >; > sudo docker run \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}"":""/output"" \; > google/deepvariant:""${BIN_VERSION}"" \; > ...; >; > The variable BIN_VERSION was specified in earlier in the steps:; >; > BIN_VERSION=""1.6.1""; >; > So, in Unix command it's equivalent to:; >; > google/deepvariant:""1.6.1"" \; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/829#issuecomment-2162210763>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BDQL2ZE35INJI4WP7BHRNK3ZG7TVPAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGIYTANZWGM>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829#issuecomment-2162307749
Usability,clear,clear,"p2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3; -v; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result; google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam; --ref; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa; --report_title MITO60_Stats --sample_name MITO60 --output_vcf; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result; --model_type ONT_R104. On Wed, Jun 12, 2024 at 12:03 PM Pi-Chuan Chang ***@***.***>; wrote:. > And, just in case the documentation isn't clear:; >; > This part:; >; > sudo docker run \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}"":""/output"" \; > google/deepvariant:""${BIN_VERSION}"" \; > ...; >; > The variable BIN_VERSION was specified in earlier in the steps:; >; > BIN_VERSION=""1.6.1""; >; > So, in Unix command it's equivalent to:; >; > google/deepvariant:""1.6.1"" \; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/829#issuecomment-2162210763>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BDQL2ZE35INJI4WP7BHRNK3ZG7TVPAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGIYTANZWGM>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829#issuecomment-2162307749
Availability,error,error,"l/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result; google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam; --ref; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa; --report_title MITO60_Stats --sample_name MITO60 --output_vcf; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result/deepvariant_MITO60.vcf.gz; --model_type ONT_R104. On Wed, Jun 12, 2024 at 1:02 PM JYOTI MRIDHA ***@***.***> wrote:. > Hi,; > Thanks a lot for your immediate response, i have followed the above; > instructions given by you, now the docker command is running fine, but I; > have come across a new error.; >; > *[E::hts_open_format] Failed to open file; > ""/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result""; > : Is a directory*; >; > *ValueError: UNKNOWN: Could not open variants_path:; > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result*; >; > here i have used the same path for docker run (-v; > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result); > and also same path for run_deepvariant (--output_vcf; > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvaria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829#issuecomment-2162485508
Modifiability,variab,variable,"/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3; > -v; > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result; > google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads; > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam; > --ref; > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa; > --report_title MITO60_Stats --sample_name MITO60 --output_vcf; > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result; > --model_type ONT_R104; >; >; > On Wed, Jun 12, 2024 at 12:03 PM Pi-Chuan Chang ***@***.***>; > wrote:; >; >> And, just in case the documentation isn't clear:; >>; >> This part:; >>; >> sudo docker run \; >> -v ""${INPUT_DIR}"":""/input"" \; >> -v ""${OUTPUT_DIR}"":""/output"" \; >> google/deepvariant:""${BIN_VERSION}"" \; >> ...; >>; >> The variable BIN_VERSION was specified in earlier in the steps:; >>; >> BIN_VERSION=""1.6.1""; >>; >> So, in Unix command it's equivalent to:; >>; >> google/deepvariant:""1.6.1"" \; >>; >> —; >> Reply to this email directly, view it on GitHub; >> <https://github.com/google/deepvariant/issues/829#issuecomment-2162210763>,; >> or unsubscribe; >> <https://github.com/notifications/unsubscribe-auth/BDQL2ZE35INJI4WP7BHRNK3ZG7TVPAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGIYTANZWGM>; >> .; >> You are receiving this because you were mentioned.Message ID:; >> ***@***.***>; >>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829#issuecomment-2162485508
Usability,clear,clear,"/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3; > -v; > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result; > google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads; > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam; > --ref; > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa; > --report_title MITO60_Stats --sample_name MITO60 --output_vcf; > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result; > --model_type ONT_R104; >; >; > On Wed, Jun 12, 2024 at 12:03 PM Pi-Chuan Chang ***@***.***>; > wrote:; >; >> And, just in case the documentation isn't clear:; >>; >> This part:; >>; >> sudo docker run \; >> -v ""${INPUT_DIR}"":""/input"" \; >> -v ""${OUTPUT_DIR}"":""/output"" \; >> google/deepvariant:""${BIN_VERSION}"" \; >> ...; >>; >> The variable BIN_VERSION was specified in earlier in the steps:; >>; >> BIN_VERSION=""1.6.1""; >>; >> So, in Unix command it's equivalent to:; >>; >> google/deepvariant:""1.6.1"" \; >>; >> —; >> Reply to this email directly, view it on GitHub; >> <https://github.com/google/deepvariant/issues/829#issuecomment-2162210763>,; >> or unsubscribe; >> <https://github.com/notifications/unsubscribe-auth/BDQL2ZE35INJI4WP7BHRNK3ZG7TVPAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGIYTANZWGM>; >> .; >> You are receiving this because you were mentioned.Message ID:; >> ***@***.***>; >>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/829#issuecomment-2162485508
Energy Efficiency,adapt,adapt,"@hangy1 ,. 1) You can see from the log:. ```; Reading ../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz; ```. seems like sample_name is not set correctly? Unless you replaced them? Can you confirm if you have set the values correctly by printing them before running DeepVariant?. 2) Please use absolute paths rather than relative paths when you are setting paths. so instead of using:; ```bash; gvcf = ""../results/deepVariant/{dataset}/{sample}/vcf/{sample}.deepVariant.g.vcf.gz""; ```; Use:; ```bash; gvcf = ""/path/to/vcf/{sample}.deepVariant.g.vcf.gz""; ```; Hope this helps. You can also run the [quickstart](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md) to see if you can simply copy-paste and run the command fully and then adapt it to the command you are planning to run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839#issuecomment-2195257563
Modifiability,adapt,adapt,"@hangy1 ,. 1) You can see from the log:. ```; Reading ../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz; ```. seems like sample_name is not set correctly? Unless you replaced them? Can you confirm if you have set the values correctly by printing them before running DeepVariant?. 2) Please use absolute paths rather than relative paths when you are setting paths. so instead of using:; ```bash; gvcf = ""../results/deepVariant/{dataset}/{sample}/vcf/{sample}.deepVariant.g.vcf.gz""; ```; Use:; ```bash; gvcf = ""/path/to/vcf/{sample}.deepVariant.g.vcf.gz""; ```; Hope this helps. You can also run the [quickstart](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md) to see if you can simply copy-paste and run the command fully and then adapt it to the command you are planning to run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839#issuecomment-2195257563
Testability,log,log,"@hangy1 ,. 1) You can see from the log:. ```; Reading ../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz; ```. seems like sample_name is not set correctly? Unless you replaced them? Can you confirm if you have set the values correctly by printing them before running DeepVariant?. 2) Please use absolute paths rather than relative paths when you are setting paths. so instead of using:; ```bash; gvcf = ""../results/deepVariant/{dataset}/{sample}/vcf/{sample}.deepVariant.g.vcf.gz""; ```; Use:; ```bash; gvcf = ""/path/to/vcf/{sample}.deepVariant.g.vcf.gz""; ```; Hope this helps. You can also run the [quickstart](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md) to see if you can simply copy-paste and run the command fully and then adapt it to the command you are planning to run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839#issuecomment-2195257563
Usability,simpl,simply,"@hangy1 ,. 1) You can see from the log:. ```; Reading ../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz; ```. seems like sample_name is not set correctly? Unless you replaced them? Can you confirm if you have set the values correctly by printing them before running DeepVariant?. 2) Please use absolute paths rather than relative paths when you are setting paths. so instead of using:; ```bash; gvcf = ""../results/deepVariant/{dataset}/{sample}/vcf/{sample}.deepVariant.g.vcf.gz""; ```; Use:; ```bash; gvcf = ""/path/to/vcf/{sample}.deepVariant.g.vcf.gz""; ```; Hope this helps. You can also run the [quickstart](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md) to see if you can simply copy-paste and run the command fully and then adapt it to the command you are planning to run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839#issuecomment-2195257563
Usability,simpl,simply,"@hangy1 , can you please run the [quickstart](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md) by simply copy-pasting the commands in your system? That way we could pinpoint the issue in a controlled case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/839#issuecomment-2207033890
Usability,clear,clearing,"Hi @pichuan, thanks for clearing this up! When you get the chance, please let me know what to look for in the call_variants -output. Also, I'm not sure I understand the format, using zcat I get many very short lines. I see AD, DP and VAF but not sure how to read variant positions / probabilities. . Many thanks! ; -Karoliina",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849#issuecomment-2227870805
Deployability,install,install,"Nice catch! Thank you! It does seem that there are different number of outputs, though I've (to my knowledge) only used the commands from the full run with 19 cpus and --dry_run=true. I've cleared the output directories and am running the full command from the beginning. Which nvidia-driver - cuda combination do you run deepvariant with? I'm looking into the gpu -problem, I'm thinking I need to install an older nvidia-driver and cuda. Currently the driver is 555.42.02, but looking at this https://docs.nvidia.com/deploy/cuda-compatibility/#id1 , it's not compatible with cuda 11.3.1 that the deepvariant:1.6.1-gpu is using.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849#issuecomment-2230253485
Usability,clear,cleared,"Nice catch! Thank you! It does seem that there are different number of outputs, though I've (to my knowledge) only used the commands from the full run with 19 cpus and --dry_run=true. I've cleared the output directories and am running the full command from the beginning. Which nvidia-driver - cuda combination do you run deepvariant with? I'm looking into the gpu -problem, I'm thinking I need to install an older nvidia-driver and cuda. Currently the driver is 555.42.02, but looking at this https://docs.nvidia.com/deploy/cuda-compatibility/#id1 , it's not compatible with cuda 11.3.1 that the deepvariant:1.6.1-gpu is using.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/849#issuecomment-2230253485
Deployability,update,updated,"@pioneer-pi , the deepvariant manuscript from six years ago. Since then, the benchmarking methods, tools and data all has been updated. The latest version of bechmarking is GIAB v4.2.1 for HG002 that you can find [here](https://ftp.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/). I would suggest reading the following manuscripts if you are interested to learn about these benchmarks:. 1) https://cell.com/cell-genomics/fulltext/S2666-979X(22)00058-1; 2) https://www.cell.com/cell-genomics/fulltext/S2666-979X-2200057-X; 3) https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1.abstract. The latest data can be found in: . https://storage.googleapis.com/brain-genomics-public/research/sequencing/fastq/. https://storage.googleapis.com/brain-genomics-public/research/sequencing/grch37/. https://storage.googleapis.com/brain-genomics-public/research/sequencing/grch38/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/852#issuecomment-2238167683
Testability,benchmark,benchmarking,"@pioneer-pi , the deepvariant manuscript from six years ago. Since then, the benchmarking methods, tools and data all has been updated. The latest version of bechmarking is GIAB v4.2.1 for HG002 that you can find [here](https://ftp.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/). I would suggest reading the following manuscripts if you are interested to learn about these benchmarks:. 1) https://cell.com/cell-genomics/fulltext/S2666-979X(22)00058-1; 2) https://www.cell.com/cell-genomics/fulltext/S2666-979X-2200057-X; 3) https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1.abstract. The latest data can be found in: . https://storage.googleapis.com/brain-genomics-public/research/sequencing/fastq/. https://storage.googleapis.com/brain-genomics-public/research/sequencing/grch37/. https://storage.googleapis.com/brain-genomics-public/research/sequencing/grch38/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/852#issuecomment-2238167683
Usability,learn,learn,"@pioneer-pi , the deepvariant manuscript from six years ago. Since then, the benchmarking methods, tools and data all has been updated. The latest version of bechmarking is GIAB v4.2.1 for HG002 that you can find [here](https://ftp.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/). I would suggest reading the following manuscripts if you are interested to learn about these benchmarks:. 1) https://cell.com/cell-genomics/fulltext/S2666-979X(22)00058-1; 2) https://www.cell.com/cell-genomics/fulltext/S2666-979X-2200057-X; 3) https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1.abstract. The latest data can be found in: . https://storage.googleapis.com/brain-genomics-public/research/sequencing/fastq/. https://storage.googleapis.com/brain-genomics-public/research/sequencing/grch37/. https://storage.googleapis.com/brain-genomics-public/research/sequencing/grch38/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/852#issuecomment-2238167683
Availability,down,downloaded,"@poddarharsh15, can you please check if the files were downloaded correctly and if their sizes look good. The case studies are designed in a way that you can simply copy-paste the commands and it should work. I just tested the case study and it worked on my end.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/853#issuecomment-2238236950
Testability,test,tested,"@poddarharsh15, can you please check if the files were downloaded correctly and if their sizes look good. The case studies are designed in a way that you can simply copy-paste the commands and it should work. I just tested the case study and it worked on my end.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/853#issuecomment-2238236950
Usability,simpl,simply,"@poddarharsh15, can you please check if the files were downloaded correctly and if their sizes look good. The case studies are designed in a way that you can simply copy-paste the commands and it should work. I just tested the case study and it worked on my end.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/853#issuecomment-2238236950
Usability,user-friendly,user-friendly,"> Hi @vera-gomes , One possible way to get around the RAM issue is to split the run into two or more, using the `--regions` flag (For example, one run can run the first 8 chromosomes, and the second run can run the rest, or something like that). And then at the end you can combine the VCFs. Perhaps a user-friendly way to implement this is to let deepvariant split the job by chromosomes/regions automatically? If this mainly affects the post processing step, perhaps just make this step automatically process the output by chromosome/regions of a fixed window size?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/868#issuecomment-2293266591
Availability,error,errors,"Hi @Axze-rgb . To date, I've been hesitant to include simulated data as I'm very confident that we don't know all of the diverse and complex ways that errors happen to be able to model them. To a limited extent it might be possible to supplement training, but I'm fairly pessimistic on the approach. . For the skepticism component, one of the things we try to work on is investigating explainability - cases where the ML model has learned something that might not have been immediately obvious to a human. Hopefully we'll have some things to share there soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/872#issuecomment-2329912331
Usability,learn,learned,"Hi @Axze-rgb . To date, I've been hesitant to include simulated data as I'm very confident that we don't know all of the diverse and complex ways that errors happen to be able to model them. To a limited extent it might be possible to supplement training, but I'm fairly pessimistic on the approach. . For the skepticism component, one of the things we try to work on is investigating explainability - cases where the ML model has learned something that might not have been immediately obvious to a human. Hopefully we'll have some things to share there soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/872#issuecomment-2329912331
Usability,simpl,simplest,"Ok, then this would probably be my approach:. 1) Call variants individually on all of the samples indepedently (not using a trio caller, use DeepVariant or something) and create a combined gVCF using glnexus. Set parameters like minimum depth of 15 and GQ 20. Then find blocks that you can use as ""confident regions"". 2) Pick a few samples and apply the confident regions on them and see if you get mostly variants with GQ>=20 with them. At this point you may need to make sure they are not falling within an SV for some samples. This gives you truth VCFs. 3) Assess the F1-score of current DeepVariant using your truth vcf and bed and see how it looks. Finally train a model and see if the F1-score improves. I am sure there are better ways to do this, but, this would be the simplest and least blocking path for this experiment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/878#issuecomment-2361910524
Availability,reliab,reliable,"Hi,. Thanks for the reply, but it doesn't address my question.; My doubt is about how to generate the bam file for doing the variant calling. For the checking/validation approach, I would like to ask a few questions. > Ok, then this would probably be my approach:; > ; > 1. Call variants individually on all of the samples indepedently (not using a trio caller, use DeepVariant or something) and create a combined gVCF using glnexus. Set parameters like minimum depth of 15 and GQ 20. Then find blocks that you can use as ""confident regions"". I did medium coverage sequencing based on some literature, but since my species might have high genetic diversity I used depth higher than recommended (1x), about ~10X per individual.; https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0079667; https://onlinelibrary.wiley.com/doi/10.1111/mec.12105; https://pubmed.ncbi.nlm.nih.gov/34250668/. Then, I can't use the minimum depth you suggest of 15; my data should be good quality, hence GQ of 20 is fine, but depth of 15 is not achievable. > 2. Pick a few samples and apply the confident regions on them and see if you get mostly variants with GQ>=20 with them. At this point you may need to make sure they are not falling within an SV for some samples. This gives you truth VCFs.; I am new in Bioinformatics, could you suggest a program to use to find those ""confident blocks"". I can't think of any program for doing it.; I have short-reads not long-reads, hence finding SV is not very reliable. Any suggestion on how to check if a variant falls within a SV?. > 3. Assess the F1-score of current DeepVariant using your truth vcf and bed and see how it looks.; > ; > Finally train a model and see if the F1-score improves. I am sure there are better ways to do this, but, this would be the simplest and least blocking path for this experiment. As mentioned before, I do not have trios data. @pichuan , Is it possible to train a model without trios data?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/878#issuecomment-2364502771
Security,validat,validation,"Hi,. Thanks for the reply, but it doesn't address my question.; My doubt is about how to generate the bam file for doing the variant calling. For the checking/validation approach, I would like to ask a few questions. > Ok, then this would probably be my approach:; > ; > 1. Call variants individually on all of the samples indepedently (not using a trio caller, use DeepVariant or something) and create a combined gVCF using glnexus. Set parameters like minimum depth of 15 and GQ 20. Then find blocks that you can use as ""confident regions"". I did medium coverage sequencing based on some literature, but since my species might have high genetic diversity I used depth higher than recommended (1x), about ~10X per individual.; https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0079667; https://onlinelibrary.wiley.com/doi/10.1111/mec.12105; https://pubmed.ncbi.nlm.nih.gov/34250668/. Then, I can't use the minimum depth you suggest of 15; my data should be good quality, hence GQ of 20 is fine, but depth of 15 is not achievable. > 2. Pick a few samples and apply the confident regions on them and see if you get mostly variants with GQ>=20 with them. At this point you may need to make sure they are not falling within an SV for some samples. This gives you truth VCFs.; I am new in Bioinformatics, could you suggest a program to use to find those ""confident blocks"". I can't think of any program for doing it.; I have short-reads not long-reads, hence finding SV is not very reliable. Any suggestion on how to check if a variant falls within a SV?. > 3. Assess the F1-score of current DeepVariant using your truth vcf and bed and see how it looks.; > ; > Finally train a model and see if the F1-score improves. I am sure there are better ways to do this, but, this would be the simplest and least blocking path for this experiment. As mentioned before, I do not have trios data. @pichuan , Is it possible to train a model without trios data?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/878#issuecomment-2364502771
Usability,simpl,simplest,"Hi,. Thanks for the reply, but it doesn't address my question.; My doubt is about how to generate the bam file for doing the variant calling. For the checking/validation approach, I would like to ask a few questions. > Ok, then this would probably be my approach:; > ; > 1. Call variants individually on all of the samples indepedently (not using a trio caller, use DeepVariant or something) and create a combined gVCF using glnexus. Set parameters like minimum depth of 15 and GQ 20. Then find blocks that you can use as ""confident regions"". I did medium coverage sequencing based on some literature, but since my species might have high genetic diversity I used depth higher than recommended (1x), about ~10X per individual.; https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0079667; https://onlinelibrary.wiley.com/doi/10.1111/mec.12105; https://pubmed.ncbi.nlm.nih.gov/34250668/. Then, I can't use the minimum depth you suggest of 15; my data should be good quality, hence GQ of 20 is fine, but depth of 15 is not achievable. > 2. Pick a few samples and apply the confident regions on them and see if you get mostly variants with GQ>=20 with them. At this point you may need to make sure they are not falling within an SV for some samples. This gives you truth VCFs.; I am new in Bioinformatics, could you suggest a program to use to find those ""confident blocks"". I can't think of any program for doing it.; I have short-reads not long-reads, hence finding SV is not very reliable. Any suggestion on how to check if a variant falls within a SV?. > 3. Assess the F1-score of current DeepVariant using your truth vcf and bed and see how it looks.; > ; > Finally train a model and see if the F1-score improves. I am sure there are better ways to do this, but, this would be the simplest and least blocking path for this experiment. As mentioned before, I do not have trios data. @pichuan , Is it possible to train a model without trios data?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/878#issuecomment-2364502771
Availability,avail,available," etc. Juan Pablo Aguilar. ________________________________; From: Pi-Chuan Chang ***@***.***>; Sent: Friday, September 20, 2024 6:52:36 PM; To: google/deepvariant ***@***.***>; Cc: Aguilar Cabezas, Juan Pablo ***@***.***>; Mention ***@***.***>; Subject: [External] Re: [google/deepvariant] Retraining DeepVariant without trios data? (Issue #878). Use caution with links and attachments. Hi @desmodus1984<https://github.com/desmodus1984> ,. Fundamentally, training a DeepVariant requires truth data (truth variants and confident regions).; The core question here is: Would you be able to get truth data for the bats you're studying?. I quickly looked through your recent discussion with @kishwarshafin<https://github.com/kishwarshafin> .; I believe @kishwarshafin<https://github.com/kishwarshafin> has been trying to give you some tips on some ways to construct truth. Note that this is an advanced topic. We don't expect most of our users to train DeepVariant models, or to construct truth data. However, if you do have truth data (truth variants and confident regions), you should be able to follow the documentation https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md to try to train a model. From your description above, I still think the best way to proceed is to directly use DeepVariant release models. Once you have the callsets, try to evaluate the calls first. Even if you plan to train a model, it'll be good to have those baseline metrics available, so you know whether your trained model is working or not. Does this help? If I'm misunderstanding your question, let me know. —; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/878#issuecomment-2364726373>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AJWD2VJ75MSKTWY7ILOFVM3ZXSRLBAVCNFSM6AAAAABNXT6B26VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDGNRUG4ZDMMZXGM>.; You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/878#issuecomment-2364755195
Deployability,release,release," etc. Juan Pablo Aguilar. ________________________________; From: Pi-Chuan Chang ***@***.***>; Sent: Friday, September 20, 2024 6:52:36 PM; To: google/deepvariant ***@***.***>; Cc: Aguilar Cabezas, Juan Pablo ***@***.***>; Mention ***@***.***>; Subject: [External] Re: [google/deepvariant] Retraining DeepVariant without trios data? (Issue #878). Use caution with links and attachments. Hi @desmodus1984<https://github.com/desmodus1984> ,. Fundamentally, training a DeepVariant requires truth data (truth variants and confident regions).; The core question here is: Would you be able to get truth data for the bats you're studying?. I quickly looked through your recent discussion with @kishwarshafin<https://github.com/kishwarshafin> .; I believe @kishwarshafin<https://github.com/kishwarshafin> has been trying to give you some tips on some ways to construct truth. Note that this is an advanced topic. We don't expect most of our users to train DeepVariant models, or to construct truth data. However, if you do have truth data (truth variants and confident regions), you should be able to follow the documentation https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md to try to train a model. From your description above, I still think the best way to proceed is to directly use DeepVariant release models. Once you have the callsets, try to evaluate the calls first. Even if you plan to train a model, it'll be good to have those baseline metrics available, so you know whether your trained model is working or not. Does this help? If I'm misunderstanding your question, let me know. —; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/878#issuecomment-2364726373>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AJWD2VJ75MSKTWY7ILOFVM3ZXSRLBAVCNFSM6AAAAABNXT6B26VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDGNRUG4ZDMMZXGM>.; You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/878#issuecomment-2364755195
Usability,learn,learning-based,"Hi,. Yes, my main question has not been addressed, it is about how to make the bam files for DeepVariant. ""DeepVariant is a deep learning-based variant caller that takes aligned reads (in BAM or CRAM format)"". The documentation doesn't say anything about how to make it/them.; For instance, one can filter reads based on mapping quality- which is not mentioned in the workflow of GATK, or mark PCR duplicates and also remove them.; For example, GATK needs read group information like sample ID, sequencing technology, etc. Juan Pablo Aguilar. ________________________________; From: Pi-Chuan Chang ***@***.***>; Sent: Friday, September 20, 2024 6:52:36 PM; To: google/deepvariant ***@***.***>; Cc: Aguilar Cabezas, Juan Pablo ***@***.***>; Mention ***@***.***>; Subject: [External] Re: [google/deepvariant] Retraining DeepVariant without trios data? (Issue #878). Use caution with links and attachments. Hi @desmodus1984<https://github.com/desmodus1984> ,. Fundamentally, training a DeepVariant requires truth data (truth variants and confident regions).; The core question here is: Would you be able to get truth data for the bats you're studying?. I quickly looked through your recent discussion with @kishwarshafin<https://github.com/kishwarshafin> .; I believe @kishwarshafin<https://github.com/kishwarshafin> has been trying to give you some tips on some ways to construct truth. Note that this is an advanced topic. We don't expect most of our users to train DeepVariant models, or to construct truth data. However, if you do have truth data (truth variants and confident regions), you should be able to follow the documentation https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md to try to train a model. From your description above, I still think the best way to proceed is to directly use DeepVariant release models. Once you have the callsets, try to evaluate the calls first. Even if you plan to train a model, it'll be good to have those baseline metr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/878#issuecomment-2364755195
Deployability,update,update,"Hi,. It is actually possible to train DeepVariant on multiple GPUs, using the [MirroredStrategy](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/train.py#L112). You can find the tensorflow documentation here: [link](https://www.tensorflow.org/guide/distributed_training). . It looks like we need to update the [FAQ](https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md#can-model_train-be-run-on-multiple-gpus) to reflect that—thanks for brining it to our attention! The [training case study](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#start-train) is up-to-date, so feel to continue to reference that. It looks like it already applies the `mirrored ` strategy.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/885#issuecomment-2362016240
Usability,guid,guide,"Hi,. It is actually possible to train DeepVariant on multiple GPUs, using the [MirroredStrategy](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/train.py#L112). You can find the tensorflow documentation here: [link](https://www.tensorflow.org/guide/distributed_training). . It looks like we need to update the [FAQ](https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md#can-model_train-be-run-on-multiple-gpus) to reflect that—thanks for brining it to our attention! The [training case study](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#start-train) is up-to-date, so feel to continue to reference that. It looks like it already applies the `mirrored ` strategy.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/885#issuecomment-2362016240
