quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words,format_prompt,to_eliminate,reason
Deployability,"Hello, . I was wondering ... I know you have a mosquito model that seems to work for when there is a high heterozygosity, but I was wondering if you planned to release those mdels... We, who work with non-model organisms, would be really happy to use DeepVariant. . Sorry if this is out of place here, I didn't really know where to ask. Feel free to lock . Cheers. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/661:160,release,release,160,,https://github.com/google/deepvariant/issues/661,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello, . I was wondering ... I know you have a mosquito model that seems to work for when there is a high heterozygosity, but I was wondering if you planned to release those mdels... We, who work with non-model organisms, would be really happy to use DeepVariant. . Sorry if this is out of place here, I didn't really know where to ask. Feel free to lock . Cheers. Alex
",False,
Deployability,"Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. ; First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, ;",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793:556,install,install,556,,https://github.com/google/deepvariant/issues/793,4,['install'],"['install', 'installed']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. ; First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, ;
",False,
Deployability,"Hello, . I've been attempting to use the customized_classes_labeler to train a DeepVariant model. Specifically, I've been trying use the ""callsets"" field from the INFO field of a Genome In A Bottle VCF file. I've been working with NA12878, VCF/BED files available here: ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/latest/GRCh38/. At first, I could not make training examples using this as that field is an integer, but by making a copy of the VCF where I changed that field to be a string, I was able to make examples (using the `--labeler algorithm`, `--customized_classes_labeler_info_field_name`, and `--customized_classes_labeler_classes_list` options) and train the model. However, when I use the best model from training to predict variants, this class label information is not included in the VCF file. Am I misinterpreting how to use this customized class labeling? Any suggestions on how to incorporate this field into training and variant prediction? Thank you for your time!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/454:312,release,release,312,,https://github.com/google/deepvariant/issues/454,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello, . I've been attempting to use the customized_classes_labeler to train a DeepVariant model. Specifically, I've been trying use the ""callsets"" field from the INFO field of a Genome In A Bottle VCF file. I've been working with NA12878, VCF/BED files available here: ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/latest/GRCh38/. At first, I could not make training examples using this as that field is an integer, but by making a copy of the VCF where I changed that field to be a string, I was able to make examples (using the `--labeler algorithm`, `--customized_classes_labeler_info_field_name`, and `--customized_classes_labeler_classes_list` options) and train the model. However, when I use the best model from training to predict variants, this class label information is not included in the VCF file. Am I misinterpreting how to use this customized class labeling? Any suggestions on how to incorporate this field into training and variant prediction? Thank you for your time!
",False,
Deployability,"Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed; . /etc/profile.d/modules.sh; module load xxxxx/singularity/3.5.3. # inputs; reference=$2; bam=$1.final.bam; sampleid=$1; outdir=deepvar. # Create output directories; if [ ! -e deepvar ]; then mkdir deepvar; fi; if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches; if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi; export SINGULARITY_TMPDIR=$PWD/.singularity; export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image; if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant; singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${reference} \; --reads=${bam} \; --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \; --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \; --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/543:422,install,installed,422,,https://github.com/google/deepvariant/issues/543,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed; . /etc/profile.d/modules.sh; module load xxxxx/singularity/3.5.3. # inputs; reference=$2; bam=$1.final.bam; sampleid=$1; outdir=deepvar. # Create output directories; if [ ! -e deepvar ]; then mkdir deepvar; fi; if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches; if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi; export SINGULARITY_TMPDIR=$PWD/.singularity; export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image; if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant; singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${reference} \; --reads=${bam} \; --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \; --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \; --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt)
",False,"The text contains complete, meaningful sentences in natural language discussing issues and attempts to resolve them, such as 'When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist' and explanations about using a script, module loading, and previous fixes attempted."
Deployability,"Hello, ; Operatin system: Linux HPC ; Version: 1.3.0 ; Installation: Singularity ; Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**; **Command**; ```; `#!/bin/bash --login; #SBATCH -J AmyHouseman_deepvariant; #SBATCH -o %x.stdout.%J.%N; #SBATCH -e %x.stderr.%J.%N; #SBATCH --ntasks=1; #SBATCH --ntasks-per-node=1; #SBATCH -p compute; #SBATCH --account=scw1581; #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL); #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-23; #SBATCH --time=12:00:00; #SBATCH --mem-per-cpu=128GB. module purge; module load parallel; module load singularity; EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27; HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna; PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam; BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed; OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz; OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz; INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error.; set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \; --ref=$HG38_REFERENCE \; --reads=$PICARDMARKDUPLICATES_SORTEDBAM \; --regions=$BED_REGIONS \; --output_vcf=$OUTPUT_VCF \; --output_gvcf=$OUTPUT_GVCF \; --intermediate_results_dir=$INTERMEDIATE_RESULTS""; ```. **Error trace:**. ***** Intermediate results will be written to Polyposis_Exome_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/542:55,Install,Installation,55,,https://github.com/google/deepvariant/issues/542,1,['Install'],['Installation'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello, ; Operatin system: Linux HPC ; Version: 1.3.0 ; Installation: Singularity ; Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**; **Command**; ```; `#!/bin/bash --login; #SBATCH -J AmyHouseman_deepvariant; #SBATCH -o %x.stdout.%J.%N; #SBATCH -e %x.stderr.%J.%N; #SBATCH --ntasks=1; #SBATCH --ntasks-per-node=1; #SBATCH -p compute; #SBATCH --account=scw1581; #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL); #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-23; #SBATCH --time=12:00:00; #SBATCH --mem-per-cpu=128GB. module purge; module load parallel; module load singularity; EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27; HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna; PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam; BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed; OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz; OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz; INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error.; set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \; --ref=$HG38_REFERENCE \; --reads=$PICARDMARKDUPLICATES_SORTEDBAM \; --regions=$BED_REGIONS \; --output_vcf=$OUTPUT_VCF \; --output_gvcf=$OUTPUT_GVCF \; --intermediate_results_dir=$INTERMEDIATE_RESULTS""; ```. **Error trace:**. ***** Intermediate results will be written to Polyposis_Exome_
",False,
Deployability,"Hello, I am a pharmacy student and I am trying to use your tool. ; I have in my possession a pc with i7-6700, 32GB RAM and a RTX 2070 with CUDA cores and Tensor Cores.; 1) Can I somehow use the Tensor cores of my GPU and how?; 2) Is 32 GB enough? If I upgrade to 64gb will be better?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/157:252,upgrade,upgrade,252,,https://github.com/google/deepvariant/issues/157,1,['upgrade'],['upgrade'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello, I am a pharmacy student and I am trying to use your tool. ; I have in my possession a pc with i7-6700, 32GB RAM and a RTX 2070 with CUDA cores and Tensor Cores.; 1) Can I somehow use the Tensor cores of my GPU and how?; 2) Is 32 GB enough? If I upgrade to 64gb will be better?
",False,
Deployability,"Hello, I am attempting to compile deepvariant from source. ; running ; `./build-prereq.sh; `; returns; ```; Installing numpy with -no-binary=:all:. This will take a bit longer.; ERROR: tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ERROR: silico 1.0.1 has requirement pysam==0.8.4, but you'll have pysam 0.15.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ========== [Di Jun 18 12:55:53 CEST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ERROR: keras 2.2.2 has requirement keras_applications==1.0.4, but you'll have keras-applications 1.0.8 which is incompatible.; ERROR: keras 2.2.2 has requirement keras_preprocessing==1.0.2, but you'll have keras-preprocessing 1.1.0 which is incompatible. ```; And then ; `./build_and_test.sh`; returns; ```; ERROR: /media/urbe/MyBDrive/12-06-2019_masurca_instaGRAAL_final/deepvariant/third_party/nucleus/io/python/BUILD:309:1: C++ compilation of rule '//third_party/nucleus/io/python:hts_verbose_cclib' failed (Exit 1): gcc failed: error executing command ; (cd /home/urbe/.cache/bazel/_bazel_urbe/83a209cfb2bd2efbd35b40f0662be001/execroot/com_google_deepvariant && \; exec env - \; PATH=/bin:/usr/bin \; PWD=/proc/self/cwd \; PYTHONPATH=/home/urbe/Tools/MARVEL/bin/lib.python:/usr/local/lib.python: \; PYTHON_BIN_PATH=/home/urbe/anaconda3/bin/python \; PYTHON_LIB_PATH=/home/urbe/Tools/MARVEL/bin/lib.python \; TF_DOWNLOAD_CLANG=0 \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; TF_NEED_ROCM=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -f",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/189:108,Install,Installing,108,,https://github.com/google/deepvariant/issues/189,3,['Install'],"['Install', 'Installing']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello, I am attempting to compile deepvariant from source. ; running ; `./build-prereq.sh; `; returns; ```; Installing numpy with -no-binary=:all:. This will take a bit longer.; ERROR: tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ERROR: silico 1.0.1 has requirement pysam==0.8.4, but you'll have pysam 0.15.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ========== [Di Jun 18 12:55:53 CEST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ERROR: keras 2.2.2 has requirement keras_applications==1.0.4, but you'll have keras-applications 1.0.8 which is incompatible.; ERROR: keras 2.2.2 has requirement keras_preprocessing==1.0.2, but you'll have keras-preprocessing 1.1.0 which is incompatible. ```; And then ; `./build_and_test.sh`; returns; ```; ERROR: /media/urbe/MyBDrive/12-06-2019_masurca_instaGRAAL_final/deepvariant/third_party/nucleus/io/python/BUILD:309:1: C++ compilation of rule '//third_party/nucleus/io/python:hts_verbose_cclib' failed (Exit 1): gcc failed: error executing command ; (cd /home/urbe/.cache/bazel/_bazel_urbe/83a209cfb2bd2efbd35b40f0662be001/execroot/com_google_deepvariant && \; exec env - \; PATH=/bin:/usr/bin \; PWD=/proc/self/cwd \; PYTHONPATH=/home/urbe/Tools/MARVEL/bin/lib.python:/usr/local/lib.python: \; PYTHON_BIN_PATH=/home/urbe/anaconda3/bin/python \; PYTHON_LIB_PATH=/home/urbe/Tools/MARVEL/bin/lib.python \; TF_DOWNLOAD_CLANG=0 \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; TF_NEED_ROCM=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -f
",True,"The content consists of error messages and build logs from a compilation process. It includes command outputs, timestamps, and specific error codes related to C++ compilation. These are indicative of program logs or build system output which should be filtered out."
Deployability,"Hello, I am trying to install DeepVariant on an IBM Power 8 machine within a docker container. I get the following error during ./build_and_test.sh, which I understand is tied to Intel SSE2 instruction set. `external/libssw/src/ssw.c:38:23: fatal error: emmintrin.h: No such file or directory`. I did `export DV_USE_GCP_OPTIMIZED_TF_WHL=0` from the command line before running the compile. I also changed `DV_COPT_FLAGS` to `--copt=-Wno-sign-compare --copt=-Wno-write-strings` within settings.sh (removing the corei7 option). I am using bazel version '0.15.0-' (settings.sh is changed to reflect this). I am using scikit-learn=0.20 (run-prereq.sh changed to reflect this). pyclif was compiled from source. Is there a way to circumvent this error? The complete error message is as follows. ERROR: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/libssw/BUILD.bazel:11:1: C++ compilation of rule '@libssw//:ssw' failed (Exit 1): gcc failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/bin/python \; PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \; TF_DOWNLOAD_CLANG=0 \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction; -sections -fdata-sections -MD -MF bazel-out/ppc-opt/bin/external/libssw/_objs/ssw/external/libssw/src/ssw.pic.d -fPIC -iquote external/libssw -iquote bazel-out/ppc-opt/genfiles/external/libssw -iquote ext; ernal/bazel_tools -iquote bazel-out/ppc-opt/genfiles/external/bazel_tools -Wno-maybe-uninitial",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123:22,install,install,22,,https://github.com/google/deepvariant/issues/123,1,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello, I am trying to install DeepVariant on an IBM Power 8 machine within a docker container. I get the following error during ./build_and_test.sh, which I understand is tied to Intel SSE2 instruction set. `external/libssw/src/ssw.c:38:23: fatal error: emmintrin.h: No such file or directory`. I did `export DV_USE_GCP_OPTIMIZED_TF_WHL=0` from the command line before running the compile. I also changed `DV_COPT_FLAGS` to `--copt=-Wno-sign-compare --copt=-Wno-write-strings` within settings.sh (removing the corei7 option). I am using bazel version '0.15.0-' (settings.sh is changed to reflect this). I am using scikit-learn=0.20 (run-prereq.sh changed to reflect this). pyclif was compiled from source. Is there a way to circumvent this error? The complete error message is as follows. ERROR: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/libssw/BUILD.bazel:11:1: C++ compilation of rule '@libssw//:ssw' failed (Exit 1): gcc failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/bin/python \; PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \; TF_DOWNLOAD_CLANG=0 \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction; -sections -fdata-sections -MD -MF bazel-out/ppc-opt/bin/external/libssw/_objs/ssw/external/libssw/src/ssw.pic.d -fPIC -iquote external/libssw -iquote bazel-out/ppc-opt/genfiles/external/libssw -iquote ext; ernal/bazel_tools -iquote bazel-out/ppc-opt/genfiles/external/bazel_tools -Wno-maybe-uninitial
",False,"The provided text is a detailed error message from a build process. It includes log information such as timestamps, error codes, and compiler outputs which are typically associated with program logs or build system output. Additionally, it contains configuration settings and environment variables that relate to the build process. While there are some human-readable sentences discussing the problem and attempted solutions, the majority of the text is composed of code snippets, log messages, and technical details that fall under the category of program logs or API documentation."
Deployability,"Hello, I am trying to make a variant calling analysis with ONT data for Homo Sapiens. However, it is still running even though I started this analysis 6 days ago and it is still making examples. Could anyone help me to understand whether it is normal or I should re-run the analysis? The computer has 64-core Linux. It should be able to run the analysis for nearly one to two days based on my experiences with other variant callers. I would be very happy to get feedbacks from you! ; Thanks a lot!. Deep Variant/ Variant Calling; BIN_VERSION=1.6.1; Installation via Docker; Homo Sapiens Oxford Nanopore Whole Genome; ![Screenshot 2024-05-01 220709](https://github.com/google/deepvariant/assets/74244954/93bf098a-083d-40f5-ba41-9d876a836be3)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/814:549,Install,Installation,549,,https://github.com/google/deepvariant/issues/814,1,['Install'],['Installation'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello, I am trying to make a variant calling analysis with ONT data for Homo Sapiens. However, it is still running even though I started this analysis 6 days ago and it is still making examples. Could anyone help me to understand whether it is normal or I should re-run the analysis? The computer has 64-core Linux. It should be able to run the analysis for nearly one to two days based on my experiences with other variant callers. I would be very happy to get feedbacks from you! ; Thanks a lot!. Deep Variant/ Variant Calling; BIN_VERSION=1.6.1; Installation via Docker; Homo Sapiens Oxford Nanopore Whole Genome; ![Screenshot 2024-05-01 220709](https://github.com/google/deepvariant/assets/74244954/93bf098a-083d-40f5-ba41-9d876a836be3)
",False,"The text contains complete, meaningful sentences in natural language discussing the user's experience and questions about a computational analysis."
Deployability,"Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists.; I've googled all over and still can't solve the problem. please help me!. commandï¼š; ```; INPUT_DIR=/path1/4_Test/qingjiang/dpv; OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \; --num_shards=3 \; --model_type=PACBIO \; --ref=""${INPUT_DIR}""/QJref.fa \; --reads=""${INPUT_DIR}""/input.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; ```; errorï¼š; ```; I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LC_CTYPE = ""C.UTF-8"",; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LC_CTYPE = ""C.UTF-8"",; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s; user 0m0.211s; sys 0m0.371s; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/613:1321,install,installed,1321,,https://github.com/google/deepvariant/issues/613,2,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists.; I've googled all over and still can't solve the problem. please help me!. commandï¼š; ```; INPUT_DIR=/path1/4_Test/qingjiang/dpv; OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \; --num_shards=3 \; --model_type=PACBIO \; --ref=""${INPUT_DIR}""/QJref.fa \; --reads=""${INPUT_DIR}""/input.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; ```; errorï¼š; ```; I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LC_CTYPE = ""C.UTF-8"",; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LC_CTYPE = ""C.UTF-8"",; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s; user 0m0.211s; sys 0m0.371s; ```
",True,"The text contains error messages and log output from a command, including timestamps, warnings, and command outputs which are typically indicative of program logs or build system output."
Deployability,"Hello, I have installed all the binaries and ran all the shell scripts to install tensorflow and bazel, but after that I could not follow how to actually train the model or how to identify the snps for my files. I am sorry I am very new to deep learning. Any help would be greatly appreciate",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/63:14,install,installed,14,,https://github.com/google/deepvariant/issues/63,2,['install'],"['install', 'installed']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello, I have installed all the binaries and ran all the shell scripts to install tensorflow and bazel, but after that I could not follow how to actually train the model or how to identify the snps for my files. I am sorry I am very new to deep learning. Any help would be greatly appreciate
",False,This text contains a human-readable problem statement and request for assistance. It includes complete sentences discussing the user's experience with installation and their confusion regarding training models.
Deployability,"Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:; ```; + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/...; [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:; Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2; [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1; ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/511:109,patch,patch,109,,https://github.com/google/deepvariant/issues/511,2,"['patch', 'update']","['patch', 'update']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:; ```; + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/...; [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:; Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2; [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1; ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.
",False,
Deployability,"Hello, more of a question than an issue: what does the ""Could not create PileupImage for candidate"" mean during make_examples? What triggers it?. **Setup**; - DeepVariant version: 1.1.0 and 1.3.0; - Installation method: Singularity; - Type of data: illumina on a Pinus genome (big, repetitive genome). **Error trace** ; ```; W1203 19:21:43.514668 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:9 ; W1203 19:21:43.515001 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:83 ; W1203 19:21:43.515132 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:91 ; W1203 19:21:48.118362 140507900241664 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_4:129804; W1203 19:21:51.183064 139737683482368 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_19:82 ; W1203 19:21:51.183443 139737683482368 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_19:106 ; ```. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/512:199,Install,Installation,199,,https://github.com/google/deepvariant/issues/512,1,['Install'],['Installation'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello, more of a question than an issue: what does the ""Could not create PileupImage for candidate"" mean during make_examples? What triggers it?. **Setup**; - DeepVariant version: 1.1.0 and 1.3.0; - Installation method: Singularity; - Type of data: illumina on a Pinus genome (big, repetitive genome). **Error trace** ; ```; W1203 19:21:43.514668 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:9 ; W1203 19:21:43.515001 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:83 ; W1203 19:21:43.515132 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:91 ; W1203 19:21:48.118362 140507900241664 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_4:129804; W1203 19:21:51.183064 139737683482368 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_19:82 ; W1203 19:21:51.183443 139737683482368 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_19:106 ; ```. Thanks
",False,"The text includes a human-readable question and some descriptive content about the setup and error context, along with log traces. The question is in natural language, indicating that it's meant for a human reader to seek assistance or understanding."
Deployability,"Hello, when running DeepVariant on a machine with a GPU, we get ; [the attached error](https://github.com/google/deepvariant/files/5947987/DeepVariantError.txt); which seems to indicate that DeepVariant cannot find the samples in the working directory which is a solid state drive contained within the node. Oddly enough, when we rerun without removing the files in the /tmp directory, DeepVariant completes without error. Do you have any explanation for this? The submit command is below as system information. **Setup**; - Operating system: CentOS7, cuda/11.0; - DeepVariant version: v1.1.0; - Installation method: Singularity; - Type of data: PacBio HiFi from SQII with hg38. **Steps to reproduce:**; - Command: `singularity run --nv --bind $(readlink -f dv_wd):/wd /path/to/deepvariant/images/deepvariant_1.1.0-gpu.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/wd/hg38.ref.fasta --reads=/wd/${sample}.bam --output_vcf=/wd/${sample}.vcf --output_gvcf=/wd/${sample}.gvcf --novcf_stats_report --intermediate_results_dir=/tmp/deepvariant_tmp/$( whoami )_${sample}/ --num_shards=${threads}`; - Error trace: Included above; - We have also tried out a similar process running on another machine without a GPU, and we do not see this issue.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/422:596,Install,Installation,596,,https://github.com/google/deepvariant/issues/422,1,['Install'],['Installation'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello, when running DeepVariant on a machine with a GPU, we get ; [the attached error](https://github.com/google/deepvariant/files/5947987/DeepVariantError.txt); which seems to indicate that DeepVariant cannot find the samples in the working directory which is a solid state drive contained within the node. Oddly enough, when we rerun without removing the files in the /tmp directory, DeepVariant completes without error. Do you have any explanation for this? The submit command is below as system information. **Setup**; - Operating system: CentOS7, cuda/11.0; - DeepVariant version: v1.1.0; - Installation method: Singularity; - Type of data: PacBio HiFi from SQII with hg38. **Steps to reproduce:**; - Command: `singularity run --nv --bind $(readlink -f dv_wd):/wd /path/to/deepvariant/images/deepvariant_1.1.0-gpu.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/wd/hg38.ref.fasta --reads=/wd/${sample}.bam --output_vcf=/wd/${sample}.vcf --output_gvcf=/wd/${sample}.gvcf --novcf_stats_report --intermediate_results_dir=/tmp/deepvariant_tmp/$( whoami )_${sample}/ --num_shards=${threads}`; - Error trace: Included above; - We have also tried out a similar process running on another machine without a GPU, and we do not see this issue.
",False,"The text includes a detailed problem description, steps to reproduce the issue, error context, and relevant system information. It is written in natural language and explains a specific scenario which would be useful for others encountering the same problem."
Deployability,"Hello,. Deepvariant is reported to work well with WGS data from the Element AVITIâ„¢ System.; #623 ; https://www.biorxiv.org/content/10.1101/2023.08.11.553043v1.full.pdf. How does Deepvariant perform with AVITI WES data?; Is it possible to use the current WES model or is it still required to update the WES model?. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/703:291,update,update,291,,https://github.com/google/deepvariant/issues/703,1,['update'],['update'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello,. Deepvariant is reported to work well with WGS data from the Element AVITIâ„¢ System.; #623 ; https://www.biorxiv.org/content/10.1101/2023.08.11.553043v1.full.pdf. How does Deepvariant perform with AVITI WES data?; Is it possible to use the current WES model or is it still required to update the WES model?. Thanks!
",False,"The text includes meaningful, human-readable sentences discussing the performance of Deepvariant with different types of data and questioning the need for model updates. It does not consist of code, logs, API documentation, or other programmatic content."
Deployability,"Hello,. I am facing the below error when running built_and_test.sh. I have protocol buf built and installed in my home directory. (19:11:12) ERROR: /uufs/chpc.utah.edu/common/home/u1142888/deepvariant/bazel_tmp/_bazel_u1142888/bc41070ad1d30708841b968fbd6bc540/external/com_google_protobuf/BUILD:104:1: C++ compilation of rule '@com_google_protobuf//:protobuf' failed (Exit 1): gcc failed: error executing command . I have built the latest protobuf from source following the instructions [here](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md) and have installed it under $HOME/protobuf and updated my PATH and LD_LIBRARY_PATH accordingly. Looking for some pointers on how to resolve this issue by tweaking the build procedure. Thank you,; Ram",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/94:98,install,installed,98,,https://github.com/google/deepvariant/issues/94,3,"['install', 'update']","['installed', 'updated']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello,. I am facing the below error when running built_and_test.sh. I have protocol buf built and installed in my home directory. (19:11:12) ERROR: /uufs/chpc.utah.edu/common/home/u1142888/deepvariant/bazel_tmp/_bazel_u1142888/bc41070ad1d30708841b968fbd6bc540/external/com_google_protobuf/BUILD:104:1: C++ compilation of rule '@com_google_protobuf//:protobuf' failed (Exit 1): gcc failed: error executing command . I have built the latest protobuf from source following the instructions [here](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md) and have installed it under $HOME/protobuf and updated my PATH and LD_LIBRARY_PATH accordingly. Looking for some pointers on how to resolve this issue by tweaking the build procedure. Thank you,; Ram
",False,"The text includes a detailed error message from a build process, which is an example of program output. However, it also contains meaningful sentences discussing the problem and steps taken to resolve it, written in natural language. Since there are both code-like elements and human-readable content, we should keep it for its descriptive value."
Deployability,"Hello,. I am trying to build deepavariant on a HPC node on which all the required dependency is met except pyclif. I do not have root privileges to install it under /usr/local/clif. Hence I downloaded pyclif source code and ran the INSTALL.sh to get it successfully built and installed under $HOME and activated the pyclif virtualenv. . (clif) [test-node]$ which pyclif; ~/opt/clif/bin/pyclif. However build_and_test.sh fails with the below error. Any changes required to deepvariant build setup to pick up the pyclif installation in my home directory?; (18:02:14) ERROR: missing input file '@clif//:clif/bin/pyclif'",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/93:148,install,install,148,,https://github.com/google/deepvariant/issues/93,4,"['INSTALL', 'install']","['INSTALL', 'install', 'installation', 'installed']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello,. I am trying to build deepavariant on a HPC node on which all the required dependency is met except pyclif. I do not have root privileges to install it under /usr/local/clif. Hence I downloaded pyclif source code and ran the INSTALL.sh to get it successfully built and installed under $HOME and activated the pyclif virtualenv. . (clif) [test-node]$ which pyclif; ~/opt/clif/bin/pyclif. However build_and_test.sh fails with the below error. Any changes required to deepvariant build setup to pick up the pyclif installation in my home directory?; (18:02:14) ERROR: missing input file '@clif//:clif/bin/pyclif'
",False,"The content contains a user's explanation of their situation and problem, including steps taken to resolve an issue with installation and usage of pyclif. The text includes sentences that are meaningful and descriptive, discussing the setup process and troubleshooting."
Deployability,"Hello,. I am trying to install DeepVariant from source on Ubuntu 1.18.04. The build-prereq.sh script finished well,; but build_and_test.sh has stopped unexpectedly do not displaying any error:. ```; (18:54:51) INFO: Found applicable config definition build:linux in file /data1/SOFT/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels; (18:54:51) INFO: Found applicable config definition build:dynamic_kernels in file /data1/SOFT/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; (18:54:51) INFO: Current date is 2021-04-16; (18:54:51) INFO: Build option --build_python_zip has changed, discarding analysis cache.; (18:54:51) INFO: Analyzed target //:licenses_zip (0 packages loaded, 22 targets configured).; (18:54:51) INFO: Found 1 target...; (18:54:51) INFO: Elapsed time: 0.224s, Critical Path: 0.00s; (18:54:51) INFO: 0 processes.; + echo 'Expect a usage message:'; Expect a usage message:; + python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help; + grep /call_variants.py:; /tmp/Bazel.runfiles_5qjtwbro/runfiles/com_google_deepvariant/deepvariant/call_variants.py:; + :; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/443:23,install,install,23,,https://github.com/google/deepvariant/issues/443,1,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello,. I am trying to install DeepVariant from source on Ubuntu 1.18.04. The build-prereq.sh script finished well,; but build_and_test.sh has stopped unexpectedly do not displaying any error:. ```; (18:54:51) INFO: Found applicable config definition build:linux in file /data1/SOFT/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels; (18:54:51) INFO: Found applicable config definition build:dynamic_kernels in file /data1/SOFT/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; (18:54:51) INFO: Current date is 2021-04-16; (18:54:51) INFO: Build option --build_python_zip has changed, discarding analysis cache.; (18:54:51) INFO: Analyzed target //:licenses_zip (0 packages loaded, 22 targets configured).; (18:54:51) INFO: Found 1 target...; (18:54:51) INFO: Elapsed time: 0.224s, Critical Path: 0.00s; (18:54:51) INFO: 0 processes.; + echo 'Expect a usage message:'; Expect a usage message:; + python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help; + grep /call_variants.py:; /tmp/Bazel.runfiles_5qjtwbro/runfiles/com_google_deepvariant/deepvariant/call_variants.py:; + :; ```
",False,
Deployability,"Hello,. I am working on a next-flow pipeline and have a question about using the deep variant tool. Would it still work correctly if I split the bam file into chromosomes, or if it's not designed to be used with a splitter? Additionally, I would like to know if there would be any speed issues between the two options. Using the splitter chromosomes would be faster, but I am unsure.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/744:36,pipeline,pipeline,36,,https://github.com/google/deepvariant/issues/744,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello,. I am working on a next-flow pipeline and have a question about using the deep variant tool. Would it still work correctly if I split the bam file into chromosomes, or if it's not designed to be used with a splitter? Additionally, I would like to know if there would be any speed issues between the two options. Using the splitter chromosomes would be faster, but I am unsure.
",False,"The text contains complete sentences and questions in natural language, discussing concepts related to a bioinformatics pipeline."
Deployability,"Hello,. I apologise if my question is naive I am a beginner with neural networks. ; Do you plan to release models trained with non-humans? Like the mosquito analysis published on your blog? And would it made sense to have a kind of ""universal model""? . thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/204:99,release,release,99,,https://github.com/google/deepvariant/issues/204,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello,. I apologise if my question is naive I am a beginner with neural networks. ; Do you plan to release models trained with non-humans? Like the mosquito analysis published on your blog? And would it made sense to have a kind of ""universal model""? . thank you
",False,"The text contains complete, meaningful sentences in natural language. It includes questions and requests for information from a human perspective, discussing concepts related to neural networks."
Deployability,"Hello,. I tried running ""_run_deepvariant_keras.py_"" script from the latest release of deepvaraint and faced some issues while running the keras-based call variant module. . I used the following command to run:. `; python bazel-out/k8-opt/bin/deepvariant/run_deepvariant_keras.py --model_type=WGS --ref=ref.fa --reads=reads.bam --regions ""chr19"" --output_vcf=${OUTPUT_DIR}/output.vcf.gz --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz --intermediate_results_dir ${OUTPUT_DIR}/intermediate_results_dir --num_shards=10; `. The above command successfully ran for _make_example_ module and failed at _call_varaint_keras.py_ with the following error message.. -------------. _Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 399, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/absl_py/absl/app.py"", line 312, in run_; _run_main(main, args); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 387, in main; call_variants(; File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 344, in call_variants; model.load_weights(checkpoint_path).expect_partial(); File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler; raise e.with_traceback(filtered_tb) from None; File ""/usr/local/lib/pyt",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/636:76,release,release,76,,https://github.com/google/deepvariant/issues/636,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello,. I tried running ""_run_deepvariant_keras.py_"" script from the latest release of deepvaraint and faced some issues while running the keras-based call variant module. . I used the following command to run:. `; python bazel-out/k8-opt/bin/deepvariant/run_deepvariant_keras.py --model_type=WGS --ref=ref.fa --reads=reads.bam --regions ""chr19"" --output_vcf=${OUTPUT_DIR}/output.vcf.gz --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz --intermediate_results_dir ${OUTPUT_DIR}/intermediate_results_dir --num_shards=10; `. The above command successfully ran for _make_example_ module and failed at _call_varaint_keras.py_ with the following error message.. -------------. _Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 399, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/absl_py/absl/app.py"", line 312, in run_; _run_main(main, args); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 387, in main; call_variants(; File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 344, in call_variants; model.load_weights(checkpoint_path).expect_partial(); File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler; raise e.with_traceback(filtered_tb) from None; File ""/usr/local/lib/pyt
",False,"The text is a detailed error log, including stack traces and exception details. It contains program logs or error messages which primarily consist of debugging information rather than meaningful human-readable content."
Deployability,"Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? ; I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)?. Here's the code I'm running from the Quickstart:; ```; OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules; module load singularity; module load cuda-dcgm/2.2.9.1; module load cuda11.4/toolkit; module load cuda11.4/blas; module load cuda11.4/nsight; module load cuda11.4/profiler; module load cuda11.4/fft; source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh; conda activate TensorFlow_GPU. # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; --nv \; docker://google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir""; ```. And here's my error:; ```; 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/514:31,install,installation,31,,https://github.com/google/deepvariant/issues/514,1,['install'],['installation'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? ; I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)?. Here's the code I'm running from the Quickstart:; ```; OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules; module load singularity; module load cuda-dcgm/2.2.9.1; module load cuda11.4/toolkit; module load cuda11.4/blas; module load cuda11.4/nsight; module load cuda11.4/profiler; module load cuda11.4/fft; source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh; conda activate TensorFlow_GPU. # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; --nv \; docker://google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir""; ```. And here's my error:; ```; 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, 
",True,"The text contains code snippets, including a bash script and Python code, which are typically considered programmatic content that should be filtered out."
Deployability,"Hello,. I'm trying to run DeepVariant using ultima data (cram file).; I get information that deepvariant tool provides --enable_joint_realignment and --p_error in DeepVariant 1.5.0 release page.; But, I got error message when I am trying to use --enable_joint_realignment options.. Can I get some advice which custom channels or options I should use to run deepvariant using ultima data?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/711:181,release,release,181,,https://github.com/google/deepvariant/issues/711,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello,. I'm trying to run DeepVariant using ultima data (cram file).; I get information that deepvariant tool provides --enable_joint_realignment and --p_error in DeepVariant 1.5.0 release page.; But, I got error message when I am trying to use --enable_joint_realignment options.. Can I get some advice which custom channels or options I should use to run deepvariant using ultima data?
",False,"The text contains meaningful sentences discussing how to use DeepVariant with specific options and error messages, indicating it's human-readable content."
Deployability,"Hello,. I'm using DeepVariant docker container v1.3 to call variants using the `run_deepvariant` command. What I've done in the past to manage temp files was to create a `temp_dir` in the working directory and then use `--intermediate_results_dir temp_dir` to make DeepVariant write temp files in this custom location. However, the same approach is not working anymore for me in the new HPC system since on computing node the default temp folder stored in `$TMPDIR` is set to a special space `\localscratch` that is not among the path automatically mounted by Docker or Singularity (like \tmp) apparently. I realized that, in addition to intermediate files written to `--intermediate_results_dir`, DeepVariant writes some additional temp files to the default temp dir location (`$TMPDIR`) and this created some issues when running it in pipelines (like Nextflow). . I've created a work around by manually setting `$TMPDIR` in the sh script so that it points to another folder in the work directory, and I can see there are a bunch of small files created in there (~30Mb total) like the following; ```; Bazel.runfiles_6nvtcv_j __pycache__ tmp8rz89h3g.py tmpglc9d5x3.py tmph9ntzkbx; ```. I wonder which kind of files are written to `$TMPDIR` and if it's possible to redirect them by command line option without having to set `$TMPDIR`",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/524:837,pipeline,pipelines,837,,https://github.com/google/deepvariant/issues/524,1,['pipeline'],['pipelines'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello,. I'm using DeepVariant docker container v1.3 to call variants using the `run_deepvariant` command. What I've done in the past to manage temp files was to create a `temp_dir` in the working directory and then use `--intermediate_results_dir temp_dir` to make DeepVariant write temp files in this custom location. However, the same approach is not working anymore for me in the new HPC system since on computing node the default temp folder stored in `$TMPDIR` is set to a special space `\localscratch` that is not among the path automatically mounted by Docker or Singularity (like \tmp) apparently. I realized that, in addition to intermediate files written to `--intermediate_results_dir`, DeepVariant writes some additional temp files to the default temp dir location (`$TMPDIR`) and this created some issues when running it in pipelines (like Nextflow). . I've created a work around by manually setting `$TMPDIR` in the sh script so that it points to another folder in the work directory, and I can see there are a bunch of small files created in there (~30Mb total) like the following; ```; Bazel.runfiles_6nvtcv_j __pycache__ tmp8rz89h3g.py tmpglc9d5x3.py tmph9ntzkbx; ```. I wonder which kind of files are written to `$TMPDIR` and if it's possible to redirect them by command line option without having to set `$TMPDIR`
",False,"The text contains complete, meaningful sentences discussing the user's workflow and problem with DeepVariant in a Docker container. It includes human-readable prose about managing temporary files and issues encountered in a new HPC system."
Deployability,"Hello,. I've been trying to set up the **google/deepvariant:1.6.1-gpu** or **google/deepvariant:latest-gpu** image on a GPU instance, but I've encountered the error message mentioned below when running the **run_deepvariant** or **train** scripts, and despite generating the flags (screenshot) as expected, I believe those incompatible/missing TensorRT libraries are preventing these scripts from using the GPU. **Command used:** ; ` sudo docker run --runtime=nvidia --gpus 1 google/deepvariant:1.6.1-gpu train --help; `. **Error message:**; ```; 2024-05-08 15:11:26.358196: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64; 2024-05-08 15:11:26.358229: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; ```. ![image](https://github.com/google/deepvariant/assets/169280348/fd17bf4e-0b6c-46b7-b5e8-74a3525d07a5)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/819:1107,install,installed,1107,,https://github.com/google/deepvariant/issues/819,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello,. I've been trying to set up the **google/deepvariant:1.6.1-gpu** or **google/deepvariant:latest-gpu** image on a GPU instance, but I've encountered the error message mentioned below when running the **run_deepvariant** or **train** scripts, and despite generating the flags (screenshot) as expected, I believe those incompatible/missing TensorRT libraries are preventing these scripts from using the GPU. **Command used:** ; ` sudo docker run --runtime=nvidia --gpus 1 google/deepvariant:1.6.1-gpu train --help; `. **Error message:**; ```; 2024-05-08 15:11:26.358196: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64; 2024-05-08 15:11:26.358229: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; ```. ![image](https://github.com/google/deepvariant/assets/169280348/fd17bf4e-0b6c-46b7-b5e8-74a3525d07a5)
",False,"The text contains detailed error messages and steps taken to resolve an issue, indicating human-readable content discussing technical problems and solutions."
Deployability,"Hello,. I've been using DeepTrio for de novo variant analysis, and it's been performing excellently. However, I noticed from the logs that DeepTrio uses the CPU to prepare data, which is quite time-consuming. In my case, it took 12 hours for one trio analysis, with an additional 4 hours on the GPU. Given that renting GPU servers( usually with less CPU) is more expensive than CPU servers and access to privately owned GPU servers is limited, it seems inefficient to run lengthy CPU processes on a GPU server. It feels like a bit of a waste, and sometimes I half-jokingly feel there might be someone out there with murderous intent because of it!. Would it be possible in future updates to partition the DeepTrio analysis into separate steps? This way, CPU-intensive tasks could be completed on a CPU server, and then the job could be transferred to a GPU server for the remaining tasks. Alternatively, could the data preparation (CPU) and analysis (GPU) be run at the same time? This would help optimize resource usage and reduce costs. Thank you for considering these suggestions.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/873:680,update,updates,680,,https://github.com/google/deepvariant/issues/873,1,['update'],['updates'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello,. I've been using DeepTrio for de novo variant analysis, and it's been performing excellently. However, I noticed from the logs that DeepTrio uses the CPU to prepare data, which is quite time-consuming. In my case, it took 12 hours for one trio analysis, with an additional 4 hours on the GPU. Given that renting GPU servers( usually with less CPU) is more expensive than CPU servers and access to privately owned GPU servers is limited, it seems inefficient to run lengthy CPU processes on a GPU server. It feels like a bit of a waste, and sometimes I half-jokingly feel there might be someone out there with murderous intent because of it!. Would it be possible in future updates to partition the DeepTrio analysis into separate steps? This way, CPU-intensive tasks could be completed on a CPU server, and then the job could be transferred to a GPU server for the remaining tasks. Alternatively, could the data preparation (CPU) and analysis (GPU) be run at the same time? This would help optimize resource usage and reduce costs. Thank you for considering these suggestions.
",False,"The text contains complete, meaningful sentences in natural language discussing the user's experience with DeepTrio, including both positive feedback and constructive suggestions for optimization. It includes human-readable content without code snippets or logs."
Deployability,"Hello,. Noticed this issue with your tool DeepTrio regarding the representation of hemizygous variants in the non-pseudoautosomal (PAR) X-chromosome. This may be fixed now in 1.3? If so ignore this, but if not this is what I noticed. . Note, this is a simulated pathogenic variant from bamsurgeon, but the VCF representation is the focus of this problem. . Let's start with an IGV snapshot of the variant:; ![image](https://user-images.githubusercontent.com/16579982/154755554-3642728e-03c3-4c87-ba89-d66f0ecd6982.png). Now, I'll go into the representation from the DeepVariant --> GVCF --> GLnexus pipeline:. ## DeepVariant Pipeline:; ```; # Load singularity; module load singularity; BIN_VERSION=""1.1.0"". # Load env for bcftools; ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/; source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh; conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Number of threads; NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered); cd $SLURM_SUBMIT_DIR; WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis. ## Set working space; mkdir -p $WORKING_DIR; cd $WORKING_DIR. #### GRCh38 #### ; echo ""GRCh38 genome""; GENOME=GRCh38; FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/; FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS; BAM_DIR=$WORKING_DIR; FAMILY_ID=Case1; PROBAND_ID=Case1_proband; MOTHER_ID=Case1_mother; FATHER_ID=Case1_father; SIBLING_ID=.; PED=$FAMILY_ID.ped. MOTHER_PRESENT=true; FATHER_PRESENT=true; SIBLING_PRESENT=false. PROBAND_BAM=${PROBAND_ID}.sorted.bam; FATHER_BAM=${FATHER_ID}.sorted.bam; MOTHER_BAM=${MOTHER_ID}.sorted.bam; SIBLING_BAM=${SIBLING_ID}.sorted.bam. PROBAND_VCF=${PROBAND_ID}.vcf.gz; FATHER_VCF=${FATHER_ID}.vcf.gz; MOTHER_VCF=${MOTHER_ID}.vcf.gz; SIBL",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518:599,pipeline,pipeline,599,,https://github.com/google/deepvariant/issues/518,2,"['Pipeline', 'pipeline']","['Pipeline', 'pipeline']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello,. Noticed this issue with your tool DeepTrio regarding the representation of hemizygous variants in the non-pseudoautosomal (PAR) X-chromosome. This may be fixed now in 1.3? If so ignore this, but if not this is what I noticed. . Note, this is a simulated pathogenic variant from bamsurgeon, but the VCF representation is the focus of this problem. . Let's start with an IGV snapshot of the variant:; ![image](https://user-images.githubusercontent.com/16579982/154755554-3642728e-03c3-4c87-ba89-d66f0ecd6982.png). Now, I'll go into the representation from the DeepVariant --> GVCF --> GLnexus pipeline:. ## DeepVariant Pipeline:; ```; # Load singularity; module load singularity; BIN_VERSION=""1.1.0"". # Load env for bcftools; ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/; source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh; conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Number of threads; NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered); cd $SLURM_SUBMIT_DIR; WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis. ## Set working space; mkdir -p $WORKING_DIR; cd $WORKING_DIR. #### GRCh38 #### ; echo ""GRCh38 genome""; GENOME=GRCh38; FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/; FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS; BAM_DIR=$WORKING_DIR; FAMILY_ID=Case1; PROBAND_ID=Case1_proband; MOTHER_ID=Case1_mother; FATHER_ID=Case1_father; SIBLING_ID=.; PED=$FAMILY_ID.ped. MOTHER_PRESENT=true; FATHER_PRESENT=true; SIBLING_PRESENT=false. PROBAND_BAM=${PROBAND_ID}.sorted.bam; FATHER_BAM=${FATHER_ID}.sorted.bam; MOTHER_BAM=${MOTHER_ID}.sorted.bam; SIBLING_BAM=${SIBLING_ID}.sorted.bam. PROBAND_VCF=${PROBAND_ID}.vcf.gz; FATHER_VCF=${FATHER_ID}.vcf.gz; MOTHER_VCF=${MOTHER_ID}.vcf.gz; SIBL
",False,"The content includes human-readable sentences discussing issues and steps taken to analyze them, which should not be filtered out."
Deployability,"Hello,. Please find below the description of an issue with the flag --output_gvcf_merged in DeepTrio.; Thanks for considering this request; Fred-07. **Describe the issue:**; Running the ""DeepTrio quick start"" commands with the additional flag --output_gvcf_merged produces no error but the""merged"" file is not created.; https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-quick-start.md; All other expected files are created. **Setup**; - Operating system: Linux 3.10.0-1160.71.1.el7.x86_64; - DeepVariant version: 1.4.0; - Installation method: singularity pull from docker, LSF as batch system; - Type of data: ""DeepTrio quick start"" data. **Steps to reproduce:**; - Command: additional flag; `--output_gvcf_merged ""${OUTPUT_DIR}""/ALL.g.vcf.gz`",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/544:534,Install,Installation,534,,https://github.com/google/deepvariant/issues/544,1,['Install'],['Installation'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello,. Please find below the description of an issue with the flag --output_gvcf_merged in DeepTrio.; Thanks for considering this request; Fred-07. **Describe the issue:**; Running the ""DeepTrio quick start"" commands with the additional flag --output_gvcf_merged produces no error but the""merged"" file is not created.; https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-quick-start.md; All other expected files are created. **Setup**; - Operating system: Linux 3.10.0-1160.71.1.el7.x86_64; - DeepVariant version: 1.4.0; - Installation method: singularity pull from docker, LSF as batch system; - Type of data: ""DeepTrio quick start"" data. **Steps to reproduce:**; - Command: additional flag; `--output_gvcf_merged ""${OUTPUT_DIR}""/ALL.g.vcf.gz`
",False,
Deployability,"Hello,. We have found that a known de novo variant was missed when using DeepTrio and GLnexus in our pipeline (WGS, hg38). I know that a similar issue has already been raised and appreciate the interesting discussion on this (i.e. https://github.com/google/deepvariant/issues/440), but to recap for others this was the result of two contributing factors:. 1. DeepTrio being less confident in the de novo call for the proband than when DeepVariant is run in singleton mode on the proband. In our case, comparing the output VCFs from these two different runs we saw a reduction in the GQ score assigned to the variant from 56 when using DeepVariant in singleton mode, to only 10 when using DeepTrio.; 2. GLnexus filtering, according to the `DeepVariantWGS` configuration we were using, removing our variant of interest in the case of DeepTrio due to the low likelihood assigned to the call. To partly overcome this we are looking to switch the GLnexus configuration to `DeepVariant_unfiltered` as mentioned in https://github.com/google/deepvariant/issues/440. However we would like to further evaluate this change on a known truth set to determine the increase in false-positive calls (similar to [1] with DV-GLN-NOMOD vs DV-GLN-OPT, but for DeepTrio instead... because from what I understand that paper evaluated DeepVariant). I have seen that all three GIAB/NIST benchmark trios have been used as training data for DeepTrio so would like to ask:. 1. Were all chromosomes from these trios used to train the DeepTrio models? I believe the DeepVariant WGS training data excluded chr20-22, and the deeptrio test data uses HG001 Chr20 [2], so I assume chr20-22 were excluded from the Deeptrio models for each of the trios too and would be suitable for testing? Or any alternative suggestions for this?; 2. I understand that the DeepTrio docs aren't officially released yet, but would it be possible please to provide an overview of the workings and differences between the Child and Parent models for DeepT",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/475:101,pipeline,pipeline,101,,https://github.com/google/deepvariant/issues/475,3,"['configurat', 'pipeline']","['configuration', 'pipeline']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello,. We have found that a known de novo variant was missed when using DeepTrio and GLnexus in our pipeline (WGS, hg38). I know that a similar issue has already been raised and appreciate the interesting discussion on this (i.e. https://github.com/google/deepvariant/issues/440), but to recap for others this was the result of two contributing factors:. 1. DeepTrio being less confident in the de novo call for the proband than when DeepVariant is run in singleton mode on the proband. In our case, comparing the output VCFs from these two different runs we saw a reduction in the GQ score assigned to the variant from 56 when using DeepVariant in singleton mode, to only 10 when using DeepTrio.; 2. GLnexus filtering, according to the `DeepVariantWGS` configuration we were using, removing our variant of interest in the case of DeepTrio due to the low likelihood assigned to the call. To partly overcome this we are looking to switch the GLnexus configuration to `DeepVariant_unfiltered` as mentioned in https://github.com/google/deepvariant/issues/440. However we would like to further evaluate this change on a known truth set to determine the increase in false-positive calls (similar to [1] with DV-GLN-NOMOD vs DV-GLN-OPT, but for DeepTrio instead... because from what I understand that paper evaluated DeepVariant). I have seen that all three GIAB/NIST benchmark trios have been used as training data for DeepTrio so would like to ask:. 1. Were all chromosomes from these trios used to train the DeepTrio models? I believe the DeepVariant WGS training data excluded chr20-22, and the deeptrio test data uses HG001 Chr20 [2], so I assume chr20-22 were excluded from the Deeptrio models for each of the trios too and would be suitable for testing? Or any alternative suggestions for this?; 2. I understand that the DeepTrio docs aren't officially released yet, but would it be possible please to provide an overview of the workings and differences between the Child and Parent models for DeepT
",False,"The text contains complete, meaningful sentences in natural language discussing concepts related to variant analysis using specific tools. It includes human-written prose about problems encountered with DeepTrio and GLnexus, as well as proposed solutions and questions regarding the models."
Deployability,"Hello,. We noticed that adjacent variants of the same haplotype (i.e. MNVs) are being called as separate variants in the DeepVariant and DeepTrio outputs with VCF and gVCF files. During downstream processing these MNVs are then treated as two individual SNVs at two different loci, leading to faulty assessments. . For example two variants for a site of interest (reference TCG -> Serine) were separated between two lines in the DeepVariant/DeepTrio output VCF and then categorised as containing a nonsynonymous (T**G**G -> Tryptophan) and synonymous mutation (TC**A** -> Serine). Whereas the correct and desired way to handle this, at least for us but I imagine others too, would seem to be to recognise both mutations on a single line in the VCF as a combined substitution, which could then be identified as resulting in a stopgain (T**GA** -> Nonsense mutation). Are there plans to support these MNV calls in the DeepVariant/DeepTrio outputs? Or alternatively are there any current post-processing approaches that you may be using and can recommend to handle these cases? Can understand these may be challenging to manage in some aspects but could be important to flag given some recent literature around this topic. For reference this was using hg38 with WGS. We initially identified this using the original DeepTrio release (docker image deeptrio:1.0.1rc), but then also using the most recent DeepVariant release (via docker, v1.2). We also tested this to see whether the change to the unfiltered GLnexus config could be contributing to this for processing of DeepTrio gVCFs due to the joint genotyping parameter, but reverting back to the WGS config did not result in a merged MNV in this instance. . Many thanks,; Macabe.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/486:1321,release,release,1321,,https://github.com/google/deepvariant/issues/486,2,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello,. We noticed that adjacent variants of the same haplotype (i.e. MNVs) are being called as separate variants in the DeepVariant and DeepTrio outputs with VCF and gVCF files. During downstream processing these MNVs are then treated as two individual SNVs at two different loci, leading to faulty assessments. . For example two variants for a site of interest (reference TCG -> Serine) were separated between two lines in the DeepVariant/DeepTrio output VCF and then categorised as containing a nonsynonymous (T**G**G -> Tryptophan) and synonymous mutation (TC**A** -> Serine). Whereas the correct and desired way to handle this, at least for us but I imagine others too, would seem to be to recognise both mutations on a single line in the VCF as a combined substitution, which could then be identified as resulting in a stopgain (T**GA** -> Nonsense mutation). Are there plans to support these MNV calls in the DeepVariant/DeepTrio outputs? Or alternatively are there any current post-processing approaches that you may be using and can recommend to handle these cases? Can understand these may be challenging to manage in some aspects but could be important to flag given some recent literature around this topic. For reference this was using hg38 with WGS. We initially identified this using the original DeepTrio release (docker image deeptrio:1.0.1rc), but then also using the most recent DeepVariant release (via docker, v1.2). We also tested this to see whether the change to the unfiltered GLnexus config could be contributing to this for processing of DeepTrio gVCFs due to the joint genotyping parameter, but reverting back to the WGS config did not result in a merged MNV in this instance. . Many thanks,; Macabe.
",False,"The text contains complete, meaningful sentences in natural language discussing the impact of MNVs being called as separate variants and the downstream effects on data interpretation. It includes human-written prose explaining the issue, its implications, and questions about handling these cases."
Deployability,"Hello,. Will DeepTrio be updated for use with Oxford Nanopore data ?. Thank you, regards.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/715:25,update,updated,25,,https://github.com/google/deepvariant/issues/715,1,['update'],['updated'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello,. Will DeepTrio be updated for use with Oxford Nanopore data ?. Thank you, regards.
",False,"The text contains a complete, meaningful sentence in natural language. It is a user's question seeking information about updates and compatibility."
Deployability,"Hello,; I'm writing you because I'm trying to install deepvariant, but I'm encountering several difficulties in doing so.; I've tried at first to install through anaconda (```conda install -c bioconda deepvariant```), but I alway get the same problem:; ```. Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 497, in run_script; subprocess_call(command_args, env=env, path=dirname(path)); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/gateways/subprocess.py"", line 56, in subprocess_call; output=_format_output(command_str, path, rc, stdout, stderr)); subprocess.CalledProcessError: Command '['/bin/bash', '-x', '/PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh']' returned non-zero exit status 1.; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 327, in _execute_actions; run_script(target_prefix, Dist(pkg_data), 'post-unlink' if is_unlink else 'post-link'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 513, in run_script; raise LinkError(message); conda.exceptions.LinkError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 281, in execute; pkg_data, actions); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/cond",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252:46,install,install,46,,https://github.com/google/deepvariant/issues/252,3,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello,; I'm writing you because I'm trying to install deepvariant, but I'm encountering several difficulties in doing so.; I've tried at first to install through anaconda (```conda install -c bioconda deepvariant```), but I alway get the same problem:; ```. Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 497, in run_script; subprocess_call(command_args, env=env, path=dirname(path)); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/gateways/subprocess.py"", line 56, in subprocess_call; output=_format_output(command_str, path, rc, stdout, stderr)); subprocess.CalledProcessError: Command '['/bin/bash', '-x', '/PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh']' returned non-zero exit status 1.; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 327, in _execute_actions; run_script(target_prefix, Dist(pkg_data), 'post-unlink' if is_unlink else 'post-link'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 513, in run_script; raise LinkError(message); conda.exceptions.LinkError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 281, in execute; pkg_data, actions); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/cond
",True,"The text contains a detailed log of an error encountered while installing deepvariant through anaconda, including code snippets and tracebacks which are typical of program logs."
Deployability,"Hello,; first of all ,i do not have the permission to run docker on my machine or sudo update or install. And as i work on cluster service,so i hardly can try to tell the Administrator to update some tools because there are other users and any update to key tools may cause them some troublesome.AS i know,many people work on bioinformation use cluster service and do not have permission to do sudo update or maybe not have a docker in service,but conda can do.; so i try search conda deepvariant,and i try conda install -c bioconda deepvariant=1.0.0(on python3,and i also try other version on python2),and i find dv_make_examples.py, and i see many other guys also try conda.(https://github.com/google/deepvariant/issues/9).; when i run dv_make_examples.py on python3,i get ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and i know it is wrong with GLIBC and the solution is to update GLIBC to GLIBC_2.23 ,but i can not . i ask my Administrator and he say the glibc is too important and update it on cluster service may cause other users bug. . so is there any chance i can use deepvatiant ? and again,i can not install from source(no permission to sudo ) or docker(don't have docker on cluster service ),and i can't update glibc .; And the info are like this:; ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and strings /lib64/libc.so.6 |grep GLIBC:; GLIBC_2.2.5; GLIBC_2.2.6; GLIBC_2.3; GLIBC_2.3.2; GLIBC_2.3.3; GLIBC_2.3.4; GLIBC_2.4; GLIBC_2.5; GLIBC_2.6; GLIBC_2.7; GLIBC_2.8; GLIBC_2.9; GLIBC_2.10; GLIBC_2.11; GLIBC_2.12; GLIBC_2.13; GLIBC_2.14; GLIBC_2.15; GLIBC_2.16; GLIBC_2.17; GLIBC_PRIVATE; and the other information is :; Linux version 3.10.0-1127.18.2.el7.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 4.8.5",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/391:87,update,update,87,,https://github.com/google/deepvariant/issues/391,7,"['install', 'update']","['install', 'update']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello,; first of all ,i do not have the permission to run docker on my machine or sudo update or install. And as i work on cluster service,so i hardly can try to tell the Administrator to update some tools because there are other users and any update to key tools may cause them some troublesome.AS i know,many people work on bioinformation use cluster service and do not have permission to do sudo update or maybe not have a docker in service,but conda can do.; so i try search conda deepvariant,and i try conda install -c bioconda deepvariant=1.0.0(on python3,and i also try other version on python2),and i find dv_make_examples.py, and i see many other guys also try conda.(https://github.com/google/deepvariant/issues/9).; when i run dv_make_examples.py on python3,i get ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and i know it is wrong with GLIBC and the solution is to update GLIBC to GLIBC_2.23 ,but i can not . i ask my Administrator and he say the glibc is too important and update it on cluster service may cause other users bug. . so is there any chance i can use deepvatiant ? and again,i can not install from source(no permission to sudo ) or docker(don't have docker on cluster service ),and i can't update glibc .; And the info are like this:; ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and strings /lib64/libc.so.6 |grep GLIBC:; GLIBC_2.2.5; GLIBC_2.2.6; GLIBC_2.3; GLIBC_2.3.2; GLIBC_2.3.3; GLIBC_2.3.4; GLIBC_2.4; GLIBC_2.5; GLIBC_2.6; GLIBC_2.7; GLIBC_2.8; GLIBC_2.9; GLIBC_2.10; GLIBC_2.11; GLIBC_2.12; GLIBC_2.13; GLIBC_2.14; GLIBC_2.15; GLIBC_2.16; GLIBC_2.17; GLIBC_PRIVATE; and the other information is :; Linux version 3.10.0-1127.18.2.el7.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 4.8.5
",False,
Deployability,Hello. ; Thanks for long awaited update to Python3.; I'm trying to update our pipeline and getting error when installing CLIF.; Looks like https://storage.googleapis.com/deepvariant/packages/oss_clif_py3/oss_clif.ubuntu-18.latest.tgz is missing while https://storage.googleapis.com/deepvariant/packages/oss_clif_py3/oss_clif.ubuntu-16.latest.tgz exists.; Can you fix this please?,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/289:33,update,update,33,,https://github.com/google/deepvariant/issues/289,4,"['install', 'pipeline', 'update']","['installing', 'pipeline', 'update']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello. ; Thanks for long awaited update to Python3.; I'm trying to update our pipeline and getting error when installing CLIF.; Looks like https://storage.googleapis.com/deepvariant/packages/oss_clif_py3/oss_clif.ubuntu-18.latest.tgz is missing while https://storage.googleapis.com/deepvariant/packages/oss_clif_py3/oss_clif.ubuntu-16.latest.tgz exists.; Can you fix this please?
",False,The text contains natural language sentences and a request for help.
Deployability,"Hello. OS: Scicore Cluster, Linux; Deep Variant version:1.2.0; Installation: Singularity; Instrument: Ilumina; Data type: Whole exome sequencing analysis. I used the script given from this site:https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-exome-case-study.md. I edited the script to run in the cluster here:; ```; #!/bin/bash. #SBATCH --job-name=Deepvariant_debug; #SBATCH --cpus-per-task=2 # change this according to your needs; #SBATCH --mem=8G # change this according to your needs; #SBATCH --qos=30min # this was just for testing, but the example runs in less than 30 minutes; #SBATCH --output=myrun.o%j; #SBATCH --error=myrun.e%j. mkdir -p output; mkdir -p /scicore/home/cichon/GROUP/Ilumina/output/intermediate_results_dir. ulimit -u 10000; BIN_VERSION=""1.2.0""; # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder; export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output ; export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata ; # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add; # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container; # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif); singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \; /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \; --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \; --regions=/scicore/home/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/515:63,Install,Installation,63,,https://github.com/google/deepvariant/issues/515,1,['Install'],['Installation'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello. OS: Scicore Cluster, Linux; Deep Variant version:1.2.0; Installation: Singularity; Instrument: Ilumina; Data type: Whole exome sequencing analysis. I used the script given from this site:https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-exome-case-study.md. I edited the script to run in the cluster here:; ```; #!/bin/bash. #SBATCH --job-name=Deepvariant_debug; #SBATCH --cpus-per-task=2 # change this according to your needs; #SBATCH --mem=8G # change this according to your needs; #SBATCH --qos=30min # this was just for testing, but the example runs in less than 30 minutes; #SBATCH --output=myrun.o%j; #SBATCH --error=myrun.e%j. mkdir -p output; mkdir -p /scicore/home/cichon/GROUP/Ilumina/output/intermediate_results_dir. ulimit -u 10000; BIN_VERSION=""1.2.0""; # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder; export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output ; export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata ; # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add; # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container; # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif); singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \; /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \; --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \; --regions=/scicore/home/
",False,
Deployability,"Hello.; We encountered an error when rebuild our docker image, and it didn't build despite no changes. ```bash; [91m+ apt-get install -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev zlib1g-dev; [0mReading package lists...; Building dependency tree...; Reading state information...; zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:127,install,install,127,,https://github.com/google/deepvariant/issues/489,5,['install'],"['install', 'installable', 'installed']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello.; We encountered an error when rebuild our docker image, and it didn't build despite no changes. ```bash; [91m+ apt-get install -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev zlib1g-dev; [0mReading package lists...; Building dependency tree...; Reading state information...; zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends
",True,"The content consists of program logs or error messages, including detailed dependency installation issues in a bash script. This text is primarily for debugging purposes and does not contain meaningful human-readable sentences beyond technical details."
Deployability,"Hello; I download 40 sample in 1kgp, first I use oqfe to BWA; Then I send the CRAM to Deepvariant to call variants.; But recently, I had found a concern problem, these 40 samples execute same pipeline exactly. But the sample name of result vcf, some of it is right, last have a SRR suffix just as the next figure. And I confirm that this is the SRR suffix exactly as downloaded fastq SRR of per sample from 1KGP. And I also confirm that they all have right and same CRAM. I don't find the reason of this circumstances and I don't know there will lead to some error to my analysis?; So if someone give me some explain or advice?; very Thanks!; ![1687788687108](https://github.com/google/deepvariant/assets/63234787/2aea3b53-babd-4874-9351-e2024fbc81df)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/670:192,pipeline,pipeline,192,,https://github.com/google/deepvariant/issues/670,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hello; I download 40 sample in 1kgp, first I use oqfe to BWA; Then I send the CRAM to Deepvariant to call variants.; But recently, I had found a concern problem, these 40 samples execute same pipeline exactly. But the sample name of result vcf, some of it is right, last have a SRR suffix just as the next figure. And I confirm that this is the SRR suffix exactly as downloaded fastq SRR of per sample from 1KGP. And I also confirm that they all have right and same CRAM. I don't find the reason of this circumstances and I don't know there will lead to some error to my analysis?; So if someone give me some explain or advice?; very Thanks!; ![1687788687108](https://github.com/google/deepvariant/assets/63234787/2aea3b53-babd-4874-9351-e2024fbc81df)
",False,
Deployability,"Here is my command and the error log.; Command:; ```javascript; EXAMPLES=""/home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/NA12878_BGISEQ_PE150_5.training.examples.tfrecord-?????-of-00038.gz""; echo ""${EXAMPLES}""; OUTPUT_DIR=${EXAMPLES%/*} ; time python ./shuffle_tfrecords_beam.py ; --input_pattern_list=""${EXAMPLES}""; --output_pattern_prefix=""${OUTPUT_DIR}/training_set.with_label.shuffled"" ; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" ; --output_dataset_name=""HG001""; --runner=BundleBasedDirectRunner; ```; the error message:; ```; /home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/sample.training.examples.tfrecord-?????-of-00076.gz; /home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/direct_runner.py:342: DeprecationWarning: options is deprecated since First stable release.. References to <pipeline>.options will not be supported; pipeline.replace_all(_get_transform_overrides(pipeline.options)); INFO:root:Running pipeline with DirectRunner.; WARNING:root:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.; ERROR:root:Exception at bundle <apache_beam.runners.direct.bundle_factory._Bundle object at 0x7f86daaa07e8>, due to an exception.; Traceback (most recent call last):; File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 341, in call; finish_state); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 381, in attempt_call; result = evaluator.finish_bundle(); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/transform_evaluator.py"", line 303, in finish_bundle; bundles = _read_values_to_bundles(reader); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/transform_evaluator.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/91:1323,pipeline,pipeline,1323,,https://github.com/google/deepvariant/issues/91,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Here is my command and the error log.; Command:; ```javascript; EXAMPLES=""/home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/NA12878_BGISEQ_PE150_5.training.examples.tfrecord-?????-of-00038.gz""; echo ""${EXAMPLES}""; OUTPUT_DIR=${EXAMPLES%/*} ; time python ./shuffle_tfrecords_beam.py ; --input_pattern_list=""${EXAMPLES}""; --output_pattern_prefix=""${OUTPUT_DIR}/training_set.with_label.shuffled"" ; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" ; --output_dataset_name=""HG001""; --runner=BundleBasedDirectRunner; ```; the error message:; ```; /home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/sample.training.examples.tfrecord-?????-of-00076.gz; /home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/direct_runner.py:342: DeprecationWarning: options is deprecated since First stable release.. References to <pipeline>.options will not be supported; pipeline.replace_all(_get_transform_overrides(pipeline.options)); INFO:root:Running pipeline with DirectRunner.; WARNING:root:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.; ERROR:root:Exception at bundle <apache_beam.runners.direct.bundle_factory._Bundle object at 0x7f86daaa07e8>, due to an exception.; Traceback (most recent call last):; File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 341, in call; finish_state); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 381, in attempt_call; result = evaluator.finish_bundle(); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/transform_evaluator.py"", line 303, in finish_bundle; bundles = _read_values_to_bundles(reader); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/transform_evaluator.
",False,The text contains code samples and error logs which are primarily made up of technical details and commands.
Deployability,"Hey guys,. I got deepvariant installed with conda fine, but my run failed by needing glibc, when I installed glibc all processes get a segmentation fault, if I remove glibc it works until failing needing the dependency. Any advice? We can't install docker images on our HPC, I haven't tried converting docker to singularity, as I have no experience with that but I'm comfortable compiling from source, but couldn't find the right files/instructions. Cheers",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/179:29,install,installed,29,,https://github.com/google/deepvariant/issues/179,3,['install'],"['install', 'installed']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hey guys,. I got deepvariant installed with conda fine, but my run failed by needing glibc, when I installed glibc all processes get a segmentation fault, if I remove glibc it works until failing needing the dependency. Any advice? We can't install docker images on our HPC, I haven't tried converting docker to singularity, as I have no experience with that but I'm comfortable compiling from source, but couldn't find the right files/instructions. Cheers
",False,"The text contains a user's discussion about their installation and dependency issues, including attempts to resolve them by installing certain packages and considerations for alternative methods like Singularity. This is human-readable prose that explains the problem and seeks advice."
Deployability,"Hi , when i run call_variant , it arises this warn which means can't use the gpu,but i can make sure that the tensorflow can use the gpu.There are the screen shots of the warn and the existence of the gpu. - the gpu existence; ![image](https://github.com/google/deepvariant/assets/71956115/367b1a98-123c-48fb-b170-3f8e4aae7d30). ```python; tensorflow.test.is_gpu_available(); WARNING:tensorflow:From <stdin>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.config.list_physical_devices('GPU')` instead.; 2024-05-12 21:36:00.744470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /device:GPU:0 with 15089 MB memory: -> device: 0, name: Vega 20, pci bus id: 0000:26:00.0; True; ```. - the warn ; ![image](https://github.com/google/deepvariant/assets/71956115/246d5cfd-a9b3-4ac5-aea7-bf4c89401c76). ```shell; warnings.warn(; 2024-05-12 21:43:29.067332: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; ```. - Operating system: Linux ; - DeepVariant version: 1.6.1-gpu; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/820:1200,Install,Installation,1200,,https://github.com/google/deepvariant/issues/820,1,['Install'],['Installation'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi , when i run call_variant , it arises this warn which means can't use the gpu,but i can make sure that the tensorflow can use the gpu.There are the screen shots of the warn and the existence of the gpu. - the gpu existence; ![image](https://github.com/google/deepvariant/assets/71956115/367b1a98-123c-48fb-b170-3f8e4aae7d30). ```python; tensorflow.test.is_gpu_available(); WARNING:tensorflow:From <stdin>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.config.list_physical_devices('GPU')` instead.; 2024-05-12 21:36:00.744470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /device:GPU:0 with 15089 MB memory: -> device: 0, name: Vega 20, pci bus id: 0000:26:00.0; True; ```. - the warn ; ![image](https://github.com/google/deepvariant/assets/71956115/246d5cfd-a9b3-4ac5-aea7-bf4c89401c76). ```shell; warnings.warn(; 2024-05-12 21:43:29.067332: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; ```. - Operating system: Linux ; - DeepVariant version: 1.6.1-gpu; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?)
",False,
Deployability,Hi ; I recently installed docker version of deep variant for cancer exome analysis. I like to know if I don't use paired cancer samples for variant calling with deep variant will it be right approach,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/211:16,install,installed,16,,https://github.com/google/deepvariant/issues/211,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi ; I recently installed docker version of deep variant for cancer exome analysis. I like to know if I don't use paired cancer samples for variant calling with deep variant will it be right approach
",False,
Deployability,"Hi Deep Variant team,. I receive the below error when attempting to follow the [VCF stats report documentation](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md); however, attempting to run vcf stats report interactively inside of the docker yielded an error message that was a bit more informative, telling me that the .py file does not exist. Error message from following the documentation ; > docker: Error response from daemon: OCI runtime create failed: container_linux.go:344: starting container process caused ""exec: \""/opt/deepvariant/bin/vcf_stats_report\"": permission denied"": unknown. Error message from inside the docker; > python: can't open file '/opt/deepvariant/bin/vcf_stats_report.py': [Errno 2] No such file or directory. It seems that [lines 79 to 81 of the Dockerfile](https://github.com/google/deepvariant/blob/r0.9/Dockerfile#L79-L82) create the file called, /opt/deepvariant/bin/vcf_stats_report, but the underlying python script does not seem to be copied into the Docker. It looks like other files in the /opt/deepvariant/bin directory are copied over in [lines 42 to 50](https://github.com/google/deepvariant/blob/r0.9/Dockerfile#L42-L50), maybe a similar line needs to be added for vcf_stats_report?. Thank you for the 0.9.0 release and for being so active on Github. Iâ€™m looking forward to using this tool for my research.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/235:1289,release,release,1289,,https://github.com/google/deepvariant/issues/235,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi Deep Variant team,. I receive the below error when attempting to follow the [VCF stats report documentation](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md); however, attempting to run vcf stats report interactively inside of the docker yielded an error message that was a bit more informative, telling me that the .py file does not exist. Error message from following the documentation ; > docker: Error response from daemon: OCI runtime create failed: container_linux.go:344: starting container process caused ""exec: \""/opt/deepvariant/bin/vcf_stats_report\"": permission denied"": unknown. Error message from inside the docker; > python: can't open file '/opt/deepvariant/bin/vcf_stats_report.py': [Errno 2] No such file or directory. It seems that [lines 79 to 81 of the Dockerfile](https://github.com/google/deepvariant/blob/r0.9/Dockerfile#L79-L82) create the file called, /opt/deepvariant/bin/vcf_stats_report, but the underlying python script does not seem to be copied into the Docker. It looks like other files in the /opt/deepvariant/bin directory are copied over in [lines 42 to 50](https://github.com/google/deepvariant/blob/r0.9/Dockerfile#L42-L50), maybe a similar line needs to be added for vcf_stats_report?. Thank you for the 0.9.0 release and for being so active on Github. Iâ€™m looking forward to using this tool for my research.
",False,"The text contains a detailed explanation of an issue encountered while attempting to use the DeepVariant tool, including error messages and analysis of the Dockerfile, which is more descriptive and meaningful than code or logs."
Deployability,"Hi DeepVariant team,. I have been using DeepVariant ï¼ˆv1.1.0ï¼‰to call small variants for a human genome with approximately 50X coverage of HiFi reads. During my analysis, I noticed that DeepVariant consistently reports some false positives in some regions. For example, at position chr10:89013075-89013077, there is a SNV (see the IGV snapshot of both Illumina and HiFi reads), but DeepVariant identifies three variants at that location. The vcf records by deepvariant:; ```; chr10 89013075 . T TC 31.7 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:63:35,28:0.444444:31,0,43; chr10 89013076 . CA C 28.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:27:63:35,28:0.444444:28,0,34; chr10 89013077 . A C 30.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:18:34:0,34:1:30,0,17; ```; The IGV snapshots:. ![image](https://github.com/google/deepvariant/assets/44404441/9618a4b9-7594-42e0-bb65-173761f803a8). I haven't identified any patterns in these regions. Have you encountered similar situations before? Could you please explain the possible reasons behind these false positive calls? I have provided three examples, including small BAM files, resulting VCF files, and my code, for you to replicate and investigate. . [deepvariant_fp.zip](https://github.com/google/deepvariant/files/11735375/deepvariant_fp.zip). Other information of my test: . - Operating system: Red Hat 4.8.5-36; - DeepVariant version: 1.1.0; - Installation method (Docker, built from source, etc.): Docker; - reference : GRCh38_full_analysis_set_plus_decoy_hla.fa; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); ~50x Human HiFi seuquencing data. Thank you very much for your attention and support. I look forward to your response. Best regards,; Peng Jia",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/660:1364,Install,Installation,1364,,https://github.com/google/deepvariant/issues/660,1,['Install'],['Installation'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi DeepVariant team,. I have been using DeepVariant ï¼ˆv1.1.0ï¼‰to call small variants for a human genome with approximately 50X coverage of HiFi reads. During my analysis, I noticed that DeepVariant consistently reports some false positives in some regions. For example, at position chr10:89013075-89013077, there is a SNV (see the IGV snapshot of both Illumina and HiFi reads), but DeepVariant identifies three variants at that location. The vcf records by deepvariant:; ```; chr10 89013075 . T TC 31.7 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:63:35,28:0.444444:31,0,43; chr10 89013076 . CA C 28.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:27:63:35,28:0.444444:28,0,34; chr10 89013077 . A C 30.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:18:34:0,34:1:30,0,17; ```; The IGV snapshots:. ![image](https://github.com/google/deepvariant/assets/44404441/9618a4b9-7594-42e0-bb65-173761f803a8). I haven't identified any patterns in these regions. Have you encountered similar situations before? Could you please explain the possible reasons behind these false positive calls? I have provided three examples, including small BAM files, resulting VCF files, and my code, for you to replicate and investigate. . [deepvariant_fp.zip](https://github.com/google/deepvariant/files/11735375/deepvariant_fp.zip). Other information of my test: . - Operating system: Red Hat 4.8.5-36; - DeepVariant version: 1.1.0; - Installation method (Docker, built from source, etc.): Docker; - reference : GRCh38_full_analysis_set_plus_decoy_hla.fa; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); ~50x Human HiFi seuquencing data. Thank you very much for your attention and support. I look forward to your response. Best regards,; Peng Jia
",False,
Deployability,"Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:; ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```; rule deepvariant:; input:; bam=rules.apply_bqsr.output.bam,; ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'; output:; vcf=""results/deepvariant/{sample}.vcf.gz""; params:; model=""WES""; threads: ; 64; resources:; mem_mb=163840; log:; ""logs/deepvariant/{sample}/stdout.log""; singularity:; ""singularity/deepvariant_1.4.0.sif""; # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU; shell:; """"""; /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'; """"""; ```. Below is the begening and end of",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/602:156,pipeline,pipeline,156,,https://github.com/google/deepvariant/issues/602,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:; ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```; rule deepvariant:; input:; bam=rules.apply_bqsr.output.bam,; ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'; output:; vcf=""results/deepvariant/{sample}.vcf.gz""; params:; model=""WES""; threads: ; 64; resources:; mem_mb=163840; log:; ""logs/deepvariant/{sample}/stdout.log""; singularity:; ""singularity/deepvariant_1.4.0.sif""; # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU; shell:; """"""; /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'; """"""; ```. Below is the begening and end of
",False,"The text contains detailed descriptions of a workflow setup using Snakemake, including command lines and configuration settings. While it does include some technical content, it's not exclusively code or logs. It discusses issues encountered when running DeepVariant on an HPC cluster, providing context that is meaningful for troubleshooting and understanding pipeline behavior."
Deployability,"Hi Developers,. I get the following error when I am trying to use the latest version (V0.7.0) of deepvariant for wes analysis. I encounter the same error even I used the example given [here](https://cloud.google.com/genomics/docs/tutorials/deepvariant). ; ./deepvariant_v0.7.0_UDN644883_wes_09202018.sh; ERROR: (gcloud.alpha.genomics.pipelines.run) INVALID_ARGUMENT: Error: validating pipeline: zones and regions cannot be specified together. I have attached my script for the reference. ; [deepvariant_v0.7.0_UDN644883_wes_09202018.sh.txt](https://github.com/google/deepvariant/files/2403088/deepvariant_v0.7.0_UDN644883_wes_09202018.sh.txt). I was able to run deepvariant for wes using V0.6.1 successfully however looking at the output vcf I 'think' the script is not restricting the variants to the bed file regions. ; [deepvariant_v0.6.1_UDN644883_wes_09132018.sh.txt](https://github.com/google/deepvariant/files/2403091/deepvariant_v0.6.1_UDN644883_wes_09132018.sh.txt); [deepvariant_v0.6.1_UDN644883_wes_09132018.yaml.txt](https://github.com/google/deepvariant/files/2403092/deepvariant_v0.6.1_UDN644883_wes_09132018.yaml.txt). Thanks,; Shruti. Shruti Marwaha, PhD.; Research Engineer,; Stanford Center for Undiagnosed Diseases; Stanford University",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/96:334,pipeline,pipelines,334,,https://github.com/google/deepvariant/issues/96,2,['pipeline'],"['pipeline', 'pipelines']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi Developers,. I get the following error when I am trying to use the latest version (V0.7.0) of deepvariant for wes analysis. I encounter the same error even I used the example given [here](https://cloud.google.com/genomics/docs/tutorials/deepvariant). ; ./deepvariant_v0.7.0_UDN644883_wes_09202018.sh; ERROR: (gcloud.alpha.genomics.pipelines.run) INVALID_ARGUMENT: Error: validating pipeline: zones and regions cannot be specified together. I have attached my script for the reference. ; [deepvariant_v0.7.0_UDN644883_wes_09202018.sh.txt](https://github.com/google/deepvariant/files/2403088/deepvariant_v0.7.0_UDN644883_wes_09202018.sh.txt). I was able to run deepvariant for wes using V0.6.1 successfully however looking at the output vcf I 'think' the script is not restricting the variants to the bed file regions. ; [deepvariant_v0.6.1_UDN644883_wes_09132018.sh.txt](https://github.com/google/deepvariant/files/2403091/deepvariant_v0.6.1_UDN644883_wes_09132018.sh.txt); [deepvariant_v0.6.1_UDN644883_wes_09132018.yaml.txt](https://github.com/google/deepvariant/files/2403092/deepvariant_v0.6.1_UDN644883_wes_09132018.yaml.txt). Thanks,; Shruti. Shruti Marwaha, PhD.; Research Engineer,; Stanford Center for Undiagnosed Diseases; Stanford University
",False,
Deployability,"Hi all,. The DeepVariant case study scripts for running via binaries on CPU install the `intel-tensorflow` package. We have noticed the below error when installing this package and are looking into how this can be fixed. ```; $ pip install intel-tensorflow; ERROR: intel-tensorflow has an invalid wheel, multiple .dist-info directories found: intel_tensorflow-2.0.0.dist-info, tensorflow-2.0.0.dist-info; ```; If you run into this issue, we recommend one of the following options in the meantime:; * Use the Docker scripts instead of the binaries scripts.; * Set the [`DV_USE_GCP_OPTIMIZED_TF_WHL`](https://github.com/google/deepvariant/blob/r0.9/settings.sh#L90) variable to 0 prior to setting up DeepVariant and running the case study scripts for binaries. `intel-tensorflow` is only installed when this variable is set.; * Use the GPU scripts instead of the CPU scripts. Best,; The DeepVariant Team",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/263:76,install,install,76,,https://github.com/google/deepvariant/issues/263,4,['install'],"['install', 'installed', 'installing']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi all,. The DeepVariant case study scripts for running via binaries on CPU install the `intel-tensorflow` package. We have noticed the below error when installing this package and are looking into how this can be fixed. ```; $ pip install intel-tensorflow; ERROR: intel-tensorflow has an invalid wheel, multiple .dist-info directories found: intel_tensorflow-2.0.0.dist-info, tensorflow-2.0.0.dist-info; ```; If you run into this issue, we recommend one of the following options in the meantime:; * Use the Docker scripts instead of the binaries scripts.; * Set the [`DV_USE_GCP_OPTIMIZED_TF_WHL`](https://github.com/google/deepvariant/blob/r0.9/settings.sh#L90) variable to 0 prior to setting up DeepVariant and running the case study scripts for binaries. `intel-tensorflow` is only installed when this variable is set.; * Use the GPU scripts instead of the CPU scripts. Best,; The DeepVariant Team
",False,
Deployability,"Hi all;; Thanks for all the help getting an initial conda package in place for DeepVariant (#9) through bioconda. I wanted to follow up with some suggestions that would help make the pre-built binaries more portable as part of this process, in order of helpfulness for portability:. - Currently the binaries need a recent kernel with GLIBC > 2.23 due to pre-built htslib and other libraries. Would it be possible to build the DeepVariant libraries on an older machine to allow a wider range of system support? We build on CentOS 6 in conda to provide wider compatibility.; - main.py in the zip files hardcodes python to use `/usr/bin/python`. Would it be possible to generalize this by using the python that the zip file gets called with (`sys.executable`)? I currently patch this in the conda build: https://github.com/bioconda/bioconda-recipes/blob/0a2d467d63d011015efeef4b644e985297b6b271/recipes/deepvariant/build.sh#L22; - This is currently built against numpy 1.13 and ideally we'd want to sync with CONDA_NPY (1.12: https://github.com/bioconda/bioconda-recipes/blob/0a2d467d63d011015efeef4b644e985297b6b271/scripts/env_matrix.yml#L9). I believe building against 1.12 would make it forward compatible. An alternative to points 1 and 3 is making it easier to build DeepVariant as part of the conda build process. The major blocker here is the `clif` dependency which is difficult to build and the pre-built binaries require unpacking into `/usr`. If we could make this relocatable and easier to install globally we could build with portable binaries and adjustable numpy as part of the bioconda preparation process. Thanks again for all the help.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29:770,patch,patch,770,,https://github.com/google/deepvariant/issues/29,2,"['install', 'patch']","['install', 'patch']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi all;; Thanks for all the help getting an initial conda package in place for DeepVariant (#9) through bioconda. I wanted to follow up with some suggestions that would help make the pre-built binaries more portable as part of this process, in order of helpfulness for portability:. - Currently the binaries need a recent kernel with GLIBC > 2.23 due to pre-built htslib and other libraries. Would it be possible to build the DeepVariant libraries on an older machine to allow a wider range of system support? We build on CentOS 6 in conda to provide wider compatibility.; - main.py in the zip files hardcodes python to use `/usr/bin/python`. Would it be possible to generalize this by using the python that the zip file gets called with (`sys.executable`)? I currently patch this in the conda build: https://github.com/bioconda/bioconda-recipes/blob/0a2d467d63d011015efeef4b644e985297b6b271/recipes/deepvariant/build.sh#L22; - This is currently built against numpy 1.13 and ideally we'd want to sync with CONDA_NPY (1.12: https://github.com/bioconda/bioconda-recipes/blob/0a2d467d63d011015efeef4b644e985297b6b271/scripts/env_matrix.yml#L9). I believe building against 1.12 would make it forward compatible. An alternative to points 1 and 3 is making it easier to build DeepVariant as part of the conda build process. The major blocker here is the `clif` dependency which is difficult to build and the pre-built binaries require unpacking into `/usr`. If we could make this relocatable and easier to install globally we could build with portable binaries and adjustable numpy as part of the bioconda preparation process. Thanks again for all the help.
",False,
Deployability,"Hi folks,. Our users are running deepvariant on our HPC and resource requests are proving quite tricky for them as different stages of the pipeline seem to have different resource needs. This leaves execution nodes essentially idle for much of the time of the jobs' running. In the two days runtime you can see below that a user who has asked for 48 cpus doesn't use most of the cores for most of the time:. ![Image](https://github.com/user-attachments/assets/c631a80d-a7b3-435a-ac50-97a1743f7638). ```; singularity run deepvariant_1.6.1.sif /opt/deepvariant/bin/run_deepvariant \; --model_type ONT_R104 \; --ref /path/to/their/reference.fasta \; --reads /path/to/their/input.bam \; --sample_name unique_name \; --output_vcf /path/to/their/unique_name.vcf.gz \; --output_gvcf /path/to/their/unique_name.g.vcf.gz \; --num_shards 48; ```; Is there a way that each stage of the pipeline with discreet resource requests so that the bits that can be cleanly parallelized can go in one job submission and the stages that don't can be in separate jobs/commands, to avoid having requested and reserved resources being idle?. Cheers",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/905:139,pipeline,pipeline,139,,https://github.com/google/deepvariant/issues/905,2,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi folks,. Our users are running deepvariant on our HPC and resource requests are proving quite tricky for them as different stages of the pipeline seem to have different resource needs. This leaves execution nodes essentially idle for much of the time of the jobs' running. In the two days runtime you can see below that a user who has asked for 48 cpus doesn't use most of the cores for most of the time:. ![Image](https://github.com/user-attachments/assets/c631a80d-a7b3-435a-ac50-97a1743f7638). ```; singularity run deepvariant_1.6.1.sif /opt/deepvariant/bin/run_deepvariant \; --model_type ONT_R104 \; --ref /path/to/their/reference.fasta \; --reads /path/to/their/input.bam \; --sample_name unique_name \; --output_vcf /path/to/their/unique_name.vcf.gz \; --output_gvcf /path/to/their/unique_name.g.vcf.gz \; --num_shards 48; ```; Is there a way that each stage of the pipeline with discreet resource requests so that the bits that can be cleanly parallelized can go in one job submission and the stages that don't can be in separate jobs/commands, to avoid having requested and reserved resources being idle?. Cheers
",False,"The text contains natural language sentences discussing resource management for deepvariant pipeline, including specific commands which are relevant context but not primary content."
Deployability,"Hi guys, . I have been doing some test on DeepVariant genotyping call. I run the suit and got very good results, but as part of experiments I need to generate a cohort VCF (multi-sample). As suggested here on github, I generated GVCF for all my samples, but when I tried to use GATK's CombineGVCFs or GenotypeGVCFs neither worked because they don't recognize the alternative allele `<*>`, instead GATK moved to `<NON_REF>` on the more recent versions.; After identified the problem, I run a simple substitution using `sed` to replace all occurrences of `<*>` for `<NON_REF>` and the commands ran fine. . `zcat SAMPLE.deepvar.g.vcf.gz | sed 's/<*>/<NON_REF>/g' | bgzip -c > SAMPLE.gvcf.gz`. May I suggest this update for your software to keep the compatibility with GATK?. Best,; AndrÃ© Santos",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/83:709,update,update,709,,https://github.com/google/deepvariant/issues/83,1,['update'],['update'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi guys, . I have been doing some test on DeepVariant genotyping call. I run the suit and got very good results, but as part of experiments I need to generate a cohort VCF (multi-sample). As suggested here on github, I generated GVCF for all my samples, but when I tried to use GATK's CombineGVCFs or GenotypeGVCFs neither worked because they don't recognize the alternative allele `<*>`, instead GATK moved to `<NON_REF>` on the more recent versions.; After identified the problem, I run a simple substitution using `sed` to replace all occurrences of `<*>` for `<NON_REF>` and the commands ran fine. . `zcat SAMPLE.deepvar.g.vcf.gz | sed 's/<*>/<NON_REF>/g' | bgzip -c > SAMPLE.gvcf.gz`. May I suggest this update for your software to keep the compatibility with GATK?. Best,; AndrÃ© Santos
",False,"The text contains complete, meaningful sentences in natural language, such as expressing concern about software compatibility and suggesting an improvement. It includes human-readable content discussing genotyping calls and VCF processing."
Deployability,"Hi! The current bioconda recipe requires precompiled binaries to be available at `https://github.com/google/deepvariant/releases/download/v{{ version }}/deepvariant.zip`. I've updated the bioconda recipe to work with v1.0.0, but for a smooth update to v1.1.0, it would be excellent if you could provide zipped binaries with the release. The bioconda recipe: https://github.com/bioconda/bioconda-recipes/tree/master/recipes/deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/436:120,release,releases,120,,https://github.com/google/deepvariant/issues/436,4,"['release', 'update']","['release', 'releases', 'update', 'updated']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi! The current bioconda recipe requires precompiled binaries to be available at `https://github.com/google/deepvariant/releases/download/v{{ version }}/deepvariant.zip`. I've updated the bioconda recipe to work with v1.0.0, but for a smooth update to v1.1.0, it would be excellent if you could provide zipped binaries with the release. The bioconda recipe: https://github.com/bioconda/bioconda-recipes/tree/master/recipes/deepvariant
",False,"The text contains complete sentences in natural language discussing the update process and requirements, suitable for human reading."
Deployability,"Hi!. Is there a way to emit all sites in given regions (using a bed file) in the final VCF even if they are the same as reference? I want to use it in a pipeline in which any position not in the input VCF is assumed to be a ""no call"". Missing positions will not be interpreted as reference.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/484:153,pipeline,pipeline,153,,https://github.com/google/deepvariant/issues/484,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi!. Is there a way to emit all sites in given regions (using a bed file) in the final VCF even if they are the same as reference? I want to use it in a pipeline in which any position not in the input VCF is assumed to be a ""no call"". Missing positions will not be interpreted as reference.
",False,
Deployability,"Hi, . I had trouble running Deepvariant using conda. I ran the following command.; ```; dv_make_examples.py --sample {MY_SAMPLE} --ref {MY_FASTA}.fasta --reads {MY_BAM}.bam --logdir ./log/ --examples examples/; ```. and I got an error like this:; ```; ETA: 0s Left: 1 AVG: 0.00s local:1/0/100%/0.0s sh: 1: unzip: not found; Traceback (most recent call last):; File ""/opt/conda/envs/deepvariant/lib/python3.6/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/opt/conda/envs/deepvariant/lib/python3.6/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 252, in <module>; File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 187, in Main; File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 138, in GetRepositoriesImports; FileNotFoundError: [Errno 2] No such file or directory: '/tmp/Bazel.runfiles_qwsw52c7/runfiles'; parallel: This job failed:; /opt/conda/envs/deepvariant/bin/python /opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip --mode calling --ref E.coli_K12_MG1655.fa --reads SRR1770413.bam --examples examples//SRR1770413.tfrecord@1.gz --task 0; ```. When I installed unzip by `conda install`, the command worked fine. When using conda to install deepvariant, should it not have to be installed together?. Thanks,",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/314:1499,install,installed,1499,,https://github.com/google/deepvariant/issues/314,4,['install'],"['install', 'installed']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi, . I had trouble running Deepvariant using conda. I ran the following command.; ```; dv_make_examples.py --sample {MY_SAMPLE} --ref {MY_FASTA}.fasta --reads {MY_BAM}.bam --logdir ./log/ --examples examples/; ```. and I got an error like this:; ```; ETA: 0s Left: 1 AVG: 0.00s local:1/0/100%/0.0s sh: 1: unzip: not found; Traceback (most recent call last):; File ""/opt/conda/envs/deepvariant/lib/python3.6/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/opt/conda/envs/deepvariant/lib/python3.6/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 252, in <module>; File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 187, in Main; File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 138, in GetRepositoriesImports; FileNotFoundError: [Errno 2] No such file or directory: '/tmp/Bazel.runfiles_qwsw52c7/runfiles'; parallel: This job failed:; /opt/conda/envs/deepvariant/bin/python /opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip --mode calling --ref E.coli_K12_MG1655.fa --reads SRR1770413.bam --examples examples//SRR1770413.tfrecord@1.gz --task 0; ```. When I installed unzip by `conda install`, the command worked fine. When using conda to install deepvariant, should it not have to be installed together?. Thanks,
",False,"The text contains a detailed error message and explanation of an issue when running DeepVariant with conda. It includes a code snippet in markdown but also provides context and analysis, which makes the content meaningful and readable for humans."
Deployability,"Hi, . This is more of a support question, but I wasn't sure where else to get help. I'm trying to build and test deepvariant inside of a docker image. I know that there is already an image published to google cloud, but for my purposes I prefer to build my own image. My docker file looks like this. ; ```; FROM ubuntu:16.04. RUN set -ex \; && buildDependencies=' \; ca-certificates \; curl \; wget \; git \; apt-transport-https \; xz-utils \; bzip2 \; make \; ' \; && apt-get update \; && apt-get install -y --no-install-recommends $buildDependencies \; # gsutil; && wget https://storage.googleapis.com/pub/gsutil.tar.gz \; && tar xfz gsutil.tar.gz -C $HOME && rm gsutil.tar.gz \; && export PATH=$PATH:$HOME/gsutil \; # deepvariant; && git clone https://github.com/google/deepvariant.git \; && cd deepvariant \; && git checkout v0.4.1 \; && ./build-prereq.sh \; && ./build_and_test.sh; ```; The `build_and_test.sh` script fails with these errors:; ```; + ./build_and_test.sh; + source settings.sh; ++ export TF_CUDA_CLANG=0; ++ TF_CUDA_CLANG=0; ++ export TF_ENABLE_XLA=0; ++ TF_ENABLE_XLA=0; ++ export TF_NEED_CUDA=0; ++ TF_NEED_CUDA=0; ++ export TF_NEED_GCP=1; ++ TF_NEED_GCP=1; ++ export TF_NEED_GDR=0; ++ TF_NEED_GDR=0; ++ export TF_NEED_HDFS=0; ++ TF_NEED_HDFS=0; ++ export TF_NEED_JEMALLOC=0; ++ TF_NEED_JEMALLOC=0; ++ export TF_NEED_MKL=0; ++ TF_NEED_MKL=0; ++ export TF_NEED_MPI=0; ++ TF_NEED_MPI=0; ++ export TF_NEED_OPENCL=0; ++ TF_NEED_OPENCL=0; ++ export TF_NEED_OPENCL_SYCL=0; ++ TF_NEED_OPENCL_SYCL=0; ++ export TF_NEED_S3=0; ++ TF_NEED_S3=0; ++ export TF_NEED_VERBS=0; ++ TF_NEED_VERBS=0; ++ export TF_CUDA_VERSION=8.0; ++ TF_CUDA_VERSION=8.0; ++ export CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ export TF_CUDNN_VERSION=6; ++ TF_CUDNN_VERSION=6; ++ export CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ export DEEPVARIANT_BUCKET=gs://deepvariant; ++ DEEPVARIANT_BUCKET=gs://deepvariant; ++ export DV_P",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:477,update,update,477,,https://github.com/google/deepvariant/issues/19,3,"['install', 'update']","['install', 'install-recommends', 'update']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi, . This is more of a support question, but I wasn't sure where else to get help. I'm trying to build and test deepvariant inside of a docker image. I know that there is already an image published to google cloud, but for my purposes I prefer to build my own image. My docker file looks like this. ; ```; FROM ubuntu:16.04. RUN set -ex \; && buildDependencies=' \; ca-certificates \; curl \; wget \; git \; apt-transport-https \; xz-utils \; bzip2 \; make \; ' \; && apt-get update \; && apt-get install -y --no-install-recommends $buildDependencies \; # gsutil; && wget https://storage.googleapis.com/pub/gsutil.tar.gz \; && tar xfz gsutil.tar.gz -C $HOME && rm gsutil.tar.gz \; && export PATH=$PATH:$HOME/gsutil \; # deepvariant; && git clone https://github.com/google/deepvariant.git \; && cd deepvariant \; && git checkout v0.4.1 \; && ./build-prereq.sh \; && ./build_and_test.sh; ```; The `build_and_test.sh` script fails with these errors:; ```; + ./build_and_test.sh; + source settings.sh; ++ export TF_CUDA_CLANG=0; ++ TF_CUDA_CLANG=0; ++ export TF_ENABLE_XLA=0; ++ TF_ENABLE_XLA=0; ++ export TF_NEED_CUDA=0; ++ TF_NEED_CUDA=0; ++ export TF_NEED_GCP=1; ++ TF_NEED_GCP=1; ++ export TF_NEED_GDR=0; ++ TF_NEED_GDR=0; ++ export TF_NEED_HDFS=0; ++ TF_NEED_HDFS=0; ++ export TF_NEED_JEMALLOC=0; ++ TF_NEED_JEMALLOC=0; ++ export TF_NEED_MKL=0; ++ TF_NEED_MKL=0; ++ export TF_NEED_MPI=0; ++ TF_NEED_MPI=0; ++ export TF_NEED_OPENCL=0; ++ TF_NEED_OPENCL=0; ++ export TF_NEED_OPENCL_SYCL=0; ++ TF_NEED_OPENCL_SYCL=0; ++ export TF_NEED_S3=0; ++ TF_NEED_S3=0; ++ export TF_NEED_VERBS=0; ++ TF_NEED_VERBS=0; ++ export TF_CUDA_VERSION=8.0; ++ TF_CUDA_VERSION=8.0; ++ export CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ export TF_CUDNN_VERSION=6; ++ TF_CUDNN_VERSION=6; ++ export CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ export DEEPVARIANT_BUCKET=gs://deepvariant; ++ DEEPVARIANT_BUCKET=gs://deepvariant; ++ export DV_P
",False,"The text contains a user's problem description and Dockerfile, which are both meaningful sentences. The error logs from build_and_test.sh are included as well, but the presence of human-readable explanations about building DeepVariant in a Docker image outweighs the code snippets. Additionally, the text is discussing setup and troubleshooting, which is relevant for support purposes."
Deployability,"Hi, . Would it be possible to use Deepvariant's training pipeline to create a model that is able to call larger variants (SVs), or is there something that would fundamentally limit the use of Deepvariant's algorithm for this case? . Thanks a lot in advance for any comments!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/298:57,pipeline,pipeline,57,,https://github.com/google/deepvariant/issues/298,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi, . Would it be possible to use Deepvariant's training pipeline to create a model that is able to call larger variants (SVs), or is there something that would fundamentally limit the use of Deepvariant's algorithm for this case? . Thanks a lot in advance for any comments!
",False,"The text contains complete, meaningful sentences in natural language. It includes questions and requests for comments, which are examples of human-readable content."
Deployability,"Hi, ; when I install deepvariant by anaconda with the command ""conda install -c bioconda deepvariant"", version 0.8.0 will be installed, but I got the following errors at the end of installation:. CondaError: Downloaded bytes did not match Content-Length; url: https://conda.anaconda.org/bioconda/linux-64/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; target_path: /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; Content-Length: 229846992; downloaded bytes: 217650750. Best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/228:13,install,install,13,,https://github.com/google/deepvariant/issues/228,4,['install'],"['install', 'installation', 'installed']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi, ; when I install deepvariant by anaconda with the command ""conda install -c bioconda deepvariant"", version 0.8.0 will be installed, but I got the following errors at the end of installation:. CondaError: Downloaded bytes did not match Content-Length; url: https://conda.anaconda.org/bioconda/linux-64/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; target_path: /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; Content-Length: 229846992; downloaded bytes: 217650750. Best.
",False,"The text contains an error message from a software installation, which is considered program output and should be filtered."
Deployability,"Hi, I am trying to build DeepVariant from source, and I encounter the following issue in build_and_test. I have bazel 0.26.1 compiled from source as well. ```; (16:39:00) ERROR: Analysis of target '//deepvariant:make_examples_utils_test' failed; build aborted: . /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/bazel_tools/tools/jdk/BUILD:487:14: Configurable attribute ""actual"" doesn't match this configuration: Could not find a JDK for host execution environment, please explicitly provide one using `--host_javabase.`; ```; I tried passing the argument ""--host_javabase=@local_jdk//:jdk"" to bazel to no avail. Java:; ```; # java -version; openjdk version ""1.8.0_262""; OpenJDK Runtime Environment (build 1.8.0_262-b10); OpenJDK 64-Bit Server VM (build 25.262-b10, mixed mode); ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/355:423,configurat,configuration,423,,https://github.com/google/deepvariant/issues/355,1,['configurat'],['configuration'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi, I am trying to build DeepVariant from source, and I encounter the following issue in build_and_test. I have bazel 0.26.1 compiled from source as well. ```; (16:39:00) ERROR: Analysis of target '//deepvariant:make_examples_utils_test' failed; build aborted: . /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/bazel_tools/tools/jdk/BUILD:487:14: Configurable attribute ""actual"" doesn't match this configuration: Could not find a JDK for host execution environment, please explicitly provide one using `--host_javabase.`; ```; I tried passing the argument ""--host_javabase=@local_jdk//:jdk"" to bazel to no avail. Java:; ```; # java -version; openjdk version ""1.8.0_262""; OpenJDK Runtime Environment (build 1.8.0_262-b10); OpenJDK 64-Bit Server VM (build 25.262-b10, mixed mode); ```
",False,
Deployability,"Hi, I want to know is there any way to make deepvariant support command line pipeline, I'm expecting it could accept read from stdin and pipe result to stdout. I tried explicitly using /dev/stdout to pipe result to stdout, ; ```bash; docker run \; --gpus all \; --rm \; -v ${INPUT_DIR}:/input \; -v ${OUTPUT_DIR}:/output \; google/deepvariant:latest-gpu \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,000,100"" \; --output_vcf=/dev/stdout \; --num_shards=4 \; 2> stderr.txt; ```; it could work, but print some debug information to stdout and pollute the result; ```; ***** Running the command:*****; time seq 0 3 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@4.gz"" --regions ""chr20:10,000,000-10,000,100"" --task {}. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@4.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.; For more information, please see:; * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md; * https://github.com/tensorflow/addons; If you depend on functionality not listed there, please file an issue. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --outfile ""/dev/stdout"". ##fileformat=VCFv4.2; ##FILTER=<ID=PASS,Description=""All filters passed"">; ##FILTER=<ID=RefCall,Description=""Genotyping model thinks this site is reference."">; ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/288:77,pipeline,pipeline,77,,https://github.com/google/deepvariant/issues/288,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi, I want to know is there any way to make deepvariant support command line pipeline, I'm expecting it could accept read from stdin and pipe result to stdout. I tried explicitly using /dev/stdout to pipe result to stdout, ; ```bash; docker run \; --gpus all \; --rm \; -v ${INPUT_DIR}:/input \; -v ${OUTPUT_DIR}:/output \; google/deepvariant:latest-gpu \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,000,100"" \; --output_vcf=/dev/stdout \; --num_shards=4 \; 2> stderr.txt; ```; it could work, but print some debug information to stdout and pollute the result; ```; ***** Running the command:*****; time seq 0 3 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@4.gz"" --regions ""chr20:10,000,000-10,000,100"" --task {}. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@4.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.; For more information, please see:; * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md; * https://github.com/tensorflow/addons; If you depend on functionality not listed there, please file an issue. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --outfile ""/dev/stdout"". ##fileformat=VCFv4.2; ##FILTER=<ID=PASS,Description=""All filters passed"">; ##FILTER=<ID=RefCall,Description=""Genotyping model thinks this site is reference."">; 
",False,"The text includes commands and outputs from shell sessions, which are logs rather than meaningful prose."
Deployability,"Hi, I was working on update the deepvariant source code from ubuntu 16.04 to 18.04 and python 3.6 to python 3.8. Now I met a problem in build_release_binaries.shell scripts. . bazel build -c opt \; --output_filter=DONT_MATCH_ANYTHING \; --noshow_loading_progress \; --show_result=0 \; ${DV_COPT_FLAGS} \; --build_python_zip \; :binaries. The error is below:; [1,442 / 1,802] Compiling third_party/nucleus/protos/struct.pb.cc; 1s local ... (128 actions, 48 running); (17:42:57) [1,544 / 1,802] Compiling external/org_tensorflow/tensorflow/core/util/test_log.pb.cc; 6s local ... (128 actions, 47 running); (17:43:03) ERROR: /opt/deepvariant/deepvariant/realigner/python/BUILD:54:1: C++ compilation of rule '//deepvariant/realigner/python:ssw_cclib' failed (Exit 1): gcc failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \; exec env - \; PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/bin/python3.8 \; PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \; TF2_BEHAVIOR=1 \; TF_CONFIGURE_IOS=0 \; TF_ENABLE_XLA=1 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/deepvariant/realigner/python/_objs/ssw_cclib/ssw.pic.d '-frandom-seed=bazel-out/k8-opt/bin/deepvariant/realigner/python/_objs/ssw_cclib/ssw.pic.o' -fPIC -DTF_USE_SNAPPY -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -DHAVE_SYS_UIO_H -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote . -iquote bazel-out/k8-opt/bin -iquote external/libssw -iquote bazel-out/k8-opt/bin/external/libssw -iquote external/org_tensorflow -iquote bazel-out/k8-opt/bin/external/org_tensorflow -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_g",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441:21,update,update,21,,https://github.com/google/deepvariant/issues/441,1,['update'],['update'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi, I was working on update the deepvariant source code from ubuntu 16.04 to 18.04 and python 3.6 to python 3.8. Now I met a problem in build_release_binaries.shell scripts. . bazel build -c opt \; --output_filter=DONT_MATCH_ANYTHING \; --noshow_loading_progress \; --show_result=0 \; ${DV_COPT_FLAGS} \; --build_python_zip \; :binaries. The error is below:; [1,442 / 1,802] Compiling third_party/nucleus/protos/struct.pb.cc; 1s local ... (128 actions, 48 running); (17:42:57) [1,544 / 1,802] Compiling external/org_tensorflow/tensorflow/core/util/test_log.pb.cc; 6s local ... (128 actions, 47 running); (17:43:03) ERROR: /opt/deepvariant/deepvariant/realigner/python/BUILD:54:1: C++ compilation of rule '//deepvariant/realigner/python:ssw_cclib' failed (Exit 1): gcc failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \; exec env - \; PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/bin/python3.8 \; PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \; TF2_BEHAVIOR=1 \; TF_CONFIGURE_IOS=0 \; TF_ENABLE_XLA=1 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/deepvariant/realigner/python/_objs/ssw_cclib/ssw.pic.d '-frandom-seed=bazel-out/k8-opt/bin/deepvariant/realigner/python/_objs/ssw_cclib/ssw.pic.o' -fPIC -DTF_USE_SNAPPY -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -DHAVE_SYS_UIO_H -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote . -iquote bazel-out/k8-opt/bin -iquote external/libssw -iquote bazel-out/k8-opt/bin/external/libssw -iquote external/org_tensorflow -iquote bazel-out/k8-opt/bin/external/org_tensorflow -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_g
",False,
Deployability,"Hi, I'm running DeepVariant with BQSR -adjusted bam files. I have sequencing data for hg002 and hg005 and I have to say the validation results are very impressive!. I wanted to test the option for using the original base quality scores with:. --parse_sam_aux_fields ; --use_original_quality_scores. but get the following error: . FATAL Flags parsing error: Unknown command line flag 'parse_sam_aux_fields'; Pass --helpshort or --helpfull to see help on flags. I was running DeepVariant with docker by following the whole genome sequencing case study -tutorial, but will next test the pipeline for multi-sample variant calling for my cohort of 50 samples. I'm wondering should I realign the reads or is it possible to use the original base quality scores from BQSR adjusted bam files? I was previously using the GATK4 pipeline, but the results are so much better with DeepVariant and as a bonus, it's a million times easier (and quicker). Thanks. Karoliina",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/595:584,pipeline,pipeline,584,,https://github.com/google/deepvariant/issues/595,2,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi, I'm running DeepVariant with BQSR -adjusted bam files. I have sequencing data for hg002 and hg005 and I have to say the validation results are very impressive!. I wanted to test the option for using the original base quality scores with:. --parse_sam_aux_fields ; --use_original_quality_scores. but get the following error: . FATAL Flags parsing error: Unknown command line flag 'parse_sam_aux_fields'; Pass --helpshort or --helpfull to see help on flags. I was running DeepVariant with docker by following the whole genome sequencing case study -tutorial, but will next test the pipeline for multi-sample variant calling for my cohort of 50 samples. I'm wondering should I realign the reads or is it possible to use the original base quality scores from BQSR adjusted bam files? I was previously using the GATK4 pipeline, but the results are so much better with DeepVariant and as a bonus, it's a million times easier (and quicker). Thanks. Karoliina
",False,
Deployability,"Hi, I'm trying to visualize the pileup images generated by DeepVariant. The images for SNP sites and deletions seem to be straightforward, but I found those for insertions are rather confusing. The reference lines for insertion sites are still continuous, and at the point where the insertion happens, the bases on the sequenced reads are set to 0. Here's part of an example of a homozygous ""A->AATAAAAT"" variant, the top 5 lines are the reference lines. 250 | 30 | 180 | 250 | 250 | 100 | 250; 250 | 30 | 180 | 250 | 250 | 100 | 250; 250 | 30 | 180 | 250 | 250 | 100 | 250; 250 | 30 | 180 | 250 | 250 | 100 | 250; 250 | 30 | 180 | 250 | 250 | 100 | 250; 250 | 30 | 180 | 0 | 250 | 100 | 250; 250 | 30 | 180 | 0 | 250 | 100 | 250; 250 | 30 | 180 | 0 | 250 | 100 | 250; 250 | 30 | 180 | 0 | 250 | 100 | 250. The problem is these images are not presenting detailed infomation for the inserted sequence, and on sites where multiple insertions happen, the ""supports variant"" channel might become the only useful infomation to distinguish them.; Also, on the ""base quality"" channel, the qualities for these 0-bases are not zeros, how are these values determined?. I'm wondering if other structures of pileup images on these sites can achieve better performance, like adding 0s on the reference lines?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/306:244,continuous,continuous,244,,https://github.com/google/deepvariant/issues/306,1,['continuous'],['continuous'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi, I'm trying to visualize the pileup images generated by DeepVariant. The images for SNP sites and deletions seem to be straightforward, but I found those for insertions are rather confusing. The reference lines for insertion sites are still continuous, and at the point where the insertion happens, the bases on the sequenced reads are set to 0. Here's part of an example of a homozygous ""A->AATAAAAT"" variant, the top 5 lines are the reference lines. 250 | 30 | 180 | 250 | 250 | 100 | 250; 250 | 30 | 180 | 250 | 250 | 100 | 250; 250 | 30 | 180 | 250 | 250 | 100 | 250; 250 | 30 | 180 | 250 | 250 | 100 | 250; 250 | 30 | 180 | 250 | 250 | 100 | 250; 250 | 30 | 180 | 0 | 250 | 100 | 250; 250 | 30 | 180 | 0 | 250 | 100 | 250; 250 | 30 | 180 | 0 | 250 | 100 | 250; 250 | 30 | 180 | 0 | 250 | 100 | 250. The problem is these images are not presenting detailed infomation for the inserted sequence, and on sites where multiple insertions happen, the ""supports variant"" channel might become the only useful infomation to distinguish them.; Also, on the ""base quality"" channel, the qualities for these 0-bases are not zeros, how are these values determined?. I'm wondering if other structures of pileup images on these sites can achieve better performance, like adding 0s on the reference lines?
",False,
Deployability,"Hi, it's getting harder to build deepvariant, even using bioconda as everything it moving to python3.7 or higher.; Would it be possible to get the build and Dockerfile updated to 3.7? And/or could you provide some guidance on what is needed?. Using the docker container works perfectly. But I want to add bcftools and samtools (for example) to the container and also have it work on singularity.; thanks,; -B",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/445:168,update,updated,168,,https://github.com/google/deepvariant/issues/445,1,['update'],['updated'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi, it's getting harder to build deepvariant, even using bioconda as everything it moving to python3.7 or higher.; Would it be possible to get the build and Dockerfile updated to 3.7? And/or could you provide some guidance on what is needed?. Using the docker container works perfectly. But I want to add bcftools and samtools (for example) to the container and also have it work on singularity.; thanks,; -B
",False,
Deployability,"Hi, so sorry for asking something again. But I really want to use mostly DeepVariant for variant calling. . **Setup**; - Operating system: Red Hat Enterprise Linux 9; - DeepVariant version: 1.6.1; - Installation method (Docker, built from source, etc.): Docker; - Type of data: WES mapped to hg19. **My code:**; - Commands: ; ```; #!/bin/bash; #$ -l m_mem_free=200G; #$ -l os=rhel9; #$ -m bea; #$ -cwd; #$ -pe smp 2; #$ -o deepvariant_output.log; #$ -e deepvariant_error.log. cd path/to/deepvariant. BAM_DIR=.; VCF_DIR=deepvariant_output/; REFERENCE=Reference_HLA/human_g1k_v37_decoy.fasta. export SINGULARITY_CACHEDIR=""path/to/deepvariant/.singularity-$(whoami)""; export SINGULARITY_TMPDIR=""path/to/deepvariant/.singularity-$(whoami)"". BIN_VERSION=""1.6.1"". for BAM_FILE in ""${BAM_DIR}""/*.bam; do; # Extract the base name of the BAM file (without the directory and extension); BASE_NAME=$(basename ""${BAM_FILE}"" .bam). # Define the output VCF file name; VCF_FILE=""${VCF_DIR}/${BASE_NAME}.vcf.gz""; echo $BAM_FILE; echo $VCF_FILE; singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref $REFERENCE \; --reads $BAM_FILE \; --regions 6:32509320-32669663 \; --output_vcf $VCF_FILE \; --num_shards 12; done; ``` . - Error trace: ; ```; ***** Running the command:*****; time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Reference_HLA/chr6_hg19.fa"" --reads ""./MDC05_1463_3.final.bam"" --examples ""/tmp/7361351.1.gpu.q/tmpzsp9g_vq/make_examples.tfrecord@12.gz"" --channels ""insert_size"" --regions ""chr6:32509320-32669663"" --task {}. [libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/wire_format_lite.cc:584] String field 'nucleus.genomics.v1.Program.command_line' contains invalid UTF-8 data when serializing a protocol buffer. Use the 'bytes' type if you intend to send raw bytes.; [libprotobuf ERROR external/com_google_protobuf/src/g",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/870:199,Install,Installation,199,,https://github.com/google/deepvariant/issues/870,1,['Install'],['Installation'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi, so sorry for asking something again. But I really want to use mostly DeepVariant for variant calling. . **Setup**; - Operating system: Red Hat Enterprise Linux 9; - DeepVariant version: 1.6.1; - Installation method (Docker, built from source, etc.): Docker; - Type of data: WES mapped to hg19. **My code:**; - Commands: ; ```; #!/bin/bash; #$ -l m_mem_free=200G; #$ -l os=rhel9; #$ -m bea; #$ -cwd; #$ -pe smp 2; #$ -o deepvariant_output.log; #$ -e deepvariant_error.log. cd path/to/deepvariant. BAM_DIR=.; VCF_DIR=deepvariant_output/; REFERENCE=Reference_HLA/human_g1k_v37_decoy.fasta. export SINGULARITY_CACHEDIR=""path/to/deepvariant/.singularity-$(whoami)""; export SINGULARITY_TMPDIR=""path/to/deepvariant/.singularity-$(whoami)"". BIN_VERSION=""1.6.1"". for BAM_FILE in ""${BAM_DIR}""/*.bam; do; # Extract the base name of the BAM file (without the directory and extension); BASE_NAME=$(basename ""${BAM_FILE}"" .bam). # Define the output VCF file name; VCF_FILE=""${VCF_DIR}/${BASE_NAME}.vcf.gz""; echo $BAM_FILE; echo $VCF_FILE; singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref $REFERENCE \; --reads $BAM_FILE \; --regions 6:32509320-32669663 \; --output_vcf $VCF_FILE \; --num_shards 12; done; ``` . - Error trace: ; ```; ***** Running the command:*****; time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Reference_HLA/chr6_hg19.fa"" --reads ""./MDC05_1463_3.final.bam"" --examples ""/tmp/7361351.1.gpu.q/tmpzsp9g_vq/make_examples.tfrecord@12.gz"" --channels ""insert_size"" --regions ""chr6:32509320-32669663"" --task {}. [libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/wire_format_lite.cc:584] String field 'nucleus.genomics.v1.Program.command_line' contains invalid UTF-8 data when serializing a protocol buffer. Use the 'bytes' type if you intend to send raw bytes.; [libprotobuf ERROR external/com_google_protobuf/src/g
",True,"The content includes code snippets and program logs, which fit into the categories that should be filtered out."
Deployability,"Hi, with WGS (mm10, but the documentation says WGS human model should work for this), I got make_examples running fine with 4 cores and shards, but as soon as I go above 4 I get a strange lag. I'm running this recommended command:. ```; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads bamlink \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; echo ""Done.""; echo. ```; With 4, I get a nice stdout update per shard, first a description of the contigs:. `I0201 17:01:29.167016 140183113676544 make_examples.py:946] Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrUn_GL456370', u'chrUn_GL456379', u'chrUn_GL456366', u'chrUn_GL456368', u'chr",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/150:712,update,update,712,,https://github.com/google/deepvariant/issues/150,1,['update'],['update'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi, with WGS (mm10, but the documentation says WGS human model should work for this), I got make_examples running fine with 4 cores and shards, but as soon as I go above 4 I get a strange lag. I'm running this recommended command:. ```; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads bamlink \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; echo ""Done.""; echo. ```; With 4, I get a nice stdout update per shard, first a description of the contigs:. `I0201 17:01:29.167016 140183113676544 make_examples.py:946] Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrUn_GL456370', u'chrUn_GL456379', u'chrUn_GL456366', u'chrUn_GL456368', u'chr
",False,"The text contains a detailed explanation of the user's experience with running make_examples, including the commands used and observed behaviors when increasing the number of cores. The content is in natural language and provides context about the problem being encountered, making it meaningful and relevant for human readers."
Deployability,"Hi,. Are there any plans to extend DeepVariant to somatic variant calling? The current model seems to be inherently diploid. What is the training time for the released versions of DeepVariant? The Supplementary information from the Nature paper mentions something about ""80 hours"" but does not specify which kind of hardware was used?. Do you have any numbers on how much the neural network improves the accuracy as compared to the raw (over-sensitive) variant calls output after the haplotype-aware realignment step?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/241:159,release,released,159,,https://github.com/google/deepvariant/issues/241,1,['release'],['released'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi,. Are there any plans to extend DeepVariant to somatic variant calling? The current model seems to be inherently diploid. What is the training time for the released versions of DeepVariant? The Supplementary information from the Nature paper mentions something about ""80 hours"" but does not specify which kind of hardware was used?. Do you have any numbers on how much the neural network improves the accuracy as compared to the raw (over-sensitive) variant calls output after the haplotype-aware realignment step?
",False,
Deployability,"Hi,. Are there any plans to upgrade to python3? Py2 has been deprecated, which makes deepvariant kind of obsolete.. Thanks; M",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/285:28,upgrade,upgrade,28,,https://github.com/google/deepvariant/issues/285,1,['upgrade'],['upgrade'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi,. Are there any plans to upgrade to python3? Py2 has been deprecated, which makes deepvariant kind of obsolete.. Thanks; M
",False,The text contains a meaningful sentence in natural language discussing the deprecation of Python 2 and its impact on DeepVariant.
Deployability,"Hi,. I am trying to get the Docker image of DeepVariant to work on CentOS 7.; The same image works just fine on Ubuntu. Many thanks, Alf. $ cat /etc/redhat-release; CentOS Linux release 7.4.1708 (Core); $ docker --version; Docker version 1.13.1, build 6e3bb8e/1.13.1. $ docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.0. root@dade8141a904:/# /opt/deepvariant/bin/make_examples \; > --mode calling \; > --ref /dv2/input/ucsc.hg19.chr20.unittest.fasta.gz \; > --reads /dv2/input/NA12878_S1.chr20.10_10p1mb.bam \; > --examples output.examples.tfrecord \; > --regions ""chr20:10,000,000-10,010,000""; [W::hts_idx_load2] The index file is older than the data file: /dv2/input/NA12878_S1.chr20.10_10p1mb.bam.bai; 2018-10-12 09:57:22.558003: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring:; I1012 09:57:22.558365 139655512114944 genomics_reader.py:213] Reading /dv2/input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_dIz6Sx/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_dIz6Sx/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_dIz6Sx/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1074, in make_examples_runner; resource_monitor = resources.ResourceMonitor().start(); File ""/tmp/Bazel.runfiles_dIz6Sx/runfiles/com_google_deepvariant/deepvariant/resources.py"", line 59, in __init__; self.metrics_pb = self._initial_metrics_protobuf(); File ""/tmp/Bazel.runfiles_dIz6Sx/runfiles/com_google_deepvariant/deepvariant/resources.py"", line 72, in _initial_metrics_protobuf; cpu_frequency_mhz=_get_cpu_frequency(),; File ""/tmp/Bazel.runfiles_dIz6Sx/runfiles/com_google_deepvariant/deepvarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/104:156,release,release,156,,https://github.com/google/deepvariant/issues/104,2,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi,. I am trying to get the Docker image of DeepVariant to work on CentOS 7.; The same image works just fine on Ubuntu. Many thanks, Alf. $ cat /etc/redhat-release; CentOS Linux release 7.4.1708 (Core); $ docker --version; Docker version 1.13.1, build 6e3bb8e/1.13.1. $ docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.0. root@dade8141a904:/# /opt/deepvariant/bin/make_examples \; > --mode calling \; > --ref /dv2/input/ucsc.hg19.chr20.unittest.fasta.gz \; > --reads /dv2/input/NA12878_S1.chr20.10_10p1mb.bam \; > --examples output.examples.tfrecord \; > --regions ""chr20:10,000,000-10,010,000""; [W::hts_idx_load2] The index file is older than the data file: /dv2/input/NA12878_S1.chr20.10_10p1mb.bam.bai; 2018-10-12 09:57:22.558003: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring:; I1012 09:57:22.558365 139655512114944 genomics_reader.py:213] Reading /dv2/input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_dIz6Sx/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_dIz6Sx/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_dIz6Sx/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1074, in make_examples_runner; resource_monitor = resources.ResourceMonitor().start(); File ""/tmp/Bazel.runfiles_dIz6Sx/runfiles/com_google_deepvariant/deepvariant/resources.py"", line 59, in __init__; self.metrics_pb = self._initial_metrics_protobuf(); File ""/tmp/Bazel.runfiles_dIz6Sx/runfiles/com_google_deepvariant/deepvariant/resources.py"", line 72, in _initial_metrics_protobuf; cpu_frequency_mhz=_get_cpu_frequency(),; File ""/tmp/Bazel.runfiles_dIz6Sx/runfiles/com_google_deepvariant/deepvarian
",False,
Deployability,"Hi,. I am trying to run DeepVariant 1.2.0 on a few human samples PacBio HiFi data (about 30x coverage per sample). I first ran my samples through the [PEPPER-Margin pipeline r0.4](https://github.com/kishwarshafin/pepper) to get a haplotagged BAM file. Then I ran DeepVariant as follows:; ```; singularity exec -B ${SOME_PATHS} deepvariant_1.2.0.sif bash /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref ${PATH_TO_REF} --reads MARGIN_PHASED.PEPPER_SNP_MARGIN.happlotagged.bam --output_vcf sample.vcf.gz --output_gvcf sample.g.vcf.gz --num_shards 24 --make_examples_extra_args=""realign_reads=false,min_mapping_quality=5"" --sample_name MYSAMPLE --use-hp-information;; ```. I have two problems:; 1. Right from the beginning (`CALL VARIANT MODULE SELECTED`), for each interval processed. I get thousands of `READ TAG: n_elements is zero` messages in the console. What does it mean and is it a problem or just a warning?; 2. I allocate 200GB of RAM for per job and they all seem to systematically fail on memory. I do not recall DeepVariant using that much memory in the past but I might be wrong. Is 200GB too light for a human genome PacBio Hifi 30x coverage dataset?. Thank you for your help,; Guillaume",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/490:165,pipeline,pipeline,165,,https://github.com/google/deepvariant/issues/490,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi,. I am trying to run DeepVariant 1.2.0 on a few human samples PacBio HiFi data (about 30x coverage per sample). I first ran my samples through the [PEPPER-Margin pipeline r0.4](https://github.com/kishwarshafin/pepper) to get a haplotagged BAM file. Then I ran DeepVariant as follows:; ```; singularity exec -B ${SOME_PATHS} deepvariant_1.2.0.sif bash /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref ${PATH_TO_REF} --reads MARGIN_PHASED.PEPPER_SNP_MARGIN.happlotagged.bam --output_vcf sample.vcf.gz --output_gvcf sample.g.vcf.gz --num_shards 24 --make_examples_extra_args=""realign_reads=false,min_mapping_quality=5"" --sample_name MYSAMPLE --use-hp-information;; ```. I have two problems:; 1. Right from the beginning (`CALL VARIANT MODULE SELECTED`), for each interval processed. I get thousands of `READ TAG: n_elements is zero` messages in the console. What does it mean and is it a problem or just a warning?; 2. I allocate 200GB of RAM for per job and they all seem to systematically fail on memory. I do not recall DeepVariant using that much memory in the past but I might be wrong. Is 200GB too light for a human genome PacBio Hifi 30x coverage dataset?. Thank you for your help,; Guillaume
",False,"The text contains meaningful sentences and concerns about running DeepVariant, including steps taken and specific issues encountered. It is not code or logs but a user's problem description."
Deployability,"Hi,. I am using [Pepper-MARGIN-DeepVariant r0.7](https://github.com/kishwarshafin/pepper) with custom made models for Pepper-SNP, Pepper-HP and DeepVariant. As far as I know, this release uses DeepVariant 1.2. I have run this pipeline successfully for a small cohort of about 100 genomes but when merging the GVCF files, [GLnexus 1.4.1](https://github.com/dnanexus-rnd/GLnexus) (with config `DeepVariant`) complains that at least one variant is missing PL values. I tracked down the issue to one GVCF were the record is:; ```; CHR	POS	.	A	G,<*>	9.9	NoCall	.	GT:GQ:DP:AD:VAF:PL	./.:0:5:0,0,0:0,0:0,0,990,990,990; ```; We can see that this GVCF record has 5 PL values where there should be 6. The corresponding record in the VCF file is:; ```; CHR POS .	A	G	9.9	refCall	.	GT:GQ:DP:AD:VAF:C	./.:0:5:0:0:DV; ```; The VCF record indicates that the variant call was issued by DeepVariant. . Any ideas what could be the issue here?. Thank you for your help,; Guillaume",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/521:180,release,release,180,,https://github.com/google/deepvariant/issues/521,2,"['pipeline', 'release']","['pipeline', 'release']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi,. I am using [Pepper-MARGIN-DeepVariant r0.7](https://github.com/kishwarshafin/pepper) with custom made models for Pepper-SNP, Pepper-HP and DeepVariant. As far as I know, this release uses DeepVariant 1.2. I have run this pipeline successfully for a small cohort of about 100 genomes but when merging the GVCF files, [GLnexus 1.4.1](https://github.com/dnanexus-rnd/GLnexus) (with config `DeepVariant`) complains that at least one variant is missing PL values. I tracked down the issue to one GVCF were the record is:; ```; CHR	POS	.	A	G,<*>	9.9	NoCall	.	GT:GQ:DP:AD:VAF:PL	./.:0:5:0,0,0:0,0:0,0,990,990,990; ```; We can see that this GVCF record has 5 PL values where there should be 6. The corresponding record in the VCF file is:; ```; CHR POS .	A	G	9.9	refCall	.	GT:GQ:DP:AD:VAF:C	./.:0:5:0:0:DV; ```; The VCF record indicates that the variant call was issued by DeepVariant. . Any ideas what could be the issue here?. Thank you for your help,; Guillaume
",False,"The text contains complete, meaningful sentences in natural language discussing issues with a bioinformatics pipeline and steps taken to diagnose the problem. It includes human-readable explanations of errors encountered and their implications."
Deployability,"Hi,. I am working on a [deepvariant pipeline](https://github.com/nf-core/deepvariant/tree/dev) written in nextflow which installs deepvariant via conda. The command for `make_examples` will evaluate to something like this:; ```bash; time seq 0 !{numberShardsMinusOne} | \; parallel --eta --halt 2 \; python /opt/conda/pkgs/deepvariant-0.7.0-py27h5d9141f_0/share/deepvariant-0.7.0-0/binaries/DeepVariant/0.7.0/DeepVariant-0.7.0+cl-208818123/make_examples.zip \; --mode calling \; --ref chr20.fa.gz \; --reads test.bam \; --examples shardedExamples/examples.tfrecord@2.gz; --task {}; ```. I noticed that there is `dv_make_examples.py` on the PATH. Does this executable carry out the same/similar function as `python make_examples.zip`. If so how could I modify the code above to make it work?. Many thanks in advance",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/101:36,pipeline,pipeline,36,,https://github.com/google/deepvariant/issues/101,2,"['install', 'pipeline']","['installs', 'pipeline']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi,. I am working on a [deepvariant pipeline](https://github.com/nf-core/deepvariant/tree/dev) written in nextflow which installs deepvariant via conda. The command for `make_examples` will evaluate to something like this:; ```bash; time seq 0 !{numberShardsMinusOne} | \; parallel --eta --halt 2 \; python /opt/conda/pkgs/deepvariant-0.7.0-py27h5d9141f_0/share/deepvariant-0.7.0-0/binaries/DeepVariant/0.7.0/DeepVariant-0.7.0+cl-208818123/make_examples.zip \; --mode calling \; --ref chr20.fa.gz \; --reads test.bam \; --examples shardedExamples/examples.tfrecord@2.gz; --task {}; ```. I noticed that there is `dv_make_examples.py` on the PATH. Does this executable carry out the same/similar function as `python make_examples.zip`. If so how could I modify the code above to make it work?. Many thanks in advance
",False,
Deployability,"Hi,. I don't know if this is the place to report issues with running the docker pipeline on the google cloud, I have been following instructions at [https://cloud.google.com/life-sciences/docs/tutorials/deepvariant#console_1](https://cloud.google.com/life-sciences/docs/tutorials/deepvariant#console_1) and it mostly works, but the second command within the big docker call dies with some python error. . That page indicates I should email google directly at google-genomics-contact@google.com, but this address bounces, which is why I came here. Anyway I have all the commands and error messages etc, so let me know if this is the right place for that and I will post. Thanks,; Ariel",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/399:80,pipeline,pipeline,80,,https://github.com/google/deepvariant/issues/399,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi,. I don't know if this is the place to report issues with running the docker pipeline on the google cloud, I have been following instructions at [https://cloud.google.com/life-sciences/docs/tutorials/deepvariant#console_1](https://cloud.google.com/life-sciences/docs/tutorials/deepvariant#console_1) and it mostly works, but the second command within the big docker call dies with some python error. . That page indicates I should email google directly at google-genomics-contact@google.com, but this address bounces, which is why I came here. Anyway I have all the commands and error messages etc, so let me know if this is the right place for that and I will post. Thanks,; Ariel
",False,The text contains a human-readable problem description with relevant context and natural language sentences.
Deployability,"Hi,. I have encountered a somewhat unexpected behavior related to the sample name written to the output VCF (possibly caused by the presence of chrEBV?!). **Describe the issue:**; Sample name ""default"" is used in the output VCF for ""chrEBV"", all other chromosome VCFs contain correct sample name (chr1-chr22,chrX,chrY,chrM). **Setup**; - Operating system: CentOS; - DeepVariant version: v1.2; - Installation method (Docker, built from source, etc.): Singularity v3.5.2; - Type of data: PacBio HiFi, Sequel-II. **Steps to reproduce:**; - Command:; ```; /opt/deepvariant/bin/run_deepvariant \; --model_type=""PACBIO"" \; --regions ""$CHROM""; [ ... otherwise default options ...]; ```; - note: the input BAM alignment contains the sample name in the header (i.e. `SM:HG002`); - Error trace: (n/a - run finishes). **Does the quick start test work on your system?**; can't be used to reproduce the problem. **Any additional context:**; In the log for each chromosome run, I see that all chromosomes except chrEBV are listed in lines like this; ```; I0112 10:31:04.917984 47443049531200 \; make_examples_core.py:236] \; Task 6/12: Common contigs are [ HERE: list of all chromosomes except for chrEBV]; ```. Best,; Peter",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/603:395,Install,Installation,395,,https://github.com/google/deepvariant/issues/603,1,['Install'],['Installation'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi,. I have encountered a somewhat unexpected behavior related to the sample name written to the output VCF (possibly caused by the presence of chrEBV?!). **Describe the issue:**; Sample name ""default"" is used in the output VCF for ""chrEBV"", all other chromosome VCFs contain correct sample name (chr1-chr22,chrX,chrY,chrM). **Setup**; - Operating system: CentOS; - DeepVariant version: v1.2; - Installation method (Docker, built from source, etc.): Singularity v3.5.2; - Type of data: PacBio HiFi, Sequel-II. **Steps to reproduce:**; - Command:; ```; /opt/deepvariant/bin/run_deepvariant \; --model_type=""PACBIO"" \; --regions ""$CHROM""; [ ... otherwise default options ...]; ```; - note: the input BAM alignment contains the sample name in the header (i.e. `SM:HG002`); - Error trace: (n/a - run finishes). **Does the quick start test work on your system?**; can't be used to reproduce the problem. **Any additional context:**; In the log for each chromosome run, I see that all chromosomes except chrEBV are listed in lines like this; ```; I0112 10:31:04.917984 47443049531200 \; make_examples_core.py:236] \; Task 6/12: Common contigs are [ HERE: list of all chromosomes except for chrEBV]; ```. Best,; Peter
",False,"The text includes descriptive sentences about an issue encountered with the sample name in DeepVariant's output VCF. It explains the problem, setup details, steps to reproduce, and additional context. The content is in natural language, providing a detailed explanation rather than code or logs."
Deployability,"Hi,. The singularity containers are on version 0.9.0, can these be updated to 1.0.0?. Cheers,. Max H.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/353:67,update,updated,67,,https://github.com/google/deepvariant/issues/353,1,['update'],['updated'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi,. The singularity containers are on version 0.9.0, can these be updated to 1.0.0?. Cheers,. Max H.
",False,The text contains a complete sentence in natural language that is not primarily code or logs.
Deployability,"Hi,; I installed deepvariant from conda. However, when I run dv_make_examples.py all jobs fail, and in their log is reported as:. ```; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>; from deepvariant import make_examples_core; File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>; from etils import epath; ModuleNotFoundError: No module named 'etils' . ```; Do you have any suggestions?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/669:7,install,installed,7,,https://github.com/google/deepvariant/issues/669,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi,; I installed deepvariant from conda. However, when I run dv_make_examples.py all jobs fail, and in their log is reported as:. ```; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>; from deepvariant import make_examples_core; File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>; from etils import epath; ModuleNotFoundError: No module named 'etils' . ```; Do you have any suggestions?
",False,"The text contains a detailed error log that includes stack trace information and code-related errors. It also includes human-readable sentences discussing the installation process and the problem encountered, making it a mixture of both. However, since the primary content is an error log and stack trace, which are program logs or error messages, this should be filtered out as per evaluation criteria."
Deployability,"Hi. . I recently ran deep variant with a collection of WGS samples. It seems to have run through the whole pipeline producing the g.vcf.gz and vcf.gz files, however there was no visual_report.html file? . /nrnb/opt/singularity-3.3.0/bin/singularity run --nv /nrnb/opt/singularity-containers/deepvariant_gpu_0.8.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref --reads=$bam --regions $chr --output_vcf=$x.$chr.vcf.gz --output_gvcf=$x.$chr.g.vcf.gz --num_shards=3. Do I have to include a specific output tag in order to get the file? Based on the docs, it should automatically be produced?. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/290:107,pipeline,pipeline,107,,https://github.com/google/deepvariant/issues/290,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi. . I recently ran deep variant with a collection of WGS samples. It seems to have run through the whole pipeline producing the g.vcf.gz and vcf.gz files, however there was no visual_report.html file? . /nrnb/opt/singularity-3.3.0/bin/singularity run --nv /nrnb/opt/singularity-containers/deepvariant_gpu_0.8.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref --reads=$bam --regions $chr --output_vcf=$x.$chr.vcf.gz --output_gvcf=$x.$chr.g.vcf.gz --num_shards=3. Do I have to include a specific output tag in order to get the file? Based on the docs, it should automatically be produced?. Thanks.
",False,"The text contains a user's query about running a pipeline and whether a specific file is generated, which includes natural language sentences discussing concepts and ideas."
Deployability,"Hi. I'm running a pipeline but get an error on ""make_examples"" stage. Could you help me to debug possible cause, please?. Here is a log:; ```; [W::hts_idx_load2] The index file is older than the data file: /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam.bai; 2019-08-14 12:36:51.456603: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; WARNING: Logging before flag parsing goes to stderr.; I0814 12:36:53.581777 140158089049856 genomics_reader.py:174] Reading /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam with NativeSamReader; I0814 12:36:54.201131 140158089049856 make_examples.py:1024] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam.bai; 2019-08-14 12:36:58.286794: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I0814 12:36:59.914532 140158089049856 genomics_reader.py:174] Reading /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam with NativeSamReader; I0814 12:45:36.568115 140158089049856 make_examples.py:946] Common contigs are [u'LKUA01000001.1', u'LKUA01000002.1', ...<ANOTHER 300k NAMES>..., u'LKUA01311038.1', u'LKUA01311039.1']; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/cannabis-3k-vcf/staging/SRS1107973_LKUA01/staging/examples/0/examples_output.tfrecord@512.gz --reads /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam --ref /mnt/google/.google/input/cannabis-3k/reference/LKUA01/LKUA01.fa --task 8; ```. For me it looks that the error message is `parallel: This job failed:` and failure doesn't relate to the warnings at the beginning of the file?. Regards,",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/207:18,pipeline,pipeline,18,,https://github.com/google/deepvariant/issues/207,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi. I'm running a pipeline but get an error on ""make_examples"" stage. Could you help me to debug possible cause, please?. Here is a log:; ```; [W::hts_idx_load2] The index file is older than the data file: /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam.bai; 2019-08-14 12:36:51.456603: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; WARNING: Logging before flag parsing goes to stderr.; I0814 12:36:53.581777 140158089049856 genomics_reader.py:174] Reading /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam with NativeSamReader; I0814 12:36:54.201131 140158089049856 make_examples.py:1024] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam.bai; 2019-08-14 12:36:58.286794: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I0814 12:36:59.914532 140158089049856 genomics_reader.py:174] Reading /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam with NativeSamReader; I0814 12:45:36.568115 140158089049856 make_examples.py:946] Common contigs are [u'LKUA01000001.1', u'LKUA01000002.1', ...<ANOTHER 300k NAMES>..., u'LKUA01311038.1', u'LKUA01311039.1']; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/cannabis-3k-vcf/staging/SRS1107973_LKUA01/staging/examples/0/examples_output.tfrecord@512.gz --reads /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam --ref /mnt/google/.google/input/cannabis-3k/reference/LKUA01/LKUA01.fa --task 8; ```. For me it looks that the error message is `parallel: This job failed:` and failure doesn't relate to the warnings at the beginning of the file?. Regards,
",False,"The text contains a human-readable sentence asking for help in debugging an error, as well as some log messages. The content includes both meaningful sentences and program logs."
Deployability,"Hi. when I try to pull the deepvariant docker image via singularity using the following command:; singularity pull docker://google/deepvariant:""1.3.0""; it return the following error:; WARNING: pull for Docker Hub is not guaranteed to produce the; WARNING: same image on repeated pull. Use Singularity Registry; WARNING: (shub://) to pull exactly equivalent images.; ERROR Authentication error, exiting.; Cleaning up...; ERROR: pulling container failed!. could you please help. I don't have the permission to install docker.; Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/513:508,install,install,508,,https://github.com/google/deepvariant/issues/513,1,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi. when I try to pull the deepvariant docker image via singularity using the following command:; singularity pull docker://google/deepvariant:""1.3.0""; it return the following error:; WARNING: pull for Docker Hub is not guaranteed to produce the; WARNING: same image on repeated pull. Use Singularity Registry; WARNING: (shub://) to pull exactly equivalent images.; ERROR Authentication error, exiting.; Cleaning up...; ERROR: pulling container failed!. could you please help. I don't have the permission to install docker.; Thanks
",False,"The content includes a human-readable problem description, error messages, and request for assistance. It consists of complete sentences in natural language discussing an issue with Docker image pulling using Singularity."
Deployability,"Hi.; I read this post and came here. [https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction](https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction). I'd like to apply the process to my pipeline. But the above example ingores indel errors and considers only regions with no known variants. Does this repo has sequencing error correction part?; If then, can I use the part only?. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/760:315,pipeline,pipeline,315,,https://github.com/google/deepvariant/issues/760,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi.; I read this post and came here. [https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction](https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction). I'd like to apply the process to my pipeline. But the above example ingores indel errors and considers only regions with no known variants. Does this repo has sequencing error correction part?; If then, can I use the part only?. Thank you.
",False,
Deployability,"Hi.; I'm running the pipeline on a CRAM file. I read that the pipeline works with CRAM files, so I guess that's not the issue.; Can you assist in any way?; Thanks. **Setup**; - Operating system: Ubuntu 20.04.5 LTS; - DeepVariant version: 1.4.0; - Installation method: Docker; - Type of data: I have no information about the sequencing instrument, the reference genome was GRCh38. **Steps to reproduce:**; - Command:; ```; BIN_VERSION=""1.4.0"". sudo docker run \; -v ""input"":""/input"" \; -v ""output"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=/input/GCF_000001405.26_GRCh38_genomic1.fa.gz \; --reads=/input/1115492_23181_0_0.cram \; --regions ""chr3:10,049,322-10,156,156"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=5 ; ```; - Error trace:; ; > parallel: This job failed:; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options; samples_in_order, sample_role_to_train = one_sample_from_flags(; File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags; sample_name = make_examples_core.assign_sample_name(; File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 134, in assign_sample_name; with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:; File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader; retu",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/588:21,pipeline,pipeline,21,,https://github.com/google/deepvariant/issues/588,3,"['Install', 'pipeline']","['Installation', 'pipeline']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi.; I'm running the pipeline on a CRAM file. I read that the pipeline works with CRAM files, so I guess that's not the issue.; Can you assist in any way?; Thanks. **Setup**; - Operating system: Ubuntu 20.04.5 LTS; - DeepVariant version: 1.4.0; - Installation method: Docker; - Type of data: I have no information about the sequencing instrument, the reference genome was GRCh38. **Steps to reproduce:**; - Command:; ```; BIN_VERSION=""1.4.0"". sudo docker run \; -v ""input"":""/input"" \; -v ""output"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=/input/GCF_000001405.26_GRCh38_genomic1.fa.gz \; --reads=/input/1115492_23181_0_0.cram \; --regions ""chr3:10,049,322-10,156,156"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=5 ; ```; - Error trace:; ; > parallel: This job failed:; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options; samples_in_order, sample_role_to_train = one_sample_from_flags(; File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags; sample_name = make_examples_core.assign_sample_name(; File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 134, in assign_sample_name; with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:; File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader; retu
",False,
Deployability,"Hi;; I would like to build DeepVariant on CentOS 7. I have installed dependencies of the Centos version corresponding to run-prereq.sh. But I also cann't run copying binaries on my local machines which have installed CentOS 7. The wrror message indicate it cann't find some dependent package in environment.; the error message:; ```; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_w1ath6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 41, in <module>; from deepvariant import pileup_image; File ""/tmp/Bazel.runfiles_w1ath6/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 42, in <module>; from third_party.nucleus.util import ranges; File ""/tmp/Bazel.runfiles_w1ath6/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 42, in <module>; from third_party.nucleus.io import bed; File ""/tmp/Bazel.runfiles_w1ath6/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 79, in <module>; from third_party.nucleus.io.python import bed_reader; ImportError: libbz2.so.1.0: cannot open shared object file: No such file or directory; ```; I know we can use docker to run DeepVariant on CentOS 7. But There are some reason why I cann't use docker to run DeepVariant. Did you try to build DeepVariant for CentOS 7. Or, you know who have build DeepVariant with some way on CentOS 7. If you know, can give me some adviceï¼Ÿ; Thanks a lot,; Simon.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/95:59,install,installed,59,,https://github.com/google/deepvariant/issues/95,2,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Hi;; I would like to build DeepVariant on CentOS 7. I have installed dependencies of the Centos version corresponding to run-prereq.sh. But I also cann't run copying binaries on my local machines which have installed CentOS 7. The wrror message indicate it cann't find some dependent package in environment.; the error message:; ```; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_w1ath6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 41, in <module>; from deepvariant import pileup_image; File ""/tmp/Bazel.runfiles_w1ath6/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 42, in <module>; from third_party.nucleus.util import ranges; File ""/tmp/Bazel.runfiles_w1ath6/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 42, in <module>; from third_party.nucleus.io import bed; File ""/tmp/Bazel.runfiles_w1ath6/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 79, in <module>; from third_party.nucleus.io.python import bed_reader; ImportError: libbz2.so.1.0: cannot open shared object file: No such file or directory; ```; I know we can use docker to run DeepVariant on CentOS 7. But There are some reason why I cann't use docker to run DeepVariant. Did you try to build DeepVariant for CentOS 7. Or, you know who have build DeepVariant with some way on CentOS 7. If you know, can give me some adviceï¼Ÿ; Thanks a lot,; Simon.
",False,"The text contains complete sentences in natural language discussing the user's experience and problem when trying to install DeepVariant on CentOS 7. It includes human-readable explanations of errors and attempts made by the user, as well as questions seeking advice from others who might have experience with the same issue."
Deployability,"I am currently running DeepVariant on CCS data after installing via Docker on an AWS instance. The make_examples step of the pipeline is taking much longer than expected. I have performed this in the past with 30X Illumina Data, and it has taken a few hours. This has been running for about a week on 23X coverage PacBio HiFi with a 16 core machine (CPU optimized), and I was wondering if that was expected. Below, I have the command issued:. `sudo time seq 0 $((N_SHARDS-1)) | sudo parallel --eta --halt 2 --joblog ""${LOGDIR}""/log --res ""${LOGDIR}"" sudo docker run -v ${HOME}:${HOME} -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/make_examples --mode calling --ref=/input/ucsc.hg38.no_alts.fasta --reads=/input/hg00733_ccs_to_hg38.bam --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" --task {}`. All variables listed have been set as expected. When I ssh in to the node I can see that it is running python in parallel and writing to the proper output files, but it is just taking forever to process anything. Any help would be greatly appreciated!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/208:53,install,installing,53,,https://github.com/google/deepvariant/issues/208,2,"['install', 'pipeline']","['installing', 'pipeline']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: I am currently running DeepVariant on CCS data after installing via Docker on an AWS instance. The make_examples step of the pipeline is taking much longer than expected. I have performed this in the past with 30X Illumina Data, and it has taken a few hours. This has been running for about a week on 23X coverage PacBio HiFi with a 16 core machine (CPU optimized), and I was wondering if that was expected. Below, I have the command issued:. `sudo time seq 0 $((N_SHARDS-1)) | sudo parallel --eta --halt 2 --joblog ""${LOGDIR}""/log --res ""${LOGDIR}"" sudo docker run -v ${HOME}:${HOME} -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/make_examples --mode calling --ref=/input/ucsc.hg38.no_alts.fasta --reads=/input/hg00733_ccs_to_hg38.bam --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" --task {}`. All variables listed have been set as expected. When I ssh in to the node I can see that it is running python in parallel and writing to the proper output files, but it is just taking forever to process anything. Any help would be greatly appreciated!
",False,"The text contains complete, meaningful sentences in natural language discussing issues with processing time, commands used, and observations made during an experiment or data analysis."
Deployability,"I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10.; I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed.; On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ?. Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not; unavailable for this platform. Please install CLIF at; https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**.; A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/160:25,install,install,25,,https://github.com/google/deepvariant/issues/160,6,['install'],"['install', 'installed']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10.; I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed.; On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ?. Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not; unavailable for this platform. Please install CLIF at; https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**.; A tiny cosmetic error, but can be easily fixed. Thank you!
",False,
Deployability,"I am following the [Building from sources](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-build-test.md) tutorial. I cloned the repository, then started building and got the following errors:. ```; viniws@woese:~/Code/deepvariant$ ./build-prereq.sh ; ========== Load config settings. ; ========== [qua set 26 16:42:55 -03 2018] Stage 'Install the runtime packages' starting ; ========== Load config settings. ; ========== [qua set 26 16:42:55 -03 2018] Stage 'Misc setup' starting ; ========== [qua set 26 16:42:55 -03 2018] Stage 'Update package list' starting ; E: The repository 'http://apt.postgresql.org/pub/repos/apt YOUR_UBUNTU_VERSION_HERE-pgdg Release' does not have a Release file. ; E: The repository 'http://ppa.launchpad.net/gnome-terminator/ppa/ubuntu bionic Release' does not have a Release file ; ```. And after:. ```; + source settings.sh; ++ export DV_USE_PREINSTALLED_TF=0; ++ DV_USE_PREINSTALLED_TF=0; ++ export TF_CUDA_CLANG=0; ++ TF_CUDA_CLANG=0; ++ export TF_ENABLE_XLA=0; ++ TF_ENABLE_XLA=0; ++ export TF_NEED_CUDA=0; ++ TF_NEED_CUDA=0; ++ export TF_NEED_GCP=1; ++ TF_NEED_GCP=1; ++ export TF_NEED_GDR=0; ++ TF_NEED_GDR=0; ++ export TF_NEED_HDFS=0; ++ TF_NEED_HDFS=0; ++ export TF_NEED_JEMALLOC=0; ++ TF_NEED_JEMALLOC=0; ++ export TF_NEED_MKL=0; ++ TF_NEED_MKL=0; ++ export TF_NEED_MPI=0; ++ TF_NEED_MPI=0; ++ export TF_NEED_OPENCL=0; ++ TF_NEED_OPENCL=0; ++ export TF_NEED_OPENCL_SYCL=0; ++ TF_NEED_OPENCL_SYCL=0; ++ export TF_NEED_S3=0; ++ TF_NEED_S3=0; ++ export TF_NEED_VERBS=0; ++ TF_NEED_VERBS=0; ++ export TF_CUDA_VERSION=8.0; ++ TF_CUDA_VERSION=8.0; ++ export CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ export TF_CUDNN_VERSION=6; ++ TF_CUDNN_VERSION=6; ++ export CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ DV_BAZEL_VERSION=0.15.0; ++ export DEEPVARIANT_BUCKET=gs://deepvariant; ++ DEEPVARIANT_BUCKET=gs://deepvariant; ++ export DV_PACKAGE_BUCKET_PATH=gs://deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/98:353,Install,Install,353,,https://github.com/google/deepvariant/issues/98,6,"['Install', 'Release', 'Update']","['Install', 'Release', 'Update']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: I am following the [Building from sources](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-build-test.md) tutorial. I cloned the repository, then started building and got the following errors:. ```; viniws@woese:~/Code/deepvariant$ ./build-prereq.sh ; ========== Load config settings. ; ========== [qua set 26 16:42:55 -03 2018] Stage 'Install the runtime packages' starting ; ========== Load config settings. ; ========== [qua set 26 16:42:55 -03 2018] Stage 'Misc setup' starting ; ========== [qua set 26 16:42:55 -03 2018] Stage 'Update package list' starting ; E: The repository 'http://apt.postgresql.org/pub/repos/apt YOUR_UBUNTU_VERSION_HERE-pgdg Release' does not have a Release file. ; E: The repository 'http://ppa.launchpad.net/gnome-terminator/ppa/ubuntu bionic Release' does not have a Release file ; ```. And after:. ```; + source settings.sh; ++ export DV_USE_PREINSTALLED_TF=0; ++ DV_USE_PREINSTALLED_TF=0; ++ export TF_CUDA_CLANG=0; ++ TF_CUDA_CLANG=0; ++ export TF_ENABLE_XLA=0; ++ TF_ENABLE_XLA=0; ++ export TF_NEED_CUDA=0; ++ TF_NEED_CUDA=0; ++ export TF_NEED_GCP=1; ++ TF_NEED_GCP=1; ++ export TF_NEED_GDR=0; ++ TF_NEED_GDR=0; ++ export TF_NEED_HDFS=0; ++ TF_NEED_HDFS=0; ++ export TF_NEED_JEMALLOC=0; ++ TF_NEED_JEMALLOC=0; ++ export TF_NEED_MKL=0; ++ TF_NEED_MKL=0; ++ export TF_NEED_MPI=0; ++ TF_NEED_MPI=0; ++ export TF_NEED_OPENCL=0; ++ TF_NEED_OPENCL=0; ++ export TF_NEED_OPENCL_SYCL=0; ++ TF_NEED_OPENCL_SYCL=0; ++ export TF_NEED_S3=0; ++ TF_NEED_S3=0; ++ export TF_NEED_VERBS=0; ++ TF_NEED_VERBS=0; ++ export TF_CUDA_VERSION=8.0; ++ TF_CUDA_VERSION=8.0; ++ export CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ export TF_CUDNN_VERSION=6; ++ TF_CUDNN_VERSION=6; ++ export CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ DV_BAZEL_VERSION=0.15.0; ++ export DEEPVARIANT_BUCKET=gs://deepvariant; ++ DEEPVARIANT_BUCKET=gs://deepvariant; ++ export DV_PACKAGE_BUCKET_PATH=gs://deepvar
",False,"The text contains a mix of command outputs and configuration settings. While there are some error messages, the content also includes exports of variables and scripts, which suggests it's part of a build process or configuration file rather than a natural explanation. There is no coherent narrative or meaningful sentences discussing concepts or ideas in a human-readable way."
Deployability,"I am running DeepVariant on google cloud following the wiki (Cost-optimized configuration): https://cloud.google.com/genomics/deepvariant, and it works for one of our sample. . Now I have several bam files for several samples. I can run them one by one. But I wonder is there some options to set a list of bam files? Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/33:76,configurat,configuration,76,,https://github.com/google/deepvariant/issues/33,1,['configurat'],['configuration'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: I am running DeepVariant on google cloud following the wiki (Cost-optimized configuration): https://cloud.google.com/genomics/deepvariant, and it works for one of our sample. . Now I have several bam files for several samples. I can run them one by one. But I wonder is there some options to set a list of bam files? Thank you.
",False,"The text contains complete, meaningful sentences in natural language discussing the process and considerations when running DeepVariant on Google Cloud, including specific steps and questions about handling multiple BAM files."
Deployability,"I am trying to build DeepVariant from source, and **trying to use a custom python installation rather than the standard one.** However, ```bazel test ``` fails because it tries to use the standard library python. The requisite python is accessible as ""python"" because it is in the PATH variable, but bazel seems to ignore that and looks for python in the standard location. I am not an expert in bazel by any means, so any help in how to get around this issue is greatly appreciated. Here is the command used for build (all necessary libraries have been compiled. I didn't use run-prereq.sh and build-prereq.sh, but I installed them manually). Command used (this was edited into build_and_test.sh, and build_and_test.sh was run after the edits); ```; bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \; deepvariant/...; ```. settings.sh was changed as follows:; ```; export DV_USE_PREINSTALLED_TF=""1""; export TF_NEED_GCP=0; export CUDNN_INSTALL_PATH=""/usr""; export DV_GPU_BUILD=""1""; export DV_INSTALL_GPU_DRIVERS=""0""; export PYTHON_BIN_PATH='/opt/at11.0/bin/python'; export PYTHON_LIB_PATH='/opt/at11.0/lib64/python3.6/site-packages'; export USE_DEFAULT_PYTHON_LIB_PATH=0; export DV_COPT_FLAGS=""--copt=-mcpu=native --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS""; ```. Error trace:; ```; (15:44:57) ERROR: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/org_tensorflow/tensorflow/core/BUILD:2762:1: Executing genrule @org_tensorflow//tensorflow/core:version_info_gen failed (Exit 1): bash failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; CUDA_TOOLKIT_PATH=/usr/local/cuda-10.0 \; GCC_HOST_COMPILER_PATH=/opt/at11.0/bin/gcc \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/opt/at11.0/bin:/opt/at11.0/sbin:/usr/local/nvidia/bin:/usr/loca",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/356:82,install,installation,82,,https://github.com/google/deepvariant/issues/356,2,['install'],"['installation', 'installed']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: I am trying to build DeepVariant from source, and **trying to use a custom python installation rather than the standard one.** However, ```bazel test ``` fails because it tries to use the standard library python. The requisite python is accessible as ""python"" because it is in the PATH variable, but bazel seems to ignore that and looks for python in the standard location. I am not an expert in bazel by any means, so any help in how to get around this issue is greatly appreciated. Here is the command used for build (all necessary libraries have been compiled. I didn't use run-prereq.sh and build-prereq.sh, but I installed them manually). Command used (this was edited into build_and_test.sh, and build_and_test.sh was run after the edits); ```; bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \; deepvariant/...; ```. settings.sh was changed as follows:; ```; export DV_USE_PREINSTALLED_TF=""1""; export TF_NEED_GCP=0; export CUDNN_INSTALL_PATH=""/usr""; export DV_GPU_BUILD=""1""; export DV_INSTALL_GPU_DRIVERS=""0""; export PYTHON_BIN_PATH='/opt/at11.0/bin/python'; export PYTHON_LIB_PATH='/opt/at11.0/lib64/python3.6/site-packages'; export USE_DEFAULT_PYTHON_LIB_PATH=0; export DV_COPT_FLAGS=""--copt=-mcpu=native --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS""; ```. Error trace:; ```; (15:44:57) ERROR: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/org_tensorflow/tensorflow/core/BUILD:2762:1: Executing genrule @org_tensorflow//tensorflow/core:version_info_gen failed (Exit 1): bash failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; CUDA_TOOLKIT_PATH=/usr/local/cuda-10.0 \; GCC_HOST_COMPILER_PATH=/opt/at11.0/bin/gcc \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/opt/at11.0/bin:/opt/at11.0/sbin:/usr/local/nvidia/bin:/usr/loca
",True,"The text contains logs and build system output, including command outputs, error messages, and configuration changes which are indicative of programmatic documentation rather than meaningful human-readable content."
Deployability,"I am trying to install DeepVariant on an IBM Power 8 system within a docker container. The docker container has the following Bazel version installed: 0.15.0- (https://github.com/bazelbuild/bazel/releases/tag/0.15.0) . I installed tensorflow r1.11 from source inside the docker container for CPU-only execution. This same source-code is placed so that it is seen by the build-prereq.sh script. I set the `export DV_USE_PREINSTALLED_TF=1`. In settings.sh, I changed DV_BAZEL_VERSION to DV_BAZEL_VERSION=""0.15.0-"" (to match the bazel version above). I also removed the corei7 option in DV_COPT_FLAGS. . In build-prereq.sh, I hard-coded the following in: `DV_PLATFORM=""ubuntu-16""`, since `lsb_release` didn't match the case statement conditions there. The following is the result `lsb_release`.; root@1f07cee05809:~/deepvariant# lsb_release; LSB Version: core-9.20160110ubuntu0.2-noarch:core-9.20160110ubuntu0.2-ppc64el:security-9.20160110ubuntu0.2-noarch:security-9.20160110ubuntu0.2-ppc64el. After these changes, build-prereq.sh runs fine. However, build_and_test.sh fails with the following error:; (03:21:40) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:424:1: ClifProtoLibraryGeneration third_party/nucleus/protos/reads_pyclif.h failed (Exit 2): proto failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/genfiles/third_party/nucleus/protos/reads_pyclif.cc -h bazel-out/ppc-opt/genfiles/third_party/nucleus/protos/reads_pyclif.h '--strip_dir=bazel; -out/ppc-opt/genfiles' '--source_dir='\''.'\''' third_party/nucleus/protos/reads.proto); bazel-out/host/bin/external/clif/proto: 3: bazel-out/host/bin/external/clif/proto: __requires__: not found; bazel-out/host/bin/external/clif/proto: 4: bazel-out/host/bin/external/clif/proto: import: not found; bazel-out/host/bin/external/clif/proto: 5: bazel-out/host/bin/external/clif/p",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/122:15,install,install,15,,https://github.com/google/deepvariant/issues/122,4,"['install', 'release']","['install', 'installed', 'releases']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: I am trying to install DeepVariant on an IBM Power 8 system within a docker container. The docker container has the following Bazel version installed: 0.15.0- (https://github.com/bazelbuild/bazel/releases/tag/0.15.0) . I installed tensorflow r1.11 from source inside the docker container for CPU-only execution. This same source-code is placed so that it is seen by the build-prereq.sh script. I set the `export DV_USE_PREINSTALLED_TF=1`. In settings.sh, I changed DV_BAZEL_VERSION to DV_BAZEL_VERSION=""0.15.0-"" (to match the bazel version above). I also removed the corei7 option in DV_COPT_FLAGS. . In build-prereq.sh, I hard-coded the following in: `DV_PLATFORM=""ubuntu-16""`, since `lsb_release` didn't match the case statement conditions there. The following is the result `lsb_release`.; root@1f07cee05809:~/deepvariant# lsb_release; LSB Version: core-9.20160110ubuntu0.2-noarch:core-9.20160110ubuntu0.2-ppc64el:security-9.20160110ubuntu0.2-noarch:security-9.20160110ubuntu0.2-ppc64el. After these changes, build-prereq.sh runs fine. However, build_and_test.sh fails with the following error:; (03:21:40) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:424:1: ClifProtoLibraryGeneration third_party/nucleus/protos/reads_pyclif.h failed (Exit 2): proto failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/genfiles/third_party/nucleus/protos/reads_pyclif.cc -h bazel-out/ppc-opt/genfiles/third_party/nucleus/protos/reads_pyclif.h '--strip_dir=bazel; -out/ppc-opt/genfiles' '--source_dir='\''.'\''' third_party/nucleus/protos/reads.proto); bazel-out/host/bin/external/clif/proto: 3: bazel-out/host/bin/external/clif/proto: __requires__: not found; bazel-out/host/bin/external/clif/proto: 4: bazel-out/host/bin/external/clif/proto: import: not found; bazel-out/host/bin/external/clif/proto: 5: bazel-out/host/bin/external/clif/p
",False,"The text contains a detailed description of installation steps and configurations for DeepVariant on an IBM Power 8 system using Docker. It includes human-readable explanations of the setup process, version specifications, and error messages encountered during the build process. The content is explanatory and provides context for someone attempting to replicate the installation, making it meaningful and not merely code or logs."
Deployability,"I am trying to install deepvariant within a singularity container using recipe build. I have successfully complied google-sdk within the container, when it comes to the point where its runs ""./build-prereq.sh"" script, it terminates with this error: . **Setting up unzip (6.0-9ubuntu1) ...; Setting up zip (3.0-8) ...; Setting up zlib1g-dev:amd64 (1:1.2.8.dfsg-1ubuntu1) ...; Processing triggers for libc-bin (2.19-0ubuntu6) ...; Processing triggers for sgml-base (1.26+nmu4ubuntu1) ...; ========== [Tue Apr 17 00:09:23 UTC 2018] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; E: Unable to locate package python-wheel; ABORT: Aborting with RETVAL=255; Cleaning up...**. I am using singularity within in vagrant on my mac and here is my recipe file:. **Bootstrap: shub; From: singularityhub/ubuntu. %runscript; exec echo ""The runscript is the containers default runtime command!"". %files; # /home/vanessa/Desktop/hello-kitty.txt # copied to root of container; # /home/vanessa/Desktop/party_dinosaur.gif /opt/the-party-dino.gif #. %environment. export CLOUD_SDK_REPO=""cloud-sdk-$(lsb_release -c -s)"". %labels; AUTHOR mnoon@email.arizona.edu. %post; apt-get update && apt-get -y install python2.7 git wget curl ; mkdir /data; echo ""The post section is where you can install, and configure your container."". echo ""deb http://packages.cloud.google.com/apt $CLOUD_SDK_REPO main"" > /etc/apt/sources.list.d/google-cloud-sdk.list; curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -. apt-get update && apt-get install -y google-cloud-sdk; ##download deepVariant scripts and run them; git clone https://github.com/google/deepvariant.git; cd deepvariant; ; ./build-prereq.sh. ./build_and_test.sh. ./run-prereq.sh**. I have no idea whats causing this error. any help will be appreciated. -M",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/66:15,install,install,15,,https://github.com/google/deepvariant/issues/66,7,"['Install', 'install', 'update']","['Install', 'install', 'update']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: I am trying to install deepvariant within a singularity container using recipe build. I have successfully complied google-sdk within the container, when it comes to the point where its runs ""./build-prereq.sh"" script, it terminates with this error: . **Setting up unzip (6.0-9ubuntu1) ...; Setting up zip (3.0-8) ...; Setting up zlib1g-dev:amd64 (1:1.2.8.dfsg-1ubuntu1) ...; Processing triggers for libc-bin (2.19-0ubuntu6) ...; Processing triggers for sgml-base (1.26+nmu4ubuntu1) ...; ========== [Tue Apr 17 00:09:23 UTC 2018] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; E: Unable to locate package python-wheel; ABORT: Aborting with RETVAL=255; Cleaning up...**. I am using singularity within in vagrant on my mac and here is my recipe file:. **Bootstrap: shub; From: singularityhub/ubuntu. %runscript; exec echo ""The runscript is the containers default runtime command!"". %files; # /home/vanessa/Desktop/hello-kitty.txt # copied to root of container; # /home/vanessa/Desktop/party_dinosaur.gif /opt/the-party-dino.gif #. %environment. export CLOUD_SDK_REPO=""cloud-sdk-$(lsb_release -c -s)"". %labels; AUTHOR mnoon@email.arizona.edu. %post; apt-get update && apt-get -y install python2.7 git wget curl ; mkdir /data; echo ""The post section is where you can install, and configure your container."". echo ""deb http://packages.cloud.google.com/apt $CLOUD_SDK_REPO main"" > /etc/apt/sources.list.d/google-cloud-sdk.list; curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -. apt-get update && apt-get install -y google-cloud-sdk; ##download deepVariant scripts and run them; git clone https://github.com/google/deepvariant.git; cd deepvariant; ; ./build-prereq.sh. ./build_and_test.sh. ./run-prereq.sh**. I have no idea whats causing this error. any help will be appreciated. -M
",False,"The text contains detailed, meaningful sentences discussing the installation process and errors encountered when using Singularity within Vagrant on a Mac. It includes human-readable explanations of the problem and steps taken to troubleshoot, such as compiling the Google SDK and running scripts. The content is not primarily logs, code snippets, or API documentation but rather a narrative explaining the installation issues and attempts to resolve them."
Deployability,I am trying to use bioconda Deepvariant (https://anaconda.org/bioconda/deepvariant ) on a cluster with CentOS 7. I am getting this error. ERROR conda.core.link:_execute(507): An error occurred while installing package 'bioconda::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /mnt/home/mansourt/miniconda3/envs/deepVar/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137:199,install,installing,199,,https://github.com/google/deepvariant/issues/137,1,['install'],['installing'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: I am trying to use bioconda Deepvariant (https://anaconda.org/bioconda/deepvariant ) on a cluster with CentOS 7. I am getting this error. ERROR conda.core.link:_execute(507): An error occurred while installing package 'bioconda::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /mnt/home/mansourt/miniconda3/envs/deepVar/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>
",False,"The text includes a user error message from Conda, which is part of the installation process. This provides information about the failure during package installation and points to additional troubleshooting steps."
Deployability,"I am using Ubuntu 16. I got binaries from file:; https://github.com/google/deepvariant/releases/download/v0.7.0/deepvariant.zip. **I run run-prereq.sh first and warning message appears:** ; Cloning into 'tensorflow'...; Switched to a new branch 'r1.9'; Extracting Bazel installation...; WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"". **When I run ./build_and_test.sh command, an error appears:**; ./build_and_test.sh: line 54: bazel: command not found. **When I run ./run-prereq.sh command, it stops at ""unable to re-open stdin:""** ; debconf: unable to initialize frontend: Dialog; debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.); debconf: falling back to frontend: Readline; debconf: unable to initialize frontend: Readline; debconf: (This frontend requires a controlling tty.); debconf: falling back to frontend: Teletype; dpkg-preconfigure: unable to re-open stdin: ; debconf: unable to initialize frontend: Dialog; debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.); debconf: falling back to frontend: Readline; debconf: unable to initialize frontend: Readline; debconf: (This frontend requires a controlling tty.); debconf: falling back to frontend: Teletype; dpkg-preconfigure: unable to re-open stdin: . As I can see the problem is bazel installation and already some ways of resolving the problem were suggested - one of suggestion was to change .txt.sh file, another one to manually install bazel package (which seems to me regarding instructions on bazel site not a straightforward approach). I am running DeepVariant on a cluster, therefore would be very grateful for any more straightforward; suggestion. Thank you very much.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/92:87,release,releases,87,,https://github.com/google/deepvariant/issues/92,4,"['install', 'release']","['install', 'installation', 'releases']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: I am using Ubuntu 16. I got binaries from file:; https://github.com/google/deepvariant/releases/download/v0.7.0/deepvariant.zip. **I run run-prereq.sh first and warning message appears:** ; Cloning into 'tensorflow'...; Switched to a new branch 'r1.9'; Extracting Bazel installation...; WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"". **When I run ./build_and_test.sh command, an error appears:**; ./build_and_test.sh: line 54: bazel: command not found. **When I run ./run-prereq.sh command, it stops at ""unable to re-open stdin:""** ; debconf: unable to initialize frontend: Dialog; debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.); debconf: falling back to frontend: Readline; debconf: unable to initialize frontend: Readline; debconf: (This frontend requires a controlling tty.); debconf: falling back to frontend: Teletype; dpkg-preconfigure: unable to re-open stdin: ; debconf: unable to initialize frontend: Dialog; debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.); debconf: falling back to frontend: Readline; debconf: unable to initialize frontend: Readline; debconf: (This frontend requires a controlling tty.); debconf: falling back to frontend: Teletype; dpkg-preconfigure: unable to re-open stdin: . As I can see the problem is bazel installation and already some ways of resolving the problem were suggested - one of suggestion was to change .txt.sh file, another one to manually install bazel package (which seems to me regarding instructions on bazel site not a straightforward approach). I am running DeepVariant on a cluster, therefore would be very grateful for any more straightforward; suggestion. Thank you very much.
",False,
Deployability,"I currently tried to write some script to train the new model. The make_example script have created labeled data. But the model_train script have some problem. I made some labeled examples using the WGS case study data of Deepvariant on chr20. I used the 0.6.0 version of released DeepVariant model as a started training model. **The make_examples script is:**; `python ../bin/make_examples.zip \; --mode training \; --ref ""file/ucsc.hg19.chr20.unittest.fasta.gz"" \; --reads ""file/NA12878_S1.chr20.10_10p1mb.bam"" \; --confident_regions ""file/test_nist.b37_chr20_100kbp_at_10mb.bed"" \; --truth_variants ""file/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz"" \; --examples ""output/examples.tfrecord.gz""; `; **my-training-dataset.pbtxt file:**; `name: ""my-training-dataset""; tfrecord_path: ""/home/suanfa/Documents/wangpeng/testmake_examples/output/examples.tfrecord.gz""; num_examples: 1`. **The model_train script is:**; `python ../bin/model_train.zip \; --dataset_config_pbtxt ""./my-training-dataset.pbtxt"" \; --start_from_checkpoint ""/my/path/of/DeepVariant/deepvariant-model-wes_and_wgs/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt""; `. **The following error have happened while the model_train.zip is invoked:**; > I0502 10:58:51.903573 139632719935232 model_train.py:182] Initializing model from checkpoint at /home/suanfa/Documents/source/DeepVariant/deepvariant-model-wes_and_wgs/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt; 2018-05-02 10:58:56.347500: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA; 2018-05-02 10:58:57.263635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:; name: Tesla P100-PCIE-12GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285; pciBusID: 0000:3b:00.0; totalMemory: 11.91GiB freeMemory: 11.62GiB; 2018-05-02 10:58:57.263682: I tensorflow/core/common_runtim",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/69:272,release,released,272,,https://github.com/google/deepvariant/issues/69,1,['release'],['released'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: I currently tried to write some script to train the new model. The make_example script have created labeled data. But the model_train script have some problem. I made some labeled examples using the WGS case study data of Deepvariant on chr20. I used the 0.6.0 version of released DeepVariant model as a started training model. **The make_examples script is:**; `python ../bin/make_examples.zip \; --mode training \; --ref ""file/ucsc.hg19.chr20.unittest.fasta.gz"" \; --reads ""file/NA12878_S1.chr20.10_10p1mb.bam"" \; --confident_regions ""file/test_nist.b37_chr20_100kbp_at_10mb.bed"" \; --truth_variants ""file/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz"" \; --examples ""output/examples.tfrecord.gz""; `; **my-training-dataset.pbtxt file:**; `name: ""my-training-dataset""; tfrecord_path: ""/home/suanfa/Documents/wangpeng/testmake_examples/output/examples.tfrecord.gz""; num_examples: 1`. **The model_train script is:**; `python ../bin/model_train.zip \; --dataset_config_pbtxt ""./my-training-dataset.pbtxt"" \; --start_from_checkpoint ""/my/path/of/DeepVariant/deepvariant-model-wes_and_wgs/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt""; `. **The following error have happened while the model_train.zip is invoked:**; > I0502 10:58:51.903573 139632719935232 model_train.py:182] Initializing model from checkpoint at /home/suanfa/Documents/source/DeepVariant/deepvariant-model-wes_and_wgs/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt; 2018-05-02 10:58:56.347500: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA; 2018-05-02 10:58:57.263635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:; name: Tesla P100-PCIE-12GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285; pciBusID: 0000:3b:00.0; totalMemory: 11.91GiB freeMemory: 11.62GiB; 2018-05-02 10:58:57.263682: I tensorflow/core/common_runtim
",False,
Deployability,"I found is for version 0.9, and I am unsure how to proceed with version 1.6.1. The command I used is the following: . ```; docker run \; -v ${HOME}:${HOME} \; google/deepvariant:1.6.1 \; train \; --config=${HOME}/dv_config.py:base \; --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \; --config.num_epochs=10 \; --config.learning_rate=0.001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=tpu \; --config.batch_size=1024; ```. However, I am not an expert with TPUs, and this is entirely new to me. Below is the error I encountered. Do you have any suggestions or can you direct me to an updated tutorial to follow?. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; I0627 21:18:43.707066 139683296487232 train.py:92] Running with debug=False; I0627 21:18:43.707488 139683296487232 train.py:100] Use TPU at local; I0627 21:18:43.707705 139683296487232 train.py:103] experiment_dir: /home/gambardella/training_chk; INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.; I0627 21:18:43.707828 139683296487232 tpu_strategy_util.py:57] Deallocate tpu buffers before initializing tpu system.; INFO:tensorflow:Initializing the TPU system: local; I0627 21:18:43.846984 139683296487232 tpu_strategy_util.py:81] Initializing the TPU system: local; 2024-06-27 21:18:43.848149: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized wi",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/841:1155,release,release,1155,,https://github.com/google/deepvariant/issues/841,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: I found is for version 0.9, and I am unsure how to proceed with version 1.6.1. The command I used is the following: . ```; docker run \; -v ${HOME}:${HOME} \; google/deepvariant:1.6.1 \; train \; --config=${HOME}/dv_config.py:base \; --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \; --config.num_epochs=10 \; --config.learning_rate=0.001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=tpu \; --config.batch_size=1024; ```. However, I am not an expert with TPUs, and this is entirely new to me. Below is the error I encountered. Do you have any suggestions or can you direct me to an updated tutorial to follow?. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; I0627 21:18:43.707066 139683296487232 train.py:92] Running with debug=False; I0627 21:18:43.707488 139683296487232 train.py:100] Use TPU at local; I0627 21:18:43.707705 139683296487232 train.py:103] experiment_dir: /home/gambardella/training_chk; INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.; I0627 21:18:43.707828 139683296487232 tpu_strategy_util.py:57] Deallocate tpu buffers before initializing tpu system.; INFO:tensorflow:Initializing the TPU system: local; I0627 21:18:43.846984 139683296487232 tpu_strategy_util.py:81] Initializing the TPU system: local; 2024-06-27 21:18:43.848149: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized wi
",False,"The text contains natural language sentences discussing the user's confusion about versioning and their inexperience with TPUs, along with a command snippet. The content includes a human-readable error log which may include technical terms but is presented in a way that isn't purely code or logs."
Deployability,"I get the following error when running the example provided in the quick start document:. merge_overlaps() got an unexpected keyword argument 'strict'. Any advice as to how I can resolve the issue is greatly appreciated. ```; This is the context of the error.; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/data/hs37d5.fa.gz"" --reads ""/input/data/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" `--gvcf` ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""20"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; I1220 08:40:22.262234 46912496321664 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I1220 08:40:22.268675 46912496321664 genomics_reader.py:223] Reading /input/data/HG002_NIST_150bp_chr20_downsampled_30x.bam with NativeSamReader; I1220 08:40:22.272100 46912496321664 make_examples.py:1324] Preparing inputs; I1220 08:40:22.280786 46912496321664 genomics_reader.py:223] Reading /input/data/HG002_NIST_150bp_chr20_downsampled_30x.bam with NativeSamReader; I1220 08:40:22.292714 46912496321664 make_examples.py:1248] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_UJ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/255:811,install,installed,811,,https://github.com/google/deepvariant/issues/255,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: I get the following error when running the example provided in the quick start document:. merge_overlaps() got an unexpected keyword argument 'strict'. Any advice as to how I can resolve the issue is greatly appreciated. ```; This is the context of the error.; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/data/hs37d5.fa.gz"" --reads ""/input/data/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" `--gvcf` ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""20"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; I1220 08:40:22.262234 46912496321664 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I1220 08:40:22.268675 46912496321664 genomics_reader.py:223] Reading /input/data/HG002_NIST_150bp_chr20_downsampled_30x.bam with NativeSamReader; I1220 08:40:22.272100 46912496321664 make_examples.py:1324] Preparing inputs; I1220 08:40:22.280786 46912496321664 genomics_reader.py:223] Reading /input/data/HG002_NIST_150bp_chr20_downsampled_30x.bam with NativeSamReader; I1220 08:40:22.292714 46912496321664 make_examples.py:1248] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_UJ
",False,
Deployability,I had tried install the soft using condaï¼Œ but it was failed finally. Besides dose GPU necessary ï¼Ÿ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/169:12,install,install,12,,https://github.com/google/deepvariant/issues/169,1,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: I had tried install the soft using condaï¼Œ but it was failed finally. Besides dose GPU necessary ï¼Ÿ
",False,
Deployability,"I have WGS data (about 200x) and WES data (about 1000x) of the same individual.; Ideally I would like to merge the 2 datasets and run DeepVariant with --model_type=WGS on the merged data and obtain one VCF file. Or is the model behind ""--model_type=WES"" really a different machine learning model (ML) trained on real Exome data?; I could imagine that such a ML model would learn a slightly different sequencing error model specific for sequencing data derived from target enrichment (hybridization probes) as the ones used for WES. Thank you for your advice. **Setup**; - Operating system: Ubuntu 18.04; - DeepVariant version: r0.10; - Installation method (Docker, built from source, etc.): Docker; - Type of data: Illumina WGS and WES data",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/338:636,Install,Installation,636,,https://github.com/google/deepvariant/issues/338,1,['Install'],['Installation'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: I have WGS data (about 200x) and WES data (about 1000x) of the same individual.; Ideally I would like to merge the 2 datasets and run DeepVariant with --model_type=WGS on the merged data and obtain one VCF file. Or is the model behind ""--model_type=WES"" really a different machine learning model (ML) trained on real Exome data?; I could imagine that such a ML model would learn a slightly different sequencing error model specific for sequencing data derived from target enrichment (hybridization probes) as the ones used for WES. Thank you for your advice. **Setup**; - Operating system: Ubuntu 18.04; - DeepVariant version: r0.10; - Installation method (Docker, built from source, etc.): Docker; - Type of data: Illumina WGS and WES data
",False,
Deployability,"I have try to install the DeepVariant, but failed. I cannot install the GoogleCloud at first, and then cannot also install the software. I come from China. Any solutions to the problem. I hope you can give some help. Thank you. I downloaded a versioned archive for Cloud SDK. When installing googlecloud, some module error pops. ; File ""xxxxx/install.py"", line 8, in <module> import bootstrapping; File ""xxxxx/install.py"", line 9, in <module> import setup; File ""xxxxx/install.py"", line 38, in <module> from googlecloudsdk.core.util import platforms; ImportError: No module named googlecloudsdk.core.util; I also tried the apt-get install, however due to network striction, I cannot install it by online command.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/17:14,install,install,14,,https://github.com/google/deepvariant/issues/17,9,['install'],"['install', 'installing']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: I have try to install the DeepVariant, but failed. I cannot install the GoogleCloud at first, and then cannot also install the software. I come from China. Any solutions to the problem. I hope you can give some help. Thank you. I downloaded a versioned archive for Cloud SDK. When installing googlecloud, some module error pops. ; File ""xxxxx/install.py"", line 8, in <module> import bootstrapping; File ""xxxxx/install.py"", line 9, in <module> import setup; File ""xxxxx/install.py"", line 38, in <module> from googlecloudsdk.core.util import platforms; ImportError: No module named googlecloudsdk.core.util; I also tried the apt-get install, however due to network striction, I cannot install it by online command.
",False,
Deployability,"I managed to start training on a GPU, but it took too much time. Now, I am attempting to train DeepVariant on a TPU v3-8 VM. However, the most recent tutorial I found is for version 0.9, and I am unsure how to proceed with version 1.6.1. The command I used is the following: . ```; docker run \; -v ${HOME}:${HOME} \; google/deepvariant:1.6.1 \; train \; --config=${HOME}/dv_config.py:base \; --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \; --config.num_epochs=10 \; --config.learning_rate=0.001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=tpu \; --config.batch_size=1024; ```. However, I am not an expert with TPUs, and this is entirely new to me. Below is the error I encountered. Do you have any suggestions or can you direct me to an updated tutorial to follow?. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; I0627 21:18:43.707066 139683296487232 train.py:92] Running with debug=False; I0627 21:18:43.707488 139683296487232 train.py:100] Use TPU at local; I0627 21:18:43.707705 139683296487232 train.py:103] experiment_dir: /home/gambardella/training_chk; INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.; I0627 21:18:43.707828 139683296487232 tpu_strategy_util.py:57] Deallocate tpu buffers before initializing tpu system.; INFO:tensorflow:Initializing the TPU system: local; I0627 21:18:43.846984 139683296487232 tpu_strategy_util.py:",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/841:901,update,updated,901,,https://github.com/google/deepvariant/issues/841,1,['update'],['updated'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: I managed to start training on a GPU, but it took too much time. Now, I am attempting to train DeepVariant on a TPU v3-8 VM. However, the most recent tutorial I found is for version 0.9, and I am unsure how to proceed with version 1.6.1. The command I used is the following: . ```; docker run \; -v ${HOME}:${HOME} \; google/deepvariant:1.6.1 \; train \; --config=${HOME}/dv_config.py:base \; --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \; --config.num_epochs=10 \; --config.learning_rate=0.001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=tpu \; --config.batch_size=1024; ```. However, I am not an expert with TPUs, and this is entirely new to me. Below is the error I encountered. Do you have any suggestions or can you direct me to an updated tutorial to follow?. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; I0627 21:18:43.707066 139683296487232 train.py:92] Running with debug=False; I0627 21:18:43.707488 139683296487232 train.py:100] Use TPU at local; I0627 21:18:43.707705 139683296487232 train.py:103] experiment_dir: /home/gambardella/training_chk; INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.; I0627 21:18:43.707828 139683296487232 tpu_strategy_util.py:57] Deallocate tpu buffers before initializing tpu system.; INFO:tensorflow:Initializing the TPU system: local; I0627 21:18:43.846984 139683296487232 tpu_strategy_util.py:
",False,
Deployability,"I noticed a mention of VG giraffe evaluation in the 1.5 changelog. . Has your team evaluated the impact of performing indel realignment prior to variant calling VG giraffe-generated bamfiles? This is what was done in the vg giraffe-DeepVariant paper. . In my hands, the indel realignment step significantly adds to run time. If your team does not think it meaningfully improves accuracy beyond make_example's built in realignment algorithm, then I could remove the step from my pipeline. That would be welcome news!. -Joe Lalli",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/629:478,pipeline,pipeline,478,,https://github.com/google/deepvariant/issues/629,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: I noticed a mention of VG giraffe evaluation in the 1.5 changelog. . Has your team evaluated the impact of performing indel realignment prior to variant calling VG giraffe-generated bamfiles? This is what was done in the vg giraffe-DeepVariant paper. . In my hands, the indel realignment step significantly adds to run time. If your team does not think it meaningfully improves accuracy beyond make_example's built in realignment algorithm, then I could remove the step from my pipeline. That would be welcome news!. -Joe Lalli
",False,"The content includes natural language sentences discussing concepts and ideas regarding VG giraffe evaluation, variant calling, and potential impact of indel realignment in a research context."
Deployability,"I ran deeptrio on a trio WGS data. I got the gvcf and vcf for parent 1 and 2 but I didn't get output from child. There were no error messages that I could find as to why. The output seems complete. **Setup**; - Operating system: Windows 10; - DeepVariant version: DeepTrio version 1.1.0; - Installation method: Docker; - Type of data: Illumina, GRCh38, trio WGS. **Steps to reproduce:**; - Command:; `/opt/deepvariant/bin/deeptrio/run_deeptrio . - --model_type=WGS ; - --ref=GRCh38_full_analysis_set_plus_decoy_hla.fa ; - --reads_child=20A0012672_P_GRCh38.bam ; - --reads_parent1=20A0012673_M_GRCh38.bam ; - --reads_parent2=NBVY8432_GRCh38.bam; - --output_vcf_child 20A0012672_P_GRCh38_deeptrio.vcf.gz ; - --output_vcf_parent1 20A0012673_M_GRCh38_deeptrio.vcf.gz ; - --output_vcf_parent2 NBVY8432_GRCh38_deeptrio.vcf.gz ; - --sample_name_child '20A0012672_P' ; - --sample_name_parent1 '20A0012673_M' ; - --sample_name_parent2 'NBVY8432' ; - --num_shards $(nproc) ; - --intermediate_results_dir ../home/tmp ; - --output_gvcf_child 20A0012672_P_GRCh38_deeptrio.gvcf.gz ; - --output_gvcf_parent1 20A0012673_M_GRCh38_deeptrio.gvcf.gz ; - --output_gvcf_parent2 NBVY8432_GRCh38_deeptrio.gvcf.gz`. - Error trace: NA",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/431:290,Install,Installation,290,,https://github.com/google/deepvariant/issues/431,1,['Install'],['Installation'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: I ran deeptrio on a trio WGS data. I got the gvcf and vcf for parent 1 and 2 but I didn't get output from child. There were no error messages that I could find as to why. The output seems complete. **Setup**; - Operating system: Windows 10; - DeepVariant version: DeepTrio version 1.1.0; - Installation method: Docker; - Type of data: Illumina, GRCh38, trio WGS. **Steps to reproduce:**; - Command:; `/opt/deepvariant/bin/deeptrio/run_deeptrio . - --model_type=WGS ; - --ref=GRCh38_full_analysis_set_plus_decoy_hla.fa ; - --reads_child=20A0012672_P_GRCh38.bam ; - --reads_parent1=20A0012673_M_GRCh38.bam ; - --reads_parent2=NBVY8432_GRCh38.bam; - --output_vcf_child 20A0012672_P_GRCh38_deeptrio.vcf.gz ; - --output_vcf_parent1 20A0012673_M_GRCh38_deeptrio.vcf.gz ; - --output_vcf_parent2 NBVY8432_GRCh38_deeptrio.vcf.gz ; - --sample_name_child '20A0012672_P' ; - --sample_name_parent1 '20A0012673_M' ; - --sample_name_parent2 'NBVY8432' ; - --num_shards $(nproc) ; - --intermediate_results_dir ../home/tmp ; - --output_gvcf_child 20A0012672_P_GRCh38_deeptrio.gvcf.gz ; - --output_gvcf_parent1 20A0012673_M_GRCh38_deeptrio.gvcf.gz ; - --output_gvcf_parent2 NBVY8432_GRCh38_deeptrio.gvcf.gz`. - Error trace: NA
",False,"The text contains complete sentences discussing the process and setup, including commands and details. It's not code or logs but descriptive content."
Deployability,"I ran my DeepVariant and Deeptrio pipeline following the ""quick start"" guidance, and I noticed that in my real trio case analysis, the variants called by Deeptrio outnumbers those called by DeepVariant (especially the RefCalls), for both the parents and child. Why did this happen? And I also noticed that in issue #699 your team recommand to perform trio analysis either through DeepVariant+GLnexus or Deeptrio with truth sets to be compared. I wonder how to use ""truth set"" (and what does the truth set means? like dataset from GIAB?) to check my Deeptrio results? And which method will you consider as the best in both accuracy and time cost in trio analysis? Really appreciate that if your team could answer these questions!. **Setup**; - Operating system: linux; - DeepVariant version: 1.5.0 (in Deeptrio as well); - Installation method: Singularity version; - Type of data: WES",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/704:34,pipeline,pipeline,34,,https://github.com/google/deepvariant/issues/704,2,"['Install', 'pipeline']","['Installation', 'pipeline']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: I ran my DeepVariant and Deeptrio pipeline following the ""quick start"" guidance, and I noticed that in my real trio case analysis, the variants called by Deeptrio outnumbers those called by DeepVariant (especially the RefCalls), for both the parents and child. Why did this happen? And I also noticed that in issue #699 your team recommand to perform trio analysis either through DeepVariant+GLnexus or Deeptrio with truth sets to be compared. I wonder how to use ""truth set"" (and what does the truth set means? like dataset from GIAB?) to check my Deeptrio results? And which method will you consider as the best in both accuracy and time cost in trio analysis? Really appreciate that if your team could answer these questions!. **Setup**; - Operating system: linux; - DeepVariant version: 1.5.0 (in Deeptrio as well); - Installation method: Singularity version; - Type of data: WES
",False,
Deployability,"I rebuilt docker images from instruction on https://github.com/google/deepvariant/issues/99#issuecomment-428366972. ```; gcloud builds submit \; --project ""${PROJECT_ID}"" \; --config cloudbuild.yaml \; --substitutions TAG_NAME=""${VERSION_NUMBER}"" \; --timeout 2h .; ```. I see three images on GCP Container Registry:. 1. **deepvariant**; 1. **deepvariant_gpu**; 1. **deepvariant_runner**. After finishing make_examples, now I am running calll_variant, but seems my rebuilt deepvariant_gpu image doesn't have CUDA installed, seeing such error. ```; ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory; ```. The command I used:; ```; ( time nvidia-docker run -v /home/${USER}:/home/${USER} gcr.io/my_project/deepvariant_gpu:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${CALL_VARIANTS_OUTPUT}"" \; --examples ""${EXAMPLES}"" \; --checkpoint ""${MODEL}""; ) | tee ""${LOG_DIR}/call_variants.log"" 2>&1; ```. I confirmed this is NOT an issue with gcr.io/deepvariant-docker/deepvariant_gpu, which means that it's just my rebuilt image missing CUDA driver. How should I modify the build command to build an image with CUDA driver, please?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/102:513,install,installed,513,,https://github.com/google/deepvariant/issues/102,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: I rebuilt docker images from instruction on https://github.com/google/deepvariant/issues/99#issuecomment-428366972. ```; gcloud builds submit \; --project ""${PROJECT_ID}"" \; --config cloudbuild.yaml \; --substitutions TAG_NAME=""${VERSION_NUMBER}"" \; --timeout 2h .; ```. I see three images on GCP Container Registry:. 1. **deepvariant**; 1. **deepvariant_gpu**; 1. **deepvariant_runner**. After finishing make_examples, now I am running calll_variant, but seems my rebuilt deepvariant_gpu image doesn't have CUDA installed, seeing such error. ```; ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory; ```. The command I used:; ```; ( time nvidia-docker run -v /home/${USER}:/home/${USER} gcr.io/my_project/deepvariant_gpu:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${CALL_VARIANTS_OUTPUT}"" \; --examples ""${EXAMPLES}"" \; --checkpoint ""${MODEL}""; ) | tee ""${LOG_DIR}/call_variants.log"" 2>&1; ```. I confirmed this is NOT an issue with gcr.io/deepvariant-docker/deepvariant_gpu, which means that it's just my rebuilt image missing CUDA driver. How should I modify the build command to build an image with CUDA driver, please?
",False,"The text contains complete sentences discussing issues and solutions in natural language, including error messages and steps taken by the user."
Deployability,"I recently took four cram files (one family) and put them through the deep variant pipeline in accordance with the documentation. I then followed the instructions for ""Best practices for multi-sample variant calling with DeepVariant (WES trio demonstration)"" and tried two sets of of ""trios"" and one comparison containing all four. Each of them however came up with a substanial amount of mendelian constraints which I'll list below:. Checking: /home/username/deepvariant-run/output/RBAs.cohort.vcf.gz; Family: [2114337 + 2114302] -> [2115432]; 12 non-pass records were skipped; Concordance 2115432: F:127216/127862 (99.49%) M:121292/121882 (99.52%) F+M:79650/80917 (98.43%); Sample 2115432 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling.; 4249/338863 (1.25%) records did not conform to expected call ploidy; 228759/338863 (67.51%) records were variant in at least 1 family member and checked for Mendelian constraints; 147067/228759 (64.29%) records had indeterminate consistency status due to incomplete calls; 1838/228759 (0.80%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBNs.cohort.vcf.gz; Family: [2114337 + 2114302] -> [2009617]; 18 non-pass records were skipped; Concordance 2009617: F:124581/125097 (99.59%) M:120027/120545 (99.57%) F+M:79289/80523 (98.47%); Sample 2009617 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling.; 3705/295693 (1.25%) records did not conform to expected call ploidy; 184648/295693 (62.45%) records were variant in at least 1 family member and checked for Mendelian constraints; 103541/184648 (56.07%) records had indeterminate consistency status due to incomplete calls; 1603/184648 (0.87%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBNAs.cohort.vcf.gz; Family: [2114337 + 2114302] -> [2009617, 2115432]; 24 non-pass records were ski",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/311:83,pipeline,pipeline,83,,https://github.com/google/deepvariant/issues/311,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: I recently took four cram files (one family) and put them through the deep variant pipeline in accordance with the documentation. I then followed the instructions for ""Best practices for multi-sample variant calling with DeepVariant (WES trio demonstration)"" and tried two sets of of ""trios"" and one comparison containing all four. Each of them however came up with a substanial amount of mendelian constraints which I'll list below:. Checking: /home/username/deepvariant-run/output/RBAs.cohort.vcf.gz; Family: [2114337 + 2114302] -> [2115432]; 12 non-pass records were skipped; Concordance 2115432: F:127216/127862 (99.49%) M:121292/121882 (99.52%) F+M:79650/80917 (98.43%); Sample 2115432 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling.; 4249/338863 (1.25%) records did not conform to expected call ploidy; 228759/338863 (67.51%) records were variant in at least 1 family member and checked for Mendelian constraints; 147067/228759 (64.29%) records had indeterminate consistency status due to incomplete calls; 1838/228759 (0.80%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBNs.cohort.vcf.gz; Family: [2114337 + 2114302] -> [2009617]; 18 non-pass records were skipped; Concordance 2009617: F:124581/125097 (99.59%) M:120027/120545 (99.57%) F+M:79289/80523 (98.47%); Sample 2009617 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling.; 3705/295693 (1.25%) records did not conform to expected call ploidy; 184648/295693 (62.45%) records were variant in at least 1 family member and checked for Mendelian constraints; 103541/184648 (56.07%) records had indeterminate consistency status due to incomplete calls; 1603/184648 (0.87%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBNAs.cohort.vcf.gz; Family: [2114337 + 2114302] -> [2009617, 2115432]; 24 non-pass records were ski
",False,"The text contains detailed descriptions of processing steps, including the use of specific software and methods, as well as the analysis results. It provides context about the data handling and outcomes, which are meaningful to understanding the process."
Deployability,"I tried to build deepvariant through my own docker image. I installed deepvariant and openvino toolkit. But, when I run _call_variants.zip_ script, I get the error **_name 'optimize_for_inference_lib' is not defined_**. I was retracing steps and came to conclusion that `from openvino.inference_engine import StatusCode` part of the code is failing. StatusCode cannot be imported. Have you ever encountered the same problem ?. I installed DeepVariant1.1.0 version via Docker using Ubuntu 18.04. The command I run was ; `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32 ` . and got this error:; ```; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants; checkpoint_path, input_fn=tf_dataset, model=model); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__; freeze_graph(model, checkpoint_path, tensor_shape); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, i",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/432:60,install,installed,60,,https://github.com/google/deepvariant/issues/432,2,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: I tried to build deepvariant through my own docker image. I installed deepvariant and openvino toolkit. But, when I run _call_variants.zip_ script, I get the error **_name 'optimize_for_inference_lib' is not defined_**. I was retracing steps and came to conclusion that `from openvino.inference_engine import StatusCode` part of the code is failing. StatusCode cannot be imported. Have you ever encountered the same problem ?. I installed DeepVariant1.1.0 version via Docker using Ubuntu 18.04. The command I run was ; `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32 ` . and got this error:; ```; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants; checkpoint_path, input_fn=tf_dataset, model=model); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__; freeze_graph(model, checkpoint_path, tensor_shape); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, i
",False,"The text contains detailed error logs and stack traces from a Python script. It also includes code snippets with file paths and function calls, which are indicative of program output or logging."
Deployability,"I used DeepVariant to call SNPs and Indels with the HG002 Pacbio Revio benchmark data download from https://human-pangenomics.s3.amazonaws.com/submissions/80d00e88-7a92-46d8-88c7-48f1486e11ed--HG002_PACBIO_REVIO/, while the hap.py result showed Indels have very low precision (0.653927) and recall (0.884985) and SNPs seemed to be normal having 0.998 precision and recall. Type	Filter	TRUTH.TOTAL	TRUTH.TP	TRUTH.FN	QUERY.TOTAL	QUERY.FP	QUERY.UNK	FP.gtMETRIC.Recall	METRIC.Precision	METRIC.Frac_NA	METRIC.F1_Score; INDEL	ALL	523034	462877	60157	1215487	252834	484908	16813	0.884985	0.653927	0.398941	0.75211; INDEL	PASS	523034	462877	60157	1215487	252834	484908	16813	0.884985	0.653927	0.398941	0.75211; SNP	ALL	3352818	3349190	3628	3960152	8899	597545	864	0.998918	0.997354	0.150889	0.998135; SNP	PASS	3352818	3349190	3628	3960152	8899	597545	864	0.998918	0.997354	0.150889	0.998135. I running the pipeline with aligner minimap2 v2.24 with parameters ""-L --MD -Y -a -x map-hifi --secondary=no"" and calling SNVs with deepVariant v1.3.0 with --model_type=PACBIO.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/641:898,pipeline,pipeline,898,,https://github.com/google/deepvariant/issues/641,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: I used DeepVariant to call SNPs and Indels with the HG002 Pacbio Revio benchmark data download from https://human-pangenomics.s3.amazonaws.com/submissions/80d00e88-7a92-46d8-88c7-48f1486e11ed--HG002_PACBIO_REVIO/, while the hap.py result showed Indels have very low precision (0.653927) and recall (0.884985) and SNPs seemed to be normal having 0.998 precision and recall. Type	Filter	TRUTH.TOTAL	TRUTH.TP	TRUTH.FN	QUERY.TOTAL	QUERY.FP	QUERY.UNK	FP.gtMETRIC.Recall	METRIC.Precision	METRIC.Frac_NA	METRIC.F1_Score; INDEL	ALL	523034	462877	60157	1215487	252834	484908	16813	0.884985	0.653927	0.398941	0.75211; INDEL	PASS	523034	462877	60157	1215487	252834	484908	16813	0.884985	0.653927	0.398941	0.75211; SNP	ALL	3352818	3349190	3628	3960152	8899	597545	864	0.998918	0.997354	0.150889	0.998135; SNP	PASS	3352818	3349190	3628	3960152	8899	597545	864	0.998918	0.997354	0.150889	0.998135. I running the pipeline with aligner minimap2 v2.24 with parameters ""-L --MD -Y -a -x map-hifi --secondary=no"" and calling SNVs with deepVariant v1.3.0 with --model_type=PACBIO.
",False,
Deployability,"I was able to build the docker image last week, but this week the build fails at binary creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04; - DeepVariant version: 1.4.0; - Building docker image locally. **Steps to reproduce:**; - Command: `docker build .` in source directory (no modifications); - Error trace: ; ; ```; #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting; #16 1484.7 Extracting Bazel installation...; #16 1487.8 Starting local Bazel server and connecting to it...; #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc); #16 1489.8 (21:51:01) INFO: Options provided by the client:; #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80; #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:; #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec; #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:; #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:479,install,installation,479,,https://github.com/google/deepvariant/issues/608,1,['install'],['installation'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: I was able to build the docker image last week, but this week the build fails at binary creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04; - DeepVariant version: 1.4.0; - Building docker image locally. **Steps to reproduce:**; - Command: `docker build .` in source directory (no modifications); - Error trace: ; ; ```; #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting; #16 1484.7 Extracting Bazel installation...; #16 1487.8 Starting local Bazel server and connecting to it...; #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc); #16 1489.8 (21:51:01) INFO: Options provided by the client:; #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80; #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:; #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec; #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:; #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler
",True,"The text is a log of build steps and error messages from Bazel, including timestamps, warnings, and configuration details. It does not contain meaningful sentences in natural language but rather consists primarily of program logs or API documentation."
Deployability,"I was planning to analyse runs of homozygosity in a genome assembly with Plink using DeepVariant for the variant calling. Unfortunately Plink needs basepair resolution in the input vcf file, e.g. a vcf/gvcf file that includes homozygous reference calls as well. I tried different options in DeepVariant but there seems to be no option for basepair resolution. GATK has the option to output this (-ERC BP_RESOLUTION) but since DeepVariant is more accurate and much faster I was wondering if would it be possible to add such a feature in the future? . **Setup**; - Operating system: CentOS; - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Genome assembly plus PacBio HIFI or Illumina WGS",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/571:621,Install,Installation,621,,https://github.com/google/deepvariant/issues/571,1,['Install'],['Installation'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: I was planning to analyse runs of homozygosity in a genome assembly with Plink using DeepVariant for the variant calling. Unfortunately Plink needs basepair resolution in the input vcf file, e.g. a vcf/gvcf file that includes homozygous reference calls as well. I tried different options in DeepVariant but there seems to be no option for basepair resolution. GATK has the option to output this (-ERC BP_RESOLUTION) but since DeepVariant is more accurate and much faster I was wondering if would it be possible to add such a feature in the future? . **Setup**; - Operating system: CentOS; - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Genome assembly plus PacBio HIFI or Illumina WGS
",False,
Deployability,"I was trying to follow the quick start guide. While running the run-prereq.sh file, I got; `========== Load config settings.`; `========== [Sun Jan 28 13:47:50 EST 2018] Stage 'Misc setup' starting`; `========== [Sun Jan 28 13:47:50 EST 2018] Stage 'Update package list' starting`; `sudo: apt-get: command not found`; Then, I realize it is because I am running it on mac. Is there any quick fix to this problem?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/44:250,Update,Update,250,,https://github.com/google/deepvariant/issues/44,1,['Update'],['Update'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: I was trying to follow the quick start guide. While running the run-prereq.sh file, I got; `========== Load config settings.`; `========== [Sun Jan 28 13:47:50 EST 2018] Stage 'Misc setup' starting`; `========== [Sun Jan 28 13:47:50 EST 2018] Stage 'Update package list' starting`; `sudo: apt-get: command not found`; Then, I realize it is because I am running it on mac. Is there any quick fix to this problem?
",False,
Deployability,"I would like to try to make a custom singularity version of deepvariant in which is integrated with some custom scripts, do you know where I could find the singularity recepie of deepvariant? It would be a good starting point for me. Thank you in advance for any suggestion,",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/862:84,integrat,integrated,84,,https://github.com/google/deepvariant/issues/862,1,['integrat'],['integrated'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: I would like to try to make a custom singularity version of deepvariant in which is integrated with some custom scripts, do you know where I could find the singularity recepie of deepvariant? It would be a good starting point for me. Thank you in advance for any suggestion,
",False,"The text contains complete sentences and requests for information, which are meaningful and human-readable."
Deployability,"I would rather not install the DeepVariant dependencies into my global python environment. When the python dependencies are installed into a virtual environment, make_examples.zip cannot find tensorflow. The steps below show the error. Any suggestions?. ```; $ mkvirtualenv -p /usr/bin/python2.7 DeepVariant.2.7; (DeepVariant.2.7) $ cd bin; bash run-prereq.sh; cd -. (DeepVariant.2.7) $ python bin/make_examples.zip --mode calling --ref ""${REF}"" --reads ""${BAM}"" --regions ""chr20:10,000,000-10,010,000"" --examples ""${OUTPUT_DIR}/examples.tfrecord.gz""; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_r1oZvM/runfiles/genomics/deepvariant/make_examples.py"", line 38, in <module>; import tensorflow as tf; ImportError: No module named tensorflow; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/25:19,install,install,19,,https://github.com/google/deepvariant/issues/25,2,['install'],"['install', 'installed']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: I would rather not install the DeepVariant dependencies into my global python environment. When the python dependencies are installed into a virtual environment, make_examples.zip cannot find tensorflow. The steps below show the error. Any suggestions?. ```; $ mkvirtualenv -p /usr/bin/python2.7 DeepVariant.2.7; (DeepVariant.2.7) $ cd bin; bash run-prereq.sh; cd -. (DeepVariant.2.7) $ python bin/make_examples.zip --mode calling --ref ""${REF}"" --reads ""${BAM}"" --regions ""chr20:10,000,000-10,010,000"" --examples ""${OUTPUT_DIR}/examples.tfrecord.gz""; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_r1oZvM/runfiles/genomics/deepvariant/make_examples.py"", line 38, in <module>; import tensorflow as tf; ImportError: No module named tensorflow; ```
",False,
Deployability,"I'm opening another issue to follow up on:; https://github.com/google/deepvariant/issues/132#issuecomment-482956117; (The original thread is getting a bit too long, and this issue seems more specific). Relevant system information in another comment:; https://github.com/google/deepvariant/issues/132#issuecomment-483551683; ""The super computer has an OS CentOS Linux release 7.6.1810 (LSB Version: core-4.1-amd64:core-4.1-noarch) and singularity version 2.5.2; I created the image on Amazon instance with Ubuntu 16.04. I tried using singularity version 2.5.2 & 2.6.0 but both did not help"". Adding @drtamermansour here as well.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/178:367,release,release,367,,https://github.com/google/deepvariant/issues/178,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: I'm opening another issue to follow up on:; https://github.com/google/deepvariant/issues/132#issuecomment-482956117; (The original thread is getting a bit too long, and this issue seems more specific). Relevant system information in another comment:; https://github.com/google/deepvariant/issues/132#issuecomment-483551683; ""The super computer has an OS CentOS Linux release 7.6.1810 (LSB Version: core-4.1-amd64:core-4.1-noarch) and singularity version 2.5.2; I created the image on Amazon instance with Ubuntu 16.04. I tried using singularity version 2.5.2 & 2.6.0 but both did not help"". Adding @drtamermansour here as well.
",False,
Deployability,"I'm trying to build a scatter gather implementation of make_calls -> call_variants -> post_processing, without using GNU parallel, and since multiple shards of call_variants, even with num_readers set to 1, increases the system load well beyond the number of cores, I'd like to try to limit this to a 1:1 ratio where one shard produces a system load of 1. Is this possible?. This is essentially what my pipeline looks like today: https://github.com/oskarvid/wdl_deepvariant/blob/master/deepvar-simple-SG.wdl; I say essentially because I've made insignificant changes, like added --num_readers for example. . One way would be to try to combine all tfrecord files into one, because wdl cannot use your method of using all output files from make_calls as input files for call_variants, inputFile@#shards.gz doesn't compute for wdl, and using wdl's normal way of handling multiple input files, i.e ""--examples ${sep="" --examples "" InputFile}"" doesn't work either since call_variants only takes the last ""--examples"" as input when there are many ""--examples"" in the command. Regarding combining the tfrecord files before they're used as input for call_variants, I'm not familiar enough with tensorflow to know if it's at all possible, and a quick google search didn't return anything fruitful. Is it possible to combine many tfrecord files into one?. Is it easier to try to limit the number of threads per process instead of trying to combine the tfrecord files? Or is there a third method that solves this problem better?. And thanks for a great tool!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/49:403,pipeline,pipeline,403,,https://github.com/google/deepvariant/issues/49,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: I'm trying to build a scatter gather implementation of make_calls -> call_variants -> post_processing, without using GNU parallel, and since multiple shards of call_variants, even with num_readers set to 1, increases the system load well beyond the number of cores, I'd like to try to limit this to a 1:1 ratio where one shard produces a system load of 1. Is this possible?. This is essentially what my pipeline looks like today: https://github.com/oskarvid/wdl_deepvariant/blob/master/deepvar-simple-SG.wdl; I say essentially because I've made insignificant changes, like added --num_readers for example. . One way would be to try to combine all tfrecord files into one, because wdl cannot use your method of using all output files from make_calls as input files for call_variants, inputFile@#shards.gz doesn't compute for wdl, and using wdl's normal way of handling multiple input files, i.e ""--examples ${sep="" --examples "" InputFile}"" doesn't work either since call_variants only takes the last ""--examples"" as input when there are many ""--examples"" in the command. Regarding combining the tfrecord files before they're used as input for call_variants, I'm not familiar enough with tensorflow to know if it's at all possible, and a quick google search didn't return anything fruitful. Is it possible to combine many tfrecord files into one?. Is it easier to try to limit the number of threads per process instead of trying to combine the tfrecord files? Or is there a third method that solves this problem better?. And thanks for a great tool!
",False,"The text contains complete sentences and human-readable prose discussing the user's pipeline setup and their attempts to optimize it. It includes questions about combining TFRecord files and alternative approaches to system load issues. The content is explanatory and in natural language, making it meaningful for human reading."
Deployability,"I'm trying to output gVCF's via DeepVariant as described by the tutorial here: https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-gvcf-support.md . Is there a way that I could just modify the gcp_deepvariant_runner.py script (linked below) rather than having to manually run the commands step by step? I have many BAM files to process and running the pipeline manually is intractable. https://github.com/googlegenomics/gcp-deepvariant-runner/blob/master/gcp_deepvariant_runner.py. I'm guessing I would need to fork the gcp-deepvariant-runner repo, edit the python file, then push the new repo to some sort of container registry? Any guidance here would be much appreciated.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/193:365,pipeline,pipeline,365,,https://github.com/google/deepvariant/issues/193,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: I'm trying to output gVCF's via DeepVariant as described by the tutorial here: https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-gvcf-support.md . Is there a way that I could just modify the gcp_deepvariant_runner.py script (linked below) rather than having to manually run the commands step by step? I have many BAM files to process and running the pipeline manually is intractable. https://github.com/googlegenomics/gcp-deepvariant-runner/blob/master/gcp_deepvariant_runner.py. I'm guessing I would need to fork the gcp-deepvariant-runner repo, edit the python file, then push the new repo to some sort of container registry? Any guidance here would be much appreciated.
",False,
Deployability,"I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook ; n1-standard-64 - 64v CPUs - 240GB RAM; - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.): docker deepvariant ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:; export BIN_VERSION=""1.5.0""; export INPUT_DIR=""/home/jupyter/input""; export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna""; export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam""; export OUTPUT_DIR=""/home/jupyter/output""; export OUTPUT_VCF=""HG003.deepvariant.vcf.gz""; export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md; ; I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. ; Is there a reason for the differences in runtime? ; ; Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/735:328,Install,Installation,328,,https://github.com/google/deepvariant/issues/735,2,"['Install', 'configurat']","['Installation', 'configuration']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook ; n1-standard-64 - 64v CPUs - 240GB RAM; - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.): docker deepvariant ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:; export BIN_VERSION=""1.5.0""; export INPUT_DIR=""/home/jupyter/input""; export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna""; export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam""; export OUTPUT_DIR=""/home/jupyter/output""; export OUTPUT_VCF=""HG003.deepvariant.vcf.gz""; export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md; ; I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. ; Is there a reason for the differences in runtime? ; ; Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md
",False,"The text contains complete, meaningful sentences discussing issues with WES sample replication and runtime differences."
Deployability,"If I generated a VCF file for a trio (with a father using deeptrio) or a solo male (using deepvariant) without --haploid_contigs=""chrX,chrY"" and/or --par_regions_bed parameters. Can I fix the VCF after the run is finished, let's say by targeted calling?. And as a suggestion, it would be nice if the algorithm takes care of this automatically :). - Operating system: Linux; - DeepVariant version: 1.6.0 and 1.6.1; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PacBio, Hg38",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/825:416,Install,Installation,416,,https://github.com/google/deepvariant/issues/825,1,['Install'],['Installation'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: If I generated a VCF file for a trio (with a father using deeptrio) or a solo male (using deepvariant) without --haploid_contigs=""chrX,chrY"" and/or --par_regions_bed parameters. Can I fix the VCF after the run is finished, let's say by targeted calling?. And as a suggestion, it would be nice if the algorithm takes care of this automatically :). - Operating system: Linux; - DeepVariant version: 1.6.0 and 1.6.1; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PacBio, Hg38
",False,"The text contains sentences discussing how to handle a VCF file after generating it using specific tools and parameters, as well as suggestions for the algorithm. It includes human-readable content about configurations and potential improvements."
Deployability,Install intel-tensorflow with custom wheel.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/264:0,Install,Install,0,,https://github.com/google/deepvariant/pull/264,1,['Install'],['Install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Install intel-tensorflow with custom wheel.
",False,
Deployability,Install precompiled binaries,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252:0,Install,Install,0,,https://github.com/google/deepvariant/issues/252,1,['Install'],['Install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Install precompiled binaries
",False,
Deployability,Installation error with intel-tensorflow,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/263:0,Install,Installation,0,,https://github.com/google/deepvariant/issues/263,1,['Install'],['Installation'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Installation error with intel-tensorflow
",False,The text mentions an installation error which is a type of log or error message.
Deployability,Installation of 0.8.0 using bioconda:; ```; conda create -n deepvariant deepvariant; ```. fails on CentOS 6.6 with: . ```; CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/platform/gsutil/third_party/six/.travis.yml'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/platform/gsutil/third_party/six/README.rst'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/platform/gsutil/third_party/six/documentation/Makefile'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/platform/gsutil/third_party/six/documentation/conf.pyc'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/platform/gsutil/third_party/six/documentation/index.rst'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/platform/gsutil/third_party/six/setup.cfg'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177:0,Install,Installation,0,,https://github.com/google/deepvariant/issues/177,1,['Install'],['Installation'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Installation of 0.8.0 using bioconda:; ```; conda create -n deepvariant deepvariant; ```. fails on CentOS 6.6 with: . ```; CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/platform/gsutil/third_party/six/.travis.yml'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/platform/gsutil/third_party/six/README.rst'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/platform/gsutil/third_party/six/documentation/Makefile'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/platform/gsutil/third_party/six/documentation/conf.pyc'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/platform/gsutil/third_party/six/documentation/index.rst'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/platform/gsutil/third_party/six/setup.cfg'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3
",True,"The content consists of program logs or error messages, which are time-stamped and contain specific error codes (CondaVerificationError). These are part of the system's output when an installation fails, indicating potential issues with package corruption or dependency problems. It does not contain meaningful human-readable sentences discussing concepts or ideas."
Deployability,"Installed DeepVariant 1.5.0 from bioconda using micromamba. Running make examples command produces the following error:; ```; micromamba run --name dv /opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0; Traceback (most recent call last):; File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>; File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main; File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call; with Popen(*popenargs, **kwargs) as p:; File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__; restore_signals, start_new_session); File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child; raise child_exception_type(errno_num, err_msg, err_filename); FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'; ```. I noticed inside the bazel .zip files the python binary is hard-coded:; /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVariant was installed using micromamba:. ```; micro",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/664:0,Install,Installed,0,,https://github.com/google/deepvariant/issues/664,1,['Install'],['Installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Installed DeepVariant 1.5.0 from bioconda using micromamba. Running make examples command produces the following error:; ```; micromamba run --name dv /opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0; Traceback (most recent call last):; File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>; File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main; File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call; with Popen(*popenargs, **kwargs) as p:; File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__; restore_signals, start_new_session); File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child; raise child_exception_type(errno_num, err_msg, err_filename); FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'; ```. I noticed inside the bazel .zip files the python binary is hard-coded:; /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVariant was installed using micromamba:. ```; micro
",False,"The content includes error messages from a command execution and discussion of specific technical issues related to Python binary paths. While these are code-related, they provide context for troubleshooting installation issues, which is meaningful for human readers trying to resolve the problem."
Deployability,Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2),MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657:0,Install,Installing,0,,https://github.com/google/deepvariant/issues/657,1,['Install'],['Installing'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2)
",False,"The text provides instructions for installing software, which is a practical and readable guide."
Deployability,Is there a document release for DeepVariant used in PrecisionFDA v2 (the hybrid version)?. https://precision.fda.gov/challenges/10/view/results,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/336:20,release,release,20,,https://github.com/google/deepvariant/issues/336,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Is there a document release for DeepVariant used in PrecisionFDA v2 (the hybrid version)?. https://precision.fda.gov/challenges/10/view/results
",False,
Deployability,"Iâ€™m new to working with computers tools like DeepVariant. Iâ€™m trying to build DeepVariant using Docker on a Mac M1 and am encountering issues with the Dockerfile during the Bazel build process. I want to ensure compatibility with ARM64 architecture. **Docker version**: Docker version 27.1.1, build 6312585; **Bazel Version**: 7.3.1; **MacBook Model**: M1 chip (ARM64 architecture). **Error**: ; ![IMG_3267](https://github.com/user-attachments/assets/11e28824-b941-42cc-9d33-7e9155a03543); ![IMG_3268](https://github.com/user-attachments/assets/4e923de6-99d5-43ee-80c6-29b32504527d). **My Dockerfilee code**:. ```; # Base image suitable for ARM64 architecture; FROM arm64v8/ubuntu:latest AS base. # Prevent interactive prompts; ENV DEBIAN_FRONTEND=noninteractive. # Install necessary packages; RUN apt-get update && \; apt-get install -y \; git \; curl \; unzip \; wget \; openjdk-17-jdk \; build-essential \; bzip2 \; python3-pip \; parallel && \; apt-get clean && \; rm -rf /var/lib/apt/lists/*. # Install Bazel (adjust version as needed); RUN curl -LO ""https://github.com/bazelbuild/bazel/releases/download/7.3.1/bazel-7.3.1-linux-arm64"" && \; chmod +x bazel-7.3.1-linux-arm64 && \; mv bazel-7.3.1-linux-arm64 /usr/local/bin/bazel. # Install Conda; RUN curl -LO ""https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-aarch64.sh"" && \; bash Miniconda3-latest-Linux-aarch64.sh -b -p /opt/miniconda && \; rm Miniconda3-latest-Linux-aarch64.sh. # Setup Conda environment; ENV PATH=""/opt/miniconda/bin:${PATH}"". RUN conda config --add channels defaults && \; conda config --add channels bioconda && \; conda config --add channels conda-forge && \; conda create -n bio bioconda::bcftools bioconda::samtools -y && \; conda clean -a. # Clone DeepVariant and build; FROM base AS builder. # Clone the DeepVariant repository; RUN git clone https://github.com/google/deepvariant.git /opt/deepvariant && \; cd /opt/deepvariant && \; git checkout tags/v1.6.1. # Run Bazel build with additional flags to ski",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/871:766,Install,Install,766,,https://github.com/google/deepvariant/issues/871,3,"['Install', 'install', 'update']","['Install', 'install', 'update']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Iâ€™m new to working with computers tools like DeepVariant. Iâ€™m trying to build DeepVariant using Docker on a Mac M1 and am encountering issues with the Dockerfile during the Bazel build process. I want to ensure compatibility with ARM64 architecture. **Docker version**: Docker version 27.1.1, build 6312585; **Bazel Version**: 7.3.1; **MacBook Model**: M1 chip (ARM64 architecture). **Error**: ; ![IMG_3267](https://github.com/user-attachments/assets/11e28824-b941-42cc-9d33-7e9155a03543); ![IMG_3268](https://github.com/user-attachments/assets/4e923de6-99d5-43ee-80c6-29b32504527d). **My Dockerfilee code**:. ```; # Base image suitable for ARM64 architecture; FROM arm64v8/ubuntu:latest AS base. # Prevent interactive prompts; ENV DEBIAN_FRONTEND=noninteractive. # Install necessary packages; RUN apt-get update && \; apt-get install -y \; git \; curl \; unzip \; wget \; openjdk-17-jdk \; build-essential \; bzip2 \; python3-pip \; parallel && \; apt-get clean && \; rm -rf /var/lib/apt/lists/*. # Install Bazel (adjust version as needed); RUN curl -LO ""https://github.com/bazelbuild/bazel/releases/download/7.3.1/bazel-7.3.1-linux-arm64"" && \; chmod +x bazel-7.3.1-linux-arm64 && \; mv bazel-7.3.1-linux-arm64 /usr/local/bin/bazel. # Install Conda; RUN curl -LO ""https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-aarch64.sh"" && \; bash Miniconda3-latest-Linux-aarch64.sh -b -p /opt/miniconda && \; rm Miniconda3-latest-Linux-aarch64.sh. # Setup Conda environment; ENV PATH=""/opt/miniconda/bin:${PATH}"". RUN conda config --add channels defaults && \; conda config --add channels bioconda && \; conda config --add channels conda-forge && \; conda create -n bio bioconda::bcftools bioconda::samtools -y && \; conda clean -a. # Clone DeepVariant and build; FROM base AS builder. # Clone the DeepVariant repository; RUN git clone https://github.com/google/deepvariant.git /opt/deepvariant && \; cd /opt/deepvariant && \; git checkout tags/v1.6.1. # Run Bazel build with additional flags to ski
",False,"The text includes human-readable sentences discussing the user's experience and problem, including their setup with Docker and Bazel, along with error descriptions. It also contains code snippets that are part of their attempt to solve the issue but presented in a way that may not strictly follow formatting rules for code (missing backticks or syntax highlighting), making it more descriptive than formal code. Therefore, while there is some code content, it's accompanied by meaningful explanations and context that make it valuable for understanding the user's issue."
Deployability,"M11163_L7_PE_output_intermediate in docker. ****. ***** Running the command:*****; ```; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam"" --examples ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz"" --gvcf ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz"" --regions ""Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; I0614 20:20:45.812468 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam with NativeSamReader; W0614 20:20:45.812704 47288495204160 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument.; I0614 20:20:45.822298 47288495204160 make_examples_core.py:239] Preparing inputs; I0614 20:20:45.836667 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/1",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/542:3051,install,installed,3051,,https://github.com/google/deepvariant/issues/542,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: M11163_L7_PE_output_intermediate in docker. ****. ***** Running the command:*****; ```; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam"" --examples ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz"" --gvcf ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz"" --regions ""Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; I0614 20:20:45.812468 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam with NativeSamReader; W0614 20:20:45.812704 47288495204160 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument.; I0614 20:20:45.822298 47288495204160 make_examples_core.py:239] Preparing inputs; I0614 20:20:45.836667 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/1
",True,"The text consists primarily of code snippets or program logs. It includes command invocations, warnings from perl, and log messages from Python scripts (e.g., I0614 20:20:45). These are indicative of program output rather than meaningful human-readable content."
Deployability,"Model for calling whole exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard; IMAGE_VERSION=0.7.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-b \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --regions gs://canis/CNR-data/CDS-canonical.bed \; --bam gs://canis/CNR-data/TLE_a_001.bam \; --bai gs://canis/CNR-data/TLE_a_001.bam.bai \; --ref gs://genomics-public-data/references/GRCh38_Verily/GRCh38_Verily_v1.genome.fa \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones us-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```. I get the following error: ; ```; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run; _run_make_examples(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/13939489157244551677"" failed: executing pipeline: Execution failed: action 5: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION); details:; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116:2289,pipeline,pipeline,2289,,https://github.com/google/deepvariant/issues/116,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Model for calling whole exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard; IMAGE_VERSION=0.7.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-b \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --regions gs://canis/CNR-data/CDS-canonical.bed \; --bam gs://canis/CNR-data/TLE_a_001.bam \; --bai gs://canis/CNR-data/TLE_a_001.bam.bai \; --ref gs://genomics-public-data/references/GRCh38_Verily/GRCh38_Verily_v1.genome.fa \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones us-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```. I get the following error: ; ```; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run; _run_make_examples(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/13939489157244551677"" failed: executing pipeline: Execution failed: action 5: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION); details:; ```
",True,"The text contains code snippets, logs, and error messages. It includes command lines, API calls, configuration settings, and error outputs which are typical of programmatic documentation."
Deployability,"My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do?. ++ which python3; + PYTHON=/usr/local/bin/python3; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""); -- Checking for module 'protobuf'; -- No package 'protobuf' found; CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):; A required package was not found; Call Stack (most recent call first):; /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal); clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules); clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/737:94,install,installing,94,,https://github.com/google/deepvariant/issues/737,2,['install'],"['installed', 'installing']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do?. ++ which python3; + PYTHON=/usr/local/bin/python3; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""); -- Checking for module 'protobuf'; -- No package 'protobuf' found; CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):; A required package was not found; Call Stack (most recent call first):; /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal); clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules); clif/CMakeLists.txt:22 (include)
",False,
Deployability,"Note: this PR is on the gh-pages branch, meant for an update for the goo.gl/deepvariant blog. It's submitted by a DeepVariant team member.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/244:54,update,update,54,,https://github.com/google/deepvariant/pull/244,3,['update'],['update'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Note: this PR is on the gh-pages branch, meant for an update for the goo.gl/deepvariant blog. It's submitted by a DeepVariant team member.
",False,The text describes a pull request (PR) and its purpose as part of a blog update.
Deployability,Numpy installation problem,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/394:6,install,installation,6,,https://github.com/google/deepvariant/issues/394,1,['install'],['installation'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Numpy installation problem
",False,
Deployability,OpenVINO integration,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363:9,integrat,integration,9,,https://github.com/google/deepvariant/pull/363,1,['integrat'],['integration'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: OpenVINO integration
",True,"The given text is a programming-related term or topic, possibly indicating that it is related to code, software development, or technical documentation. This does not necessarily mean elimination, but requires closer examination for the presence of actual human-readable sentences or descriptive content."
Deployability,"Our cluster environment only allows us to run Singularity containers (due to not running as root) instead of Docker containers. I've been converting the DeepVariant provided docker images to Singularity using version 0.7.0 which worked well. With version 0.7.2 the python imports break due to the location of where the python packages are installed. In version 0.7.0 the python requirements were installed in /usr/local/lib/python2.7/dist-packages/, but now the requirements are installed into /root/.local/lib/python2.7/site-packages/ which does not get copied over into the Singularity container. This might be because of Singularity being designed to run not as root. Would you be able to change the install location back to /usr/local/lib/python2.7/dist-packages/ or provide working Singularity containers?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/132:339,install,installed,339,,https://github.com/google/deepvariant/issues/132,4,['install'],"['install', 'installed']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Our cluster environment only allows us to run Singularity containers (due to not running as root) instead of Docker containers. I've been converting the DeepVariant provided docker images to Singularity using version 0.7.0 which worked well. With version 0.7.2 the python imports break due to the location of where the python packages are installed. In version 0.7.0 the python requirements were installed in /usr/local/lib/python2.7/dist-packages/, but now the requirements are installed into /root/.local/lib/python2.7/site-packages/ which does not get copied over into the Singularity container. This might be because of Singularity being designed to run not as root. Would you be able to change the install location back to /usr/local/lib/python2.7/dist-packages/ or provide working Singularity containers?
",False,
Deployability,Questions about GLnexus integration and DeepTrio training data for config evaluation,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/475:24,integrat,integration,24,,https://github.com/google/deepvariant/issues/475,1,['integrat'],['integration'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Questions about GLnexus integration and DeepTrio training data for config evaluation
",False,"The text consists of questions related to the integration of GLnexus and the use of DeepTrio training data for configuration evaluation. These are meaningful sentences discussing integration and data usage, which should not be filtered out."
Deployability,"R_IMAGE_GPU}"", \; STAGING_FOLDER_NAME=""${STAGING_FOLDER_NAME}"", \; OUTPUT_FILE_NAME=""${OUTPUT_FILE_NAME}"" \; | tr -d '[:space:]'`; ```. I execute `./runner.sh`, and a few minutes later I can tell with `gcloud alpha genomics operations describe` that it's failed. That output is [attached](https://github.com/google/deepvariant/files/1835589/describe.out.txt). . I can see in it several distinct potential errors: . 1. `11: Docker run failed: command failed: [03/21/2018 23:29:54 INFO gcp_deepvariant_runner.py] Running make_examples...`; 2. ` [03/21/2018 23:29:54 WARNING __init__.py] file_cache is unavailable when using oauth2client >= 4.0.0`; 3. `[u'Error in job call-varia--root--180321-233157-28 - code 9: Quota CPUS exceeded in region us-central1']`. The `...-stderr.log` file written to `staging-folder` also begins with the errors; ```; /tmp/ggp-896952821: line 16: type: gsutil: not found; debconf: delaying package configuration, since apt-utils is not installed; debconf: delaying package configuration, since apt-utils is not installed; W: GPG error: http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 3746C208A7317B0F; W: The repository 'http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease' is not signed.; debconf: delaying package configuration, since apt-utils is not installed; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 663 100 663 0 0 5012 0 --:--:-- --:--:-- --:--:-- 5022; debconf: delaying package configuration, since apt-utils is not installed; WARNING: Logging before flag parsing goes to stderr.; ```. But I then see many messages about candidate variants it's found. . The directory `staging-folder/examples/0/` also includes 8 `.gz` files like `examples_output.tfrecord-00007-of-00008.gz`. . Can you help me figure out what I'm doing wrong?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/60:3499,configurat,configuration,3499,,https://github.com/google/deepvariant/issues/60,4,"['configurat', 'install']","['configuration', 'installed']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: R_IMAGE_GPU}"", \; STAGING_FOLDER_NAME=""${STAGING_FOLDER_NAME}"", \; OUTPUT_FILE_NAME=""${OUTPUT_FILE_NAME}"" \; | tr -d '[:space:]'`; ```. I execute `./runner.sh`, and a few minutes later I can tell with `gcloud alpha genomics operations describe` that it's failed. That output is [attached](https://github.com/google/deepvariant/files/1835589/describe.out.txt). . I can see in it several distinct potential errors: . 1. `11: Docker run failed: command failed: [03/21/2018 23:29:54 INFO gcp_deepvariant_runner.py] Running make_examples...`; 2. ` [03/21/2018 23:29:54 WARNING __init__.py] file_cache is unavailable when using oauth2client >= 4.0.0`; 3. `[u'Error in job call-varia--root--180321-233157-28 - code 9: Quota CPUS exceeded in region us-central1']`. The `...-stderr.log` file written to `staging-folder` also begins with the errors; ```; /tmp/ggp-896952821: line 16: type: gsutil: not found; debconf: delaying package configuration, since apt-utils is not installed; debconf: delaying package configuration, since apt-utils is not installed; W: GPG error: http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 3746C208A7317B0F; W: The repository 'http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease' is not signed.; debconf: delaying package configuration, since apt-utils is not installed; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 663 100 663 0 0 5012 0 --:--:-- --:--:-- --:--:-- 5022; debconf: delaying package configuration, since apt-utils is not installed; WARNING: Logging before flag parsing goes to stderr.; ```. But I then see many messages about candidate variants it's found. . The directory `staging-folder/examples/0/` also includes 8 `.gz` files like `examples_output.tfrecord-00007-of-00008.gz`. . Can you help me figure out what I'm doing wrong?
",False,"The text contains a mix of human-readable descriptions and logs. It discusses the user's experience running a script, error messages they encountered, and their questions about the process. While there are some log snippets and technical details, the overall content is explanatory and meant to be read by humans."
Deployability,Rebuilt deepvariant_gpu docker image doesn't seem to have CUDA driver installed,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/102:70,install,installed,70,,https://github.com/google/deepvariant/issues/102,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Rebuilt deepvariant_gpu docker image doesn't seem to have CUDA driver installed
",False,"The text is a human-readable sentence explaining that the Docker image for DeepVariant GPU doesn't have the CUDA driver installed. It's a complete, meaningful statement in natural language and provides clear information."
Deployability,Run OpenVINO processing in separate thread which let's to use common logging (https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 + https://github.com/google/deepvariant/pull/363/commits/9e69c4096fac8ddb788c3d29e4405fc50e85d1e3) . Please ignore test scripts from `.github/workflows` - they are not a part of patch but just used for validation.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/393:352,patch,patch,352,,https://github.com/google/deepvariant/pull/393,1,['patch'],['patch'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Run OpenVINO processing in separate thread which let's to use common logging (https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 + https://github.com/google/deepvariant/pull/363/commits/9e69c4096fac8ddb788c3d29e4405fc50e85d1e3) . Please ignore test scripts from `.github/workflows` - they are not a part of patch but just used for validation.
",False,The text includes complete sentences discussing the process of running OpenVINO in a separate thread and references to specific GitHub pull requests and commits. It also mentions ignoring test scripts from .github/workflows.
Deployability,Soooo hard to install,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/169:14,install,install,14,,https://github.com/google/deepvariant/issues/169,1,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Soooo hard to install
",False,"Complete, meaningful sentence discussing a challenging installation process."
Deployability,"Sorry it's not an issue of the package per se, but is it possible for you to supply the shell scripts below for building the package on RedHat systems? I tried to install the package with conda on a RedHat system, but it didn't work. When I tried to build the package from the sources, I ran into some issues. Many system libraries and utilities, such as zlib1g-dev, python3-distutils, are not available or possible in different names. Without or not familiar with a Ubuntu system, it's hard to figure out what would be the equivalent. It'd be enough to have the scripts working for one version of RedHat and it'd be much easier for people familiar with RedHat to modify the scripts for a different version of RedHat. Thanks. build_and_test.sh; build-prereq.sh; build_release_binaries.sh; run-prereq.sh; settings.sh",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/740:163,install,install,163,,https://github.com/google/deepvariant/issues/740,1,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Sorry it's not an issue of the package per se, but is it possible for you to supply the shell scripts below for building the package on RedHat systems? I tried to install the package with conda on a RedHat system, but it didn't work. When I tried to build the package from the sources, I ran into some issues. Many system libraries and utilities, such as zlib1g-dev, python3-distutils, are not available or possible in different names. Without or not familiar with a Ubuntu system, it's hard to figure out what would be the equivalent. It'd be enough to have the scripts working for one version of RedHat and it'd be much easier for people familiar with RedHat to modify the scripts for a different version of RedHat. Thanks. build_and_test.sh; build-prereq.sh; build_release_binaries.sh; run-prereq.sh; settings.sh
",False,The text contains meaningful sentences discussing installation issues on RedHat systems and requests for shell scripts. It is human-readable prose explaining problems and seeking help.
Deployability,"TF_CUDA_VERSION=""10.0""; export CUDA_TOOLKIT_PATH=""/usr/local/cuda""; export TF_CUDNN_VERSION=""7""; export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant.; DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that; # `bazel` will find the latest version of bazel installed in the user's home; # directory. This is set in setting.sh as all DeepVariant scripts source; # settings.sh and assume that `bazel` will find the right version.; export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts.; export DEEPVARIANT_BUCKET=""gs://deepvariant""; export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages""; export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a; # named release version. Set it to an already existing value in the environment; # (allowing command line control of the build), defaulting to 0 (release build).; # Note that setting this to 1 implies that the C++ code in DeepVariant will be; # build using the master branch and not the pinned version to avoid; # incompatibilities between TensorFlow C++ used to build DeepVariant and the; # tf-nightly wheel.; export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not; # the same as the python version of TensorFlow we use, but should be similar or; # we risk having version incompatibilities between our C++ code and the Python; # code we use at runtime.; if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; export DV_CPP_TENSORFLOW_TAG=""master""; else; export DV_CPP_TENSORFLOW_TAG=""r1.12""; fi; export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0""; export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0""; export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an alr",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145:4223,release,release,4223,,https://github.com/google/deepvariant/issues/145,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: TF_CUDA_VERSION=""10.0""; export CUDA_TOOLKIT_PATH=""/usr/local/cuda""; export TF_CUDNN_VERSION=""7""; export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant.; DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that; # `bazel` will find the latest version of bazel installed in the user's home; # directory. This is set in setting.sh as all DeepVariant scripts source; # settings.sh and assume that `bazel` will find the right version.; export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts.; export DEEPVARIANT_BUCKET=""gs://deepvariant""; export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages""; export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a; # named release version. Set it to an already existing value in the environment; # (allowing command line control of the build), defaulting to 0 (release build).; # Note that setting this to 1 implies that the C++ code in DeepVariant will be; # build using the master branch and not the pinned version to avoid; # incompatibilities between TensorFlow C++ used to build DeepVariant and the; # tf-nightly wheel.; export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not; # the same as the python version of TensorFlow we use, but should be similar or; # we risk having version incompatibilities between our C++ code and the Python; # code we use at runtime.; if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; export DV_CPP_TENSORFLOW_TAG=""master""; else; export DV_CPP_TENSORFLOW_TAG=""r1.12""; fi; export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0""; export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0""; export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an alr
",True,"The content consists primarily of environment variable assignments, code snippets, and configuration settings that are typically found in configuration files or build systems. It includes lines with export statements and conditional checks which are indicative of programmatic setup instructions rather than human-readable prose."
Deployability,Tensorflow .whl is not installing during build,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/30:23,install,installing,23,,https://github.com/google/deepvariant/issues/30,1,['install'],['installing'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Tensorflow .whl is not installing during build
",False,"The content is a concise statement indicating an issue during the installation of a Tensorflow wheel file, which is relevant and meaningful for human-readable context related to installation problems."
Deployability,The deep variant wrapper dv_call_variants.py crushing when installed using conda.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/573:59,install,installed,59,,https://github.com/google/deepvariant/issues/573,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: The deep variant wrapper dv_call_variants.py crushing when installed using conda.
",True,"The text appears to be a code snippet or program output, as it includes terms like 'crushing' which might be a typo for 'crashes' and refers to installation via conda. This suggests an error message or log related to the installation process of a Python package."
Deployability,"The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is; https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/789:61,install,install,61,,https://github.com/google/deepvariant/issues/789,1,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is; https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.
",False,The content includes a human-readable explanation discussing the issue with the script and the recommended solution.
Deployability,"The very_sensitive_caller seems to be a new feature in the release 0.9.0, what's the difference between the very sensitive caller and the usual one?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/256:59,release,release,59,,https://github.com/google/deepvariant/issues/256,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: The very_sensitive_caller seems to be a new feature in the release 0.9.0, what's the difference between the very sensitive caller and the usual one?
",False,The text is a meaningful human-readable sentence that explains the difference between two features in a release.
Deployability,"There are some problems while running the ./build-prereq.sh:; ```; + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0; Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0); Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0); Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0); Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2); Installing collected packages: pyparsing; Attempting uninstall: pyparsing; Found existing installation: pyparsing 3.1.1; Uninstalling pyparsing-3.1.1:; Successfully uninstalled pyparsing-3.1.1; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; + DV_PLATFORM=ubuntu-20.04; + ln -sf /usr/bin/python3.8 /usr/local/bin/python3; + cd; + rm -rf clif; + proxychains git clone https://github.com/google/clif.git; ProxyChains-3.1 (http://proxychains.sf.net); Cloning into 'clif'...; |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK; remote: Enumerating objects: 5846, done.; remote: Counting objec",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739:86,install,install,86,,https://github.com/google/deepvariant/issues/739,3,"['Install', 'install']","['Installing', 'install', 'installation']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: There are some problems while running the ./build-prereq.sh:; ```; + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0; Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0); Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0); Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0); Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2); Installing collected packages: pyparsing; Attempting uninstall: pyparsing; Found existing installation: pyparsing 3.1.1; Uninstalling pyparsing-3.1.1:; Successfully uninstalled pyparsing-3.1.1; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; + DV_PLATFORM=ubuntu-20.04; + ln -sf /usr/bin/python3.8 /usr/local/bin/python3; + cd; + rm -rf clif; + proxychains git clone https://github.com/google/clif.git; ProxyChains-3.1 (http://proxychains.sf.net); Cloning into 'clif'...; |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK; remote: Enumerating objects: 5846, done.; remote: Counting objec
",True,"The text contains logs from a shell script (e.g., pip install commands, error messages), which are primarily code-related outputs and not meaningful human-readable sentences."
Deployability,This is a blog update by the DeepVariant team. Note: we are not taking pull requests at this time.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/276:15,update,update,15,,https://github.com/google/deepvariant/pull/276,1,['update'],['update'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: This is a blog update by the DeepVariant team. Note: we are not taking pull requests at this time.
",False,"It's a blog update, which contains meaningful sentences and explanatory content."
Deployability,This is an internal pull request that is meant to only update the `gh_pages` branch.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/621:55,update,update,55,,https://github.com/google/deepvariant/pull/621,1,['update'],['update'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: This is an internal pull request that is meant to only update the `gh_pages` branch.
",False,
Deployability,"Traceback (most recent call last):; File ""get-pip.py"", line 32992, in <module>; main(); File ""get-pip.py"", line 135, in main; bootstrap(tmpdir=tmpdir); File ""get-pip.py"", line 111, in bootstrap; monkeypatch_for_cert(tmpdir); File ""get-pip.py"", line 92, in monkeypatch_for_cert; from pip._internal.commands.install import InstallCommand; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>; ModuleNotFoundError: No module named 'distutils.cmd'",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/730:306,install,install,306,,https://github.com/google/deepvariant/issues/730,3,"['Install', 'configurat', 'install']","['InstallCommand', 'configuration', 'install']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Traceback (most recent call last):; File ""get-pip.py"", line 32992, in <module>; main(); File ""get-pip.py"", line 135, in main; bootstrap(tmpdir=tmpdir); File ""get-pip.py"", line 111, in bootstrap; monkeypatch_for_cert(tmpdir); File ""get-pip.py"", line 92, in monkeypatch_for_cert; from pip._internal.commands.install import InstallCommand; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>; ModuleNotFoundError: No module named 'distutils.cmd'
",True,The given text is a stack trace from an error message. It includes file paths and timestamps which are typical characteristics of program logs or error messages.
Deployability,Update README.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/375:0,Update,Update,0,,https://github.com/google/deepvariant/pull/375,1,['Update'],['Update'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Update README.
",False,
Deployability,Update TensorFlow,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/511:0,Update,Update,0,,https://github.com/google/deepvariant/issues/511,1,['Update'],['Update'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Update TensorFlow
",False,"The content is a command meant to be executed by a system, not human-readable text."
Deployability,Update deepvariant-docker.md,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/424:0,Update,Update,0,,https://github.com/google/deepvariant/pull/424,1,['Update'],['Update'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Update deepvariant-docker.md
",True,"The file appears to be a markdown document related to Docker setup for DeepVariant. While this may contain useful information, it's more of a technical guide rather than human-readable prose. However, since the content is not purely code or logs, and seems to be explanatory, it might need further review."
Deployability,Update deepvariant-pacbio-model-case-study.md,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/473:0,Update,Update,0,,https://github.com/google/deepvariant/pull/473,1,['Update'],['Update'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Update deepvariant-pacbio-model-case-study.md
",True,"The file path 'content/update/deepvariant-pacbio-model-case-study.md' indicates that the task is about updating a case study document related to a specific model. This suggests that the content might be primarily technical documentation, code-related updates, or version control metadata, all of which should be filtered out."
Deployability,Update documentation.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/397:0,Update,Update,0,,https://github.com/google/deepvariant/pull/397,1,['Update'],['Update'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Update documentation.
",False,
Deployability,Update issue templates,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/327:0,Update,Update,0,,https://github.com/google/deepvariant/pull/327,1,['Update'],['Update'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Update issue templates
",False,
Deployability,"Update runtime - when I updated this last time, I used a newer run toâ€¦",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/350:0,Update,Update,0,,https://github.com/google/deepvariant/pull/350,2,"['Update', 'update']","['Update', 'updated']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Update runtime - when I updated this last time, I used a newer run toâ€¦
",True,This text is a code snippet or sample as it contains syntax that indicates a programming context.
Deployability,Update the README.md.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/373:0,Update,Update,0,,https://github.com/google/deepvariant/pull/373,1,['Update'],['Update'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Update the README.md.
",False,
Deployability,Updated the introduction for the ML4H blog post.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/245:0,Update,Updated,0,,https://github.com/google/deepvariant/pull/245,1,['Update'],['Updated'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Updated the introduction for the ML4H blog post.
",False,
Deployability,Updates to shuffle_tfrecords_beam script for SparkRunner,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/365:0,Update,Updates,0,,https://github.com/google/deepvariant/pull/365,1,['Update'],['Updates'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Updates to shuffle_tfrecords_beam script for SparkRunner
",False,"The text is a commit message. It includes an action (updates) followed by the file name, which indicates it's related to code changes rather than meaningful content."
Deployability,Updates to underlying tools in docker,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/755:0,Update,Updates,0,,https://github.com/google/deepvariant/issues/755,1,['Update'],['Updates'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Updates to underlying tools in docker
",False,
Deployability,"Upon running dv_make_examples.py -h , dv_call_variants.py -h , or dv_postprocess_variants.py -h , python told me there is a syntax error around a f-string. **Setup**; - Operating system: Centos; - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): bioconda. **Steps to reproduce:**; - Command: dv_make_examples.py -h",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/627:227,Install,Installation,227,,https://github.com/google/deepvariant/issues/627,1,['Install'],['Installation'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Upon running dv_make_examples.py -h , dv_call_variants.py -h , or dv_postprocess_variants.py -h , python told me there is a syntax error around a f-string. **Setup**; - Operating system: Centos; - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): bioconda. **Steps to reproduce:**; - Command: dv_make_examples.py -h
",False,
Deployability,"Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; I0703 17:18:45.810746 140322304501504 data_providers.py:376] self.input_map_threads=48; W0703 17:18:45.810852 140322304501504 deprecation.py:323] From /tmp/Bazel.runfiles_m65fk12g/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; I0703 17:18:46.328745 140322304501504 estimator.py:1147] Calling model_fn.; W0703 17:18:46.330687 140322304501504 deprecation.py:323] From /tmp/Bazel.runfiles_m65fk12g/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Deprecated in favor of operator or tf.math.divide.; W0703 17:18:46.333346 140322304501504 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; I0703 17:18:50.712157 140322304501504 estimator.py:1149] Done calling model_fn.; I0703 17:18:51.788142 140322304501504 monitored_session.py:240] Graph was finalized.; ```; We are on Docker 19.03.11 on Debian 10 and executed deepvariant with ```docker run --gpus all ....``` (proposed way of using nvidia docker for Docker version >19.03 in the nvidia docker docs). Our installed Nvidia driver version is 418.113. Thanks in advance for your help,. Sebastian",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/321:6242,install,installed,6242,,https://github.com/google/deepvariant/issues/321,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; I0703 17:18:45.810746 140322304501504 data_providers.py:376] self.input_map_threads=48; W0703 17:18:45.810852 140322304501504 deprecation.py:323] From /tmp/Bazel.runfiles_m65fk12g/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; I0703 17:18:46.328745 140322304501504 estimator.py:1147] Calling model_fn.; W0703 17:18:46.330687 140322304501504 deprecation.py:323] From /tmp/Bazel.runfiles_m65fk12g/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Deprecated in favor of operator or tf.math.divide.; W0703 17:18:46.333346 140322304501504 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; I0703 17:18:50.712157 140322304501504 estimator.py:1149] Done calling model_fn.; I0703 17:18:51.788142 140322304501504 monitored_session.py:240] Graph was finalized.; ```; We are on Docker 19.03.11 on Debian 10 and executed deepvariant with ```docker run --gpus all ....``` (proposed way of using nvidia docker for Docker version >19.03 in the nvidia docker docs). Our installed Nvidia driver version is 418.113. Thanks in advance for your help,. Sebastian
",True,"The text consists primarily of program logs, error messages, and deprecation warnings from a Tensorflow or deep learning framework. It includes timestamps, log identifiers, and instructions related to code updates and optimizations."
Deployability,"WARE, EVEN IF ADVISED OF THE; # POSSIBILITY OF SUCH DAMAGE. # Source this file---these options are needed for TF config and for; # successive bazel runs. # Set this to 1 if the system image already has TensorFlow preinstalled. This; # will skip the installation of TensorFlow.; export DV_USE_PREINSTALLED_TF=""${DV_USE_PREINSTALLED_TF:-0}"". export TF_CUDA_CLANG=0; export TF_ENABLE_XLA=1; export TF_NEED_CUDA=1; export TF_NEED_GCP=1; export TF_NEED_GDR=0; export TF_NEED_HDFS=0; export TF_NEED_JEMALLOC=0; export TF_NEED_MKL=1; export TF_NEED_MPI=0; export TF_NEED_OPENCL=0; export TF_NEED_OPENCL_SYCL=0; export TF_NEED_S3=1; export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1; export TF_CUDA_VERSION=""10.0""; export CUDA_TOOLKIT_PATH=""/usr/local/cuda""; export TF_CUDNN_VERSION=""7""; export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant.; DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that; # `bazel` will find the latest version of bazel installed in the user's home; # directory. This is set in setting.sh as all DeepVariant scripts source; # settings.sh and assume that `bazel` will find the right version.; export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts.; export DEEPVARIANT_BUCKET=""gs://deepvariant""; export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages""; export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a; # named release version. Set it to an already existing value in the environment; # (allowing command line control of the build), defaulting to 0 (release build).; # Note that setting this to 1 implies that the C++ code in DeepVariant will be; # build using the master branch and not the pinned version to avoid; # incompatibilities between TensorFlow C++ used to build DeepVariant and the; # tf-nightly wheel.; ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145:3532,install,installed,3532,,https://github.com/google/deepvariant/issues/145,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: WARE, EVEN IF ADVISED OF THE; # POSSIBILITY OF SUCH DAMAGE. # Source this file---these options are needed for TF config and for; # successive bazel runs. # Set this to 1 if the system image already has TensorFlow preinstalled. This; # will skip the installation of TensorFlow.; export DV_USE_PREINSTALLED_TF=""${DV_USE_PREINSTALLED_TF:-0}"". export TF_CUDA_CLANG=0; export TF_ENABLE_XLA=1; export TF_NEED_CUDA=1; export TF_NEED_GCP=1; export TF_NEED_GDR=0; export TF_NEED_HDFS=0; export TF_NEED_JEMALLOC=0; export TF_NEED_MKL=1; export TF_NEED_MPI=0; export TF_NEED_OPENCL=0; export TF_NEED_OPENCL_SYCL=0; export TF_NEED_S3=1; export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1; export TF_CUDA_VERSION=""10.0""; export CUDA_TOOLKIT_PATH=""/usr/local/cuda""; export TF_CUDNN_VERSION=""7""; export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant.; DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that; # `bazel` will find the latest version of bazel installed in the user's home; # directory. This is set in setting.sh as all DeepVariant scripts source; # settings.sh and assume that `bazel` will find the right version.; export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts.; export DEEPVARIANT_BUCKET=""gs://deepvariant""; export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages""; export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a; # named release version. Set it to an already existing value in the environment; # (allowing command line control of the build), defaulting to 0 (release build).; # Note that setting this to 1 implies that the C++ code in DeepVariant will be; # build using the master branch and not the pinned version to avoid; # incompatibilities between TensorFlow C++ used to build DeepVariant and the; # tf-nightly wheel.; 
",True,"The text consists primarily of shell script commands, export statements, and configuration settings. It includes code snippets (export DV_USE_PREINSTALLED_TF etc.), variables, and setup instructions for a system configuration. There are no complete sentences or explanatory content; instead, it's focused on system configuration and variable exports which align with the evaluation criteria for elimination."
Deployability,"We are not taking pull requests at this time.; (Note: this PR is on the gh-pages branch, meant for an update for the goo.gl/deepvariant blog. It's submitted by @pichuan , a DeepVariant team member.)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/216:102,update,update,102,,https://github.com/google/deepvariant/pull/216,1,['update'],['update'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: We are not taking pull requests at this time.; (Note: this PR is on the gh-pages branch, meant for an update for the goo.gl/deepvariant blog. It's submitted by @pichuan , a DeepVariant team member.)
",False,"The text contains a complete, meaningful sentence discussing the state of pull requests and their purpose in the context of the project."
Deployability,We are not taking pull requests at this time.; This update is pushed by the team member (pichuan@) to the gh-pages for https://goo.gl/deepvariant.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/654:52,update,update,52,,https://github.com/google/deepvariant/pull/654,1,['update'],['update'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: We are not taking pull requests at this time.; This update is pushed by the team member (pichuan@) to the gh-pages for https://goo.gl/deepvariant.
",False,"This text contains a natural language sentence explaining that they are not accepting pull requests. It includes a human-readable explanation about their current state regarding pull requests, which is meaningful and relevant."
Deployability,"When I run build_and_test.sh, I get the following isssues; ```. Extracting Bazel installation...; .............................; (12:58:42) INFO: Current date is 2018-03-20; (13:01:02) ERROR: /home/vinay/deepvariant/deepvariant/BUILD:564:1: no such package '@org_tensorflow_slim//': Unexpected end of ZLIB input stream and referenced by '//deepvariant:modeling'; (13:01:02) ERROR: /home/vinay/deepvariant/deepvariant/BUILD:564:1: no such package '@org_tensorflow_slim//': Unexpected end of ZLIB input stream and referenced by '//deepvariant:modeling'; (13:01:02) ERROR: /home/vinay/deepvariant/deepvariant/BUILD:564:1: no such package '@org_tensorflow_slim//': Unexpected end of ZLIB input stream and referenced by '//deepvariant:modeling'; (13:01:03) ERROR: Analysis of target '//deepvariant:binaries' failed; build aborted: no such package '@org_tensorflow_slim//': Unexpected end of ZLIB input stream; (13:01:03) INFO: Elapsed time: 146.946s; (13:01:03) FAILED: Build did NOT complete successfully (60 packages loaded); Fetching https://mirror.bazel.build/ufpr.dl.sourceforge.net/project/giflib/giflib-5.1.4.tar.gz; 26,415b 43s; Fetching https://mirror.bazel.build/github.com/libjpeg-turbo/libjpeg-turbo/archive/1.5.1.tar.gz; 32,588b 42s; Fetching https://mirror.bazel.build/www.kurims.kyoto-u.ac.jp/~ooura/fft.tgz; 20,092b 40s; (13:01:03) ERROR: Couldn't start the build. Unable to run tests. ```. Please Help",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/59:81,install,installation,81,,https://github.com/google/deepvariant/issues/59,1,['install'],['installation'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: When I run build_and_test.sh, I get the following isssues; ```. Extracting Bazel installation...; .............................; (12:58:42) INFO: Current date is 2018-03-20; (13:01:02) ERROR: /home/vinay/deepvariant/deepvariant/BUILD:564:1: no such package '@org_tensorflow_slim//': Unexpected end of ZLIB input stream and referenced by '//deepvariant:modeling'; (13:01:02) ERROR: /home/vinay/deepvariant/deepvariant/BUILD:564:1: no such package '@org_tensorflow_slim//': Unexpected end of ZLIB input stream and referenced by '//deepvariant:modeling'; (13:01:02) ERROR: /home/vinay/deepvariant/deepvariant/BUILD:564:1: no such package '@org_tensorflow_slim//': Unexpected end of ZLIB input stream and referenced by '//deepvariant:modeling'; (13:01:03) ERROR: Analysis of target '//deepvariant:binaries' failed; build aborted: no such package '@org_tensorflow_slim//': Unexpected end of ZLIB input stream; (13:01:03) INFO: Elapsed time: 146.946s; (13:01:03) FAILED: Build did NOT complete successfully (60 packages loaded); Fetching https://mirror.bazel.build/ufpr.dl.sourceforge.net/project/giflib/giflib-5.1.4.tar.gz; 26,415b 43s; Fetching https://mirror.bazel.build/github.com/libjpeg-turbo/libjpeg-turbo/archive/1.5.1.tar.gz; 32,588b 42s; Fetching https://mirror.bazel.build/www.kurims.kyoto-u.ac.jp/~ooura/fft.tgz; 20,092b 40s; (13:01:03) ERROR: Couldn't start the build. Unable to run tests. ```. Please Help
",False,"The text contains error messages and logs from a build process, which are primarily code-related and should be filtered out. The content includes timestamps, error codes, stack traces, and commands related to the build and test process. These are indicative of program logs or compiler/interpreter output."
Deployability,When I run the pipeline on females I get lots of PASSED variants on chrY. Why is that?,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/196:15,pipeline,pipeline,15,,https://github.com/google/deepvariant/issues/196,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: When I run the pipeline on females I get lots of PASSED variants on chrY. Why is that?
",False,"The text contains a complete, meaningful sentence in natural language asking for an explanation."
Deployability,"When I tried to create a VM with 8 GPUs using this command line:; export IMAGE_FAMILY=""tf-latest-gpu""; export ZONE=""us-west1-a""; export INSTANCE_NAME=""deep""; export INSTANCE_TYPE=""n1-standard-8""; gcloud compute instances create $INSTANCE_NAME \; --zone=$ZONE \; --image-family=$IMAGE_FAMILY \; --image-project=deeplearning-platform-release \; --maintenance-policy=TERMINATE \; --accelerator=""type=nvidia-tesla-p100,count=8"" \; --machine-type=$INSTANCE_TYPE \; --boot-disk-size=200GB \; --metadata=""install-nvidia-driver=True"". I got this error:; ERROR: (gcloud.compute.instances.create) Could not fetch resource:; - Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '8'. Number of accelerator cards attached to an instance must be one of [1, 2, 4]. My Quota shows that I have access to 8 P100 GPUs:; ![screenshot from 2019-01-04 12-53-33](https://user-images.githubusercontent.com/19914123/50707923-cd8b4d80-101f-11e9-976d-b04df93f0f26.png)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/136:332,release,release,332,,https://github.com/google/deepvariant/issues/136,2,"['install', 'release']","['install-nvidia-driver', 'release']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: When I tried to create a VM with 8 GPUs using this command line:; export IMAGE_FAMILY=""tf-latest-gpu""; export ZONE=""us-west1-a""; export INSTANCE_NAME=""deep""; export INSTANCE_TYPE=""n1-standard-8""; gcloud compute instances create $INSTANCE_NAME \; --zone=$ZONE \; --image-family=$IMAGE_FAMILY \; --image-project=deeplearning-platform-release \; --maintenance-policy=TERMINATE \; --accelerator=""type=nvidia-tesla-p100,count=8"" \; --machine-type=$INSTANCE_TYPE \; --boot-disk-size=200GB \; --metadata=""install-nvidia-driver=True"". I got this error:; ERROR: (gcloud.compute.instances.create) Could not fetch resource:; - Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '8'. Number of accelerator cards attached to an instance must be one of [1, 2, 4]. My Quota shows that I have access to 8 P100 GPUs:; ![screenshot from 2019-01-04 12-53-33](https://user-images.githubusercontent.com/19914123/50707923-cd8b4d80-101f-11e9-976d-b04df93f0f26.png)
",False,"The text contains a detailed error message from a command line tool, including specific technical information and an image link. While the content is not purely code or logs, it provides information that is useful for debugging and understanding the issue."
Deployability,"When I used version 1.6.1 for source code compilation, an error related to the numpy library occurred. I suspect this is due to incompatibility with TensorFlow. I tried using other versions of the numpy library, but the issue persisted. ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Fri 02 Aug 2024 02:19:28 PM CST] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Fri 02 Aug 2024 02:19:28 PM CST] Stage 'Misc setup' starting; ========== [Fri 02 Aug 2024 02:20:04 PM CST] Stage 'Update package list' starting; ========== [Fri 02 Aug 2024 02:20:06 PM CST] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Fri 02 Aug 2024 02:20:10 PM CST] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2213k 100 2213k 0 0 1634k 0 0:00:01 0:00:01 --:--:-- 1634k; Collecting pip; Using cached pip-24.2-py3-none-any.whl.metadata (3.6 kB); Using cached pip-24.2-py3-none-any.whl (1.8 MB); Installing collected packages: pip; WARNING: The scripts pip, pip3 and pip3.10 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-24.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning. [notice] A new release of pip is available: 24.0 -> 24.2; [notice] To update, run: pip install --upgrade pip; Python 3.10.14; pi",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:403,Install,Install,403,,https://github.com/google/deepvariant/issues/859,4,"['Install', 'Update']","['Install', 'Update']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: When I used version 1.6.1 for source code compilation, an error related to the numpy library occurred. I suspect this is due to incompatibility with TensorFlow. I tried using other versions of the numpy library, but the issue persisted. ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Fri 02 Aug 2024 02:19:28 PM CST] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Fri 02 Aug 2024 02:19:28 PM CST] Stage 'Misc setup' starting; ========== [Fri 02 Aug 2024 02:20:04 PM CST] Stage 'Update package list' starting; ========== [Fri 02 Aug 2024 02:20:06 PM CST] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Fri 02 Aug 2024 02:20:10 PM CST] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2213k 100 2213k 0 0 1634k 0 0:00:01 0:00:01 --:--:-- 1634k; Collecting pip; Using cached pip-24.2-py3-none-any.whl.metadata (3.6 kB); Using cached pip-24.2-py3-none-any.whl (1.8 MB); Installing collected packages: pip; WARNING: The scripts pip, pip3 and pip3.10 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-24.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning. [notice] A new release of pip is available: 24.0 -> 24.2; [notice] To update, run: pip install --upgrade pip; Python 3.10.14; pi
",False,"The text contains a mixture of descriptive sentences about error handling and script execution along with some code-like outputs. While there are logs and API documentation elements, the presence of meaningful human-readable content makes it not purely code or logs."
Deployability,"When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:; `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine.; /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**; - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa); - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: docker run google/deepvariant:1.4.0; - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine.; /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; CPU information from /proc/cpuinfo; product: Common KVM processor; vendor: Intel Corp.; physical id: 2; bus info: cpu@1; width: 64 bits; capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552:428,Install,Installation,428,,https://github.com/google/deepvariant/issues/552,1,['Install'],['Installation'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:; `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine.; /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**; - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa); - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: docker run google/deepvariant:1.4.0; - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine.; /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; CPU information from /proc/cpuinfo; product: Common KVM processor; vendor: Intel Corp.; physical id: 2; bus info: cpu@1; width: 64 bits; capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti
",True,"The content consists primarily of program logs or error messages (timestamps, error codes, stack traces) and configuration information."
Deployability,"With the release of version 1.0.0, it is stated that DeepVariant will not support somatic variant calling because the only genotypes supported are hom-alt, het, and hom-ref. . Is there the potential for a deep variant somatic variant caller in the future? ; or ; Could individuals produce somatic variants using a matched-normal approach? ie by calling variants on the germline and tumor(s) and extracting variants found in the tumor only?; Thank you for your time",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/351:9,release,release,9,,https://github.com/google/deepvariant/issues/351,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: With the release of version 1.0.0, it is stated that DeepVariant will not support somatic variant calling because the only genotypes supported are hom-alt, het, and hom-ref. . Is there the potential for a deep variant somatic variant caller in the future? ; or ; Could individuals produce somatic variants using a matched-normal approach? ie by calling variants on the germline and tumor(s) and extracting variants found in the tumor only?; Thank you for your time
",False,"The text contains complete, meaningful sentences discussing concepts related to variant calling and potential future developments."
Deployability,YTHON_BIN_PATH=/usr/bin/python; ++ PYTHON_BIN_PATH=/usr/bin/python; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; + bazel; build_and_test.sh: line 39: bazel: command not found; + PATH=/home/solokopi/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/...; Unexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ . solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ sudo bash build-prereq.sh; [sudo] password for solokopi: ; ========== Load config settings.; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:30:03 CST] Stage 'Install the runtime packages' starting; ========== Load config settings.; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:30:03 CST] Stage 'Misc setup' starting; Hit:1 http://mirrors.aliyun.com/ubuntu xenial InRelease; Hit:2 http://mirrors.aliyun.com/ubuntu xenial-updates InRelease ; Get:3 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB] ; Ign:4 http://dl.google.com/linux/chrome/deb stable InRelease ; Hit:5 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Hit:6 http://dl.google.com/linux/chrome/deb stable Release ; Hit:7 http://mirrors.aliyun.com/ubuntu xenial-security InRelease ; Err:9 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:802::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:802::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:3903,Install,Install,3903,,https://github.com/google/deepvariant/issues/89,1,['Install'],['Install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: YTHON_BIN_PATH=/usr/bin/python; ++ PYTHON_BIN_PATH=/usr/bin/python; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; + bazel; build_and_test.sh: line 39: bazel: command not found; + PATH=/home/solokopi/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/...; Unexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ . solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ sudo bash build-prereq.sh; [sudo] password for solokopi: ; ========== Load config settings.; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:30:03 CST] Stage 'Install the runtime packages' starting; ========== Load config settings.; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:30:03 CST] Stage 'Misc setup' starting; Hit:1 http://mirrors.aliyun.com/ubuntu xenial InRelease; Hit:2 http://mirrors.aliyun.com/ubuntu xenial-updates InRelease ; Get:3 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB] ; Ign:4 http://dl.google.com/linux/chrome/deb stable InRelease ; Hit:5 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Hit:6 http://dl.google.com/linux/chrome/deb stable Release ; Hit:7 http://mirrors.aliyun.com/ubuntu xenial-security InRelease ; Err:9 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:802::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:802::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has 
",False,
Deployability,_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; + bazel; build_and_test.sh: line 39: bazel: command not found; + PATH=/home/solokopi/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/...; Unexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ . solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ sudo bash build-prereq.sh; [sudo] password for solokopi: ; ========== Load config settings.; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:30:03 CST] Stage 'Install the runtime packages' starting; ========== Load config settings.; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:30:03 CST] Stage 'Misc setup' starting; Hit:1 http://mirrors.aliyun.com/ubuntu xenial InRelease; Hit:2 http://mirrors.aliyun.com/ubuntu xenial-updates InRelease ; Get:3 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB] ; Ign:4 http://dl.google.com/linux/chrome/deb stable InRelease ; Hit:5 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Hit:6 http://dl.google.com/linux/chrome/deb stable Release ; Hit:7 http://mirrors.aliyun.com/ubuntu xenial-security InRelease ; Err:9 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:802::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:802::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Pac,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:4153,update,updates,4153,,https://github.com/google/deepvariant/issues/89,1,['update'],['updates'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: _FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; + bazel; build_and_test.sh: line 39: bazel: command not found; + PATH=/home/solokopi/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/...; Unexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ . solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ sudo bash build-prereq.sh; [sudo] password for solokopi: ; ========== Load config settings.; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:30:03 CST] Stage 'Install the runtime packages' starting; ========== Load config settings.; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:30:03 CST] Stage 'Misc setup' starting; Hit:1 http://mirrors.aliyun.com/ubuntu xenial InRelease; Hit:2 http://mirrors.aliyun.com/ubuntu xenial-updates InRelease ; Get:3 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB] ; Ign:4 http://dl.google.com/linux/chrome/deb stable InRelease ; Hit:5 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Hit:6 http://dl.google.com/linux/chrome/deb stable Release ; Hit:7 http://mirrors.aliyun.com/ubuntu xenial-security InRelease ; Err:9 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:802::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:802::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Pac
",False,"The text includes a mix of command outputs, logs, error messages, and configuration file contents. While there are some natural language sentences present, the majority consists of program output which should be filtered out."
Deployability,"_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --postprocess_variants_disk_gb 200 \; --gcsfuse ""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions europe-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". And i get the following error:. 07:03:22 Stopped running ""-c timeout=10; elapsed=0; seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""mkdir -p ./input-gcsfused-{} && gcsfuse --implicit-dirs \""${GCS_BUCKET}\"" /input-gcsfused-{}\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""until mountpoint -q /input-gcsfused-{}; do test \""${elapsed}\"" -lt \""${timeout}\"" || fail \""Time out waiting for gcsfuse mount points\""; sleep 1; elapsed=$((elapsed+1)); done\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""/opt/deepvariant/bin/make_examples --mode calling --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --reads \""/input-gcsfused-{}/${BAM}\"" --ref \""${INPUT_REF}\"" --task {} --gvcf \""${GVCF}\""/gvcf_output.tfrecord@\""${SHARDS}\"".gz\"""": exit status 127: bash: gcsfuse: command not found. Is it possible to identify the problem/typo?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/214:1636,pipeline,pipelines,1636,,https://github.com/google/deepvariant/issues/214,1,['pipeline'],['pipelines'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: _IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --postprocess_variants_disk_gb 200 \; --gcsfuse ""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions europe-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". And i get the following error:. 07:03:22 Stopped running ""-c timeout=10; elapsed=0; seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""mkdir -p ./input-gcsfused-{} && gcsfuse --implicit-dirs \""${GCS_BUCKET}\"" /input-gcsfused-{}\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""until mountpoint -q /input-gcsfused-{}; do test \""${elapsed}\"" -lt \""${timeout}\"" || fail \""Time out waiting for gcsfuse mount points\""; sleep 1; elapsed=$((elapsed+1)); done\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""/opt/deepvariant/bin/make_examples --mode calling --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --reads \""/input-gcsfused-{}/${BAM}\"" --ref \""${INPUT_REF}\"" --task {} --gvcf \""${GVCF}\""/gvcf_output.tfrecord@\""${SHARDS}\"".gz\"""": exit status 127: bash: gcsfuse: command not found. Is it possible to identify the problem/typo?
",False,"The text contains a detailed error message indicating that 'gcsfuse' was not found, which is a specific issue. However, this message includes natural language explaining the problem and attempting to help resolve it ('Is it possible to identify the problem/typo?'). Therefore, it's meaningful content."
Deployability,"_postproc.py"", line 154, in _raw_next; not_done = self._cc_iterable.PythonNext(record); RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/DeepVariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /work/cjm124/SWFst/DeepVariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/make_examples.tfrecord@12.gz --channels insert_size --gvcf /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@12.gz --regions chr20:10,000,000-10,010,000 --task 0; ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```; singularity exec DeepVariant_1.6.1.sif bash; pip install --user google-nucleus; run_deepvariant --model_type=WGS \; 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; 	--regions ""chr20:10,000,000-10,010,000"" \; 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; 	--num_shards=12; ```. Error:. ```; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>; import tensorflow as tf; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>; from tensorflow.python.tools import module_util as _module_util; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>; from tensorflow.python.ea",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812:4524,install,installing,4524,,https://github.com/google/deepvariant/issues/812,1,['install'],['installing'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: _postproc.py"", line 154, in _raw_next; not_done = self._cc_iterable.PythonNext(record); RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/DeepVariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /work/cjm124/SWFst/DeepVariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/make_examples.tfrecord@12.gz --channels insert_size --gvcf /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@12.gz --regions chr20:10,000,000-10,010,000 --task 0; ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```; singularity exec DeepVariant_1.6.1.sif bash; pip install --user google-nucleus; run_deepvariant --model_type=WGS \; 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; 	--regions ""chr20:10,000,000-10,010,000"" \; 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; 	--num_shards=12; ```. Error:. ```; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>; import tensorflow as tf; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>; from tensorflow.python.tools import module_util as _module_util; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>; from tensorflow.python.ea
",False,
Deployability,"_type: zero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}; ##bcftools_viewVersion=1.10.2+htslib-1.10.2; ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022; ```. ### Variant line; ```; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband; X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:..; ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:; Pipeline; ```; # Load singularity; module load singularity; BIN_VERSION=""1.1.0"". # Load env for bcftools; ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/; source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh; conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped; export SINGULARITY_CACHEDIR=$PWD; singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Number of threads; NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered); cd $SLURM_SUBMIT_DIR; WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis/. ## Set working space; mkdir -p $WORKING_DIR; cd $WORKING_DIR. #### GRCh38 #### ; echo ""GRCh38 genome""; GENOME=GRCh38; FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/; FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS; BAM_DIR=$WORKING_DIR; Case_ID=Case1; FAMILY_ID=$Case_I",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518:6826,pipeline,pipeline,6826,,https://github.com/google/deepvariant/issues/518,2,"['Pipeline', 'pipeline']","['Pipeline', 'pipeline']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: _type: zero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}; ##bcftools_viewVersion=1.10.2+htslib-1.10.2; ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022; ```. ### Variant line; ```; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband; X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:..; ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:; Pipeline; ```; # Load singularity; module load singularity; BIN_VERSION=""1.1.0"". # Load env for bcftools; ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/; source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh; conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped; export SINGULARITY_CACHEDIR=$PWD; singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Number of threads; NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered); cd $SLURM_SUBMIT_DIR; WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis/. ## Set working space; mkdir -p $WORKING_DIR; cd $WORKING_DIR. #### GRCh38 #### ; echo ""GRCh38 genome""; GENOME=GRCh38; FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/; FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS; BAM_DIR=$WORKING_DIR; Case_ID=Case1; FAMILY_ID=$Case_I
",False,
Deployability,"`javascript; EXAMPLES=""/home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/NA12878_BGISEQ_PE150_5.training.examples.tfrecord-?????-of-00038.gz""; echo ""${EXAMPLES}""; OUTPUT_DIR=${EXAMPLES%/*} ; time python ./shuffle_tfrecords_beam.py ; --input_pattern_list=""${EXAMPLES}""; --output_pattern_prefix=""${OUTPUT_DIR}/training_set.with_label.shuffled"" ; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" ; --output_dataset_name=""HG001""; --runner=BundleBasedDirectRunner; ```; the error message:; ```; /home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/sample.training.examples.tfrecord-?????-of-00076.gz; /home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/direct_runner.py:342: DeprecationWarning: options is deprecated since First stable release.. References to <pipeline>.options will not be supported; pipeline.replace_all(_get_transform_overrides(pipeline.options)); INFO:root:Running pipeline with DirectRunner.; WARNING:root:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.; ERROR:root:Exception at bundle <apache_beam.runners.direct.bundle_factory._Bundle object at 0x7f86daaa07e8>, due to an exception.; Traceback (most recent call last):; File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 341, in call; finish_state); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 381, in attempt_call; result = evaluator.finish_bundle(); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/transform_evaluator.py"", line 303, in finish_bundle; bundles = _read_values_to_bundles(reader); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/transform_evaluator.py"", line 293, in _read_values_to_bundles; read_res",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/91:1361,pipeline,pipeline,1361,,https://github.com/google/deepvariant/issues/91,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: `javascript; EXAMPLES=""/home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/NA12878_BGISEQ_PE150_5.training.examples.tfrecord-?????-of-00038.gz""; echo ""${EXAMPLES}""; OUTPUT_DIR=${EXAMPLES%/*} ; time python ./shuffle_tfrecords_beam.py ; --input_pattern_list=""${EXAMPLES}""; --output_pattern_prefix=""${OUTPUT_DIR}/training_set.with_label.shuffled"" ; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" ; --output_dataset_name=""HG001""; --runner=BundleBasedDirectRunner; ```; the error message:; ```; /home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/sample.training.examples.tfrecord-?????-of-00076.gz; /home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/direct_runner.py:342: DeprecationWarning: options is deprecated since First stable release.. References to <pipeline>.options will not be supported; pipeline.replace_all(_get_transform_overrides(pipeline.options)); INFO:root:Running pipeline with DirectRunner.; WARNING:root:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.; ERROR:root:Exception at bundle <apache_beam.runners.direct.bundle_factory._Bundle object at 0x7f86daaa07e8>, due to an exception.; Traceback (most recent call last):; File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 341, in call; finish_state); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 381, in attempt_call; result = evaluator.finish_bundle(); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/transform_evaluator.py"", line 303, in finish_bundle; bundles = _read_values_to_bundles(reader); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/transform_evaluator.py"", line 293, in _read_values_to_bundles; read_res
",False,This text consists of code snippets and logs which are primarily programmatic and not meant for human reading.
Deployability,"a.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ; ; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/exceptions.py"", line 640, in conda_exception_handler; return_value = func(*args, **kwargs); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main.py"", line 140, in _main; exit_code = args.func(args, p); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main_install.py"", line 80, in execute; install(args, parser, 'install'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/install.py"", line 326, in install; execute_actions(actions, index, verbose=not context.quiet); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/plan.py"", line 828, in execute_actions; execute_instructions(plan, index, verbose); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 247, in execute_instructions; cmd(state, arg); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 108, in UNLINKLINKTRANSACTION_CMD; txn.execute(); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 297, in execute; rollback_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-po",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252:3077,install,install,3077,,https://github.com/google/deepvariant/issues/252,1,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: a.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ; ; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/exceptions.py"", line 640, in conda_exception_handler; return_value = func(*args, **kwargs); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main.py"", line 140, in _main; exit_code = args.func(args, p); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main_install.py"", line 80, in execute; install(args, parser, 'install'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/install.py"", line 326, in install; execute_actions(actions, index, verbose=not context.quiet); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/plan.py"", line 828, in execute_actions; execute_instructions(plan, index, verbose); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 247, in execute_instructions; cmd(state, arg); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 108, in UNLINKLINKTRANSACTION_CMD; txn.execute(); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 297, in execute; rollback_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-po
",True,"The text consists primarily of an error message from a program, including details like timestamps, tracebacks, and specific commands. This is indicative of program logs or error messages which should be filtered out."
Deployability,a/ubuntu bionic InRelease [20.8 kB]; Get:3 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB]; Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]; Get:6 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic/main amd64 Packages [19.3 kB]; Get:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]; Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]; Get:9 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]; Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [638 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2801 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:4504,update,updates,4504,,https://github.com/google/deepvariant/issues/489,1,['update'],['updates'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: a/ubuntu bionic InRelease [20.8 kB]; Get:3 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB]; Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]; Get:6 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic/main amd64 Packages [19.3 kB]; Get:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]; Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]; Get:9 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]; Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [638 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2801 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; 
",False,
Deployability,"ach memory. It is out of memory of my machine. My machine have 376G memory. Here is my command and the error log.; Command:; ```javascript; EXAMPLES=""/home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/NA12878_BGISEQ_PE150_5.training.examples.tfrecord-?????-of-00038.gz""; echo ""${EXAMPLES}""; OUTPUT_DIR=${EXAMPLES%/*} ; time python ./shuffle_tfrecords_beam.py ; --input_pattern_list=""${EXAMPLES}""; --output_pattern_prefix=""${OUTPUT_DIR}/training_set.with_label.shuffled"" ; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" ; --output_dataset_name=""HG001""; --runner=BundleBasedDirectRunner; ```; the error message:; ```; /home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/sample.training.examples.tfrecord-?????-of-00076.gz; /home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/direct_runner.py:342: DeprecationWarning: options is deprecated since First stable release.. References to <pipeline>.options will not be supported; pipeline.replace_all(_get_transform_overrides(pipeline.options)); INFO:root:Running pipeline with DirectRunner.; WARNING:root:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.; ERROR:root:Exception at bundle <apache_beam.runners.direct.bundle_factory._Bundle object at 0x7f86daaa07e8>, due to an exception.; Traceback (most recent call last):; File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 341, in call; finish_state); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 381, in attempt_call; result = evaluator.finish_bundle(); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/transform_evaluator.py"", line 303, in finish_bundle; bundles = _read_values_to_bundles(reader); File ""/home/suanfa/virtualenv_beam/local",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/91:1236,pipeline,pipeline,1236,,https://github.com/google/deepvariant/issues/91,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ach memory. It is out of memory of my machine. My machine have 376G memory. Here is my command and the error log.; Command:; ```javascript; EXAMPLES=""/home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/NA12878_BGISEQ_PE150_5.training.examples.tfrecord-?????-of-00038.gz""; echo ""${EXAMPLES}""; OUTPUT_DIR=${EXAMPLES%/*} ; time python ./shuffle_tfrecords_beam.py ; --input_pattern_list=""${EXAMPLES}""; --output_pattern_prefix=""${OUTPUT_DIR}/training_set.with_label.shuffled"" ; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" ; --output_dataset_name=""HG001""; --runner=BundleBasedDirectRunner; ```; the error message:; ```; /home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/sample.training.examples.tfrecord-?????-of-00076.gz; /home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/direct_runner.py:342: DeprecationWarning: options is deprecated since First stable release.. References to <pipeline>.options will not be supported; pipeline.replace_all(_get_transform_overrides(pipeline.options)); INFO:root:Running pipeline with DirectRunner.; WARNING:root:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.; ERROR:root:Exception at bundle <apache_beam.runners.direct.bundle_factory._Bundle object at 0x7f86daaa07e8>, due to an exception.; Traceback (most recent call last):; File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 341, in call; finish_state); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 381, in attempt_call; result = evaluator.finish_bundle(); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/transform_evaluator.py"", line 303, in finish_bundle; bundles = _read_values_to_bundles(reader); File ""/home/suanfa/virtualenv_beam/local
",True,"The text consists primarily of a command and its associated error log, which are code snippets and logs. It includes specific paths and commands that appear to be part of a script or program, along with an error message indicating issues with the implementation such as deprecation warnings and exceptions."
Deployability,"ages (1.11.0); Requirement already satisfied: sklearn in /usr/local/lib/python2.7/dist-packages (0.0); Requirement already satisfied: scikit-learn in /usr/local/lib/python2.7/dist-packages (from sklearn) (0.19.2); Requirement already satisfied: pandas in /usr/local/lib/python2.7/dist-packages (0.23.4); Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (1.14.0); Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (2.7.3); Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas) (2018.5); Requirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0); Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.7); Requirement already up-to-date: google-api-python-client in /usr/local/lib/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/d",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:17071,upgrade,upgrade,17071,,https://github.com/google/deepvariant/issues/89,1,['upgrade'],['upgrade'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ages (1.11.0); Requirement already satisfied: sklearn in /usr/local/lib/python2.7/dist-packages (0.0); Requirement already satisfied: scikit-learn in /usr/local/lib/python2.7/dist-packages (from sklearn) (0.19.2); Requirement already satisfied: pandas in /usr/local/lib/python2.7/dist-packages (0.23.4); Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (1.14.0); Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (2.7.3); Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas) (2018.5); Requirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0); Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.7); Requirement already up-to-date: google-api-python-client in /usr/local/lib/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/d
",False,
Deployability,"ain(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. Here is the supported CPU instructions of host:. ```sh; # cat /proc/cpuinfo; processor : 0; vendor_id : GenuineIntel; cpu family : 6; model : 30; model name : Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz; stepping : 5; microcode : 0xa; cpu MHz : 1197.018; cache size : 8192 KB; physical id : 0; siblings : 8; core id : 0; cpu cores : 4; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 11; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid dtherm ida flush_l1d; bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs; bogomips : 5851.92; clflush size : 64; cache_alignment : 64; address sizes : 36 bits physical, 48 bits virtual; power management:; ```. I also tried create a env & install on host by `conda install -c bioconda deepvariant`, but it pop-up the same error.; And Deepvariant v0.10.0 also have the same error. Please kindly give me some advice about this thank you. Best,; Jerry",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/345:4454,install,install,4454,,https://github.com/google/deepvariant/issues/345,2,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ain(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. Here is the supported CPU instructions of host:. ```sh; # cat /proc/cpuinfo; processor : 0; vendor_id : GenuineIntel; cpu family : 6; model : 30; model name : Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz; stepping : 5; microcode : 0xa; cpu MHz : 1197.018; cache size : 8192 KB; physical id : 0; siblings : 8; core id : 0; cpu cores : 4; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 11; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid dtherm ida flush_l1d; bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs; bogomips : 5851.92; clflush size : 64; cache_alignment : 64; address sizes : 36 bits physical, 48 bits virtual; power management:; ```. I also tried create a env & install on host by `conda install -c bioconda deepvariant`, but it pop-up the same error.; And Deepvariant v0.10.0 also have the same error. Please kindly give me some advice about this thank you. Best,; Jerry
",False,"The text contains a mix of natural language and code or command snippets. However, it primarily consists of user error messages related to software installation and usage, which include commands and logs but also contain explanatory text. The evaluation criteria mention that such logs and API descriptions should be filtered out, but since the text is explaining an issue and includes human-readable content discussing problems and their solutions, it should not be eliminated."
Deployability,"alled; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)...; 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK;",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:2110,install,installed,2110,,https://github.com/google/deepvariant/issues/489,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: alled; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)...; 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK;
",False,
Deployability,"also happens if you use ""chr20"" for a BAM where contig names don't have ""chr""s (or vice versa).; ```. Then I try to downgrade intervaltree from 3.0.2 to 2.1.0. . ```; chungtsai_su@seqslab:~/src/deepvariant$ pip show intervaltree; Name: intervaltree; Version: 3.0.2; Summary: Editable interval tree data structure for Python 2 and 3; Home-page: https://github.com/chaimleib/intervaltree; Author: Chaim Leib Halbert, Konstantin Tretyakov; Author-email: chaim.leib.halbert@gmail.com; License: Apache License, Version 2.0; Location: /home/chungtsai_su/.local/lib/python2.7/site-packages; Requires: sortedcontainers; Required-by:; chungtsai_su@seqslab:~/quickstart-output$ pip uninstall intervaltree; Uninstalling intervaltree-3.0.2:; Would remove:; /home/chungtsai_su/.local/lib/python2.7/site-packages/intervaltree-3.0.2.dist-info/*; /home/chungtsai_su/.local/lib/python2.7/site-packages/intervaltree/*; Proceed (y/n)? Y; Successfully uninstalled intervaltree-3.0.2; chungtsai_su@seqslab:~/src/deepvariant$ pip install --user 'intervaltree==2.1.0'; Collecting intervaltree==2.1.0; Requirement already satisfied: sortedcontainers in /home/chungtsai_su/.local/lib/python2.7/site-packages (from intervaltree==2.1.0) (2.1.0); Installing collected packages: intervaltree; Successfully installed intervaltree-2.1.0; ```; Then the problem is solved. ; ```; chungtsai_su@seqslab:~/src/deepvariant$ ./bazel-bin/deepvariant/make_examples --mode calling --ref ""${REF}"" --reads ""${BAM}"" --regions ""chr20:10,000,000-10,010,000"" --examples ""${OUTPUT_DIR}/examples.tfrecord.gz""; 2018-12-20 07:17:31.678190: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring:; I1220 07:17:31.678396 140029649073920 genomics_reader.py:213] Reading /home/chungtsai_su/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1220 07:17:31.681643 140029649073920 make_examples.py:1080] Preparing inputs; 2018-12-20 07:17:31.682071: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized S",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/131:3083,install,install,3083,,https://github.com/google/deepvariant/issues/131,1,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: also happens if you use ""chr20"" for a BAM where contig names don't have ""chr""s (or vice versa).; ```. Then I try to downgrade intervaltree from 3.0.2 to 2.1.0. . ```; chungtsai_su@seqslab:~/src/deepvariant$ pip show intervaltree; Name: intervaltree; Version: 3.0.2; Summary: Editable interval tree data structure for Python 2 and 3; Home-page: https://github.com/chaimleib/intervaltree; Author: Chaim Leib Halbert, Konstantin Tretyakov; Author-email: chaim.leib.halbert@gmail.com; License: Apache License, Version 2.0; Location: /home/chungtsai_su/.local/lib/python2.7/site-packages; Requires: sortedcontainers; Required-by:; chungtsai_su@seqslab:~/quickstart-output$ pip uninstall intervaltree; Uninstalling intervaltree-3.0.2:; Would remove:; /home/chungtsai_su/.local/lib/python2.7/site-packages/intervaltree-3.0.2.dist-info/*; /home/chungtsai_su/.local/lib/python2.7/site-packages/intervaltree/*; Proceed (y/n)? Y; Successfully uninstalled intervaltree-3.0.2; chungtsai_su@seqslab:~/src/deepvariant$ pip install --user 'intervaltree==2.1.0'; Collecting intervaltree==2.1.0; Requirement already satisfied: sortedcontainers in /home/chungtsai_su/.local/lib/python2.7/site-packages (from intervaltree==2.1.0) (2.1.0); Installing collected packages: intervaltree; Successfully installed intervaltree-2.1.0; ```; Then the problem is solved. ; ```; chungtsai_su@seqslab:~/src/deepvariant$ ./bazel-bin/deepvariant/make_examples --mode calling --ref ""${REF}"" --reads ""${BAM}"" --regions ""chr20:10,000,000-10,010,000"" --examples ""${OUTPUT_DIR}/examples.tfrecord.gz""; 2018-12-20 07:17:31.678190: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring:; I1220 07:17:31.678396 140029649073920 genomics_reader.py:213] Reading /home/chungtsai_su/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1220 07:17:31.681643 140029649073920 make_examples.py:1080] Preparing inputs; 2018-12-20 07:17:31.682071: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized S
",True,"The text contains command outputs and logs which are primarily related to package installations, version control, and error messages. These logs include pip show, pip uninstall, pip install commands and their outputs, as well as Bazel build commands with warnings and information. Such content is typically generated by scripts or tools and not intended for human reading as narrative text."
Deployability,"arallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/public2/courses/ec3121/shareddata/Pomacea_canaliculata/refgenome/GCF_003073045.1_ASM307304v1_genomic.fna"" --reads ""/public2/courses/ec3121/shareddata/Pomacea_canaliculata/wgs/FSL10-M.bam"" --examples ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/make_examples.tfrecord@2.gz"" --channels ""insert_size"" --gvcf ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/gvcf.tfrecord@2.gz"" --regions ""NC_037590.1:200,000-950,000"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LC_CTYPE = ""C.UTF-8"",; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LC_CTYPE = ""C.UTF-8"",; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; 2024-01-05 15:53:39.096475: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 15:53:39.096611: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-01-05 15:53:39.226747: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:4480,install,installed,4480,,https://github.com/google/deepvariant/issues/761,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: arallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/public2/courses/ec3121/shareddata/Pomacea_canaliculata/refgenome/GCF_003073045.1_ASM307304v1_genomic.fna"" --reads ""/public2/courses/ec3121/shareddata/Pomacea_canaliculata/wgs/FSL10-M.bam"" --examples ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/make_examples.tfrecord@2.gz"" --channels ""insert_size"" --gvcf ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/gvcf.tfrecord@2.gz"" --regions ""NC_037590.1:200,000-950,000"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LC_CTYPE = ""C.UTF-8"",; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LC_CTYPE = ""C.UTF-8"",; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; 2024-01-05 15:53:39.096475: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 15:53:39.096611: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-01-05 15:53:39.226747: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.
",True,"The text consists of logs from a program, including timestamps, warnings about missing libraries, and error messages related to TensorRT and CUDA. These are indicative of program output rather than meaningful human-readable content."
Deployability,"ate mapped to a different chr; 0 + 0 with mate mapped to a different chr (mapQ>=5); ```. ; **Steps to reproduce:**; - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :; ; ```; E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'; 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0; 2023-06-20 22:48:40.272881: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX; PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M; 2023-06-20 22:48:40.272889: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRA",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666:2881,pipeline,pipeline,2881,,https://github.com/google/deepvariant/issues/666,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ate mapped to a different chr; 0 + 0 with mate mapped to a different chr (mapQ>=5); ```. ; **Steps to reproduce:**; - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :; ; ```; E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'; 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0; 2023-06-20 22:48:40.272881: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX; PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M; 2023-06-20 22:48:40.272889: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRA
",False,"The text contains a detailed error message and logs which include technical details, timestamps, and warnings. It also includes commands for reproducing the issue and steps to follow. While there are some sentences in natural language, the majority of the content is programmatic logs or error messages that should be filtered out."
Deployability,"ate_variables(); ; self._bitgen.state = <void *>&self.rng_state; self._bitgen.next_uint64 = &philox_uint64; ^; ------------------------------------------------------------; ; _philox.pyx:195:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to the type of the value being assigned.; Processing numpy/random/_bounded_integers.pxd.in; Processing numpy/random/_common.pyx; Processing numpy/random/_philox.pyx; Traceback (most recent call last):; File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 235, in <module>; main(); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 231, in main; find_process_files(root_dir); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 222, in find_process_files; process(root_dir, fromfile, tofile, function, hash_db); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 188, in process; processor_function(fromfile, tofile); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 77, in process_pyx; subprocess.check_call(; File ""/public/home/zhanghl3/miniconda3/envs/deepvariant/lib/python3.10/subprocess.py"", line 369, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['/public/home/zhanghl3/miniconda3/envs/deepvariant/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_philox.c', '_philox.pyx']' returned non-zero exit status 1.; Cythonizing sources; Traceback (most recent call last):; File ""/usr/local/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>; main(); File ""/usr/local/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main; json_out['return_val'] = ho",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:9246,install,install-,9246,,https://github.com/google/deepvariant/issues/859,1,['install'],['install-'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ate_variables(); ; self._bitgen.state = <void *>&self.rng_state; self._bitgen.next_uint64 = &philox_uint64; ^; ------------------------------------------------------------; ; _philox.pyx:195:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to the type of the value being assigned.; Processing numpy/random/_bounded_integers.pxd.in; Processing numpy/random/_common.pyx; Processing numpy/random/_philox.pyx; Traceback (most recent call last):; File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 235, in <module>; main(); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 231, in main; find_process_files(root_dir); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 222, in find_process_files; process(root_dir, fromfile, tofile, function, hash_db); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 188, in process; processor_function(fromfile, tofile); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 77, in process_pyx; subprocess.check_call(; File ""/public/home/zhanghl3/miniconda3/envs/deepvariant/lib/python3.10/subprocess.py"", line 369, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['/public/home/zhanghl3/miniconda3/envs/deepvariant/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_philox.c', '_philox.pyx']' returned non-zero exit status 1.; Cythonizing sources; Traceback (most recent call last):; File ""/usr/local/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>; main(); File ""/usr/local/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main; json_out['return_val'] = ho
",True,"The text is a series of error messages and logs from a build process. It includes traces of stack traces, timestamps, and command outputs which are typical signs of program compilation or execution errors."
Deployability,"b/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:15 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf-nightly-gpu as it is not installed.; Skipping tensorflow-gpu as it is not installed.; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:03:04 --:--:-- 0; curl: (56)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:18007,upgrade,upgrade,18007,,https://github.com/google/deepvariant/issues/89,1,['upgrade'],['upgrade'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: b/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:15 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf-nightly-gpu as it is not installed.; Skipping tensorflow-gpu as it is not installed.; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:03:04 --:--:-- 0; curl: (56)
",True,"The text is a log output from an installation process, which includes detailed package information and command outputs. This is typically considered programmatic output rather than meaningful human-readable content."
Deployability,binary releases on github,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/8:7,release,releases,7,,https://github.com/google/deepvariant/issues/8,1,['release'],['releases'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: binary releases on github
",False,
Deployability,bioconda installation v0.7.2,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137:9,install,installation,9,,https://github.com/google/deepvariant/issues/137,1,['install'],['installation'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: bioconda installation v0.7.2
",False,The text is a version identifier and does not contain any meaningful human-readable sentences.
Deployability,"ble; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http:/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:1517,install,installed,1517,,https://github.com/google/deepvariant/issues/489,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ble; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http:/
",True,"The text consists primarily of program logs or error messages, including dependency lists and unresolved package issues, which are typically found in log files or build system outputs."
Deployability,"buntu.com/ubuntu focal-security InRelease; Get:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease [5526 B]; Get:6 https://apt.llvm.org/focal llvm-toolchain-focal-11/main amd64 Packages [9008 B]; Fetched 14.5 kB in 13s (1133 B/s); Reading package lists...; + apt-get update -qq -y; + apt-get install -qq -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev python3-pyparsing zlib1g-dev; E: Unable to correct problems, you have held broken packages. real 0m54.858s; user 0m12.058s; sys 0m4.272s; The command '/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel' returned a non-zero code: 100. ```. According to this link: https://apt.llvm.org/ only 12 and 13 version are mensioned.; ```; Bionic LTS (18.04) - Last update : Mon, 11 Oct 2021 13:24:17 UTC / Revision: 20211011091508+7ae8f392a161; # i386 not available; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic main; # 12; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-12 main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic-12 main; # 13; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-13 main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic-13 main; Focal (20.04) LTS - Last update : Sun, 10 Oct 2021 23:59:52 UTC / Revision: 20211010053033+67964fc4b241; # i386 not available; deb http://apt.llvm.org/focal/ llvm-toolchain-focal main; deb-src http://apt.llvm.org/focal/ llvm-toolchain-focal main; # 12; deb http://apt.llvm.org/focal/ llvm-toolchain-focal-12 main; deb-src http://apt.llvm.org/focal/ llvm-toolchain-focal-12 main; # 13; deb http://apt.llvm.org/focal/ llvm-toolchain-focal-13 main; deb-src http://apt.llvm.org/focal/ llvm-toolchain-focal-13 main. ```; `llvm-toolchain-bionic-11` was changed today.; ![image](https://user-images.githubusercontent.com/41360525/136817825-71faa887-08bb-49e7-9126-036e6412d90d.png). Any help?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:9680,update,update,9680,,https://github.com/google/deepvariant/issues/489,1,['update'],['update'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: buntu.com/ubuntu focal-security InRelease; Get:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease [5526 B]; Get:6 https://apt.llvm.org/focal llvm-toolchain-focal-11/main amd64 Packages [9008 B]; Fetched 14.5 kB in 13s (1133 B/s); Reading package lists...; + apt-get update -qq -y; + apt-get install -qq -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev python3-pyparsing zlib1g-dev; E: Unable to correct problems, you have held broken packages. real 0m54.858s; user 0m12.058s; sys 0m4.272s; The command '/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel' returned a non-zero code: 100. ```. According to this link: https://apt.llvm.org/ only 12 and 13 version are mensioned.; ```; Bionic LTS (18.04) - Last update : Mon, 11 Oct 2021 13:24:17 UTC / Revision: 20211011091508+7ae8f392a161; # i386 not available; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic main; # 12; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-12 main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic-12 main; # 13; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-13 main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic-13 main; Focal (20.04) LTS - Last update : Sun, 10 Oct 2021 23:59:52 UTC / Revision: 20211010053033+67964fc4b241; # i386 not available; deb http://apt.llvm.org/focal/ llvm-toolchain-focal main; deb-src http://apt.llvm.org/focal/ llvm-toolchain-focal main; # 12; deb http://apt.llvm.org/focal/ llvm-toolchain-focal-12 main; deb-src http://apt.llvm.org/focal/ llvm-toolchain-focal-12 main; # 13; deb http://apt.llvm.org/focal/ llvm-toolchain-focal-13 main; deb-src http://apt.llvm.org/focal/ llvm-toolchain-focal-13 main. ```; `llvm-toolchain-bionic-11` was changed today.; ![image](https://user-images.githubusercontent.com/41360525/136817825-71faa887-08bb-49e7-9126-036e6412d90d.png). Any help?
",False,"The text contains a mix of programmatic content and human-readable text. While it includes command outputs, API documentation, and version information, it also has user questions and some descriptive text. However, the main part is system outputs and not primarily meaningful sentences."
Deployability,"buntu2).; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:1373,install,installed,1373,,https://github.com/google/deepvariant/issues/489,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: buntu2).; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu
",False,"The text contains a human-readable explanation of the error and potential causes, including suggestions for resolving the issue. It also includes dependency information which may be relevant but is not purely code or logs."
Deployability,"cal/lib/python2.7/dist-packages (from sklearn) (0.19.2); Requirement already satisfied: pandas in /usr/local/lib/python2.7/dist-packages (0.23.4); Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (1.14.0); Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (2.7.3); Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas) (2018.5); Requirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0); Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.7); Requirement already up-to-date: google-api-python-client in /usr/local/lib/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/loc",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:17226,upgrade,upgrade,17226,,https://github.com/google/deepvariant/issues/89,1,['upgrade'],['upgrade'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: cal/lib/python2.7/dist-packages (from sklearn) (0.19.2); Requirement already satisfied: pandas in /usr/local/lib/python2.7/dist-packages (0.23.4); Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (1.14.0); Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (2.7.3); Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas) (2018.5); Requirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0); Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.7); Requirement already up-to-date: google-api-python-client in /usr/local/lib/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/loc
",False,"The text contains detailed dependency listings which are part of package management and version control, not meaningful human-readable content."
Deployability,"ch4/path.to.mydir/ \; docker://google/deepvariant:1.4.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref c_elegans.PRJNA13758.WS245.genomic.fa \; --reads aln13448198.pbmm2.bam \; --output_vcf aln13448198.pbmm2.dv.vcf.gz \; --num_shards 64; ```. Here's a long snippet of slurm output:; ```; INFO: Using cached SIF image; INFO: Converting SIF file to temporary sandbox...; I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****; time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]; # this part is likely unimportant. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LC_CTYPE = ""C.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LC_CTYPE = ""C.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C""). #[snip]. 2023-02-17 20:13:17.235641: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0; I0217 20:13:17.235805 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader; I0217 20:13:17.268698 23456243894080 make_examples_core.py:243] Task 18/64: Preparing inputs; 2023-02-17 20:13:17.371669: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0; I0217 20:13:17.371811 23456243894080 genomics_reader.py:222] Reading /scratch4/jwan",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/614:1658,install,installed,1658,,https://github.com/google/deepvariant/issues/614,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ch4/path.to.mydir/ \; docker://google/deepvariant:1.4.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref c_elegans.PRJNA13758.WS245.genomic.fa \; --reads aln13448198.pbmm2.bam \; --output_vcf aln13448198.pbmm2.dv.vcf.gz \; --num_shards 64; ```. Here's a long snippet of slurm output:; ```; INFO: Using cached SIF image; INFO: Converting SIF file to temporary sandbox...; I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****; time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]; # this part is likely unimportant. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LC_CTYPE = ""C.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LC_CTYPE = ""C.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C""). #[snip]. 2023-02-17 20:13:17.235641: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0; I0217 20:13:17.235805 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader; I0217 20:13:17.268698 23456243894080 make_examples_core.py:243] Task 18/64: Preparing inputs; 2023-02-17 20:13:17.371669: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0; I0217 20:13:17.371811 23456243894080 genomics_reader.py:222] Reading /scratch4/jwan
",True,"The text consists primarily of program logs or error messages, including timestamps, log levels, and code-related warnings. There are also API documentation elements like parameter lists and return types mentioned in the logs. Additionally, there are code snippets and syntax markers such as Perl warnings about locale settings and SamReader errors."
Deployability,"ckstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. I have installed the DeepVariant image according to: . BIN_VERSION=""0.8.0""; sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". When I run the script test: . OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""0.8.0""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ ; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. The following error happens:. FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.; Pass --helpshort or --helpfull to see help on flags.; ./run_deepvariant.sh: line 12: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. I tried it on three different",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/223:1136,update,update,1136,,https://github.com/google/deepvariant/issues/223,2,"['install', 'update']","['install', 'update']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ckstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. I have installed the DeepVariant image according to: . BIN_VERSION=""0.8.0""; sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". When I run the script test: . OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""0.8.0""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ ; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. The following error happens:. FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.; Pass --helpshort or --helpfull to see help on flags.; ./run_deepvariant.sh: line 12: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. I tried it on three different
",True,"The text consists of command lines and script logs, including commands like 'mkdir -p', 'wget', 'sudo apt-get install', 'docker pull', and 'run_deepvariant' with various parameters. It also contains error messages related to command execution failures. This content is more programmatic or log-like, not containing meaningful human-readable sentences in natural language."
Deployability,"client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:15 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf-nightly-gpu as it is not installed.; Skipping tensorflow-gpu as it is not installed.; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:03:04 --:--:-- 0; curl: (56) GnuTLS recv error (-54): Error in the pull function.; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ . solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ s",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:18177,upgrade,upgrade,18177,,https://github.com/google/deepvariant/issues/89,1,['upgrade'],['upgrade'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:15 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf-nightly-gpu as it is not installed.; Skipping tensorflow-gpu as it is not installed.; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:03:04 --:--:-- 0; curl: (56) GnuTLS recv error (-54): Error in the pull function.; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ . solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ s
",False,
Deployability,"client. '; RuntimeError: Missing runtime dependency on the Google API client. Run `pip install cloud-tpu-client` to fix. ```; However, cloud-tpu-client is not actually the problem. The issue is that `google.api_core.client_options` is not found when being imported from `googleapiclient.discovery`. The issue appears to be the [python3.3 _ _ init _ _.py trap](http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with this patch by calling it discovery.patch and using this Dockerfile:. ```; ARG VERSION=1.1.0.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469:3790,patch,patch,3790,,https://github.com/google/deepvariant/issues/469,1,['patch'],['patch'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: client. '; RuntimeError: Missing runtime dependency on the Google API client. Run `pip install cloud-tpu-client` to fix. ```; However, cloud-tpu-client is not actually the problem. The issue is that `google.api_core.client_options` is not found when being imported from `googleapiclient.discovery`. The issue appears to be the [python3.3 _ _ init _ _.py trap](http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with this patch by calling it discovery.patch and using this Dockerfile:. ```; ARG VERSION=1.1.0.
",False,
Deployability,command line pipeline support?,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/288:13,pipeline,pipeline,13,,https://github.com/google/deepvariant/issues/288,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: command line pipeline support?
",False,The text is a human-readable question regarding command line pipeline support. It contains a meaningful sentence in natural language.
Deployability,conda install dv without model PACBIO,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/395:6,install,install,6,,https://github.com/google/deepvariant/issues/395,1,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: conda install dv without model PACBIO
",False,
Deployability,conda installation problem,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177:6,install,installation,6,,https://github.com/google/deepvariant/issues/177,1,['install'],['installation'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: conda installation problem
",False,"The text contains a complete and meaningful sentence about a conda installation problem. It is human-readable and provides context, which should not be eliminated."
Deployability,conda installed deepvariant libm.so.6: version `GLIBC_2.23' not found,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/391:6,install,installed,6,,https://github.com/google/deepvariant/issues/391,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: conda installed deepvariant libm.so.6: version `GLIBC_2.23' not found
",False,
Deployability,"d Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2213k 100 2213k 0 0 1634k 0 0:00:01 0:00:01 --:--:-- 1634k; Collecting pip; Using cached pip-24.2-py3-none-any.whl.metadata (3.6 kB); Using cached pip-24.2-py3-none-any.whl (1.8 MB); Installing collected packages: pip; WARNING: The scripts pip, pip3 and pip3.10 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-24.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning. [notice] A new release of pip is available: 24.0 -> 24.2; [notice] To update, run: pip install --upgrade pip; Python 3.10.14; pip 24.0 from /usr/local/lib/python3.10/site-packages/pip (python 3.10); ========== [Fri 02 Aug 2024 02:20:22 PM CST] Stage 'Install python3 packages' starting; error: subprocess-exited-with-error; ; Ã— Preparing metadata (pyproject.toml) did not run successfully.; â”‚ exit code: 1; â•°â”€> [78 lines of output]; Running from numpy source directory.; setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates; run_build = parse_setuppy_commands(); performance hint: _common.pyx:275:19: Exception check after calling 'random_func' will always require the GIL to be acquired. Declare 'random_func' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:299:19: Exception check after calling 'random_func' will always require the GIL to be acquired. Declare 'random_func' as 'noexcept' if you control the definition a",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:1943,update,update,1943,,https://github.com/google/deepvariant/issues/859,3,"['install', 'update', 'upgrade']","['install', 'update', 'upgrade']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: d Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2213k 100 2213k 0 0 1634k 0 0:00:01 0:00:01 --:--:-- 1634k; Collecting pip; Using cached pip-24.2-py3-none-any.whl.metadata (3.6 kB); Using cached pip-24.2-py3-none-any.whl (1.8 MB); Installing collected packages: pip; WARNING: The scripts pip, pip3 and pip3.10 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-24.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning. [notice] A new release of pip is available: 24.0 -> 24.2; [notice] To update, run: pip install --upgrade pip; Python 3.10.14; pip 24.0 from /usr/local/lib/python3.10/site-packages/pip (python 3.10); ========== [Fri 02 Aug 2024 02:20:22 PM CST] Stage 'Install python3 packages' starting; error: subprocess-exited-with-error; ; Ã— Preparing metadata (pyproject.toml) did not run successfully.; â”‚ exit code: 1; â•°â”€> [78 lines of output]; Running from numpy source directory.; setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates; run_build = parse_setuppy_commands(); performance hint: _common.pyx:275:19: Exception check after calling 'random_func' will always require the GIL to be acquired. Declare 'random_func' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:299:19: Exception check after calling 'random_func' will always require the GIL to be acquired. Declare 'random_func' as 'noexcept' if you control the definition a
",False,"The text contains pip installation output, which is similar to compiler/interpreter warnings and build system outputs that should be filtered out."
Deployability,"d"" ""${CALL_VARIANTS_SHARD_INDEX}"")""-of-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARDS}"")"".gz\n --checkpoint ""${MODEL}""/model.ckpt\n --batch_size 512\n']; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 888, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 875, in run; _run_call_variants(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 472, in _run_call_variants; _run_call_variants_with_pipelines_api(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 464, in _run_call_variants_with_pipelines_api; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 350, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""projects/ms-deepvariant/operations/23423423423423443"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). 4. Changing to 0.7.2rc gives following error: ; [12/12/2018 13:12:23 INFO gcp_deepvariant_runner.py] Running make_examples...; [12/12/2018 13:31:21 INFO gcp_deepvariant_runner.py] make_examples is done!; [12/12/2018 13:31:21 INFO gcp_deepvariant_runner.py] Running call_variants...; [12/12/2018 13:33:54 ERROR gcp_deepvariant_runner.py] Job failed with error {...........cutout...; 13:33:48 Stopped running ""-c /opt/deepvariant/bin/call_variants --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --outfile \""${CALLED_VARIANTS}\""/call_variants_output.tfrecord-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARD_INDEX}\"")\""-of-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARDS}\"")\"".gz --checkpoint \""${MODEL}\""/model.ckpt --batch_size 512"": exit status 1: turn self._sess_creator.create_session(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:4276,pipeline,pipeline,4276,,https://github.com/google/deepvariant/issues/129,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: d"" ""${CALL_VARIANTS_SHARD_INDEX}"")""-of-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARDS}"")"".gz\n --checkpoint ""${MODEL}""/model.ckpt\n --batch_size 512\n']; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 888, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 875, in run; _run_call_variants(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 472, in _run_call_variants; _run_call_variants_with_pipelines_api(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 464, in _run_call_variants_with_pipelines_api; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 350, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""projects/ms-deepvariant/operations/23423423423423443"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). 4. Changing to 0.7.2rc gives following error: ; [12/12/2018 13:12:23 INFO gcp_deepvariant_runner.py] Running make_examples...; [12/12/2018 13:31:21 INFO gcp_deepvariant_runner.py] make_examples is done!; [12/12/2018 13:31:21 INFO gcp_deepvariant_runner.py] Running call_variants...; [12/12/2018 13:33:54 ERROR gcp_deepvariant_runner.py] Job failed with error {...........cutout...; 13:33:48 Stopped running ""-c /opt/deepvariant/bin/call_variants --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --outfile \""${CALLED_VARIANTS}\""/call_variants_output.tfrecord-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARD_INDEX}\"")\""-of-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARDS}\"")\"".gz --checkpoint \""${MODEL}\""/model.ckpt --batch_size 512"": exit status 1: turn self._sess_creator.create_session(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/
",False,"The text contains a mix of command lines and error messages that are part of a program's execution logs, including stack traces and error codes. However, there are also some natural language sentences discussing the process, such as 'Running make_examples...' and 'make_examples is done!'. The majority of the text is not in human-readable prose but rather consists of log entries and command outputs."
Deployability,"dd channels bioconda && \; conda config --add channels conda-forge && \; conda create -n bio bioconda::bcftools bioconda::samtools -y && \; conda clean -a. # Clone DeepVariant and build; FROM base AS builder. # Clone the DeepVariant repository; RUN git clone https://github.com/google/deepvariant.git /opt/deepvariant && \; cd /opt/deepvariant && \; git checkout tags/v1.6.1. # Run Bazel build with additional flags to skip problematic configurations; RUN bazel build -c opt --noincremental --experimental_action_listener= //deepvariant:make_examples //deepvariant:call_variants //deepvariant:postprocess_variants || { \; echo ""Bazel build failed""; \; exit 1; }. # Final image; FROM base AS final. # Set environment variables; ENV VERSION=1.6.0; ENV PYTHON_VERSION=3.8; ENV PATH=""/opt/miniconda/bin:${PATH}"". # Install Python packages; RUN pip install --upgrade pip setuptools wheel --timeout=120 && \; pip install jaxlib jax --timeout=120 --extra-index-url https://storage.googleapis.com/jax-releases/jax_releases.html. # Copy DeepVariant binaries from the builder stage; COPY --from=builder /opt/deepvariant /opt/deepvariant; WORKDIR /opt/deepvariant. # Ensure executable scripts are correctly set up; RUN BASH_HEADER='#!/bin/bash' && \; for script in make_examples call_variants call_variants_slim postprocess_variants vcf_stats_report show_examples runtime_by_region_vis multisample_make_examples labeled_examples_to_vcf make_examples_somatic train run_deepvariant run_deepsomatic; do \; printf ""%s\n%s\n"" ""${BASH_HEADER}"" ""python3 /opt/deepvariant/bin/${script}.zip \""$@\"""" > /opt/deepvariant/bin/${script} && \; chmod +x /opt/deepvariant/bin/${script}; \; done. # Copy licenses and other necessary files; # Ensure these paths and URLs are correct and accessible; # Replace with valid URLs or remove if not needed; ADD https://storage.googleapis.com/deepvariant/models/DeepVariant/1.6.0/savedmodels/deepvariant.hybrid.savedmodel/saved_model.pb /models/; WORKDIR /opt/deepvariant/bin/; COPY --fro",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/871:2572,release,releases,2572,,https://github.com/google/deepvariant/issues/871,1,['release'],['releases'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: dd channels bioconda && \; conda config --add channels conda-forge && \; conda create -n bio bioconda::bcftools bioconda::samtools -y && \; conda clean -a. # Clone DeepVariant and build; FROM base AS builder. # Clone the DeepVariant repository; RUN git clone https://github.com/google/deepvariant.git /opt/deepvariant && \; cd /opt/deepvariant && \; git checkout tags/v1.6.1. # Run Bazel build with additional flags to skip problematic configurations; RUN bazel build -c opt --noincremental --experimental_action_listener= //deepvariant:make_examples //deepvariant:call_variants //deepvariant:postprocess_variants || { \; echo ""Bazel build failed""; \; exit 1; }. # Final image; FROM base AS final. # Set environment variables; ENV VERSION=1.6.0; ENV PYTHON_VERSION=3.8; ENV PATH=""/opt/miniconda/bin:${PATH}"". # Install Python packages; RUN pip install --upgrade pip setuptools wheel --timeout=120 && \; pip install jaxlib jax --timeout=120 --extra-index-url https://storage.googleapis.com/jax-releases/jax_releases.html. # Copy DeepVariant binaries from the builder stage; COPY --from=builder /opt/deepvariant /opt/deepvariant; WORKDIR /opt/deepvariant. # Ensure executable scripts are correctly set up; RUN BASH_HEADER='#!/bin/bash' && \; for script in make_examples call_variants call_variants_slim postprocess_variants vcf_stats_report show_examples runtime_by_region_vis multisample_make_examples labeled_examples_to_vcf make_examples_somatic train run_deepvariant run_deepsomatic; do \; printf ""%s\n%s\n"" ""${BASH_HEADER}"" ""python3 /opt/deepvariant/bin/${script}.zip \""$@\"""" > /opt/deepvariant/bin/${script} && \; chmod +x /opt/deepvariant/bin/${script}; \; done. # Copy licenses and other necessary files; # Ensure these paths and URLs are correct and accessible; # Replace with valid URLs or remove if not needed; ADD https://storage.googleapis.com/deepvariant/models/DeepVariant/1.6.0/savedmodels/deepvariant.hybrid.savedmodel/saved_model.pb /models/; WORKDIR /opt/deepvariant/bin/; COPY --fro
",False,
Deployability,deepvariant software installed using mamba does not run correctly ;lchmod error;No such file or directory: '/usr/bin/python3' error,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/598:21,install,installed,21,,https://github.com/google/deepvariant/issues/598,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: deepvariant software installed using mamba does not run correctly ;lchmod error;No such file or directory: '/usr/bin/python3' error
",True,"The text consists of a command that failed (mamba install, chmod error, and a file not found error), which are logs or error messages rather than meaningful human-readable sentences."
Deployability,"deepvariant-1; OUTPUT_BUCKET=gs://mbh-deepvariant-ouput-vcf; STAGING_FOLDER_NAME=staging_folder1; OUTPUT_FILE_NAME=HR090610illuminagr38DeepVariant.vcf; # Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard; IMAGE_VERSION=0.8.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west2-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://mbh-bam-files1/HR090610.final.bam \; --bai gs://mbh-bam-files1/HR090610.final.bam.bai \; --ref gs://mbh-bam-files1/GCA_000001405.28_GRCh38.p13_genomic.fa \; --shards 224 \; --make_examples_workers 7 \; --make_examples_cores_per_worker 32 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 7 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 200 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west2 \; --docker-image gcr.io/cloud-lifesciences/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". The log file is attached, but part of it is also pasted below. Is it saying that there is a mismatch between the .fai and .fa files for the reference or between the reference and the bam file? The .fai file was created from the .fa file using samtools index command. . ValueError: Reference contigs span 3270284521 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Con",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/225:1447,pipeline,pipeline,1447,,https://github.com/google/deepvariant/issues/225,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: deepvariant-1; OUTPUT_BUCKET=gs://mbh-deepvariant-ouput-vcf; STAGING_FOLDER_NAME=staging_folder1; OUTPUT_FILE_NAME=HR090610illuminagr38DeepVariant.vcf; # Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard; IMAGE_VERSION=0.8.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west2-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://mbh-bam-files1/HR090610.final.bam \; --bai gs://mbh-bam-files1/HR090610.final.bam.bai \; --ref gs://mbh-bam-files1/GCA_000001405.28_GRCh38.p13_genomic.fa \; --shards 224 \; --make_examples_workers 7 \; --make_examples_cores_per_worker 32 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 7 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 200 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west2 \; --docker-image gcr.io/cloud-lifesciences/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". The log file is attached, but part of it is also pasted below. Is it saying that there is a mismatch between the .fai and .fa files for the reference or between the reference and the bam file? The .fai file was created from the .fa file using samtools index command. . ValueError: Reference contigs span 3270284521 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Con
",False,"The text contains complete, meaningful sentences and explanations discussing the relationship between .fai and .fa files and an error encountered during processing. It includes natural language describing the issue and provides context for understanding the problem."
Deployability,"ds=1 ; ```; I faced the error:; ```; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --task {}. 2019-09-11 14:44:44.030589: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m2.456s; user	0m1.443s; sys	0m1.926s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --task {}' returned non-zero exit status 1; ```. I googled for solution also asked you before and found may be compiling from source code would works for me. I FAILED TO DO THIS.; Is still any chance to use install and work with DeepVariant?. Please consider that a number of potential users -like me- are biologist with limited knowledge of informatics. ; Thanks in advance.; Hamid",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/217:2561,install,install,2561,,https://github.com/google/deepvariant/issues/217,1,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ds=1 ; ```; I faced the error:; ```; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --task {}. 2019-09-11 14:44:44.030589: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m2.456s; user	0m1.443s; sys	0m1.926s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --task {}' returned non-zero exit status 1; ```. I googled for solution also asked you before and found may be compiling from source code would works for me. I FAILED TO DO THIS.; Is still any chance to use install and work with DeepVariant?. Please consider that a number of potential users -like me- are biologist with limited knowledge of informatics. ; Thanks in advance.; Hamid
",False,"The text includes natural language sentences discussing user experiences, problems encountered, and requests for help, which meet the criteria for meaningful human-readable content."
Deployability,dv_* file syntax error from bioconda installed 1.4.0 version DeepVariant,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/627:37,install,installed,37,,https://github.com/google/deepvariant/issues/627,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: dv_* file syntax error from bioconda installed 1.4.0 version DeepVariant
",False,"The text contains an error message related to a specific software package (DeepVariant) and its installation, which is part of program logs or error messages."
Deployability,"e are some problems while running the ./build-prereq.sh:; ```; + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0; Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0); Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0); Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0); Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2); Installing collected packages: pyparsing; Attempting uninstall: pyparsing; Found existing installation: pyparsing 3.1.1; Uninstalling pyparsing-3.1.1:; Successfully uninstalled pyparsing-3.1.1; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; + DV_PLATFORM=ubuntu-20.04; + ln -sf /usr/bin/python3.8 /usr/local/bin/python3; + cd; + rm -rf clif; + proxychains git clone https://github.com/google/clif.git; ProxyChains-3.1 (http://proxychains.sf.net); Cloning into 'clif'...; |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK; remote: Enumerating objects: 5846, done.; remote: Counting objects: ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739:1049,install,installed,1049,,https://github.com/google/deepvariant/issues/739,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: e are some problems while running the ./build-prereq.sh:; ```; + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0; Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0); Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0); Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0); Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2); Installing collected packages: pyparsing; Attempting uninstall: pyparsing; Found existing installation: pyparsing 3.1.1; Uninstalling pyparsing-3.1.1:; Successfully uninstalled pyparsing-3.1.1; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; + DV_PLATFORM=ubuntu-20.04; + ln -sf /usr/bin/python3.8 /usr/local/bin/python3; + cd; + rm -rf clif; + proxychains git clone https://github.com/google/clif.git; ProxyChains-3.1 (http://proxychains.sf.net); Cloning into 'clif'...; |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK; remote: Enumerating objects: 5846, done.; remote: Counting objects: 
",True,The text contains pip installation logs and dependency resolution errors which are related to program logs or build system output.
Deployability,"e creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled""; parallel: This job failed:; docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0; docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled""; ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):; ```; docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant; See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md.; ```; I do have the `gcloud alpha genomics pipelines` example working, so this isnâ€™t absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud, I would very much appreciate it. Thank you very much,; Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:7854,pipeline,pipelines,7854,,https://github.com/google/deepvariant/issues/171,1,['pipeline'],['pipelines'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: e creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled""; parallel: This job failed:; docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0; docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled""; ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):; ```; docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant; See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md.; ```; I do have the `gcloud alpha genomics pipelines` example working, so this isnâ€™t absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud, I would very much appreciate it. Thank you very much,; Charles
",False,"The text contains a mixture of human-readable sentences and logs. It includes descriptions about the user's setup, attempts to run DeepVariant, and requests for assistance, which are meaningful. Additionally, there are log excerpts, but they don't form the majority of the content. The overall content is a blend of natural language discussion and some technical logs, making it suitable to keep as readable text."
Deployability,"e following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong?; My cmd:; ```; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref ${fasta} \; --reads ${Input.bam} \; --output_vcf output/output.vcf.gz \; --output_gvcf output/output.g.vcf.gz \; --num_shards 32 \; --intermediate_results_dir output/intermediate_results_dir \; --regions chr20 \; --customized_model model/weights-51-0.995354.ckpt; ```. Error message:; ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco; rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started.; I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]; I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False; /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/725:1072,release,release,1072,,https://github.com/google/deepvariant/issues/725,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: e following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong?; My cmd:; ```; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref ${fasta} \; --reads ${Input.bam} \; --output_vcf output/output.vcf.gz \; --output_gvcf output/output.g.vcf.gz \; --num_shards 32 \; --intermediate_results_dir output/intermediate_results_dir \; --regions chr20 \; --customized_model model/weights-51-0.995354.ckpt; ```. Error message:; ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco; rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started.; I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]; I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False; /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7
",False,
Deployability,"e term ""DeepVariant"" or ""Google"" any where. I apologize for missing something that a lot of people might know, but would you mind explaining how the results in the [PrecisionFDA results table](https://precision.fda.gov/challenges/truth/results) match DeepVariant?. **2)** While it has been hard for me explain myself precisely, I have been concerned that there was somehow over-fitting for some datasets reporting extremely high accuracy. In the PrecisionFDA challenge, I think it should probably be mentioned that there are multiple programs with high percentages, including different workflows that actually use the same variant caller (like GATK) and no strategy was ""best"" for all the criteria defined. **3)** While I don't know the precise way in which you could have a lack of independence for training versus test datasets, I think there are other benchmarks that better match what I would expect for more typical results. For example, [in this paper](https://www.nature.com/articles/s41587-019-0054-x), it says ""_In the high-confidence regions, when comparing these pipelines to each other (https://precision.fda.gov/jobs/job-FJpqBP80F3YyfJG02bQzPJBj, link immediately accessible by requesting an account), they agreed on 99.7% of SNVs and 98.7% of indels. Outside the high-confidence regions (https://precision.fda.gov/jobs/job-FJpqJF80F3YyXqz6Kv8Q1BQK), they agreed with each other on only 76.5% of SNVs and 78.7% of indels_."". **4)** You mention ""_No filtering is needed beyond setting your preferred minimum quality threshold_."". While I don't currently have much first-hand experience (although that is on my long-term ""to-do"" list), I didn't think this was true. For example, unless I am missing something, this table reports a very high ""Failed Filters"" count for DeepVariant versus GATK:. https://www.nature.com/articles/s41598-018-36177-7/tables/1. Do you have other metrics with comparisons on completely independent samples?. Thank you very much for your help!. Sincerely,; Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165:1332,pipeline,pipelines,1332,,https://github.com/google/deepvariant/issues/165,1,['pipeline'],['pipelines'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: e term ""DeepVariant"" or ""Google"" any where. I apologize for missing something that a lot of people might know, but would you mind explaining how the results in the [PrecisionFDA results table](https://precision.fda.gov/challenges/truth/results) match DeepVariant?. **2)** While it has been hard for me explain myself precisely, I have been concerned that there was somehow over-fitting for some datasets reporting extremely high accuracy. In the PrecisionFDA challenge, I think it should probably be mentioned that there are multiple programs with high percentages, including different workflows that actually use the same variant caller (like GATK) and no strategy was ""best"" for all the criteria defined. **3)** While I don't know the precise way in which you could have a lack of independence for training versus test datasets, I think there are other benchmarks that better match what I would expect for more typical results. For example, [in this paper](https://www.nature.com/articles/s41587-019-0054-x), it says ""_In the high-confidence regions, when comparing these pipelines to each other (https://precision.fda.gov/jobs/job-FJpqBP80F3YyfJG02bQzPJBj, link immediately accessible by requesting an account), they agreed on 99.7% of SNVs and 98.7% of indels. Outside the high-confidence regions (https://precision.fda.gov/jobs/job-FJpqJF80F3YyXqz6Kv8Q1BQK), they agreed with each other on only 76.5% of SNVs and 78.7% of indels_."". **4)** You mention ""_No filtering is needed beyond setting your preferred minimum quality threshold_."". While I don't currently have much first-hand experience (although that is on my long-term ""to-do"" list), I didn't think this was true. For example, unless I am missing something, this table reports a very high ""Failed Filters"" count for DeepVariant versus GATK:. https://www.nature.com/articles/s41598-018-36177-7/tables/1. Do you have other metrics with comparisons on completely independent samples?. Thank you very much for your help!. Sincerely,; Charles
",False,"The text contains complete, meaningful sentences in natural language discussing concepts and ideas about the PrecisionFDA results matching DeepVariant, concerns about over-fitting, and references to other studies. It also includes a sincere thank you and apology, which are indicators of human-written prose."
Deployability,"e/ref/GIAB/HG005/hs37d5/ \; --contain \; /paedyl01/disk1/louisshe/tools/DeepVariant/deepvariant_1.6.1.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/ref/hs37d5/hs37d5.fa \; --reads=/input_reads/HG005.hs37d5.30x.bam \; --output_vcf=/output/HG005.dv.vcf.gz \; --output_gvcf=/output/HG005.dv.g.vcf.gz \; --num_shards=10 \; --intermediate_results_dir=/tmp \; --logging_dir=/output/log \; --dry_run=false \; --par_regions_bed=/ref/hg19/ucsc.hg19.par.bed \; --haploid_contigs=""chrX,chrY""; ```; - Error trace:; Error trace below is from `HG005_deppvariant.log`. No error prompts prior to this step.; ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examp. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Ker. For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; I0619 14:57:56.059498 47403021002560 call_variants.py:563] Total 1 writing processes started.; I0619 14:57:56.063244 47403021002560 dv_utils.py:370] From /tmp/make_examples.tfrecord-00000-of-00010.gz.example_info; I0619 14:57:56.063441 47403021002560 call_variants.py:588] Shape of input examples: [100, 221, 7]; I0619 14:57:56.063909 47403021002560 call_variants.py:592] Use saved model: True; 2024-06-19 14:57:57.916727: F tensorflow/tsl/platform/env.cc:391] Check failed: -1 != path_length (-1 vs. -1); Fatal Python error: Aborted. Current thread 0x00002b1ce03a6740 (most recent call first):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py"", line 500 in _import_graph_de; File ""/usr/local/lib/pytho",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/833:2099,release,release,2099,,https://github.com/google/deepvariant/issues/833,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: e/ref/GIAB/HG005/hs37d5/ \; --contain \; /paedyl01/disk1/louisshe/tools/DeepVariant/deepvariant_1.6.1.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/ref/hs37d5/hs37d5.fa \; --reads=/input_reads/HG005.hs37d5.30x.bam \; --output_vcf=/output/HG005.dv.vcf.gz \; --output_gvcf=/output/HG005.dv.g.vcf.gz \; --num_shards=10 \; --intermediate_results_dir=/tmp \; --logging_dir=/output/log \; --dry_run=false \; --par_regions_bed=/ref/hg19/ucsc.hg19.par.bed \; --haploid_contigs=""chrX,chrY""; ```; - Error trace:; Error trace below is from `HG005_deppvariant.log`. No error prompts prior to this step.; ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examp. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Ker. For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; I0619 14:57:56.059498 47403021002560 call_variants.py:563] Total 1 writing processes started.; I0619 14:57:56.063244 47403021002560 dv_utils.py:370] From /tmp/make_examples.tfrecord-00000-of-00010.gz.example_info; I0619 14:57:56.063441 47403021002560 call_variants.py:588] Shape of input examples: [100, 221, 7]; I0619 14:57:56.063909 47403021002560 call_variants.py:592] Use saved model: True; 2024-06-19 14:57:57.916727: F tensorflow/tsl/platform/env.cc:391] Check failed: -1 != path_length (-1 vs. -1); Fatal Python error: Aborted. Current thread 0x00002b1ce03a6740 (most recent call first):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py"", line 500 in _import_graph_de; File ""/usr/local/lib/pytho
",True,"The provided text consists primarily of logs and error messages from a program, including timestamps, error codes, and stack traces. Additionally, it contains API documentation details such as parameter lists and command-line arguments which are typical in logs. The content is technical and not meant for human-readable purposes."
Deployability,ease [242 kB]; Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]; Get:6 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic/main amd64 Packages [19.3 kB]; Get:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]; Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]; Get:9 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]; Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [638 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2801 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archi,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:4594,update,updates,4594,,https://github.com/google/deepvariant/issues/489,1,['update'],['updates'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ease [242 kB]; Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]; Get:6 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic/main amd64 Packages [19.3 kB]; Get:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]; Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]; Get:9 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]; Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [638 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2801 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archi
",False,"The text contains a series of package download lines which appear to be part of an APT update log. These logs are not primarily code, logs, or API documentation, but rather the process of updating software packages. However, these may not constitute as 'meaningful human-readable sentences.' The content is more technical and related to package management, which could be considered somewhat programmatic in nature. Given the evaluation criteria, this text might lean towards elimination because it's more about system processes than human-readable prose. However, since it doesn't contain code snippets or logs with detailed error messages, it may not strictly fall into the 'logs' category but rather as technical output."
Deployability,"ec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-too",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:1793,install,installed,1793,,https://github.com/google/deepvariant/issues/489,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-too
",True,"The text contains log output and error messages from a package dependency check, including timestamps and error codes, which fit into the category of program logs or error messages that should be filtered out."
Deployability,"eepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/DeepVariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /work/cjm124/SWFst/DeepVariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/make_examples.tfrecord@12.gz --channels insert_size --gvcf /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@12.gz --regions chr20:10,000,000-10,010,000 --task 0; ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```; singularity exec DeepVariant_1.6.1.sif bash; pip install --user google-nucleus; run_deepvariant --model_type=WGS \; 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; 	--regions ""chr20:10,000,000-10,010,000"" \; 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; 	--num_shards=12; ```. Error:. ```; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>; import tensorflow as tf; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>; from tensorflow.python.tools import module_util as _module_util; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>; from tensorflow.python.eager import context; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>; from tensorflow.core.framework import function_pb2; File ""/usr/local/lib/p",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812:4653,install,install,4653,,https://github.com/google/deepvariant/issues/812,1,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: eepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/DeepVariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /work/cjm124/SWFst/DeepVariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/make_examples.tfrecord@12.gz --channels insert_size --gvcf /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@12.gz --regions chr20:10,000,000-10,010,000 --task 0; ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```; singularity exec DeepVariant_1.6.1.sif bash; pip install --user google-nucleus; run_deepvariant --model_type=WGS \; 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; 	--regions ""chr20:10,000,000-10,010,000"" \; 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; 	--num_shards=12; ```. Error:. ```; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>; import tensorflow as tf; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>; from tensorflow.python.tools import module_util as _module_util; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>; from tensorflow.python.eager import context; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>; from tensorflow.core.framework import function_pb2; File ""/usr/local/lib/p
",True,"The content contains command lines and error outputs that appear to be logs or code snippets. The presence of commands like 'pip install --user google-nucleus;' and detailed error messages such as 'Traceback (most recent call last):' indicates that this is program output, likely from a terminal or log file."
Deployability,"epVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0; Traceback (most recent call last):; File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>; File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main; File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call; with Popen(*popenargs, **kwargs) as p:; File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__; restore_signals, start_new_session); File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child; raise child_exception_type(errno_num, err_msg, err_filename); FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'; ```. I noticed inside the bazel .zip files the python binary is hard-coded:; /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVariant was installed using micromamba:. ```; micromamba create -n dv && micromamba install -y -n dv -f /tmp/env_deepvariant.yaml; ``` ; With environment file:; ```; name: dv; channels:; - conda-forge; - bioconda; - defaults; dependencies:; - python=3.6; - deepvariant=1.5.0; ```. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/664:1962,install,installed,1962,,https://github.com/google/deepvariant/issues/664,2,['install'],"['install', 'installed']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: epVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0; Traceback (most recent call last):; File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>; File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main; File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call; with Popen(*popenargs, **kwargs) as p:; File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__; restore_signals, start_new_session); File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child; raise child_exception_type(errno_num, err_msg, err_filename); FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'; ```. I noticed inside the bazel .zip files the python binary is hard-coded:; /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVariant was installed using micromamba:. ```; micromamba create -n dv && micromamba install -y -n dv -f /tmp/env_deepvariant.yaml; ``` ; With environment file:; ```; name: dv; channels:; - conda-forge; - bioconda; - defaults; dependencies:; - python=3.6; - deepvariant=1.5.0; ```. Thanks
",False,"The text contains a detailed discussion about the installation and configuration of DeepVariant, including attempts to override the Python binary used. This is explanatory prose discussing technical concepts and steps, making it meaningful for human readers."
Deployability,"epends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)...; 151.101.114.49, 2a04:4e42:1b::56",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:1983,install,installed,1983,,https://github.com/google/deepvariant/issues/489,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: epends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)...; 151.101.114.49, 2a04:4e42:1b::56
",False,"The text contains a mixture of dependency-related error messages and bash commands attempting to install packages. However, the content is not purely code or logs; it includes attempts at installation steps which are more like human-readable problem-solving actions."
Deployability,"er version**: Docker version 27.1.1, build 6312585; **Bazel Version**: 7.3.1; **MacBook Model**: M1 chip (ARM64 architecture). **Error**: ; ![IMG_3267](https://github.com/user-attachments/assets/11e28824-b941-42cc-9d33-7e9155a03543); ![IMG_3268](https://github.com/user-attachments/assets/4e923de6-99d5-43ee-80c6-29b32504527d). **My Dockerfilee code**:. ```; # Base image suitable for ARM64 architecture; FROM arm64v8/ubuntu:latest AS base. # Prevent interactive prompts; ENV DEBIAN_FRONTEND=noninteractive. # Install necessary packages; RUN apt-get update && \; apt-get install -y \; git \; curl \; unzip \; wget \; openjdk-17-jdk \; build-essential \; bzip2 \; python3-pip \; parallel && \; apt-get clean && \; rm -rf /var/lib/apt/lists/*. # Install Bazel (adjust version as needed); RUN curl -LO ""https://github.com/bazelbuild/bazel/releases/download/7.3.1/bazel-7.3.1-linux-arm64"" && \; chmod +x bazel-7.3.1-linux-arm64 && \; mv bazel-7.3.1-linux-arm64 /usr/local/bin/bazel. # Install Conda; RUN curl -LO ""https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-aarch64.sh"" && \; bash Miniconda3-latest-Linux-aarch64.sh -b -p /opt/miniconda && \; rm Miniconda3-latest-Linux-aarch64.sh. # Setup Conda environment; ENV PATH=""/opt/miniconda/bin:${PATH}"". RUN conda config --add channels defaults && \; conda config --add channels bioconda && \; conda config --add channels conda-forge && \; conda create -n bio bioconda::bcftools bioconda::samtools -y && \; conda clean -a. # Clone DeepVariant and build; FROM base AS builder. # Clone the DeepVariant repository; RUN git clone https://github.com/google/deepvariant.git /opt/deepvariant && \; cd /opt/deepvariant && \; git checkout tags/v1.6.1. # Run Bazel build with additional flags to skip problematic configurations; RUN bazel build -c opt --noincremental --experimental_action_listener= //deepvariant:make_examples //deepvariant:call_variants //deepvariant:postprocess_variants || { \; echo ""Bazel build failed""; \; exit 1; }. # Final image; ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/871:1237,Install,Install,1237,,https://github.com/google/deepvariant/issues/871,1,['Install'],['Install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: er version**: Docker version 27.1.1, build 6312585; **Bazel Version**: 7.3.1; **MacBook Model**: M1 chip (ARM64 architecture). **Error**: ; ![IMG_3267](https://github.com/user-attachments/assets/11e28824-b941-42cc-9d33-7e9155a03543); ![IMG_3268](https://github.com/user-attachments/assets/4e923de6-99d5-43ee-80c6-29b32504527d). **My Dockerfilee code**:. ```; # Base image suitable for ARM64 architecture; FROM arm64v8/ubuntu:latest AS base. # Prevent interactive prompts; ENV DEBIAN_FRONTEND=noninteractive. # Install necessary packages; RUN apt-get update && \; apt-get install -y \; git \; curl \; unzip \; wget \; openjdk-17-jdk \; build-essential \; bzip2 \; python3-pip \; parallel && \; apt-get clean && \; rm -rf /var/lib/apt/lists/*. # Install Bazel (adjust version as needed); RUN curl -LO ""https://github.com/bazelbuild/bazel/releases/download/7.3.1/bazel-7.3.1-linux-arm64"" && \; chmod +x bazel-7.3.1-linux-arm64 && \; mv bazel-7.3.1-linux-arm64 /usr/local/bin/bazel. # Install Conda; RUN curl -LO ""https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-aarch64.sh"" && \; bash Miniconda3-latest-Linux-aarch64.sh -b -p /opt/miniconda && \; rm Miniconda3-latest-Linux-aarch64.sh. # Setup Conda environment; ENV PATH=""/opt/miniconda/bin:${PATH}"". RUN conda config --add channels defaults && \; conda config --add channels bioconda && \; conda config --add channels conda-forge && \; conda create -n bio bioconda::bcftools bioconda::samtools -y && \; conda clean -a. # Clone DeepVariant and build; FROM base AS builder. # Clone the DeepVariant repository; RUN git clone https://github.com/google/deepvariant.git /opt/deepvariant && \; cd /opt/deepvariant && \; git checkout tags/v1.6.1. # Run Bazel build with additional flags to skip problematic configurations; RUN bazel build -c opt --noincremental --experimental_action_listener= //deepvariant:make_examples //deepvariant:call_variants //deepvariant:postprocess_variants || { \; echo ""Bazel build failed""; \; exit 1; }. # Final image; 
",False,"The text includes a detailed Dockerfile that describes the setup process, including base images, installed packages, configuration steps and commands. It is a human-readable explanation of how to build an environment suitable for DeepVariant, which contains complete sentences discussing the process in natural language."
Deployability,erse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; D,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:5978,upgrade,upgraded,5978,,https://github.com/google/deepvariant/issues/489,1,['upgrade'],['upgraded'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: erse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; D
",False,"The provided text contains a series of commands and package information, which includes code-like elements such as package versioning and dependency lists. However, the content also consists of meaningful sentences that describe the process of updating packages and attempting to install a specific tool, including error messages related to dependencies. Therefore, while it does include some code-like structures, it is not solely composed of such elements. The presence of human-readable explanations about the installation attempt makes it suitable for retention."
Deployability,"ervice,so i hardly can try to tell the Administrator to update some tools because there are other users and any update to key tools may cause them some troublesome.AS i know,many people work on bioinformation use cluster service and do not have permission to do sudo update or maybe not have a docker in service,but conda can do.; so i try search conda deepvariant,and i try conda install -c bioconda deepvariant=1.0.0(on python3,and i also try other version on python2),and i find dv_make_examples.py, and i see many other guys also try conda.(https://github.com/google/deepvariant/issues/9).; when i run dv_make_examples.py on python3,i get ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and i know it is wrong with GLIBC and the solution is to update GLIBC to GLIBC_2.23 ,but i can not . i ask my Administrator and he say the glibc is too important and update it on cluster service may cause other users bug. . so is there any chance i can use deepvatiant ? and again,i can not install from source(no permission to sudo ) or docker(don't have docker on cluster service ),and i can't update glibc .; And the info are like this:; ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and strings /lib64/libc.so.6 |grep GLIBC:; GLIBC_2.2.5; GLIBC_2.2.6; GLIBC_2.3; GLIBC_2.3.2; GLIBC_2.3.3; GLIBC_2.3.4; GLIBC_2.4; GLIBC_2.5; GLIBC_2.6; GLIBC_2.7; GLIBC_2.8; GLIBC_2.9; GLIBC_2.10; GLIBC_2.11; GLIBC_2.12; GLIBC_2.13; GLIBC_2.14; GLIBC_2.15; GLIBC_2.16; GLIBC_2.17; GLIBC_PRIVATE; and the other information is :; Linux version 3.10.0-1127.18.2.el7.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) ) #1 SMP Sun Jul 26 15:27:06 UTC 2020; conda 4.9.2; Python 3.7.6. i really hope you can help me.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/391:1246,install,install,1246,,https://github.com/google/deepvariant/issues/391,2,"['install', 'update']","['install', 'update']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ervice,so i hardly can try to tell the Administrator to update some tools because there are other users and any update to key tools may cause them some troublesome.AS i know,many people work on bioinformation use cluster service and do not have permission to do sudo update or maybe not have a docker in service,but conda can do.; so i try search conda deepvariant,and i try conda install -c bioconda deepvariant=1.0.0(on python3,and i also try other version on python2),and i find dv_make_examples.py, and i see many other guys also try conda.(https://github.com/google/deepvariant/issues/9).; when i run dv_make_examples.py on python3,i get ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and i know it is wrong with GLIBC and the solution is to update GLIBC to GLIBC_2.23 ,but i can not . i ask my Administrator and he say the glibc is too important and update it on cluster service may cause other users bug. . so is there any chance i can use deepvatiant ? and again,i can not install from source(no permission to sudo ) or docker(don't have docker on cluster service ),and i can't update glibc .; And the info are like this:; ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and strings /lib64/libc.so.6 |grep GLIBC:; GLIBC_2.2.5; GLIBC_2.2.6; GLIBC_2.3; GLIBC_2.3.2; GLIBC_2.3.3; GLIBC_2.3.4; GLIBC_2.4; GLIBC_2.5; GLIBC_2.6; GLIBC_2.7; GLIBC_2.8; GLIBC_2.9; GLIBC_2.10; GLIBC_2.11; GLIBC_2.12; GLIBC_2.13; GLIBC_2.14; GLIBC_2.15; GLIBC_2.16; GLIBC_2.17; GLIBC_PRIVATE; and the other information is :; Linux version 3.10.0-1127.18.2.el7.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) ) #1 SMP Sun Jul 26 15:27:06 UTC 2020; conda 4.9.2; Python 3.7.6. i really hope you can help me.
",False,"The text contains a detailed problem description with attempted solutions and context, which is meaningful human-readable content."
Deployability,"es can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. 0K ... 100% 37.6M=0s. 2021-10-11 15:29:12 (37.6 MB/s",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:6958,install,installed,6958,,https://github.com/google/deepvariant/issues/489,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: es can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. 0K ... 100% 37.6M=0s. 2021-10-11 15:29:12 (37.6 MB/s
",True,"The content consists primarily of program logs or error messages, including package dependency issues and build warnings."
Deployability,"etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest version (2.7.12-1~16.04).; python-pip is already the newest version (8.1.1-2ubuntu0.4).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-date: pip in /usr/local/lib/python2.7/dist-packages (18.0); ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:09 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/l",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:12387,Install,Install,12387,,https://github.com/google/deepvariant/issues/89,1,['Install'],['Install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest version (2.7.12-1~16.04).; python-pip is already the newest version (8.1.1-2ubuntu0.4).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-date: pip in /usr/local/lib/python2.7/dist-packages (18.0); ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:09 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/l
",True,"The text consists of log output from an installation process. It includes timestamps, package names, and system messages which are typical of logs. There is no meaningful human-readable sentences outside of the log entries."
Deployability,"examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --outfile \""${CALLED_VARIANTS}\""/call_variants_output.tfrecord-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARD_INDEX}\"")\""-of-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARDS}\"")\"".gz --checkpoint \""${MODEL}\""/model.ckpt --batch_size 512""; 13:33:48 Started running ""-c gsutil -q cp /google/logs/output gs://ms_bam/deep_output/stage/logs/call_variants/0""; 13:33:50 Stopped running ""-c gsutil -q cp /google/logs/output gs://ms_bam/deep_output/stage/logs/call_variants/0""; 13:33:50 Execution failed: action 4: unexpected exit status 1 was not ignored; 13:33:51 Worker released; ""run"": operation ""projects/ms-deepvariant/operations/234234234234"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION); . Job args: ['pipelines', '--project', 'ms-deepvariant', 'run', '--attempts', '2', '--pvm-attempts', '0', '--boot-disk-size', '50', '--output-interval', '60s', '--zones', 'europe-west1-*', '--name', 'call_variants', '--vm-labels', 'dv-job-name=call_variants', '--output', 'gs://ms_bam/deep_output/stage/logs/call_variants/0', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.7.2rc', '--inputs', 'EXAMPLES=gs://ms_bam/deep_output/stage/examples/0/*', '--outputs', 'CALLED_VARIANTS=gs://ms_bam/deep_output/stage/called_variants/*', '--machine-type', 'custom-8-30720', '--disk-size', '30', '--set', 'MODEL=gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard/', '--set', 'SHARDS=8', '--set', 'CALL_VARIANTS_SHARD_INDEX=0', '--set', 'CALL_VARIANTS_SHARDS=1', '--command', '\n/opt/deepvariant/bin/call_variants\n --examples ""${EXAMPLES}""/examples_output.tfrecord@""${SHARDS}"".gz\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARD_INDEX}"")""-of-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARDS}"")"".gz\n --checkpoint ""${MODEL}""/model.ckpt\n --batch_size 512\n']; [12/12/2018 13:33:54 ERROR gcp_de",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:7086,pipeline,pipelines,7086,,https://github.com/google/deepvariant/issues/129,1,['pipeline'],['pipelines'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --outfile \""${CALLED_VARIANTS}\""/call_variants_output.tfrecord-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARD_INDEX}\"")\""-of-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARDS}\"")\"".gz --checkpoint \""${MODEL}\""/model.ckpt --batch_size 512""; 13:33:48 Started running ""-c gsutil -q cp /google/logs/output gs://ms_bam/deep_output/stage/logs/call_variants/0""; 13:33:50 Stopped running ""-c gsutil -q cp /google/logs/output gs://ms_bam/deep_output/stage/logs/call_variants/0""; 13:33:50 Execution failed: action 4: unexpected exit status 1 was not ignored; 13:33:51 Worker released; ""run"": operation ""projects/ms-deepvariant/operations/234234234234"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION); . Job args: ['pipelines', '--project', 'ms-deepvariant', 'run', '--attempts', '2', '--pvm-attempts', '0', '--boot-disk-size', '50', '--output-interval', '60s', '--zones', 'europe-west1-*', '--name', 'call_variants', '--vm-labels', 'dv-job-name=call_variants', '--output', 'gs://ms_bam/deep_output/stage/logs/call_variants/0', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.7.2rc', '--inputs', 'EXAMPLES=gs://ms_bam/deep_output/stage/examples/0/*', '--outputs', 'CALLED_VARIANTS=gs://ms_bam/deep_output/stage/called_variants/*', '--machine-type', 'custom-8-30720', '--disk-size', '30', '--set', 'MODEL=gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard/', '--set', 'SHARDS=8', '--set', 'CALL_VARIANTS_SHARD_INDEX=0', '--set', 'CALL_VARIANTS_SHARDS=1', '--command', '\n/opt/deepvariant/bin/call_variants\n --examples ""${EXAMPLES}""/examples_output.tfrecord@""${SHARDS}"".gz\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARD_INDEX}"")""-of-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARDS}"")"".gz\n --checkpoint ""${MODEL}""/model.ckpt\n --batch_size 512\n']; [12/12/2018 13:33:54 ERROR gcp_de
",True,"The text contains logs from a pipeline execution, including timestamps and error messages related to the pipeline's operation. These are typically not meaningful for human readers without additional context, making them hard to understand without technical knowledge."
Deployability,"f my machine. My machine have 376G memory. Here is my command and the error log.; Command:; ```javascript; EXAMPLES=""/home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/NA12878_BGISEQ_PE150_5.training.examples.tfrecord-?????-of-00038.gz""; echo ""${EXAMPLES}""; OUTPUT_DIR=${EXAMPLES%/*} ; time python ./shuffle_tfrecords_beam.py ; --input_pattern_list=""${EXAMPLES}""; --output_pattern_prefix=""${OUTPUT_DIR}/training_set.with_label.shuffled"" ; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" ; --output_dataset_name=""HG001""; --runner=BundleBasedDirectRunner; ```; the error message:; ```; /home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/sample.training.examples.tfrecord-?????-of-00076.gz; /home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/direct_runner.py:342: DeprecationWarning: options is deprecated since First stable release.. References to <pipeline>.options will not be supported; pipeline.replace_all(_get_transform_overrides(pipeline.options)); INFO:root:Running pipeline with DirectRunner.; WARNING:root:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.; ERROR:root:Exception at bundle <apache_beam.runners.direct.bundle_factory._Bundle object at 0x7f86daaa07e8>, due to an exception.; Traceback (most recent call last):; File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 341, in call; finish_state); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 381, in attempt_call; result = evaluator.finish_bundle(); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/transform_evaluator.py"", line 303, in finish_bundle; bundles = _read_values_to_bundles(reader); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apac",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/91:1277,pipeline,pipeline,1277,,https://github.com/google/deepvariant/issues/91,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: f my machine. My machine have 376G memory. Here is my command and the error log.; Command:; ```javascript; EXAMPLES=""/home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/NA12878_BGISEQ_PE150_5.training.examples.tfrecord-?????-of-00038.gz""; echo ""${EXAMPLES}""; OUTPUT_DIR=${EXAMPLES%/*} ; time python ./shuffle_tfrecords_beam.py ; --input_pattern_list=""${EXAMPLES}""; --output_pattern_prefix=""${OUTPUT_DIR}/training_set.with_label.shuffled"" ; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" ; --output_dataset_name=""HG001""; --runner=BundleBasedDirectRunner; ```; the error message:; ```; /home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/sample.training.examples.tfrecord-?????-of-00076.gz; /home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/direct_runner.py:342: DeprecationWarning: options is deprecated since First stable release.. References to <pipeline>.options will not be supported; pipeline.replace_all(_get_transform_overrides(pipeline.options)); INFO:root:Running pipeline with DirectRunner.; WARNING:root:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.; ERROR:root:Exception at bundle <apache_beam.runners.direct.bundle_factory._Bundle object at 0x7f86daaa07e8>, due to an exception.; Traceback (most recent call last):; File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 341, in call; finish_state); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 381, in attempt_call; result = evaluator.finish_bundle(); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/transform_evaluator.py"", line 303, in finish_bundle; bundles = _read_values_to_bundles(reader); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apac
",False,"The text contains complete, meaningful sentences in natural language discussing concepts or ideas about the machine's memory and command execution, as well as an error log which includes detailed information about a pipeline error. However, this is not primarily code snippets or logs that should be filtered out but rather explanatory content alongside some technical details."
Deployability,"f you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. 0K ... 100% 37.6M=0s. 2021-10-11 15:29:12 (37.6 MB/s) - written to stdout [3145/3145]. OK; ++ lsb_release -sc; ++ lsb_release -sc; + add-apt-repository 'deb http://apt.llvm.org/focal/ llvm-toolchain-focal-11 main'; Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease; Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease; Hit:4 http://archiv",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:7218,install,installed,7218,,https://github.com/google/deepvariant/issues/489,2,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: f you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. 0K ... 100% 37.6M=0s. 2021-10-11 15:29:12 (37.6 MB/s) - written to stdout [3145/3145]. OK; ++ lsb_release -sc; ++ lsb_release -sc; + add-apt-repository 'deb http://apt.llvm.org/focal/ llvm-toolchain-focal-11 main'; Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease; Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease; Hit:4 http://archiv
",False,"The text includes a detailed error message indicating unmet dependencies during package installation, which is part of the build process and not human-readable prose. It also contains code snippets and command outputs. Therefore, this content should be filtered out."
Deployability,get install -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev zlib1g-dev; [0mReading package lists...; Building dependency tree...; Reading state information...; zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; De,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:1113,install,installed,1113,,https://github.com/google/deepvariant/issues/489,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: get install -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev zlib1g-dev; [0mReading package lists...; Building dependency tree...; Reading state information...; zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; De
",True,"The text consists of a series of shell commands and package installation output. It includes dependency resolution information such as missing packages, error messages, and unmet dependencies. This type of content is typically associated with program logs or build system outputs which are considered non-meaningful human-readable sentences."
Deployability,"gzi` files:. ```; module load nixpkgs/16.09; module load gcc/7.3.0; module load samtools/1.9; bgzip c_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_elegans/PRJEB28388/c_elegans.PRJEB28388.WS274.annotations.gff3.gz; bgzip -d c_elegans.PRJEB28388.WS274.annotations.gff3.gz; gff2bed < c_elegans.PRJEB28388.WS274.annotations.gff3 > c_elegans.PRJEB28388.WS274.annotations.bed; rm c_elegans.PRJEB28388.WS274.annotations.gff3; ```. The `.vcf.gz` file I download from [CeNDR](https://www.elegansvariation.org/data/release/latest) (comparable to the [DGV database in humans](http://dgv.tcag.ca/dgv/app/home)) then generate its index file `vcf.gz.tbi`:. ```; wget https://storage.googleapis.com/elegansvariation.org/releases/20180527/variation/WI.20180527.impute.vcf.gz; module load nixpkgs/16.09; module load gcc/7.3.0; module load htslib/1.9; tabix -p vcf WI.20180527.impute.vcf.gz; ```. Now my input directory looks like:. ```; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; c_elegans.PRJEB28388.WS274.annotations.bed; WI.20180527.impute.vcf.gz; WI.20180527.impute.vcf.gz.tbi; c_elegans.PRJEB28388.WS274.genomic.fa; c_elegans.PRJEB28388.WS274.genomic.fa.fai; c_elegans.PRJEB28388.WS274.genomic.fa.gz; c_elegans.PRJEB28388.WS274.genomic.fa.gz.fai; c_elegans.PRJEB28388.WS274.genomic.fa.gz.gzi; ```. Now that I think I have all the appropriate input files in my `INPUT_DIR` I will try to run the code again:. ```; [31mFATAL: [0m Image file already exists: ""deepvariant_0.10.0.sif"" - will not overwrite; time=""2020-03-31T18:35:24-07:00"" level=warning msg=""\""/run/user/3019658\"" directory set by $XDG_RUNTIME_DIR does not exist. Either create the directory or unset $XDG_RUNTIME_DIR.: sta",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292:9055,release,releases,9055,,https://github.com/google/deepvariant/issues/292,1,['release'],['releases'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: gzi` files:. ```; module load nixpkgs/16.09; module load gcc/7.3.0; module load samtools/1.9; bgzip c_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_elegans/PRJEB28388/c_elegans.PRJEB28388.WS274.annotations.gff3.gz; bgzip -d c_elegans.PRJEB28388.WS274.annotations.gff3.gz; gff2bed < c_elegans.PRJEB28388.WS274.annotations.gff3 > c_elegans.PRJEB28388.WS274.annotations.bed; rm c_elegans.PRJEB28388.WS274.annotations.gff3; ```. The `.vcf.gz` file I download from [CeNDR](https://www.elegansvariation.org/data/release/latest) (comparable to the [DGV database in humans](http://dgv.tcag.ca/dgv/app/home)) then generate its index file `vcf.gz.tbi`:. ```; wget https://storage.googleapis.com/elegansvariation.org/releases/20180527/variation/WI.20180527.impute.vcf.gz; module load nixpkgs/16.09; module load gcc/7.3.0; module load htslib/1.9; tabix -p vcf WI.20180527.impute.vcf.gz; ```. Now my input directory looks like:. ```; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; c_elegans.PRJEB28388.WS274.annotations.bed; WI.20180527.impute.vcf.gz; WI.20180527.impute.vcf.gz.tbi; c_elegans.PRJEB28388.WS274.genomic.fa; c_elegans.PRJEB28388.WS274.genomic.fa.fai; c_elegans.PRJEB28388.WS274.genomic.fa.gz; c_elegans.PRJEB28388.WS274.genomic.fa.gz.fai; c_elegans.PRJEB28388.WS274.genomic.fa.gz.gzi; ```. Now that I think I have all the appropriate input files in my `INPUT_DIR` I will try to run the code again:. ```; [31mFATAL: [0m Image file already exists: ""deepvariant_0.10.0.sif"" - will not overwrite; time=""2020-03-31T18:35:24-07:00"" level=warning msg=""\""/run/user/3019658\"" directory set by $XDG_RUNTIME_DIR does not exist. Either create the directory or unset $XDG_RUNTIME_DIR.: sta
",False,"The text includes some code snippets and commands, but also contains sentences that are meaningful and explanatory. The presence of natural language discussions about file management and processing steps suggests it is not purely programmatic content."
Deployability,"h the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. ; First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, ; Haley . Here is the error traceback: ; `Traceback (most recent call last):; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793:1152,pipeline,pipeline,1152,,https://github.com/google/deepvariant/issues/793,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: h the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. ; First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, ; Haley . Here is the error traceback: ; `Traceback (most recent call last):; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11
",False,"The content contains a mix of natural language text and technical details about configuration issues, error handling, and troubleshooting. However, the presence of code snippets or logs is not prominent. The user is seeking advice on version compatibility and has described their process in a narrative form, which includes procedural steps and problem-solving attempts. Additionally, the text includes some questions regarding the proper usage of tools (e.g., make_examples), which are part of the educational content surrounding the use of DeepVariant. Therefore, this text should not be eliminated as it primarily contains explanatory and descriptive content intended to help others understand and resolve issues with their setup."
Deployability,"hain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:6822,install,installed,6822,,https://github.com/google/deepvariant/issues/489,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: hain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200
",True,"The text consists primarily of program logs or error messages including timestamps, dependency issues, and package installation errors which are typical in log files."
Deployability,"he command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/public2/courses/ec3121/shareddata/Pomacea_canaliculata/refgenome/GCF_003073045.1_ASM307304v1_genomic.fna"" --infile ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/call_variants_output.tfrecord.gz"" --outfile ""./outputgpu/output.vcf.gz"" --cpus ""2"" --gvcf_outfile ""./outputgpu/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/gvcf.tfrecord@2.gz"". 2024-01-05 16:00:59.661436: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 16:00:59.661893: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-01-05 16:01:06.236791: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; I0105 16:01:06.304423 140416700553024 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default; I0105 16:01:06.676597 140416700553024 postprocess_variants.py:1313] CVO sorting took 0.006136405467987061 minutes; I0105 16:01:06.677379 140416700553024 postprocess_variants.py:1316] Transforming call_variants_output to variants.; I0105 16:01:06.677495 140416700553024 postprocess_variants.py:1318] Using 2 CPUs for parallelization of variant transformation.; I0105 16:01:06.808352 140416700553024 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default; I0105 16:01:08.209710 140416700553024 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:16136,install,installed,16136,,https://github.com/google/deepvariant/issues/761,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: he command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/public2/courses/ec3121/shareddata/Pomacea_canaliculata/refgenome/GCF_003073045.1_ASM307304v1_genomic.fna"" --infile ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/call_variants_output.tfrecord.gz"" --outfile ""./outputgpu/output.vcf.gz"" --cpus ""2"" --gvcf_outfile ""./outputgpu/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/gvcf.tfrecord@2.gz"". 2024-01-05 16:00:59.661436: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 16:00:59.661893: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-01-05 16:01:06.236791: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; I0105 16:01:06.304423 140416700553024 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default; I0105 16:01:06.676597 140416700553024 postprocess_variants.py:1313] CVO sorting took 0.006136405467987061 minutes; I0105 16:01:06.677379 140416700553024 postprocess_variants.py:1316] Transforming call_variants_output to variants.; I0105 16:01:06.677495 140416700553024 postprocess_variants.py:1318] Using 2 CPUs for parallelization of variant transformation.; I0105 16:01:06.808352 140416700553024 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default; I0105 16:01:08.209710 140416700553024 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.
",False,
Deployability,"hg38bundle/Homo_sapiens_assembly38.fasta --reads /scratch/XXXX/ONT_WGS/HH/FL9-1/FL9-1.chr10.bam --output_vcf /scratch/XXXX/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/FL9-1_chr10.output.vcf.gz --output_gvcf /scratch/XXXX/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/FL9-1_chr10.output.g.vcf.gz --num_shards 64 --logging_dir /scratch/XXXX/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/ --intermediate_results_dir /scratch/XXXX/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/intermediate_results. - Error trace: (if applicable); ; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LC_CTYPE = ""C.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LC_CTYPE = ""C.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_2p_bcqtz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>; from deepvariant import make_examples_core; File ""/tmp/Bazel.runfiles_2p_bcqtz/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 43, in <module>; import numpy as np; File ""/usr/local/lib/python3.8/dist-packages/numpy/__init__.py"", line 152, in <module>; from . import random; File ""/usr/local/lib/python3.8/dist-packages/numpy/random/__init__.py"", line 180, in <module>; from . import _pickle; File ""/usr/local/lib/python3.8/dist-packages/numpy/random/_pickle.py"", line 1, in <module>; from .mtrand import RandomState; File ""mtrand.pyx"", line 1, in init numpy.random.mtrand; ImportError: /usr/local/lib/python3.8/dist-packages/numpy/random/_bounded_integers.cpython-38-x86_64-linux-gnu.so: failed to map segment from shared object; Tra",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/854:1767,install,installed,1767,,https://github.com/google/deepvariant/issues/854,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: hg38bundle/Homo_sapiens_assembly38.fasta --reads /scratch/XXXX/ONT_WGS/HH/FL9-1/FL9-1.chr10.bam --output_vcf /scratch/XXXX/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/FL9-1_chr10.output.vcf.gz --output_gvcf /scratch/XXXX/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/FL9-1_chr10.output.g.vcf.gz --num_shards 64 --logging_dir /scratch/XXXX/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/ --intermediate_results_dir /scratch/XXXX/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/intermediate_results. - Error trace: (if applicable); ; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LC_CTYPE = ""C.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LC_CTYPE = ""C.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_2p_bcqtz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>; from deepvariant import make_examples_core; File ""/tmp/Bazel.runfiles_2p_bcqtz/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 43, in <module>; import numpy as np; File ""/usr/local/lib/python3.8/dist-packages/numpy/__init__.py"", line 152, in <module>; from . import random; File ""/usr/local/lib/python3.8/dist-packages/numpy/random/__init__.py"", line 180, in <module>; from . import _pickle; File ""/usr/local/lib/python3.8/dist-packages/numpy/random/_pickle.py"", line 1, in <module>; from .mtrand import RandomState; File ""mtrand.pyx"", line 1, in init numpy.random.mtrand; ImportError: /usr/local/lib/python3.8/dist-packages/numpy/random/_bounded_integers.cpython-38-x86_64-linux-gnu.so: failed to map segment from shared object; Tra
",False,"The text contains command-line arguments and flags, possibly used in a script or program. It includes file paths, options, and parameters. While it may be part of a script's usage or configuration, it is not clear if it contains meaningful human-readable sentences."
Deployability,"how to debug this would be much appreciated. ```; #!/bin/bash; set -euo pipefail; # Set common settings.; PROJECT_ID=valis-194104; OUTPUT_BUCKET=gs://canis/CNR-data; STAGING_FOLDER_NAME=deep_variant_files; OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf; # Model for calling whole exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard; IMAGE_VERSION=0.7.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-b \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --regions gs://canis/CNR-data/CDS-canonical.bed \; --bam gs://canis/CNR-data/TLE_a_001.bam \; --bai gs://canis/CNR-data/TLE_a_001.bam.bai \; --ref gs://genomics-public-data/references/GRCh38_Verily/GRCh38_Verily_v1.genome.fa \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones us-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```. I get the following error: ; ```; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run; _run_make_examples(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116:1169,pipeline,pipeline,1169,,https://github.com/google/deepvariant/issues/116,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: how to debug this would be much appreciated. ```; #!/bin/bash; set -euo pipefail; # Set common settings.; PROJECT_ID=valis-194104; OUTPUT_BUCKET=gs://canis/CNR-data; STAGING_FOLDER_NAME=deep_variant_files; OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf; # Model for calling whole exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard; IMAGE_VERSION=0.7.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-b \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --regions gs://canis/CNR-data/CDS-canonical.bed \; --bam gs://canis/CNR-data/TLE_a_001.bam \; --bai gs://canis/CNR-data/TLE_a_001.bam.bai \; --ref gs://genomics-public-data/references/GRCh38_Verily/GRCh38_Verily_v1.genome.fa \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones us-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```. I get the following error: ; ```; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run; _run_make_examples(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value
",True,The text contains code snippets and logs that are primarily technical and not meaningful human-readable sentences.
Deployability,"http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:6616,install,installed,6616,,https://github.com/google/deepvariant/issues/489,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm
",False,"The text includes detailed error messages from an APT package installation attempt and a Docker build process, which are typically log entries that describe issues in software installations or builds. These logs are not meaningful for human readers in terms of general understanding; they contain technical details about dependency conflicts, missing packages, and specific installation errors. Therefore, this text should be eliminated because it consists primarily of program logs or error messages."
Deployability,"iants --outfile ""/tmp/tmpd74of138/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpd74of138/make_examples.tfrecord@16.gz"" --checkpoint ""input/weights-51-0.995354.ckpt"". 2024-02-18 00:34:28.767569: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-18 00:34:28.768358: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; 2024-02-18 00:34:45.482939: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0218 00:34:45.513278 140119155529536 call_variants.py:471] Total 1 writing processes started.; I0218 00:34:45.536368 140119155529536 dv_utils.py:365] From /tmp/tmpd74of138/make_examples.tfrecord-00000-of-00016.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0218 00:34:45.536543 140119155529536 call_variants.py:506] Shape of input examples: [100, 221, 7]; I0218 00:34:45.537125 140119155529536 call_variants.py:510] Use saved model: False; Model: ""inceptionv3""; _______________________",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:14038,release,release,14038,,https://github.com/google/deepvariant/issues/774,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: iants --outfile ""/tmp/tmpd74of138/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpd74of138/make_examples.tfrecord@16.gz"" --checkpoint ""input/weights-51-0.995354.ckpt"". 2024-02-18 00:34:28.767569: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-18 00:34:28.768358: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; 2024-02-18 00:34:45.482939: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0218 00:34:45.513278 140119155529536 call_variants.py:471] Total 1 writing processes started.; I0218 00:34:45.536368 140119155529536 dv_utils.py:365] From /tmp/tmpd74of138/make_examples.tfrecord-00000-of-00016.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0218 00:34:45.536543 140119155529536 call_variants.py:506] Shape of input examples: [100, 221, 7]; I0218 00:34:45.537125 140119155529536 call_variants.py:510] Use saved model: False; Model: ""inceptionv3""; _______________________
",False,
Deployability,"ibgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18--",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:1860,install,installable,1860,,https://github.com/google/deepvariant/issues/489,1,['install'],['installable'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ibgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18--
",False,"The text includes a mix of natural language and dependency lists, but the majority is not purely human-readable prose. It's primarily a log or build output."
Deployability,"id filename extension; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest version (2.7.12-1~16.04).; python-pip is already the newest version (8.1.1-2ubuntu0.4).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-date: pip in /usr/local/lib/python2.7/dist-packages (18.0); ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:09 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/local/lib/python2.7/dist-packages (0.5.5); Requirement already satisfied: enum34 in /usr/local/lib/python2.7/dist-packages (1.1.6); Requirement already satisfied: sortedcontainers==1.5.3 in /usr/local/lib/python2.7/dist-packages (1.5.3); Requirement already satisfied: intervaltree in /usr/local/lib/python2.7/dist-packages (2.1.0); Requirement already satisfied: sortedcontainers in /usr/local/lib/python2.7/dist-packages (from intervaltree) (1.5.3); Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (2.0.0); Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (4.2.0); Requirement already satisfied: funcsigs>=1; python_version < ""3.3"" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.0.2); Requirement already satisfied: six>=1.9 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.11.0); Requirement already satisfied: num",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:13289,Install,Install,13289,,https://github.com/google/deepvariant/issues/89,1,['Install'],['Install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: id filename extension; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest version (2.7.12-1~16.04).; python-pip is already the newest version (8.1.1-2ubuntu0.4).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-date: pip in /usr/local/lib/python2.7/dist-packages (18.0); ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:09 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/local/lib/python2.7/dist-packages (0.5.5); Requirement already satisfied: enum34 in /usr/local/lib/python2.7/dist-packages (1.1.6); Requirement already satisfied: sortedcontainers==1.5.3 in /usr/local/lib/python2.7/dist-packages (1.5.3); Requirement already satisfied: intervaltree in /usr/local/lib/python2.7/dist-packages (2.1.0); Requirement already satisfied: sortedcontainers in /usr/local/lib/python2.7/dist-packages (from intervaltree) (1.5.3); Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (2.0.0); Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (4.2.0); Requirement already satisfied: funcsigs>=1; python_version < ""3.3"" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.0.2); Requirement already satisfied: six>=1.9 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.11.0); Requirement already satisfied: num
",False,"The provided text is a log file from an automated process, such as package installation logs. It includes timestamps, progress updates, and package management commands. These are typical of system output and not meaningful human-readable content."
Deployability,"in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```; $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory â€˜/mnt/cdw-genomeâ€™: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs r",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:5023,upgrade,upgraded,5023,,https://github.com/google/deepvariant/issues/171,3,"['install', 'upgrade']","['installed', 'upgraded']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```; $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory â€˜/mnt/cdw-genomeâ€™: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs r
",False,
Deployability,"int; saver.restore(sess, checkpoint_filename_with_path); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore; + compat.as_text(save_path)); ValueError: The passed save_path is not a valid checkpoint: gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard//model.ckpt; 13:33:48 Unexpected exit status 1 while running ""-c /opt/deepvariant/bin/call_variants --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --outfile \""${CALLED_VARIANTS}\""/call_variants_output.tfrecord-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARD_INDEX}\"")\""-of-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARDS}\"")\"".gz --checkpoint \""${MODEL}\""/model.ckpt --batch_size 512""; 13:33:48 Started running ""-c gsutil -q cp /google/logs/output gs://ms_bam/deep_output/stage/logs/call_variants/0""; 13:33:50 Stopped running ""-c gsutil -q cp /google/logs/output gs://ms_bam/deep_output/stage/logs/call_variants/0""; 13:33:50 Execution failed: action 4: unexpected exit status 1 was not ignored; 13:33:51 Worker released; ""run"": operation ""projects/ms-deepvariant/operations/234234234234"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION); . Job args: ['pipelines', '--project', 'ms-deepvariant', 'run', '--attempts', '2', '--pvm-attempts', '0', '--boot-disk-size', '50', '--output-interval', '60s', '--zones', 'europe-west1-*', '--name', 'call_variants', '--vm-labels', 'dv-job-name=call_variants', '--output', 'gs://ms_bam/deep_output/stage/logs/call_variants/0', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.7.2rc', '--inputs', 'EXAMPLES=gs://ms_bam/deep_output/stage/examples/0/*', '--outputs', 'CALLED_VARIANTS=gs://ms_bam/deep_output/stage/called_variants/*', '--machine-type', 'custom-8-30720', '--disk-size', '30', '--set', 'MODEL=gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard/', '--set', 'SHARDS=8', '--",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:6867,release,released,6867,,https://github.com/google/deepvariant/issues/129,2,"['pipeline', 'release']","['pipeline', 'released']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: int; saver.restore(sess, checkpoint_filename_with_path); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore; + compat.as_text(save_path)); ValueError: The passed save_path is not a valid checkpoint: gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard//model.ckpt; 13:33:48 Unexpected exit status 1 while running ""-c /opt/deepvariant/bin/call_variants --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --outfile \""${CALLED_VARIANTS}\""/call_variants_output.tfrecord-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARD_INDEX}\"")\""-of-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARDS}\"")\"".gz --checkpoint \""${MODEL}\""/model.ckpt --batch_size 512""; 13:33:48 Started running ""-c gsutil -q cp /google/logs/output gs://ms_bam/deep_output/stage/logs/call_variants/0""; 13:33:50 Stopped running ""-c gsutil -q cp /google/logs/output gs://ms_bam/deep_output/stage/logs/call_variants/0""; 13:33:50 Execution failed: action 4: unexpected exit status 1 was not ignored; 13:33:51 Worker released; ""run"": operation ""projects/ms-deepvariant/operations/234234234234"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION); . Job args: ['pipelines', '--project', 'ms-deepvariant', 'run', '--attempts', '2', '--pvm-attempts', '0', '--boot-disk-size', '50', '--output-interval', '60s', '--zones', 'europe-west1-*', '--name', 'call_variants', '--vm-labels', 'dv-job-name=call_variants', '--output', 'gs://ms_bam/deep_output/stage/logs/call_variants/0', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.7.2rc', '--inputs', 'EXAMPLES=gs://ms_bam/deep_output/stage/examples/0/*', '--outputs', 'CALLED_VARIANTS=gs://ms_bam/deep_output/stage/called_variants/*', '--machine-type', 'custom-8-30720', '--disk-size', '30', '--set', 'MODEL=gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard/', '--set', 'SHARDS=8', '--
",True,"This text is a program log or error message, containing timestamps and command outputs that are typical in logs, as well as error codes (e.g., 'unexpected exit status 1'). It also includes details about pipeline execution, which aligns with compiler/interpreter output."
Deployability,"ioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ; ; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/exceptions.py"", line 640, in conda_exception_handler; return_value = func(*args, **kwargs); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main.py"", line 140, in _main; exit_code = args.func(args, p); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main_install.py"", line 80, in execute; install(args, parser, 'install'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/install.py"", line 326, in install; execute_actions(actions, index, verbose=not context.quiet); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/plan.py"", line 828, in execute_actions; execute_instructions(plan, index, verbose); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 247, in execute_instructions; cmd(state, arg); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 108, in UNLINKLINKTRANSACTION_CMD; txn.execute(); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 297, in execute; rollback_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```; Hovewer",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252:3103,install,install,3103,,https://github.com/google/deepvariant/issues/252,1,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ; ; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/exceptions.py"", line 640, in conda_exception_handler; return_value = func(*args, **kwargs); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main.py"", line 140, in _main; exit_code = args.func(args, p); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main_install.py"", line 80, in execute; install(args, parser, 'install'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/install.py"", line 326, in install; execute_actions(actions, index, verbose=not context.quiet); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/plan.py"", line 828, in execute_actions; execute_instructions(plan, index, verbose); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 247, in execute_instructions; cmd(state, arg); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 108, in UNLINKLINKTRANSACTION_CMD; txn.execute(); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 297, in execute; rollback_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```; Hovewer
",False,"The text includes a detailed error message from a script, which contains logs and exception handling details. This is more of a program output rather than human-readable content."
Deployability,"iousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with this patch by calling it discovery.patch and using this Dockerfile:. ```; ARG VERSION=1.1.0. FROM google/deepvariant:""${VERSION}""-gpu. RUN python3.6 -m pip install --upgrade pip; RUN python3.6 -m pip install --upgrade --force-reinstall cloud-tpu-client. WORKDIR /opt/deepvariant. COPY discovery.patch /opt/deepvariant/; RUN patch /usr/local/lib/python3.6/dist-packages/googleapiclient/discovery.py discovery.patch. CMD [""/opt/deepvariant/bin/run_deepvariant"", ""--help""]; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469:4692,patch,patch,4692,,https://github.com/google/deepvariant/issues/469,9,"['install', 'patch', 'upgrade']","['install', 'patch', 'upgrade']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: iousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with this patch by calling it discovery.patch and using this Dockerfile:. ```; ARG VERSION=1.1.0. FROM google/deepvariant:""${VERSION}""-gpu. RUN python3.6 -m pip install --upgrade pip; RUN python3.6 -m pip install --upgrade --force-reinstall cloud-tpu-client. WORKDIR /opt/deepvariant. COPY discovery.patch /opt/deepvariant/; RUN patch /usr/local/lib/python3.6/dist-packages/googleapiclient/discovery.py discovery.patch. CMD [""/opt/deepvariant/bin/run_deepvariant"", ""--help""]; ```
",True,"The text contains code snippets, specifically patches and Dockerfile commands which are indicative of programmatic content meant for system configuration or build processes."
Deployability,"ipefail; # Set common settings.; PROJECT_ID=valis-194104; OUTPUT_BUCKET=gs://canis/CNR-data; STAGING_FOLDER_NAME=deep_variant_files; OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf; # Model for calling whole exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard; IMAGE_VERSION=0.7.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-b \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --regions gs://canis/CNR-data/CDS-canonical.bed \; --bam gs://canis/CNR-data/TLE_a_001.bam \; --bai gs://canis/CNR-data/TLE_a_001.bam.bai \; --ref gs://genomics-public-data/references/GRCh38_Verily/GRCh38_Verily_v1.genome.fa \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones us-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```. I get the following error: ; ```; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run; _run_make_examples(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""projects/valis-1",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116:1202,pipeline,pipelines,1202,,https://github.com/google/deepvariant/issues/116,1,['pipeline'],['pipelines'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ipefail; # Set common settings.; PROJECT_ID=valis-194104; OUTPUT_BUCKET=gs://canis/CNR-data; STAGING_FOLDER_NAME=deep_variant_files; OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf; # Model for calling whole exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard; IMAGE_VERSION=0.7.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-b \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --regions gs://canis/CNR-data/CDS-canonical.bed \; --bam gs://canis/CNR-data/TLE_a_001.bam \; --bai gs://canis/CNR-data/TLE_a_001.bam.bai \; --ref gs://genomics-public-data/references/GRCh38_Verily/GRCh38_Verily_v1.genome.fa \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones us-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```. I get the following error: ; ```; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run; _run_make_examples(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""projects/valis-1
",False,"The text contains code snippets and logs that may need filtering. However, it also includes a human-readable error message which could be useful for analysis. It's ambiguous whether the primary content is for humans or programmatic use."
Deployability,"irf/.tmp/tmp3vf8mpw9/call_variants_output.tfrecord.gz"" --examples ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs"". 2024-01-05 15:55:31.140705: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 15:55:31.140953: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; 2024-01-05 15:55:38.664328: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; I0105 15:55:38.709242 140372734228288 call_variants.py:471] Total 1 writing processes started.; I0105 15:55:38.765925 140372734228288 dv_utils.py:365] From /public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/make_examples.tfrecord-00000-of-00002.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0105 15:55:38.766286 140372734228288 call_variants.py:506] Shape of input examples: [100, 221, 7]; I0105 15:55:38.768594 140372734228288 call_variants.py:510] Use saved model: Tr",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:13367,release,release,13367,,https://github.com/google/deepvariant/issues/761,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: irf/.tmp/tmp3vf8mpw9/call_variants_output.tfrecord.gz"" --examples ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs"". 2024-01-05 15:55:31.140705: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 15:55:31.140953: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; 2024-01-05 15:55:38.664328: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; I0105 15:55:38.709242 140372734228288 call_variants.py:471] Total 1 writing processes started.; I0105 15:55:38.765925 140372734228288 dv_utils.py:365] From /public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/make_examples.tfrecord-00000-of-00002.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0105 15:55:38.766286 140372734228288 call_variants.py:506] Shape of input examples: [100, 221, 7]; I0105 15:55:38.768594 140372734228288 call_variants.py:510] Use saved model: Tr
",False,
Deployability,"ist.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Buildin",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:11474,Install,Install,11474,,https://github.com/google/deepvariant/issues/89,1,['Install'],['Install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ist.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Buildin
",False,"The text contains logs and warnings from an APT installation process, including timestamps and configuration file references. It also includes commands and output from a package manager, which are typical of program logs."
Deployability,"ittest.fasta.gz.gzi; ls -1 ${INPUT_DIR}; mkdir -p ${OUTPUT_DIR}; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; Here is the complete error msg:; #############################################; **Any additional context:**; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_exampl",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/678:2821,install,installed,2821,,https://github.com/google/deepvariant/issues/678,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ittest.fasta.gz.gzi; ls -1 ${INPUT_DIR}; mkdir -p ${OUTPUT_DIR}; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; Here is the complete error msg:; #############################################; **Any additional context:**; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_exampl
",False,"The text contains some natural language sentences and instructions, along with error messages. It includes commands like `ls`, `mkdir -p`, `singularity run`, `docker`, and `/opt/deepvariant/bin/run_deepvariant` which may be code or logs. However, there are also explanations such as 'Does the quick start test work on your system?' and 'Please test with...' which are in natural language. Additionally, there is an error message from Perl about locale settings. The text is a mix of commands and some human-readable content, so it's not entirely code or logs but contains meaningful sentences."
Deployability,"k start document:. merge_overlaps() got an unexpected keyword argument 'strict'. Any advice as to how I can resolve the issue is greatly appreciated. ```; This is the context of the error.; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/data/hs37d5.fa.gz"" --reads ""/input/data/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" `--gvcf` ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""20"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; I1220 08:40:22.262234 46912496321664 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I1220 08:40:22.268675 46912496321664 genomics_reader.py:223] Reading /input/data/HG002_NIST_150bp_chr20_downsampled_30x.bam with NativeSamReader; I1220 08:40:22.272100 46912496321664 make_examples.py:1324] Preparing inputs; I1220 08:40:22.280786 46912496321664 genomics_reader.py:223] Reading /input/data/HG002_NIST_150bp_chr20_downsampled_30x.bam with NativeSamReader; I1220 08:40:22.292714 46912496321664 make_examples.py:1248] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_UJ59Z1/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", lin",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/255:1072,install,installed,1072,,https://github.com/google/deepvariant/issues/255,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: k start document:. merge_overlaps() got an unexpected keyword argument 'strict'. Any advice as to how I can resolve the issue is greatly appreciated. ```; This is the context of the error.; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/data/hs37d5.fa.gz"" --reads ""/input/data/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" `--gvcf` ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""20"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; I1220 08:40:22.262234 46912496321664 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I1220 08:40:22.268675 46912496321664 genomics_reader.py:223] Reading /input/data/HG002_NIST_150bp_chr20_downsampled_30x.bam with NativeSamReader; I1220 08:40:22.272100 46912496321664 make_examples.py:1324] Preparing inputs; I1220 08:40:22.280786 46912496321664 genomics_reader.py:223] Reading /input/data/HG002_NIST_150bp_chr20_downsampled_30x.bam with NativeSamReader; I1220 08:40:22.292714 46912496321664 make_examples.py:1248] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_UJ59Z1/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", lin
",False,"The text contains a mix of natural language and log data. It includes a question about an error ('merge_overlaps() got an unexpected keyword argument 'strict'') as well as logs from a script execution, such as warnings from perl and Python code with timestamps. However, the presence of meaningful sentences discussing the issue makes it not entirely a log. Therefore, this text should be kept."
Deployability,"k.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:42:05 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Tra",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:8256,upgrade,upgraded,8256,,https://github.com/google/deepvariant/issues/89,3,"['install', 'upgrade']","['installed', 'upgraded']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: k.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:42:05 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Tra
",False,"The text provided contains detailed output from an apt package manager, including warnings about configuration issues with sources.list files and messages about installed packages. While it includes some log-like information, the content is primarily descriptive of the system's state after updates and configurations, serving as informative output for system administrators or users troubleshooting package management issues. Therefore, this text should not be eliminated."
Deployability,"kages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 47, in <module>; import numpy as np; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import core; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/core/__init__.py"", line 47, in <module>; raise ImportError(msg); ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!. Importing the multiarray numpy extension module failed. Most; likely you are trying to import a failed build of numpy.; Here is how to proceed:; - If you're working with a numpy git repository, try `git clean -xdf`; (removes all files not under version control) and rebuild numpy.; - If you are simply trying to use the numpy version that you have installed:; your installation is broken - please reinstall numpy.; - If you have already reinstalled and that did not fix the problem, then:; 1. Check that you are using the Python you expect (you're using /usr/bin/python),; and that you have no directories in your PATH or PYTHONPATH that can; interfere with the Python and numpy versions you're trying to use.; 2. If (1) looks fine, you can open a new issue at; https://github.com/numpy/numpy/issues. Please include details on:; - how you installed Python; - how you installed numpy; - your operating system; - whether or not you have multiple versions of Python installed; - if you built from source, your compiler versions and ideally a build log. Note: this error has many possible causes, so please don't comment on; an existing issue about this - open a new one instead. Original error was: libopenblas.so.0: cannot open shared object file: No such file or directory; ```. I need to run deepvariant as a non-root user via singulairty on the HPC platform. The non-GPU version works just fine.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/243:1971,install,installed,1971,,https://github.com/google/deepvariant/issues/243,3,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: kages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 47, in <module>; import numpy as np; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import core; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/core/__init__.py"", line 47, in <module>; raise ImportError(msg); ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!. Importing the multiarray numpy extension module failed. Most; likely you are trying to import a failed build of numpy.; Here is how to proceed:; - If you're working with a numpy git repository, try `git clean -xdf`; (removes all files not under version control) and rebuild numpy.; - If you are simply trying to use the numpy version that you have installed:; your installation is broken - please reinstall numpy.; - If you have already reinstalled and that did not fix the problem, then:; 1. Check that you are using the Python you expect (you're using /usr/bin/python),; and that you have no directories in your PATH or PYTHONPATH that can; interfere with the Python and numpy versions you're trying to use.; 2. If (1) looks fine, you can open a new issue at; https://github.com/numpy/numpy/issues. Please include details on:; - how you installed Python; - how you installed numpy; - your operating system; - whether or not you have multiple versions of Python installed; - if you built from source, your compiler versions and ideally a build log. Note: this error has many possible causes, so please don't comment on; an existing issue about this - open a new one instead. Original error was: libopenblas.so.0: cannot open shared object file: No such file or directory; ```. I need to run deepvariant as a non-root user via singulairty on the HPC platform. The non-GPU version works just fine.
",False,"The text contains an error message related to an ImportError and provides instructions for resolving the issue, which is explanatory and helpful."
Deployability,"ke_examples.py"", line 1074, in make_examples_runner; resource_monitor = resources.ResourceMonitor().start(); File ""/state/partition1/job-1690025/Bazel.runfiles_ISl2VA/runfiles/com_google_deepvariant/deepvariant/resources.py"", line 59, in __init__; self.metrics_pb = self._initial_metrics_protobuf(); File ""/state/partition1/job-1690025/Bazel.runfiles_ISl2VA/runfiles/com_google_deepvariant/deepvariant/resources.py"", line 72, in _initial_metrics_protobuf; cpu_frequency_mhz=_get_cpu_frequency(),; File ""/state/partition1/job-1690025/Bazel.runfiles_ISl2VA/runfiles/com_google_deepvariant/deepvariant/resources.py"", line 158, in _get_cpu_frequency; freq = psutil.cpu_freq(); File ""/opt/conda/envs/nf-core-deepvariant-1.0/lib/python2.7/site-packages/psutil/__init__.py"", line 1853, in cpu_freq; ret = _psplatform.cpu_freq(); File ""/opt/conda/envs/nf-core-deepvariant-1.0/lib/python2.7/site-packages/psutil/_pslinux.py"", line 701, in cpu_freq; ""can't find current frequency file""); NotImplementedError: can't find current frequency file; parallel: This job failed:; /opt/conda/envs/nf-core-deepvariant-1.0/bin/python /opt/conda/envs/nf-core-deepvariant-1.0/share/deepvariant-0.7.0-0/binaries/DeepVariant/0.7.0/DeepVariant-0.7.0+cl-208818123/make_examples.zip --mode calling --ref PlasmoDB-41_Pfalciparum3D7_Genome.fasta.gz --reads ISO_349.bam --regions PlasmoDB-41_Pfalciparum3D7_Genome_ISO_349.per-base.bed.gz --examples ISO_349_shardedExamples/ISO_349.bam.tfrecord@16.gz --task 10; ```. This is using the Nextflow wrapper script [nf-core/deepvariant](https://github.com/nf-core/deepvariant). The pipeline works with other input data. So it seems unlikely that it is a problem with installation or parameters. DeepVariant was installed using Conda & is using v0.7.0. . Any ideas what the problem is? . Could it be a problem to do with the input data? Perhaps the organism being used? What is the frequency file which it is referring to?. Any help would be much appreciated. Many thanks in advance,; Phil",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/191:4164,pipeline,pipeline,4164,,https://github.com/google/deepvariant/issues/191,3,"['install', 'pipeline']","['installation', 'installed', 'pipeline']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ke_examples.py"", line 1074, in make_examples_runner; resource_monitor = resources.ResourceMonitor().start(); File ""/state/partition1/job-1690025/Bazel.runfiles_ISl2VA/runfiles/com_google_deepvariant/deepvariant/resources.py"", line 59, in __init__; self.metrics_pb = self._initial_metrics_protobuf(); File ""/state/partition1/job-1690025/Bazel.runfiles_ISl2VA/runfiles/com_google_deepvariant/deepvariant/resources.py"", line 72, in _initial_metrics_protobuf; cpu_frequency_mhz=_get_cpu_frequency(),; File ""/state/partition1/job-1690025/Bazel.runfiles_ISl2VA/runfiles/com_google_deepvariant/deepvariant/resources.py"", line 158, in _get_cpu_frequency; freq = psutil.cpu_freq(); File ""/opt/conda/envs/nf-core-deepvariant-1.0/lib/python2.7/site-packages/psutil/__init__.py"", line 1853, in cpu_freq; ret = _psplatform.cpu_freq(); File ""/opt/conda/envs/nf-core-deepvariant-1.0/lib/python2.7/site-packages/psutil/_pslinux.py"", line 701, in cpu_freq; ""can't find current frequency file""); NotImplementedError: can't find current frequency file; parallel: This job failed:; /opt/conda/envs/nf-core-deepvariant-1.0/bin/python /opt/conda/envs/nf-core-deepvariant-1.0/share/deepvariant-0.7.0-0/binaries/DeepVariant/0.7.0/DeepVariant-0.7.0+cl-208818123/make_examples.zip --mode calling --ref PlasmoDB-41_Pfalciparum3D7_Genome.fasta.gz --reads ISO_349.bam --regions PlasmoDB-41_Pfalciparum3D7_Genome_ISO_349.per-base.bed.gz --examples ISO_349_shardedExamples/ISO_349.bam.tfrecord@16.gz --task 10; ```. This is using the Nextflow wrapper script [nf-core/deepvariant](https://github.com/nf-core/deepvariant). The pipeline works with other input data. So it seems unlikely that it is a problem with installation or parameters. DeepVariant was installed using Conda & is using v0.7.0. . Any ideas what the problem is? . Could it be a problem to do with the input data? Perhaps the organism being used? What is the frequency file which it is referring to?. Any help would be much appreciated. Many thanks in advance,; Phil
",False,"The text contains a user's query for troubleshooting a pipeline error. It includes natural language explaining the situation, seeking help, and describing the context of the issue. There are no code snippets or logs without meaningful explanation."
Deployability,"l	13m21.231s; user	118m36.634s; sys	25m56.983s. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""/tmp/tmpd74of138/call_variants_output.tfrecord.gz"" --outfile ""output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""output_apptainer_gpu/HG001.apptainer.gpu.output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpd74of138/gvcf.tfrecord@16.gz"". 2024-02-18 00:47:52.195457: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-18 00:47:52.196245: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-02-18 00:48:10.043945: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0218 00:48:10.133844 139719065552704 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: HG001; I0218 00:48:12.163552 139719065552704 postprocess_variants.py:1313] CVO sorting took 0.03374857902526855 minutes; I0218 00:48:12.163919 139719065552704 postprocess_variants.py:1316] Transforming call_variants_output to variants.; I0218 00:48:12.163960 139719065552704 postprocess_variants.py:1318] Using 16 CPUs for parallelization of variant transformation.; I0218 00:48:12.684920 139719065552704 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: HG001; I0218 00:48:18.996037 139719065552704 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.06664579312006633 minutes; ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:18456,install,installed,18456,,https://github.com/google/deepvariant/issues/774,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: l	13m21.231s; user	118m36.634s; sys	25m56.983s. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""/tmp/tmpd74of138/call_variants_output.tfrecord.gz"" --outfile ""output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""output_apptainer_gpu/HG001.apptainer.gpu.output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpd74of138/gvcf.tfrecord@16.gz"". 2024-02-18 00:47:52.195457: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-18 00:47:52.196245: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-02-18 00:48:10.043945: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0218 00:48:10.133844 139719065552704 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: HG001; I0218 00:48:12.163552 139719065552704 postprocess_variants.py:1313] CVO sorting took 0.03374857902526855 minutes; I0218 00:48:12.163919 139719065552704 postprocess_variants.py:1316] Transforming call_variants_output to variants.; I0218 00:48:12.163960 139719065552704 postprocess_variants.py:1318] Using 16 CPUs for parallelization of variant transformation.; I0218 00:48:12.684920 139719065552704 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: HG001; I0218 00:48:18.996037 139719065552704 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.06664579312006633 minutes; 
",False,"The text contains command execution output, timestamps, and warnings which are typical of program logs or compiler outputs. However, it also includes some descriptive text such as 'Using sample name from call_variants output. Sample name: HG001' which is in natural language. Since there's a mix of both log-like entries and some explanatory sentences, the content isn't exclusively logs or code samples."
Deployability,"l/lib/python3.8/dist-packages (3.13.0); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0); Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2); Installing collected packages: pyparsing; Attempting uninstall: pyparsing; Found existing installation: pyparsing 3.1.1; Uninstalling pyparsing-3.1.1:; Successfully uninstalled pyparsing-3.1.1; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; + DV_PLATFORM=ubuntu-20.04; + ln -sf /usr/bin/python3.8 /usr/local/bin/python3; + cd; + rm -rf clif; + proxychains git clone https://github.com/google/clif.git; ProxyChains-3.1 (http://proxychains.sf.net); Cloning into 'clif'...; |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK; remote: Enumerating objects: 5846, done.; remote: Counting objects: 100% (700/700), done.; remote: Compressing objects: 100% (111/111), done.; remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146; Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done.; Resolving deltas: 100% (4683/4683), done.; + cd clif; + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]; + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7; Note: switc",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739:1394,install,installed,1394,,https://github.com/google/deepvariant/issues/739,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: l/lib/python3.8/dist-packages (3.13.0); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0); Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2); Installing collected packages: pyparsing; Attempting uninstall: pyparsing; Found existing installation: pyparsing 3.1.1; Uninstalling pyparsing-3.1.1:; Successfully uninstalled pyparsing-3.1.1; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; + DV_PLATFORM=ubuntu-20.04; + ln -sf /usr/bin/python3.8 /usr/local/bin/python3; + cd; + rm -rf clif; + proxychains git clone https://github.com/google/clif.git; ProxyChains-3.1 (http://proxychains.sf.net); Cloning into 'clif'...; |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK; remote: Enumerating objects: 5846, done.; remote: Counting objects: 100% (700/700), done.; remote: Compressing objects: 100% (111/111), done.; remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146; Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done.; Resolving deltas: 100% (4683/4683), done.; + cd clif; + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]; + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7; Note: switc
",False,
Deployability,"la/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; ...; and processing is performed on CPUs. . Also, these details are put in the log hudreds of times:; 2024-07-03 18:27:31.862526: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; 2024-07-03 18:27:31.862557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 8308a7bb3067; 2024-07-03 18:27:31.862563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 8308a7bb3067; 2024-07-03 18:27:31.862607: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 555.42.6; 2024-07-03 18:27:31.862621: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 555.42.6; 2024-07-03 18:27:31.862626: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 555.42.6; (then repeated with every call). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; L40S card is CUDA CC = 8.9; supported since CUDA >= 11.8 currently 12.4 and 12.5; https://developer.nvidia.com/cuda-11-8-0-download-archive - installed, still doesn't work with containerized 11.3.1. tensorRT is used; https://docs.nvidia.com/deeplearning/tensorrt/support-matrix/index.html ; currently 10.1 ; version 8 in CUDA 11.8; python >= 3.8. lspci | grep -i nvidia; 0a:00.0 3D controller: NVIDIA Corporation AD102GL [L40S] (rev a1); ae:00.0 3D controller: NVIDIA Corporation AD102GL [L40S] (rev a1). nvidia-container-cli is installed (supersedes nvidia-docker - https://github.com/NVIDIA/nvidia-docker )",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/844:4675,install,installed,4675,,https://github.com/google/deepvariant/issues/844,2,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: la/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; ...; and processing is performed on CPUs. . Also, these details are put in the log hudreds of times:; 2024-07-03 18:27:31.862526: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; 2024-07-03 18:27:31.862557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 8308a7bb3067; 2024-07-03 18:27:31.862563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 8308a7bb3067; 2024-07-03 18:27:31.862607: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 555.42.6; 2024-07-03 18:27:31.862621: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 555.42.6; 2024-07-03 18:27:31.862626: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 555.42.6; (then repeated with every call). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; L40S card is CUDA CC = 8.9; supported since CUDA >= 11.8 currently 12.4 and 12.5; https://developer.nvidia.com/cuda-11-8-0-download-archive - installed, still doesn't work with containerized 11.3.1. tensorRT is used; https://docs.nvidia.com/deeplearning/tensorrt/support-matrix/index.html ; currently 10.1 ; version 8 in CUDA 11.8; python >= 3.8. lspci | grep -i nvidia; 0a:00.0 3D controller: NVIDIA Corporation AD102GL [L40S] (rev a1); ae:00.0 3D controller: NVIDIA Corporation AD102GL [L40S] (rev a1). nvidia-container-cli is installed (supersedes nvidia-docker - https://github.com/NVIDIA/nvidia-docker )
",False,
Deployability,"laptop (M3 macbook). #### PackagesNotFoundError error. The first error I get is -. ```; 1.247 Platform: linux-aarch64; 1.247 Collecting package metadata (repodata.json): ...working... done; 6.190 Solving environment: ...working... failed; 6.260; 6.260 PackagesNotFoundError: The following packages are not available from current channels:; 6.260; 6.260 - bioconda::samtools==1.15; 6.260 - bioconda::bcftools==1.15; 6.260; ```. I resolved this error by removing the version numbers. i.e., removed the `==1.15` from both the lines. #### Error in the build-prerunreq.sh script. Once, I cross the previous error, I get this error -. ```; > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:; 0.101 ========== This script is only maintained for Ubuntu 22.04.; 0.101 ========== Load config settings.; 0.103 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Install the runtime packages' starting; 0.104 ========== This script is only maintained for Ubuntu 22.04.; 0.104 ========== Load config settings.; 0.105 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Misc setup' starting; 1.955 W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-updates InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy-updates InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.c",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/902:1425,Install,Install,1425,,https://github.com/google/deepvariant/issues/902,1,['Install'],['Install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: laptop (M3 macbook). #### PackagesNotFoundError error. The first error I get is -. ```; 1.247 Platform: linux-aarch64; 1.247 Collecting package metadata (repodata.json): ...working... done; 6.190 Solving environment: ...working... failed; 6.260; 6.260 PackagesNotFoundError: The following packages are not available from current channels:; 6.260; 6.260 - bioconda::samtools==1.15; 6.260 - bioconda::bcftools==1.15; 6.260; ```. I resolved this error by removing the version numbers. i.e., removed the `==1.15` from both the lines. #### Error in the build-prerunreq.sh script. Once, I cross the previous error, I get this error -. ```; > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:; 0.101 ========== This script is only maintained for Ubuntu 22.04.; 0.101 ========== Load config settings.; 0.103 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Install the runtime packages' starting; 0.104 ========== This script is only maintained for Ubuntu 22.04.; 0.104 ========== Load config settings.; 0.105 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Misc setup' starting; 1.955 W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-updates InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy-updates InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.c
",True,"The text consists primarily of logs and error messages, including timestamps, error codes, and warnings from a build process. It also contains code snippets or script outputs that are not meaningful as standalone human-readable content."
Deployability,le example `quickstart-input`; is it possible the error is caused by that? . ```; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. ## Trying to fill in the missing input files. I used `bgzip` to convert to gzip and `faidx` to get the `.fai`/`.gzi` files:. ```; module load nixpkgs/16.09; module load gcc/7.3.0; module load samtools/1.9; bgzip c_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_elegans/PRJEB28388/c_elegans.PRJEB28388.WS274.annotations.gff3.gz; bgzip -d c_elegans.PRJEB28388.WS274.annotations.gff3.gz; gff2bed < c_elegans.PRJEB28388.WS274.annotations.gff3 > c_elegans.PRJEB28388.WS274.annotations.bed; rm c_elegans.PRJEB28388.WS274.annotations.gff3; ```. The `.vcf.gz` file I download from [CeNDR](https://www.elegansvariation.org/data/release/latest) (comparable to the [DGV database in humans](http://dgv.tcag.ca/dgv/app/home)) then generate its index file `vcf.gz.tbi`:. ```; wget https://storage.googleapis.com/elegansvariation.org/releases/20180527/variation/WI.20180527.impute.vcf.gz; module load nixpkgs/16.09; module load gcc/7.3.0; module load htslib/1.9; tabix -p vcf WI.20180527.impute.vcf.gz; ```. Now my input directory looks like:. ```; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; c_elegans.PRJEB28388.WS274.annotations.bed; WI.20180527.impute.vcf.gz; WI.20180527.impute.vcf.gz.tbi; c_elegans.PRJEB28388.WS274.geno,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292:8472,release,releases,8472,,https://github.com/google/deepvariant/issues/292,1,['release'],['releases'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: le example `quickstart-input`; is it possible the error is caused by that? . ```; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. ## Trying to fill in the missing input files. I used `bgzip` to convert to gzip and `faidx` to get the `.fai`/`.gzi` files:. ```; module load nixpkgs/16.09; module load gcc/7.3.0; module load samtools/1.9; bgzip c_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_elegans/PRJEB28388/c_elegans.PRJEB28388.WS274.annotations.gff3.gz; bgzip -d c_elegans.PRJEB28388.WS274.annotations.gff3.gz; gff2bed < c_elegans.PRJEB28388.WS274.annotations.gff3 > c_elegans.PRJEB28388.WS274.annotations.bed; rm c_elegans.PRJEB28388.WS274.annotations.gff3; ```. The `.vcf.gz` file I download from [CeNDR](https://www.elegansvariation.org/data/release/latest) (comparable to the [DGV database in humans](http://dgv.tcag.ca/dgv/app/home)) then generate its index file `vcf.gz.tbi`:. ```; wget https://storage.googleapis.com/elegansvariation.org/releases/20180527/variation/WI.20180527.impute.vcf.gz; module load nixpkgs/16.09; module load gcc/7.3.0; module load htslib/1.9; tabix -p vcf WI.20180527.impute.vcf.gz; ```. Now my input directory looks like:. ```; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; c_elegans.PRJEB28388.WS274.annotations.bed; WI.20180527.impute.vcf.gz; WI.20180527.impute.vcf.gz.tbi; c_elegans.PRJEB28388.WS274.geno
",False,"The text contains multiple code commands and file operations, but they are presented in a way that forms complete sentences explaining the process of data preparation. There are no syntax elements like code snippets or program logs. The content is descriptive and discusses steps taken to organize input files, which is human-readable and meaningful."
Deployability,"lease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; C",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:6669,install,installable,6669,,https://github.com/google/deepvariant/issues/489,2,['install'],"['installable', 'installed']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: lease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; C
",True,"The text consists primarily of program logs or error messages including timestamps, dependencies, installation issues, and package management commands. It also contains API-related content and build system output."
Deployability,"les_training_examples.dataset_config.pbtxt"" ; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --config.learning_rate=0.0001 ; --config.num_validation_examples=0 ; --config.tune_every_steps=2000 ; --experiment_dir=/home/${OUTDIR} ; --strategy=mirrored ; --config.batch_size=64 ; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. Though previous runs had higher learning rates (0.01) and batch sizes (128). Training proceeds as follows:. Training Examples: 1454377; Batch Size: 64; Epochs: 1; Steps per epoch: 22724; Steps per tune: 3162; Num train steps: 22724. **Log file**. Here is the top of the log file, including some warnings in case they are relevant:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; 2024-08-28 10:40:42.588215: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_SYSTEM_DRIVER_MISMATCH: system has unsupported display driver / cuda driver combination; I0828 10:40:42.589054 140318776715072 train.py:92] Running with debug=False; I0828 10:40:42.589343 140318776715072 train.py:100] Use TPU at local; I0828 10:40:42.589422 140318776715072 train.py:103] experiment_dir: /home/training_outs/epoch1/; WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.; W0828 10:40:42.596594 140318776715072 cross_device_ops.py:1387] There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.; IN",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:2814,release,release,2814,,https://github.com/google/deepvariant/issues/876,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: les_training_examples.dataset_config.pbtxt"" ; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --config.learning_rate=0.0001 ; --config.num_validation_examples=0 ; --config.tune_every_steps=2000 ; --experiment_dir=/home/${OUTDIR} ; --strategy=mirrored ; --config.batch_size=64 ; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. Though previous runs had higher learning rates (0.01) and batch sizes (128). Training proceeds as follows:. Training Examples: 1454377; Batch Size: 64; Epochs: 1; Steps per epoch: 22724; Steps per tune: 3162; Num train steps: 22724. **Log file**. Here is the top of the log file, including some warnings in case they are relevant:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; 2024-08-28 10:40:42.588215: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_SYSTEM_DRIVER_MISMATCH: system has unsupported display driver / cuda driver combination; I0828 10:40:42.589054 140318776715072 train.py:92] Running with debug=False; I0828 10:40:42.589343 140318776715072 train.py:100] Use TPU at local; I0828 10:40:42.589422 140318776715072 train.py:103] experiment_dir: /home/training_outs/epoch1/; WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.; W0828 10:40:42.596594 140318776715072 cross_device_ops.py:1387] There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.; IN
",False,
Deployability,"lf.rng_state.key[i] = val[i]; self.rng_state.pos = i; ; self._bitgen.state = &self.rng_state; self._bitgen.next_uint64 = &mt19937_uint64; ^; ------------------------------------------------------------; ; _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'.; Processing numpy/random/_bounded_integers.pxd.in; Processing numpy/random/_mt19937.pyx; Traceback (most recent call last):; File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>; main(); File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main; find_process_files(root_dir); File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files; process(root_dir, fromfile, tofile, function, hash_db); File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process; processor_function(fromfile, tofile); File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx; subprocess.check_call(; File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1.; Cythonizing sources; Traceback (most recent call last):; File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>; main(); File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main; json_out['return_val'] = hook(**hook_input['kwargs']); File ""/root/.local/lib/python3.9/site",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/727:1876,install,install-,1876,,https://github.com/google/deepvariant/issues/727,1,['install'],['install-'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: lf.rng_state.key[i] = val[i]; self.rng_state.pos = i; ; self._bitgen.state = &self.rng_state; self._bitgen.next_uint64 = &mt19937_uint64; ^; ------------------------------------------------------------; ; _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'.; Processing numpy/random/_bounded_integers.pxd.in; Processing numpy/random/_mt19937.pyx; Traceback (most recent call last):; File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>; main(); File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main; find_process_files(root_dir); File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files; process(root_dir, fromfile, tofile, function, hash_db); File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process; processor_function(fromfile, tofile); File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx; subprocess.check_call(; File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1.; Cythonizing sources; Traceback (most recent call last):; File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>; main(); File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main; json_out['return_val'] = hook(**hook_input['kwargs']); File ""/root/.local/lib/python3.9/site
",True,"The text contains program logs or error messages (timestamps, error codes, stack traces), which are primarily code-related output and not meaningful human-readable sentences."
Deployability,"libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 15:53:39.096611: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-01-05 15:53:39.226747: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 15:53:39.226871: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-01-05 15:53:49.941043: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; I0105 15:53:49.987410 140173517489984 genomics_reader.py:222] Reading /public2/courses/ec3121/shareddata/Pomacea_canaliculata/wgs/FSL10-M.bam with NativeSamReader; W0105 15:53:49.988560 140173517489984 make_examples_core.py:344] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument.; I0105 15:53:50.021419 140173517489984 make_examples_core.py:301] Task 0/2: Preparing inputs; I0105 15:53:50.036767 140173517489984 genomics_reader.py:222] Reading /public2/courses/ec3121/shareddata/Pomacea_canaliculata/wgs/FSL10-M.bam with NativeSamReader; I0105 15:53:50.054040 140173517489984 make_examples_core.py:301] Task 0/2: Common contigs are ['NC_037",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:5749,install,installed,5749,,https://github.com/google/deepvariant/issues/761,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 15:53:39.096611: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-01-05 15:53:39.226747: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 15:53:39.226871: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-01-05 15:53:49.941043: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; I0105 15:53:49.987410 140173517489984 genomics_reader.py:222] Reading /public2/courses/ec3121/shareddata/Pomacea_canaliculata/wgs/FSL10-M.bam with NativeSamReader; W0105 15:53:49.988560 140173517489984 make_examples_core.py:344] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument.; I0105 15:53:50.021419 140173517489984 make_examples_core.py:301] Task 0/2: Preparing inputs; I0105 15:53:50.036767 140173517489984 genomics_reader.py:222] Reading /public2/courses/ec3121/shareddata/Pomacea_canaliculata/wgs/FSL10-M.bam with NativeSamReader; I0105 15:53:50.054040 140173517489984 make_examples_core.py:301] Task 0/2: Common contigs are ['NC_037
",False,"The text contains multiple error messages and log entries related to TensorRT, CUDA, and file operations. However, it also includes some natural language content such as explanations about reading BAM files and preparing inputs for DeepVariant."
Deployability,"libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-17 23:32:31.108506: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-02-17 23:32:31.006781: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-17 23:32:31.007601: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-02-17 23:32:31.110201: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; ...; 2024-02-17 23:33:25.887517: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0217 23:33:25.933275 140533724936000 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader; I0217 23:33:25.939588 140533724936000 make_examples_core.py:301] Task 15/16: Preparing inputs; I0217 23:33:25.967685 140533724936000 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader; I0217 23:33:26.024591 140533724936000 make_example",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:7167,install,installed,7167,,https://github.com/google/deepvariant/issues/774,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-17 23:32:31.108506: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-02-17 23:32:31.006781: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-17 23:32:31.007601: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-02-17 23:32:31.110201: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; ...; 2024-02-17 23:33:25.887517: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0217 23:33:25.933275 140533724936000 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader; I0217 23:33:25.939588 140533724936000 make_examples_core.py:301] Task 15/16: Preparing inputs; I0217 23:33:25.967685 140533724936000 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader; I0217 23:33:26.024591 140533724936000 make_example
",False,"The text contains error messages and warnings related to TensorRT library loading issues, CUDA initialization failures, and file access problems in the context of a TensorBoard workflow. However, these logs are part of the debugging process and provide valuable information about potential issues with the setup or configuration of GPU libraries. While they may indicate problems, they are still meaningful for humans trying to diagnose and resolve the errors."
Deployability,"ll last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/exceptions.py"", line 640, in conda_exception_handler; return_value = func(*args, **kwargs); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main.py"", line 140, in _main; exit_code = args.func(args, p); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main_install.py"", line 80, in execute; install(args, parser, 'install'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/install.py"", line 326, in install; execute_actions(actions, index, verbose=not context.quiet); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/plan.py"", line 828, in execute_actions; execute_instructions(plan, index, verbose); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 247, in execute_instructions; cmd(state, arg); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 108, in UNLINKLINKTRANSACTION_CMD; txn.execute(); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 297, in execute; rollback_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```; Hovewer, since conda installed successfully all the dependencies, I've then tried to download the precompiled binaries and use them, but couldn't find a guide on how to install them.; Is there a page where to find guidelines on how to install the precompiled deepvariant?; If not, is there a way to fix the anaconda environment issue?. Thank you in advance,. Andrea",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252:4139,install,installed,4139,,https://github.com/google/deepvariant/issues/252,3,['install'],"['install', 'installed']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ll last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/exceptions.py"", line 640, in conda_exception_handler; return_value = func(*args, **kwargs); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main.py"", line 140, in _main; exit_code = args.func(args, p); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main_install.py"", line 80, in execute; install(args, parser, 'install'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/install.py"", line 326, in install; execute_actions(actions, index, verbose=not context.quiet); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/plan.py"", line 828, in execute_actions; execute_instructions(plan, index, verbose); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 247, in execute_instructions; cmd(state, arg); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 108, in UNLINKLINKTRANSACTION_CMD; txn.execute(); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 297, in execute; rollback_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```; Hovewer, since conda installed successfully all the dependencies, I've then tried to download the precompiled binaries and use them, but couldn't find a guide on how to install them.; Is there a page where to find guidelines on how to install the precompiled deepvariant?; If not, is there a way to fix the anaconda environment issue?. Thank you in advance,. Andrea
",False,"The text contains meaningful sentences discussing installation issues and seeking help, which are complete and human-readable. It includes questions about installing precompiled binaries and troubleshooting an Anaconda environment."
Deployability,"ltiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest version (2.7.12-1~16.04).; python-pip is already the newest version (8.1.1-2ubuntu0.4).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:11935,install,installed,11935,,https://github.com/google/deepvariant/issues/89,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ltiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:08 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest version (2.7.12-1~16.04).; python-pip is already the newest version (8.1.1-2ubuntu0.4).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt
",False,"The text contains log entries and build process information, which are not meaningful human-readable sentences."
Deployability,"markDup.bam --examples /home/paul/exome-case-study/output/HG002.examples.tfrecord@1.gz --regions /home/paul/exome-case-study/input/data/refseq.coding_exons.b37.extended50.bed --task 0; Illegal instruction (core dumped); $; ```; After digging a bit deeper, I noticed that loading the `pileup_image_native` module was causing this issue. I was curious and looked at the assembly instructions:. ```; $ gdb -ex r --args python -c ""from deepvariant.python import pileup_image_native""; Program received signal SIGILL, Illegal instruction.; 0x00007ffff5d308b4 in google::protobuf::DescriptorPool::Tables::Tables() (); from /home/paul/make-examples/runfiles/genomics/deepvariant/python/../../_solib_k8/libexternal_Sprotobuf_Uarchive_Slibprotobuf.so; (gdb) disassemble $pc,$pc+32; Dump of assembler code from 0x7ffff5d308b4 to 0x7ffff5d308d4:; => 0x00007ffff5d308b4 <_ZN6google8protobuf14DescriptorPool6TablesC2Ev+676>: vpxor %xmm0,%xmm0,%xmm0; 0x00007ffff5d308b8 <_ZN6google8protobuf14DescriptorPool6TablesC2Ev+680>: lea 0x1b0(%rbx),%rax; 0x00007ffff5d308bf <_ZN6google8protobuf14DescriptorPool6TablesC2Ev+687>: movl $0x0,0x1b0(%rbx); 0x00007ffff5d308c9 <_ZN6google8protobuf14DescriptorPool6TablesC2Ev+697>: movq $0x0,0x1b8(%rbx); End of assembler dump.; (gdb); ```. I noticed the `vpxor` instruction, which made me wonder if my CPU is enabled for AVX, so I proceeded as follows:. ```; $ grep flags /proc/cpuinfo; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 syscall nx rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc pni pclmulqdq monitor ssse3 cx16 sse4_1 sse4_2 x2apic popcnt aes hypervisor lahf_lm; $; ```. This confirmed for me that I don't have AVX support. So it would be great for the examples that will drive usage and be used by many users to learn from, if the provided libraries are compiled with the bare-minimum of CPU qualities. I think it'll make it a bit easier for many users to adopt this nice pipeline. Thanks,; Paul",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21:3775,pipeline,pipeline,3775,,https://github.com/google/deepvariant/issues/21,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: markDup.bam --examples /home/paul/exome-case-study/output/HG002.examples.tfrecord@1.gz --regions /home/paul/exome-case-study/input/data/refseq.coding_exons.b37.extended50.bed --task 0; Illegal instruction (core dumped); $; ```; After digging a bit deeper, I noticed that loading the `pileup_image_native` module was causing this issue. I was curious and looked at the assembly instructions:. ```; $ gdb -ex r --args python -c ""from deepvariant.python import pileup_image_native""; Program received signal SIGILL, Illegal instruction.; 0x00007ffff5d308b4 in google::protobuf::DescriptorPool::Tables::Tables() (); from /home/paul/make-examples/runfiles/genomics/deepvariant/python/../../_solib_k8/libexternal_Sprotobuf_Uarchive_Slibprotobuf.so; (gdb) disassemble $pc,$pc+32; Dump of assembler code from 0x7ffff5d308b4 to 0x7ffff5d308d4:; => 0x00007ffff5d308b4 <_ZN6google8protobuf14DescriptorPool6TablesC2Ev+676>: vpxor %xmm0,%xmm0,%xmm0; 0x00007ffff5d308b8 <_ZN6google8protobuf14DescriptorPool6TablesC2Ev+680>: lea 0x1b0(%rbx),%rax; 0x00007ffff5d308bf <_ZN6google8protobuf14DescriptorPool6TablesC2Ev+687>: movl $0x0,0x1b0(%rbx); 0x00007ffff5d308c9 <_ZN6google8protobuf14DescriptorPool6TablesC2Ev+697>: movq $0x0,0x1b8(%rbx); End of assembler dump.; (gdb); ```. I noticed the `vpxor` instruction, which made me wonder if my CPU is enabled for AVX, so I proceeded as follows:. ```; $ grep flags /proc/cpuinfo; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 syscall nx rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc pni pclmulqdq monitor ssse3 cx16 sse4_1 sse4_2 x2apic popcnt aes hypervisor lahf_lm; $; ```. This confirmed for me that I don't have AVX support. So it would be great for the examples that will drive usage and be used by many users to learn from, if the provided libraries are compiled with the bare-minimum of CPU qualities. I think it'll make it a bit easier for many users to adopt this nice pipeline. Thanks,; Paul
",False,The text contains meaningful sentences in natural language discussing the issues encountered and solutions taken.
Deployability,"mator.py"", line 469, in evaluate; name=name); File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 481, in _actual_eval; hooks.extend(self._convert_eval_steps_to_hooks(steps)); File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 529, in _convert_eval_steps_to_hooks; raise ValueError('Must specify steps > 0, given: {}'.format(steps)); ValueError: Must specify steps > 0, given: 0. real	0m58.116s; user	0m1.590s; sys	0m0.590s. The following is my docker setting:. > FROM ubuntu:16.04. LABEL maintainer=""aaronarchiblad@gmail.com"". ENV DV_USE_GCP_OPTIMIZED_TF_WHL=0. ADD ./deepvariant /home/deepvariant; ADD ./bin /home/bin; ADD ./models /home/models; ADD ./oss_clif.ubuntu-16.latest.tgz /; WORKDIR /home/deepvariant. RUN /home/deepvariant/run-prereq.sh; RUN apt-get -y install python2.7; RUN apt-get -y install python-dev python-pip; RUN apt-get -y install parallel; RUN python2 -m pip install --upgrade --force-reinstall pip; RUN python2 -m pip install apache-beam; RUN python2 -m pip install google-cloud-dataflow. Just in case, if this is needed, I also quote the message from model_train, not sure if this will be useful. . > I0415 07:34:19.468628 140368878327552 model_train.py:310] Set KMP_BLOCKTIME to 0; I0415 07:34:19.469649 140368878327552 model_train.py:244] TF_CONFIG None; W0415 07:34:19.491456 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.; Instructions for updating:; Use eager execution and: ; `tf.data.TFRecordDataset(path)`; I0415 07:34:19.549700 140368878327552 model_train.py:193] Running training on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.with_label.shuffled-?????-of-?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:5860,install,install,5860,,https://github.com/google/deepvariant/issues/172,6,"['install', 'upgrade']","['install', 'upgrade']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: mator.py"", line 469, in evaluate; name=name); File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 481, in _actual_eval; hooks.extend(self._convert_eval_steps_to_hooks(steps)); File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 529, in _convert_eval_steps_to_hooks; raise ValueError('Must specify steps > 0, given: {}'.format(steps)); ValueError: Must specify steps > 0, given: 0. real	0m58.116s; user	0m1.590s; sys	0m0.590s. The following is my docker setting:. > FROM ubuntu:16.04. LABEL maintainer=""aaronarchiblad@gmail.com"". ENV DV_USE_GCP_OPTIMIZED_TF_WHL=0. ADD ./deepvariant /home/deepvariant; ADD ./bin /home/bin; ADD ./models /home/models; ADD ./oss_clif.ubuntu-16.latest.tgz /; WORKDIR /home/deepvariant. RUN /home/deepvariant/run-prereq.sh; RUN apt-get -y install python2.7; RUN apt-get -y install python-dev python-pip; RUN apt-get -y install parallel; RUN python2 -m pip install --upgrade --force-reinstall pip; RUN python2 -m pip install apache-beam; RUN python2 -m pip install google-cloud-dataflow. Just in case, if this is needed, I also quote the message from model_train, not sure if this will be useful. . > I0415 07:34:19.468628 140368878327552 model_train.py:310] Set KMP_BLOCKTIME to 0; I0415 07:34:19.469649 140368878327552 model_train.py:244] TF_CONFIG None; W0415 07:34:19.491456 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.; Instructions for updating:; Use eager execution and: ; `tf.data.TFRecordDataset(path)`; I0415 07:34:19.549700 140368878327552 model_train.py:193] Running training on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.with_label.shuffled-?????-of-?
",False,
Deployability,"mbly=b37>; ##contig=<ID=chr8,length=146364022,assembly=b37>; ##contig=<ID=chr9,length=141213431,assembly=b37>; ##contig=<ID=chr10,length=135534747,assembly=b37>; ##contig=<ID=chr11,length=135006516,assembly=b37>; ##contig=<ID=chr12,length=133851895,assembly=b37>; ##contig=<ID=chr13,length=115169878,assembly=b37>; ##contig=<ID=chr14,length=107349540,assembly=b37>; ##contig=<ID=chr15,length=102531392,assembly=b37>; ##contig=<ID=chr16,length=90354753,assembly=b37>; ##contig=<ID=chr17,length=81195210,assembly=b37>; ##contig=<ID=chr18,length=78077248,assembly=b37>; ##contig=<ID=chr19,length=59128983,assembly=b37>; ##contig=<ID=chr20,length=63025520,assembly=b37>; ##contig=<ID=chr21,length=48129895,assembly=b37>; ##contig=<ID=chr22,length=51304566,assembly=b37>; ##contig=<ID=chrX,length=155270560,assembly=b37>; ##contig=<ID=chrY,length=59373566,assembly=b37>; ##contig=<ID=chrM,length=16569,assembly=b37>; ##fileDate=20160329; ##reference=human_g1k_v37.fasta; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	INTEGRATION; chr20	10000117	.	C	T	50	PASS	platforms=3;platformnames=Illumina,CG,Solid;datasets=3;datasetnames=HiSeqPE300x,CGnormal,SolidSE75bp;callsets=4;callsetnames=HiSeqPE300xfreebayes,HiSeqPE300xGATK,CGnormal,SolidSE75GATKHC;datasetsmissingcall=IonExome,SolidPE50x50bp;lowcov=CS_IonExomeTVC_lowcov,CS_SolidPE50x50GATKHC_lowcov,CS_SolidSE75GATKHC_lowcov;filt=CS_HiSeqPE300xGATK_filt	GT:PS:DP:GQ	0/1:.:706:878; chr20	10000211	.	C	T	50	PASS	platforms=2;platformnames=Illumina,CG;datasets=2;datasetnames=HiSeqPE300x,CGnormal;callsets=3;callsetnames=HiSeqPE300xfreebayes,HiSeqPE300xGATK,CGnormal;datasetsmissingcall=IonExome,SolidPE50x50bp,SolidSE75bp;lowcov=CS_IonExomeTVC_lowcov,CS_SolidPE50x50GATKHC_lowcov,CS_SolidSE75GATKHC_lowcov;filt=CS_SolidPE50x50GATKHC_filt	GT:PS:DP:GQ	0/1:.:695:984; chr20	10000439	.	T	G	50	PASS	platforms=3;platformnames=Illumina,CG,Solid;datasets=4;datasetnames=HiSeqPE300x,CGnormal,SolidPE50x50bp,SolidSE75bp;callsets=5;callsetnames=HiSeqPE300xfreebayes,HiSe",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/239:28848,INTEGRAT,INTEGRATION,28848,,https://github.com/google/deepvariant/issues/239,1,['INTEGRAT'],['INTEGRATION'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: mbly=b37>; ##contig=<ID=chr8,length=146364022,assembly=b37>; ##contig=<ID=chr9,length=141213431,assembly=b37>; ##contig=<ID=chr10,length=135534747,assembly=b37>; ##contig=<ID=chr11,length=135006516,assembly=b37>; ##contig=<ID=chr12,length=133851895,assembly=b37>; ##contig=<ID=chr13,length=115169878,assembly=b37>; ##contig=<ID=chr14,length=107349540,assembly=b37>; ##contig=<ID=chr15,length=102531392,assembly=b37>; ##contig=<ID=chr16,length=90354753,assembly=b37>; ##contig=<ID=chr17,length=81195210,assembly=b37>; ##contig=<ID=chr18,length=78077248,assembly=b37>; ##contig=<ID=chr19,length=59128983,assembly=b37>; ##contig=<ID=chr20,length=63025520,assembly=b37>; ##contig=<ID=chr21,length=48129895,assembly=b37>; ##contig=<ID=chr22,length=51304566,assembly=b37>; ##contig=<ID=chrX,length=155270560,assembly=b37>; ##contig=<ID=chrY,length=59373566,assembly=b37>; ##contig=<ID=chrM,length=16569,assembly=b37>; ##fileDate=20160329; ##reference=human_g1k_v37.fasta; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	INTEGRATION; chr20	10000117	.	C	T	50	PASS	platforms=3;platformnames=Illumina,CG,Solid;datasets=3;datasetnames=HiSeqPE300x,CGnormal,SolidSE75bp;callsets=4;callsetnames=HiSeqPE300xfreebayes,HiSeqPE300xGATK,CGnormal,SolidSE75GATKHC;datasetsmissingcall=IonExome,SolidPE50x50bp;lowcov=CS_IonExomeTVC_lowcov,CS_SolidPE50x50GATKHC_lowcov,CS_SolidSE75GATKHC_lowcov;filt=CS_HiSeqPE300xGATK_filt	GT:PS:DP:GQ	0/1:.:706:878; chr20	10000211	.	C	T	50	PASS	platforms=2;platformnames=Illumina,CG;datasets=2;datasetnames=HiSeqPE300x,CGnormal;callsets=3;callsetnames=HiSeqPE300xfreebayes,HiSeqPE300xGATK,CGnormal;datasetsmissingcall=IonExome,SolidPE50x50bp,SolidSE75bp;lowcov=CS_IonExomeTVC_lowcov,CS_SolidPE50x50GATKHC_lowcov,CS_SolidSE75GATKHC_lowcov;filt=CS_SolidPE50x50GATKHC_filt	GT:PS:DP:GQ	0/1:.:695:984; chr20	10000439	.	T	G	50	PASS	platforms=3;platformnames=Illumina,CG,Solid;datasets=4;datasetnames=HiSeqPE300x,CGnormal,SolidPE50x50bp,SolidSE75bp;callsets=5;callsetnames=HiSeqPE300xfreebayes,HiSe
",False,"The text contains multiple lines of code or data, including lines starting with 'chr20', '##fileDate', '#CHROM', etc. These are indicative of program output or configuration files, which should be filtered out. Additionally, the content includes parameter lists and format specifications typical in API documentation or data processing logs."
Deployability,"med 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>; import apache_beam as beam; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>; from apache_beam import coders; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>; from apache_beam.coders.coders import *; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>; from apache_beam.coders import coder_impl; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>; import numpy as np; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>; from . import core; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>; raise ImportError(msg); ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!. Importing the numpy C-extensions failed. This error can happen for; many reasons, often due to issues with your setup or how NumPy was; installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3""; * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect.; Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793:4088,install,installed,4088,,https://github.com/google/deepvariant/issues/793,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: med 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>; import apache_beam as beam; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>; from apache_beam import coders; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>; from apache_beam.coders.coders import *; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>; from apache_beam.coders import coder_impl; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>; import numpy as np; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>; from . import core; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>; raise ImportError(msg); ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!. Importing the numpy C-extensions failed. This error can happen for; many reasons, often due to issues with your setup or how NumPy was; installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3""; * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect.; Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`
",False,
Deployability,"mmand on Google Cloud). **1b)** I realize that it would take some time (and Iâ€™m not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)?. Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that?. **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```; $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove an",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:4102,install,installation,4102,,https://github.com/google/deepvariant/issues/171,1,['install'],['installation'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: mmand on Google Cloud). **1b)** I realize that it would take some time (and Iâ€™m not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)?. Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that?. **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```; $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove an
",False,"The text contains a mix of natural language and code snippets. However, it primarily discusses running scripts and error messages related to Google Cloud tools, which may be more technical in nature but does include some descriptive content."
Deployability,"n to raise exceptions.; performance hint: _common.pyx:1002:32: Exception check after calling 'f1' will always require the GIL to be acquired. Declare 'f1' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; ; Error compiling Cython file:; ------------------------------------------------------------; ...; self.rng_state.ctr.v[i] = counter[i]; ; self._reset_state_variables(); ; self._bitgen.state = <void *>&self.rng_state; self._bitgen.next_uint64 = &philox_uint64; ^; ------------------------------------------------------------; ; _philox.pyx:195:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to the type of the value being assigned.; Processing numpy/random/_bounded_integers.pxd.in; Processing numpy/random/_common.pyx; Processing numpy/random/_philox.pyx; Traceback (most recent call last):; File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 235, in <module>; main(); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 231, in main; find_process_files(root_dir); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 222, in find_process_files; process(root_dir, fromfile, tofile, function, hash_db); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 188, in process; processor_function(fromfile, tofile); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 77, in process_pyx; subprocess.check_call(; File ""/public/home/zhanghl3/miniconda3/envs/deepvariant/lib/python3.10/subprocess.py"", line 369, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['/public/home/zhanghl3/miniconda3/envs/deepvariant/bin/python3', '-m', 'cython', '-3', '--fa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:8801,install,install-,8801,,https://github.com/google/deepvariant/issues/859,1,['install'],['install-'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: n to raise exceptions.; performance hint: _common.pyx:1002:32: Exception check after calling 'f1' will always require the GIL to be acquired. Declare 'f1' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; ; Error compiling Cython file:; ------------------------------------------------------------; ...; self.rng_state.ctr.v[i] = counter[i]; ; self._reset_state_variables(); ; self._bitgen.state = <void *>&self.rng_state; self._bitgen.next_uint64 = &philox_uint64; ^; ------------------------------------------------------------; ; _philox.pyx:195:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to the type of the value being assigned.; Processing numpy/random/_bounded_integers.pxd.in; Processing numpy/random/_common.pyx; Processing numpy/random/_philox.pyx; Traceback (most recent call last):; File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 235, in <module>; main(); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 231, in main; find_process_files(root_dir); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 222, in find_process_files; process(root_dir, fromfile, tofile, function, hash_db); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 188, in process; processor_function(fromfile, tofile); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 77, in process_pyx; subprocess.check_call(; File ""/public/home/zhanghl3/miniconda3/envs/deepvariant/lib/python3.10/subprocess.py"", line 369, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['/public/home/zhanghl3/miniconda3/envs/deepvariant/bin/python3', '-m', 'cython', '-3', '--fa
",,[WinError 10054] An existing connection was forcibly closed by the remote host
Deployability,"n3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2213k 100 2213k 0 0 1634k 0 0:00:01 0:00:01 --:--:-- 1634k; Collecting pip; Using cached pip-24.2-py3-none-any.whl.metadata (3.6 kB); Using cached pip-24.2-py3-none-any.whl (1.8 MB); Installing collected packages: pip; WARNING: The scripts pip, pip3 and pip3.10 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-24.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning. [notice] A new release of pip is available: 24.0 -> 24.2; [notice] To update, run: pip install --upgrade pip; Python 3.10.14; pip 24.0 from /usr/local/lib/python3.10/site-packages/pip (python 3.10); ========== [Fri 02 Aug 2024 02:20:22 PM CST] Stage 'Install python3 packages' starting; error: subprocess-exited-with-error; ; Ã— Preparing metadata (pyproject.toml) did not run successfully.; â”‚ exit code: 1; â•°â”€> [78 lines of output]; Running from numpy source directory.; setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates; run_build = parse_setuppy_commands(); performance hint: _common.pyx:275:19: Exception check after calling 'random_func' will always require the GIL to be acquired. Declare 'random_func' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:299:19: Exception check after calling 'random_func' will always require the GIL to be acquired. Dec",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:1888,release,release,1888,,https://github.com/google/deepvariant/issues/859,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: n3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2213k 100 2213k 0 0 1634k 0 0:00:01 0:00:01 --:--:-- 1634k; Collecting pip; Using cached pip-24.2-py3-none-any.whl.metadata (3.6 kB); Using cached pip-24.2-py3-none-any.whl (1.8 MB); Installing collected packages: pip; WARNING: The scripts pip, pip3 and pip3.10 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-24.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning. [notice] A new release of pip is available: 24.0 -> 24.2; [notice] To update, run: pip install --upgrade pip; Python 3.10.14; pip 24.0 from /usr/local/lib/python3.10/site-packages/pip (python 3.10); ========== [Fri 02 Aug 2024 02:20:22 PM CST] Stage 'Install python3 packages' starting; error: subprocess-exited-with-error; ; Ã— Preparing metadata (pyproject.toml) did not run successfully.; â”‚ exit code: 1; â•°â”€> [78 lines of output]; Running from numpy source directory.; setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates; run_build = parse_setuppy_commands(); performance hint: _common.pyx:275:19: Exception check after calling 'random_func' will always require the GIL to be acquired. Declare 'random_func' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:299:19: Exception check after calling 'random_func' will always require the GIL to be acquired. Dec
",,[WinError 10054] An existing connection was forcibly closed by the remote host
Deployability,"nd #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432); ```; File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants; ie_estimator = OpenVINOEstimator(; File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__; freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb); File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph; graph_def = optimize_for_inference_lib.optimize_for_inference(; NameError: name 'optimize_for_inference_lib' is not defined; ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run; ```; python -c 'from tensorflow.python.tools import optimize_for_inference_lib'; ```. The real issue is openvino is not installed ; ```; python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; ModuleNotFoundError: No module named 'openvino'; ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with openvino no longer being as helpful, but then there has been some recent updates, so not sure if it is still recommended or deprecated as it has disappeared from some docs. Best,; Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/541:1282,install,installed,1282,,https://github.com/google/deepvariant/issues/541,2,"['install', 'update']","['installed', 'updates']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: nd #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432); ```; File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants; ie_estimator = OpenVINOEstimator(; File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__; freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb); File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph; graph_def = optimize_for_inference_lib.optimize_for_inference(; NameError: name 'optimize_for_inference_lib' is not defined; ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run; ```; python -c 'from tensorflow.python.tools import optimize_for_inference_lib'; ```. The real issue is openvino is not installed ; ```; python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; ModuleNotFoundError: No module named 'openvino'; ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with openvino no longer being as helpful, but then there has been some recent updates, so not sure if it is still recommended or deprecated as it has disappeared from some docs. Best,; Alex
",,[WinError 10054] An existing connection was forcibly closed by the remote host
Deployability,"ndencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - &",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:1718,install,installable,1718,,https://github.com/google/deepvariant/issues/489,1,['install'],['installable'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ndencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - &
",,[WinError 10054] An existing connection was forcibly closed by the remote host
Deployability,"nds: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)...; 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. - 100%[====================================================================",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:2238,install,installed,2238,,https://github.com/google/deepvariant/issues/489,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: nds: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)...; 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. - 100%[====================================================================
",,[WinError 10054] An existing connection was forcibly closed by the remote host
Deployability,"ne 209, in _tpu_service; raise RuntimeError('Missing runtime dependency on the Google API client. '; RuntimeError: Missing runtime dependency on the Google API client. Run `pip install cloud-tpu-client` to fix. ```; However, cloud-tpu-client is not actually the problem. The issue is that `google.api_core.client_options` is not found when being imported from `googleapiclient.discovery`. The issue appears to be the [python3.3 _ _ init _ _.py trap](http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with t",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469:3734,patch,patching,3734,,https://github.com/google/deepvariant/issues/469,1,['patch'],['patching'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ne 209, in _tpu_service; raise RuntimeError('Missing runtime dependency on the Google API client. '; RuntimeError: Missing runtime dependency on the Google API client. Run `pip install cloud-tpu-client` to fix. ```; However, cloud-tpu-client is not actually the problem. The issue is that `google.api_core.client_options` is not found when being imported from `googleapiclient.discovery`. The issue appears to be the [python3.3 _ _ init _ _.py trap](http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with t
",,[WinError 10054] An existing connection was forcibly closed by the remote host
Deployability,"nfo to /cromwell_root/tmp.cd83af44/tmpuzrx3yrs/make_examples.tfrecord-00013-of-00016.gz.example_info.json; I0203 17:23:09.199875 136895166957376 make_examples_core.py:2958] example_shape = None; I0203 17:23:09.200180 136895166957376 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 7, 9, 10]; I0203 17:23:09.201941 136895166957376 make_examples_core.py:301] Task 13/16: Found 0 candidate variants; I0203 17:23:09.202048 136895166957376 make_examples_core.py:301] Task 13/16: Created 0 examples. real 112m20.375s; user 1760m59.767s; sys 11m47.541s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/cromwell_root/tmp.cd83af44/tmpuzrx3yrs/call_variants_output.tfrecord.gz"" --examples ""/cromwell_root/tmp.cd83af44/tmpuzrx3yrs/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; I0203 17:23:14.218397 132068663560000 call_variants.py:471] Total 1 writing processes started.; W0203 17:23:14.224790 132068663560000 call_variants.py:482] Unable to read any records from /cromwell_root/tmp.cd83af44/tmpuzrx3yrs/make_examples.tfrecord@16.gz. Output will contain zero records.; I0203 17:23:14.225926 132068663560000 call_variants.py:623] Complete: call_variants.; ```. And then the program hangs there for 10+ hours (UTC time when I'm reporting is Feb. 04, 04:05, and the program still appears running). . We've observed this for both ONT and HiFi data on multiple samples, further suggesting this isn't a data issue. Thanks!; Steve",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/769:3390,release,release,3390,,https://github.com/google/deepvariant/issues/769,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: nfo to /cromwell_root/tmp.cd83af44/tmpuzrx3yrs/make_examples.tfrecord-00013-of-00016.gz.example_info.json; I0203 17:23:09.199875 136895166957376 make_examples_core.py:2958] example_shape = None; I0203 17:23:09.200180 136895166957376 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 7, 9, 10]; I0203 17:23:09.201941 136895166957376 make_examples_core.py:301] Task 13/16: Found 0 candidate variants; I0203 17:23:09.202048 136895166957376 make_examples_core.py:301] Task 13/16: Created 0 examples. real 112m20.375s; user 1760m59.767s; sys 11m47.541s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/cromwell_root/tmp.cd83af44/tmpuzrx3yrs/call_variants_output.tfrecord.gz"" --examples ""/cromwell_root/tmp.cd83af44/tmpuzrx3yrs/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; I0203 17:23:14.218397 132068663560000 call_variants.py:471] Total 1 writing processes started.; W0203 17:23:14.224790 132068663560000 call_variants.py:482] Unable to read any records from /cromwell_root/tmp.cd83af44/tmpuzrx3yrs/make_examples.tfrecord@16.gz. Output will contain zero records.; I0203 17:23:14.225926 132068663560000 call_variants.py:623] Complete: call_variants.; ```. And then the program hangs there for 10+ hours (UTC time when I'm reporting is Feb. 04, 04:05, and the program still appears running). . We've observed this for both ONT and HiFi data on multiple samples, further suggesting this isn't a data issue. Thanks!; Steve
",,[WinError 10054] An existing connection was forcibly closed by the remote host
Deployability,"ng instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```; (base) âœ” /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|â€¦5] ; 20:46 $ samtools flagstat GFX.bam ; ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads); 881297 + 0 primary; 0 + 0 secondary; 59254 + 0 supplementary; 0 + 0 duplicates; 0 + 0 primary duplicates; 940551 + 0 mapped (100.00% : N/A); 881297 + 0 primary mapped (100.00% : N/A); 0 + 0 paired in sequencing; 0 + 0 read1; 0 + 0 read2; 0 + 0 properly paired (N/A : N/A); 0 + 0 with itself and mate mapped; 0 + 0 singletons (N/A : N/A); 0 + 0 with mate mapped to a different chr; 0 + 0 with mate mapped to a different chr (mapQ>=5); ```. ; **Steps to reproduce:**; - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :; ; ```; E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'; 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0; 2023-06-20 22:48:40.27288",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666:2189,install,install,2189,,https://github.com/google/deepvariant/issues/666,1,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ng instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```; (base) âœ” /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|â€¦5] ; 20:46 $ samtools flagstat GFX.bam ; ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads); 881297 + 0 primary; 0 + 0 secondary; 59254 + 0 supplementary; 0 + 0 duplicates; 0 + 0 primary duplicates; 940551 + 0 mapped (100.00% : N/A); 881297 + 0 primary mapped (100.00% : N/A); 0 + 0 paired in sequencing; 0 + 0 read1; 0 + 0 read2; 0 + 0 properly paired (N/A : N/A); 0 + 0 with itself and mate mapped; 0 + 0 singletons (N/A : N/A); 0 + 0 with mate mapped to a different chr; 0 + 0 with mate mapped to a different chr (mapQ>=5); ```. ; **Steps to reproduce:**; - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :; ; ```; E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'; 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0; 2023-06-20 22:48:40.27288
",,[WinError 10054] An existing connection was forcibly closed by the remote host
Deployability,"ng objects: 100% (111/111), done.; remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146; Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done.; Resolving deltas: 100% (4683/4683), done.; + cd clif; + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]; + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7; Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`; + ./INSTALL.sh; +++ dirname ./INSTALL.sh; ++ cd .; ++ pwd; + CLIFSRC_DIR=/root/clif; + BUILD_DIR=/root/clif/build; + declare -a CMAKE_G_FLAG; + declare -a MAKE_PARALLELISM; + which ninja; + CMAKE_G_FLAGS=(); + MAKE_OR_NINJA=make; + MAKE_PARALLELISM=(-j 2); + [[ -r /proc/cpuinfo ]]; ++ cat /proc/cpuinfo; ++ grep -c '^processor'; + N_CPUS=32; + [[ 32 -gt 0 ]]; + MAKE_PARALLELISM=(-j $N_CPUS); + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}); + echo 'Using make for the clif backend build.'; Using make for the clif backend build.; + [[ '' =~ ^-?-h ]]; + [[ -n '' ]]; ++ which python3; + PYTHON=/usr/local/bin/python3; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739:3042,INSTALL,INSTALL,3042,,https://github.com/google/deepvariant/issues/739,1,['INSTALL'],['INSTALL'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ng objects: 100% (111/111), done.; remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146; Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done.; Resolving deltas: 100% (4683/4683), done.; + cd clif; + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]; + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7; Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`; + ./INSTALL.sh; +++ dirname ./INSTALL.sh; ++ cd .; ++ pwd; + CLIFSRC_DIR=/root/clif; + BUILD_DIR=/root/clif/build; + declare -a CMAKE_G_FLAG; + declare -a MAKE_PARALLELISM; + which ninja; + CMAKE_G_FLAGS=(); + MAKE_OR_NINJA=make; + MAKE_PARALLELISM=(-j 2); + [[ -r /proc/cpuinfo ]]; ++ cat /proc/cpuinfo; ++ grep -c '^processor'; + N_CPUS=32; + [[ 32 -gt 0 ]]; + MAKE_PARALLELISM=(-j $N_CPUS); + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}); + echo 'Using make for the clif backend build.'; Using make for the clif backend build.; + [[ '' =~ ^-?-h ]]; + [[ -n '' ]]; ++ which python3; + PYTHON=/usr/local/bin/python3; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; 
",,[WinError 10054] An existing connection was forcibly closed by the remote host
Deployability,"ng variants (and writing to temporary file) took 0.06664579312006633 minutes; I0218 00:48:39.012242 139719065552704 postprocess_variants.py:1407] Finished writing VCF and gVCF in 0.33359973033269247 minutes. real	0m59.941s; user	0m58.218s; sys	0m5.086s. ***** Running the command:*****; time /opt/deepvariant/bin/vcf_stats_report --input_vcf ""output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz"" --outfile_base ""output_apptainer_gpu/HG001.apptainer.gpu.output"". 2024-02-18 00:48:50.006549: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-18 00:48:50.008250: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-02-18 00:48:57.417490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0218 00:48:57.421117 139673283618624 genomics_reader.py:222] Reading output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz with NativeVcfReader. real	0m23.982s; user	0m12.056s; sys	0m2.006s. ```. ----------------------------------------------------------------------------------; ----------------------------------------------------------------------------------. My system is Ubuntu 22.04. I have two GPUs. . **nvidia-smi** ; ``` ; Sat Feb 17 23:40:49 2024 ; +-----------------------------------------------------------------------------+; | NVIDIA-SMI 525.147.05 Driver Version: 525.147.05 CUDA Version: 12.0 |; |-------------------------------+----------------------+----------------------+; | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |; | Fan Temp Perf P",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:20378,install,installed,20378,,https://github.com/google/deepvariant/issues/774,1,['install'],['installed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ng variants (and writing to temporary file) took 0.06664579312006633 minutes; I0218 00:48:39.012242 139719065552704 postprocess_variants.py:1407] Finished writing VCF and gVCF in 0.33359973033269247 minutes. real	0m59.941s; user	0m58.218s; sys	0m5.086s. ***** Running the command:*****; time /opt/deepvariant/bin/vcf_stats_report --input_vcf ""output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz"" --outfile_base ""output_apptainer_gpu/HG001.apptainer.gpu.output"". 2024-02-18 00:48:50.006549: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-18 00:48:50.008250: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-02-18 00:48:57.417490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0218 00:48:57.421117 139673283618624 genomics_reader.py:222] Reading output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz with NativeVcfReader. real	0m23.982s; user	0m12.056s; sys	0m2.006s. ```. ----------------------------------------------------------------------------------; ----------------------------------------------------------------------------------. My system is Ubuntu 22.04. I have two GPUs. . **nvidia-smi** ; ``` ; Sat Feb 17 23:40:49 2024 ; +-----------------------------------------------------------------------------+; | NVIDIA-SMI 525.147.05 Driver Version: 525.147.05 CUDA Version: 12.0 |; |-------------------------------+----------------------+----------------------+; | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |; | Fan Temp Perf P
",,[WinError 10054] An existing connection was forcibly closed by the remote host
