quality_attribute,keyword,matched_word,sentence,source,filename,author,repo,version,wiki,url
Deployability,release,release,"#! /usr/bin/env python; #; # Harvest the solved issues for a certain tag and print them out in a format; # which is ready to be pasted in the release notes.; #; # Copyright (c) 2024 Rene Brun and Fons Rademakers; # Author: Enrico Guiraud, Axel Naumann, Danilo Piparo; # Don't care about ""."" or ""/"" or ""-"" delimiting version number parts:; # check we did not miss anything and we did not count anything twice",MatchSource.CODE_COMMENT,.ci/get_solved_issues-github-actions.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/.ci/get_solved_issues-github-actions.py
Availability,down,downloads,"#!/usr/bin/env python3; # pylint: disable=broad-except,missing-function-docstring,line-too-long; """"""This mainly functions as a shell script, but python is used for its; superior control flow. An important requirement of the CI is easily; reproducible builds, therefore a wrapper is made for running shell; commands so that they are also logged. The log is printed when build fails/succeeds and needs to perfectly; reproduce the build when pasted into a shell. Therefore all file system; modifying code not executed from shell needs a shell equivalent; explicitly appended to the shell log.; e.g. `os.chdir(x)` requires `cd x` to be appended to the shell log """"""; # Used for uploads; # Used for downloads; # openstack.enable_logging(debug=True); # used when uploading artifacts, calculate early since build times are inconsistent; # Load CMake options from .github/workflows/root-ci-config/buildconfig/[platform].txt; # file below overwrites values from above; # The hash of the build option string is used to find existing artifacts; # with matching build options on s3 storage.; # Differentiate between macos versions: it's possible to have the same label; # for different macos versions, especially different minor versions.; # Make testing of CI in forks not impact artifacts; # Where to put the roottest directory; # Where to find the target branch; # Where to find the incoming branch; # Build artifacts should only be uploaded for full builds, and only for; # ""official"" branches (master, v?-??-??-patches), i.e. not for pull_request; # We also want to upload any successful build, even if it fails testing; # later on.; # it is difficult to use boolean flags from github actions, use strings to convey; # true/false for boolean arguments instead.; # Set argument to True if matched; # runners should never have root permissions but be on the safe side; # windows; # mac/linux/POSIX; """"""; which cmake; cmake --version; which c++ || true; c++ --version || true; uname -a || true; cat /etc/os-rele",MatchSource.CODE_COMMENT,.github/workflows/root-ci-config/build_root.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/.github/workflows/root-ci-config/build_root.py
Deployability,patch,patches,"ild fails/succeeds and needs to perfectly; reproduce the build when pasted into a shell. Therefore all file system; modifying code not executed from shell needs a shell equivalent; explicitly appended to the shell log.; e.g. `os.chdir(x)` requires `cd x` to be appended to the shell log """"""; # Used for uploads; # Used for downloads; # openstack.enable_logging(debug=True); # used when uploading artifacts, calculate early since build times are inconsistent; # Load CMake options from .github/workflows/root-ci-config/buildconfig/[platform].txt; # file below overwrites values from above; # The hash of the build option string is used to find existing artifacts; # with matching build options on s3 storage.; # Differentiate between macos versions: it's possible to have the same label; # for different macos versions, especially different minor versions.; # Make testing of CI in forks not impact artifacts; # Where to put the roottest directory; # Where to find the target branch; # Where to find the incoming branch; # Build artifacts should only be uploaded for full builds, and only for; # ""official"" branches (master, v?-??-??-patches), i.e. not for pull_request; # We also want to upload any successful build, even if it fails testing; # later on.; # it is difficult to use boolean flags from github actions, use strings to convey; # true/false for boolean arguments instead.; # Set argument to True if matched; # runners should never have root permissions but be on the safe side; # windows; # mac/linux/POSIX; """"""; which cmake; cmake --version; which c++ || true; c++ --version || true; uname -a || true; cat /etc/os-release || true; sw_vers || true; uptime || true; df || true; """"""; """"""; Just return the exit code in case of test failures instead of `die()`-ing; report test; failures in main().; """"""; # Print CMake cached config; # rebase fails unless user.email and user.name is set; """"""; get_stdout_subprocess; execute and log a command.; capture the stdout, strip white space and return",MatchSource.CODE_COMMENT,.github/workflows/root-ci-config/build_root.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/.github/workflows/root-ci-config/build_root.py
Integrability,wrap,wrapper,"#!/usr/bin/env python3; # pylint: disable=broad-except,missing-function-docstring,line-too-long; """"""This mainly functions as a shell script, but python is used for its; superior control flow. An important requirement of the CI is easily; reproducible builds, therefore a wrapper is made for running shell; commands so that they are also logged. The log is printed when build fails/succeeds and needs to perfectly; reproduce the build when pasted into a shell. Therefore all file system; modifying code not executed from shell needs a shell equivalent; explicitly appended to the shell log.; e.g. `os.chdir(x)` requires `cd x` to be appended to the shell log """"""; # Used for uploads; # Used for downloads; # openstack.enable_logging(debug=True); # used when uploading artifacts, calculate early since build times are inconsistent; # Load CMake options from .github/workflows/root-ci-config/buildconfig/[platform].txt; # file below overwrites values from above; # The hash of the build option string is used to find existing artifacts; # with matching build options on s3 storage.; # Differentiate between macos versions: it's possible to have the same label; # for different macos versions, especially different minor versions.; # Make testing of CI in forks not impact artifacts; # Where to put the roottest directory; # Where to find the target branch; # Where to find the incoming branch; # Build artifacts should only be uploaded for full builds, and only for; # ""official"" branches (master, v?-??-??-patches), i.e. not for pull_request; # We also want to upload any successful build, even if it fails testing; # later on.; # it is difficult to use boolean flags from github actions, use strings to convey; # true/false for boolean arguments instead.; # Set argument to True if matched; # runners should never have root permissions but be on the safe side; # windows; # mac/linux/POSIX; """"""; which cmake; cmake --version; which c++ || true; c++ --version || true; uname -a || true; cat /etc/os-rele",MatchSource.CODE_COMMENT,.github/workflows/root-ci-config/build_root.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/.github/workflows/root-ci-config/build_root.py
Modifiability,config,config,"#!/usr/bin/env python3; # pylint: disable=broad-except,missing-function-docstring,line-too-long; """"""This mainly functions as a shell script, but python is used for its; superior control flow. An important requirement of the CI is easily; reproducible builds, therefore a wrapper is made for running shell; commands so that they are also logged. The log is printed when build fails/succeeds and needs to perfectly; reproduce the build when pasted into a shell. Therefore all file system; modifying code not executed from shell needs a shell equivalent; explicitly appended to the shell log.; e.g. `os.chdir(x)` requires `cd x` to be appended to the shell log """"""; # Used for uploads; # Used for downloads; # openstack.enable_logging(debug=True); # used when uploading artifacts, calculate early since build times are inconsistent; # Load CMake options from .github/workflows/root-ci-config/buildconfig/[platform].txt; # file below overwrites values from above; # The hash of the build option string is used to find existing artifacts; # with matching build options on s3 storage.; # Differentiate between macos versions: it's possible to have the same label; # for different macos versions, especially different minor versions.; # Make testing of CI in forks not impact artifacts; # Where to put the roottest directory; # Where to find the target branch; # Where to find the incoming branch; # Build artifacts should only be uploaded for full builds, and only for; # ""official"" branches (master, v?-??-??-patches), i.e. not for pull_request; # We also want to upload any successful build, even if it fails testing; # later on.; # it is difficult to use boolean flags from github actions, use strings to convey; # true/false for boolean arguments instead.; # Set argument to True if matched; # runners should never have root permissions but be on the safe side; # windows; # mac/linux/POSIX; """"""; which cmake; cmake --version; which c++ || true; c++ --version || true; uname -a || true; cat /etc/os-rele",MatchSource.CODE_COMMENT,.github/workflows/root-ci-config/build_root.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/.github/workflows/root-ci-config/build_root.py
Performance,cache,cached,"ent minor versions.; # Make testing of CI in forks not impact artifacts; # Where to put the roottest directory; # Where to find the target branch; # Where to find the incoming branch; # Build artifacts should only be uploaded for full builds, and only for; # ""official"" branches (master, v?-??-??-patches), i.e. not for pull_request; # We also want to upload any successful build, even if it fails testing; # later on.; # it is difficult to use boolean flags from github actions, use strings to convey; # true/false for boolean arguments instead.; # Set argument to True if matched; # runners should never have root permissions but be on the safe side; # windows; # mac/linux/POSIX; """"""; which cmake; cmake --version; which c++ || true; c++ --version || true; uname -a || true; cat /etc/os-release || true; sw_vers || true; uptime || true; df || true; """"""; """"""; Just return the exit code in case of test failures instead of `die()`-ing; report test; failures in main().; """"""; # Print CMake cached config; # rebase fails unless user.email and user.name is set; """"""; get_stdout_subprocess; execute and log a command.; capture the stdout, strip white space and return it; die in case of failed execution unless the error_message is empty.; """"""; """"""; get_base_head_sha. Given a pull request merge commit and the incoming commit return; the commit corresponding to the head of the branch we are merging into.; """"""; """"""; relatedrepo_GetClosestMatch(REPO_NAME <repo> ORIGIN_PREFIX <originp> UPSTREAM_PREFIX <upstreamp>; FETCHURL_VARIABLE <output_url> FETCHREF_VARIABLE <output_ref>); Return the clone URL and head/tag of the closest match for `repo` (e.g. roottest), based on the; current head name. See relatedrepo_GetClosestMatch in toplevel CMakeLists.txt; """"""; # Alternatively, we could use: re.sub( ""/root(.git)*$"", """", varname); # `current_head` is a well-known branch, e.g. master, or v6-28-00-patches. Use the matching branch; # upstream as the fork repository may be out-of-sync; # Resolve the 'late",MatchSource.CODE_COMMENT,.github/workflows/root-ci-config/build_root.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/.github/workflows/root-ci-config/build_root.py
Safety,safe,safe,"e hash of the build option string is used to find existing artifacts; # with matching build options on s3 storage.; # Differentiate between macos versions: it's possible to have the same label; # for different macos versions, especially different minor versions.; # Make testing of CI in forks not impact artifacts; # Where to put the roottest directory; # Where to find the target branch; # Where to find the incoming branch; # Build artifacts should only be uploaded for full builds, and only for; # ""official"" branches (master, v?-??-??-patches), i.e. not for pull_request; # We also want to upload any successful build, even if it fails testing; # later on.; # it is difficult to use boolean flags from github actions, use strings to convey; # true/false for boolean arguments instead.; # Set argument to True if matched; # runners should never have root permissions but be on the safe side; # windows; # mac/linux/POSIX; """"""; which cmake; cmake --version; which c++ || true; c++ --version || true; uname -a || true; cat /etc/os-release || true; sw_vers || true; uptime || true; df || true; """"""; """"""; Just return the exit code in case of test failures instead of `die()`-ing; report test; failures in main().; """"""; # Print CMake cached config; # rebase fails unless user.email and user.name is set; """"""; get_stdout_subprocess; execute and log a command.; capture the stdout, strip white space and return it; die in case of failed execution unless the error_message is empty.; """"""; """"""; get_base_head_sha. Given a pull request merge commit and the incoming commit return; the commit corresponding to the head of the branch we are merging into.; """"""; """"""; relatedrepo_GetClosestMatch(REPO_NAME <repo> ORIGIN_PREFIX <originp> UPSTREAM_PREFIX <upstreamp>; FETCHURL_VARIABLE <output_url> FETCHREF_VARIABLE <output_ref>); Return the clone URL and head/tag of the closest match for `repo` (e.g. roottest), based on the; current head name. See relatedrepo_GetClosestMatch in toplevel CMakeLists.txt; """"""; ",MatchSource.CODE_COMMENT,.github/workflows/root-ci-config/build_root.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/.github/workflows/root-ci-config/build_root.py
Security,hash,hash,"#!/usr/bin/env python3; # pylint: disable=broad-except,missing-function-docstring,line-too-long; """"""This mainly functions as a shell script, but python is used for its; superior control flow. An important requirement of the CI is easily; reproducible builds, therefore a wrapper is made for running shell; commands so that they are also logged. The log is printed when build fails/succeeds and needs to perfectly; reproduce the build when pasted into a shell. Therefore all file system; modifying code not executed from shell needs a shell equivalent; explicitly appended to the shell log.; e.g. `os.chdir(x)` requires `cd x` to be appended to the shell log """"""; # Used for uploads; # Used for downloads; # openstack.enable_logging(debug=True); # used when uploading artifacts, calculate early since build times are inconsistent; # Load CMake options from .github/workflows/root-ci-config/buildconfig/[platform].txt; # file below overwrites values from above; # The hash of the build option string is used to find existing artifacts; # with matching build options on s3 storage.; # Differentiate between macos versions: it's possible to have the same label; # for different macos versions, especially different minor versions.; # Make testing of CI in forks not impact artifacts; # Where to put the roottest directory; # Where to find the target branch; # Where to find the incoming branch; # Build artifacts should only be uploaded for full builds, and only for; # ""official"" branches (master, v?-??-??-patches), i.e. not for pull_request; # We also want to upload any successful build, even if it fails testing; # later on.; # it is difficult to use boolean flags from github actions, use strings to convey; # true/false for boolean arguments instead.; # Set argument to True if matched; # runners should never have root permissions but be on the safe side; # windows; # mac/linux/POSIX; """"""; which cmake; cmake --version; which c++ || true; c++ --version || true; uname -a || true; cat /etc/os-rele",MatchSource.CODE_COMMENT,.github/workflows/root-ci-config/build_root.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/.github/workflows/root-ci-config/build_root.py
Testability,log,logged,"#!/usr/bin/env python3; # pylint: disable=broad-except,missing-function-docstring,line-too-long; """"""This mainly functions as a shell script, but python is used for its; superior control flow. An important requirement of the CI is easily; reproducible builds, therefore a wrapper is made for running shell; commands so that they are also logged. The log is printed when build fails/succeeds and needs to perfectly; reproduce the build when pasted into a shell. Therefore all file system; modifying code not executed from shell needs a shell equivalent; explicitly appended to the shell log.; e.g. `os.chdir(x)` requires `cd x` to be appended to the shell log """"""; # Used for uploads; # Used for downloads; # openstack.enable_logging(debug=True); # used when uploading artifacts, calculate early since build times are inconsistent; # Load CMake options from .github/workflows/root-ci-config/buildconfig/[platform].txt; # file below overwrites values from above; # The hash of the build option string is used to find existing artifacts; # with matching build options on s3 storage.; # Differentiate between macos versions: it's possible to have the same label; # for different macos versions, especially different minor versions.; # Make testing of CI in forks not impact artifacts; # Where to put the roottest directory; # Where to find the target branch; # Where to find the incoming branch; # Build artifacts should only be uploaded for full builds, and only for; # ""official"" branches (master, v?-??-??-patches), i.e. not for pull_request; # We also want to upload any successful build, even if it fails testing; # later on.; # it is difficult to use boolean flags from github actions, use strings to convey; # true/false for boolean arguments instead.; # Set argument to True if matched; # runners should never have root permissions but be on the safe side; # windows; # mac/linux/POSIX; """"""; which cmake; cmake --version; which c++ || true; c++ --version || true; uname -a || true; cat /etc/os-rele",MatchSource.CODE_COMMENT,.github/workflows/root-ci-config/build_root.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/.github/workflows/root-ci-config/build_root.py
Availability,down,downloaded,"#!/usr/bin/env false; """""" decorator that places function's stdout/stderr output in a; dropdown group when running on github workflows """"""; """"""; Trace command invocations and print them to reproduce builds.; """"""; """"""prints message using select graphic rendition, defaults to bold text; https://en.wikipedia.org/wiki/ANSI_escape_code#SGR_(Select_Graphic_Rendition)_parameters""""""; """"""Runs <command> in shell and appends <command> to log""""""; """"""Runs <command> in shell, capture output and appends <command> to log""""""; # Since we are capturing the result and using it in other command later,; # we don't need it for the reproducing steps.; # So no call to: log.add(command); """"""Loads cmake options from a file to a dictionary""""""; """"""Converts a dictionary of build options to string.; The output is sorted alphanumerically. example: {""builtin_xrootd""=""on"", ""alien""=""on""}; ->; '""-Dalien=on"" -Dbuiltin_xrootd=on""'; """"""; """"""Calculate the hash of the options string. If ""march=native"" is in the; list of options, make the preprocessor defines resulting from it part of; the hash.; """"""; """"""Downloads latest build artifact starting with <prefix>,; and returns the file path to the downloaded file and shell_log.""""""; # https://docs.openstack.org/api-ref/object-store/#show-container-details-and-list-objects",MatchSource.CODE_COMMENT,.github/workflows/root-ci-config/build_utils.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/.github/workflows/root-ci-config/build_utils.py
Integrability,message,message,"#!/usr/bin/env false; """""" decorator that places function's stdout/stderr output in a; dropdown group when running on github workflows """"""; """"""; Trace command invocations and print them to reproduce builds.; """"""; """"""prints message using select graphic rendition, defaults to bold text; https://en.wikipedia.org/wiki/ANSI_escape_code#SGR_(Select_Graphic_Rendition)_parameters""""""; """"""Runs <command> in shell and appends <command> to log""""""; """"""Runs <command> in shell, capture output and appends <command> to log""""""; # Since we are capturing the result and using it in other command later,; # we don't need it for the reproducing steps.; # So no call to: log.add(command); """"""Loads cmake options from a file to a dictionary""""""; """"""Converts a dictionary of build options to string.; The output is sorted alphanumerically. example: {""builtin_xrootd""=""on"", ""alien""=""on""}; ->; '""-Dalien=on"" -Dbuiltin_xrootd=on""'; """"""; """"""Calculate the hash of the options string. If ""march=native"" is in the; list of options, make the preprocessor defines resulting from it part of; the hash.; """"""; """"""Downloads latest build artifact starting with <prefix>,; and returns the file path to the downloaded file and shell_log.""""""; # https://docs.openstack.org/api-ref/object-store/#show-container-details-and-list-objects",MatchSource.CODE_COMMENT,.github/workflows/root-ci-config/build_utils.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/.github/workflows/root-ci-config/build_utils.py
Security,hash,hash,"#!/usr/bin/env false; """""" decorator that places function's stdout/stderr output in a; dropdown group when running on github workflows """"""; """"""; Trace command invocations and print them to reproduce builds.; """"""; """"""prints message using select graphic rendition, defaults to bold text; https://en.wikipedia.org/wiki/ANSI_escape_code#SGR_(Select_Graphic_Rendition)_parameters""""""; """"""Runs <command> in shell and appends <command> to log""""""; """"""Runs <command> in shell, capture output and appends <command> to log""""""; # Since we are capturing the result and using it in other command later,; # we don't need it for the reproducing steps.; # So no call to: log.add(command); """"""Loads cmake options from a file to a dictionary""""""; """"""Converts a dictionary of build options to string.; The output is sorted alphanumerically. example: {""builtin_xrootd""=""on"", ""alien""=""on""}; ->; '""-Dalien=on"" -Dbuiltin_xrootd=on""'; """"""; """"""Calculate the hash of the options string. If ""march=native"" is in the; list of options, make the preprocessor defines resulting from it part of; the hash.; """"""; """"""Downloads latest build artifact starting with <prefix>,; and returns the file path to the downloaded file and shell_log.""""""; # https://docs.openstack.org/api-ref/object-store/#show-container-details-and-list-objects",MatchSource.CODE_COMMENT,.github/workflows/root-ci-config/build_utils.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/.github/workflows/root-ci-config/build_utils.py
Testability,log,log,"#!/usr/bin/env false; """""" decorator that places function's stdout/stderr output in a; dropdown group when running on github workflows """"""; """"""; Trace command invocations and print them to reproduce builds.; """"""; """"""prints message using select graphic rendition, defaults to bold text; https://en.wikipedia.org/wiki/ANSI_escape_code#SGR_(Select_Graphic_Rendition)_parameters""""""; """"""Runs <command> in shell and appends <command> to log""""""; """"""Runs <command> in shell, capture output and appends <command> to log""""""; # Since we are capturing the result and using it in other command later,; # we don't need it for the reproducing steps.; # So no call to: log.add(command); """"""Loads cmake options from a file to a dictionary""""""; """"""Converts a dictionary of build options to string.; The output is sorted alphanumerically. example: {""builtin_xrootd""=""on"", ""alien""=""on""}; ->; '""-Dalien=on"" -Dbuiltin_xrootd=on""'; """"""; """"""Calculate the hash of the options string. If ""march=native"" is in the; list of options, make the preprocessor defines resulting from it part of; the hash.; """"""; """"""Downloads latest build artifact starting with <prefix>,; and returns the file path to the downloaded file and shell_log.""""""; # https://docs.openstack.org/api-ref/object-store/#show-container-details-and-list-objects",MatchSource.CODE_COMMENT,.github/workflows/root-ci-config/build_utils.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/.github/workflows/root-ci-config/build_utils.py
Deployability,release,released,"ling the Range operation. If the head node is; a TreeHeadNode then it is an actual RDataFrame.; range_id: The id of the current range. Needed to assign a file name to a; partial Snapshot if it was requested. Returns:; list: List of actions of the computation graph to be triggered. Each; element is some kind of promise of a result (usually an; RResultPtr). Exceptions are the 'AsNumpy' operation for which an; 'AsNumpyResult' is returned and the 'Snapshot' operation for which a; 'SnapshotResult' is returned.; """"""; # Iterate over the other nodes stored in the dictionary, skipping the head; # node. We can iterate over the values knowing that the dictionary preserves; # the order in which it was created. Thus, we traverse the graph from top; # to bottom, in order to create the RDF nodes in the right order.; # Connect the starting node with the first node of the computation graph; """"""; Trigger the computation graph. The list of actions to be performed is retrieved by calling; generate_computation_graph. Afterwards, the C++ RDF computation graph is; triggered through the `ROOT::Internal::RDF::TriggerRun` function with; the GIL released. Args:; graph: A representation of the computation graph. starting_node: The node where the generation of the; computation graph is started. Either an actual RDataFrame or the; result of a Range operation (in case of empty data source). range_id: The id of the current range. Needed to assign a; file name to a partial Snapshot if it was requested. Returns:; list: A list of objects that can be either used as or converted into; mergeable values.; """"""; # Fill the cache with the future results; # Create clones according to different types of actions; # Return a list of objects that can be later merged. In most cases this; # is still made of RResultPtrs that will then be used as input arguments; # to `ROOT::RDF::Detail::GetMergeableValue`. For `AsNumpy`, it returns; # an instance of `AsNumpyResult`. For `Snapshot`, it returns a; # `SnapshotResult`",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/ComputationGraphGenerator.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/ComputationGraphGenerator.py
Performance,perform,performed,"an RSnapshotOptions; object with the 'fLazy' data member set to 'True'. Furthermore, the current; range id needs to be appended to the input file name so that the output data; from different tasks can be distinguished. Note:; Since the file name from the original operation needs to be changed, this; function makes a deep copy of it and returns the modified copy. This is; needed in order to avoid that a task may receive as input an operation that; was previously modified by another task. In that case, the file name would; contain the range id from the other task, thus leading to create a wrong; file name in this function.; """"""; # Retrieve filename and append range boundaries; # Create a partial snapshot on the current range; # Only the first two mandatory arguments were passed; # Only the following overload is possible; # Snapshot(std::string_view treename, std::string_view filename, std::string_view columnNameRegexp = """"); # Append empty regex; # An RSnapshotOptions instance was passed as fourth argument; # Make it lazy and keep the other options; # We already appended an empty regex for the 2 mandatory arguments overload; # All other overloads have 3 mandatory arguments; # We just need to append a lazy RSnapshotOptions now; # Append RSnapshotOptions; """"""; Retrieves the concrete RDataFrame operation to be performed by; querying the 'parent_rdf_node'. Forces lazyness on any operation, so; they can be all chained before triggering the actual computation. Returns; both the call to the RDataFrame operation and the operation itself, which; are then needed when creating the list of result promises to return from; the mapper task.; """"""; """"""; Implementation of a state of the computation_graph_generator; function that is requesting systematic variations on a previously called; action. The 'parent_rdf_node' parameter is the nominal action for which; the variations are requested. The function calls; ROOT.RDF.Experimental.VariationsFor on it, which returns a; ROOT.RDF.Experimen",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/ComputationGraphGenerator.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/ComputationGraphGenerator.py
Safety,avoid,avoid," Exceptions are the 'AsNumpy'; operation which promise is an 'AsNumpyResult' and the 'Snapshot' operation; for which a 'SnapshotResult' is created and appended to the list of results.; """"""; """"""; We may need to change the attributes of some operations (currently; Snapshot and AsNumpy), to make them lazy before triggering; the computation graph. In the general case, just return the input operation.; """"""; """"""; The AsNumpy operation can be made lazy by setting the boolean keyword; argument 'lazy' to 'True'.; """"""; """"""; The Snapshot operation can be made lazy by supplying an RSnapshotOptions; object with the 'fLazy' data member set to 'True'. Furthermore, the current; range id needs to be appended to the input file name so that the output data; from different tasks can be distinguished. Note:; Since the file name from the original operation needs to be changed, this; function makes a deep copy of it and returns the modified copy. This is; needed in order to avoid that a task may receive as input an operation that; was previously modified by another task. In that case, the file name would; contain the range id from the other task, thus leading to create a wrong; file name in this function.; """"""; # Retrieve filename and append range boundaries; # Create a partial snapshot on the current range; # Only the first two mandatory arguments were passed; # Only the following overload is possible; # Snapshot(std::string_view treename, std::string_view filename, std::string_view columnNameRegexp = """"); # Append empty regex; # An RSnapshotOptions instance was passed as fourth argument; # Make it lazy and keep the other options; # We already appended an empty regex for the 2 mandatory arguments overload; # All other overloads have 3 mandatory arguments; # We just need to append a lazy RSnapshotOptions now; # Append RSnapshotOptions; """"""; Retrieves the concrete RDataFrame operation to be performed by; querying the 'parent_rdf_node'. Forces lazyness on any operation, so; they can be all c",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/ComputationGraphGenerator.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/ComputationGraphGenerator.py
Availability,down,down,"of all nodes in the; graph, is then able to generate a flat representation to be sent to the; distributed workers. Attributes:; backend: A reference to the instance of distributed backend that will; execute this computation graph. graph_nodes: A deque of references to the nodes belonging to this graph. node_counter: A counter of how many nodes were created in the graph of; this head node, starting from zero.; """"""; # It is important to have a double-ended queue because we need to; # traverse the nodes in different ways w.r.t. their insertion order.; # While pruning the graph, we need to check leaf nodes before their; # parents, so if a child is pruned then it also decrements the counter; # of children of its parent. Thus, we need a bottom-up traversal.; # While executing the graph in a task, we need to create the RDF nodes; # in the order the user requested them, e.g. starting from the; # RDataFrame itself, then calling its direct children, their children; # and so on. Thus, we need a top-down traversal.; # Uniquely identify each computation graph execution of this RDataFrame; # The uuid can be set here; # The full identifier is created at the beginning of each execution; # Internal attribute to keep track of the number of partitions. We also; # check whether it was specified by the user when creating the dataframe.; # If so, this attribute will not be updated when triggering.; # Internal RDataFrame object, useful to expose information such as; # column names.; # A dictionary where the keys are the IDs of the objects to live visualize; # and the values are the corresponding callback functions ; # This attribute only gets set in case the LiveVisualize() function is called; """"""; Remove the reference to the local RDataFrame object as soon as this; object is garbage collected. This helps avoiding conflicts between; the garbage collector, the cppyy memory regulator and the C++ object; destructor.; """"""; """"""; The number of partitions for this dataframe is updated only if th",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/HeadNode.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/HeadNode.py
Deployability,update,updated,"raph of; this head node, starting from zero.; """"""; # It is important to have a double-ended queue because we need to; # traverse the nodes in different ways w.r.t. their insertion order.; # While pruning the graph, we need to check leaf nodes before their; # parents, so if a child is pruned then it also decrements the counter; # of children of its parent. Thus, we need a bottom-up traversal.; # While executing the graph in a task, we need to create the RDF nodes; # in the order the user requested them, e.g. starting from the; # RDataFrame itself, then calling its direct children, their children; # and so on. Thus, we need a top-down traversal.; # Uniquely identify each computation graph execution of this RDataFrame; # The uuid can be set here; # The full identifier is created at the beginning of each execution; # Internal attribute to keep track of the number of partitions. We also; # check whether it was specified by the user when creating the dataframe.; # If so, this attribute will not be updated when triggering.; # Internal RDataFrame object, useful to expose information such as; # column names.; # A dictionary where the keys are the IDs of the objects to live visualize; # and the values are the corresponding callback functions ; # This attribute only gets set in case the LiveVisualize() function is called; """"""; Remove the reference to the local RDataFrame object as soon as this; object is garbage collected. This helps avoiding conflicts between; the garbage collector, the cppyy memory regulator and the C++ object; destructor.; """"""; """"""; The number of partitions for this dataframe is updated only if the user; did not initially specify one when creating the dataframe.; """"""; """"""; Prunes nodes from the graph under certain conditions. A node is pruned; if it has no children and the user has no references to it. The internal; representation of the graph is traversed in such a way so that leaf; nodes are checked before their parents.; """"""; """"""Generates a list of nodes",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/HeadNode.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/HeadNode.py
Energy Efficiency,schedul,scheduler,"ck functions ; # This attribute only gets set in case the LiveVisualize() function is called; """"""; Remove the reference to the local RDataFrame object as soon as this; object is garbage collected. This helps avoiding conflicts between; the garbage collector, the cppyy memory regulator and the C++ object; destructor.; """"""; """"""; The number of partitions for this dataframe is updated only if the user; did not initially specify one when creating the dataframe.; """"""; """"""; Prunes nodes from the graph under certain conditions. A node is pruned; if it has no children and the user has no references to it. The internal; representation of the graph is traversed in such a way so that leaf; nodes are checked before their parents.; """"""; """"""Generates a list of nodes in the graph that are actions.""""""; # This function is called after distributed execution of the graph, no; # need to repeat the pruning; """"""; Generates a dictionary holding information about all nodes in the graph.; It is then given to the distributed scheduler.; """"""; # Need to prune the graph before sending it for distributed execution; # We need to store references of each node id separately from the node; # objects themselves. In a distributed task, we need to know for any; # node which node is its parent (in order to execute an RDF operation; # on the right node). We can't serialize the reference to the parent; # node that each node object has, since that would trigger recursive; # serialization of the parent(s) of parent(s) nodes.; # Prepare a dictionary with additional information for live visualization; # Key: node_id; # Tuple containing:; # 1. Callback functions passed by the user; # 2. Index of the node in the local_nodes list; # 3. Name of the operation associated with the node; # Filter: Only include nodes requested by the user; """"""; Executes an RDataFrame computation graph on a distributed backend. The needed ingredients are:. - A collection of logical ranges in which the dataset is split. Each; range is go",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/HeadNode.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/HeadNode.py
Integrability,depend,depending," of parent(s) nodes.; # Prepare a dictionary with additional information for live visualization; # Key: node_id; # Tuple containing:; # 1. Callback functions passed by the user; # 2. Index of the node in the local_nodes list; # 3. Name of the operation associated with the node; # Filter: Only include nodes requested by the user; """"""; Executes an RDataFrame computation graph on a distributed backend. The needed ingredients are:. - A collection of logical ranges in which the dataset is split. Each; range is going to be assigned to a distributed task.; - A representation of the computation graph that the task needs to; execute.; - A way to generate an RDataFrame instance starting from the logical; range of the task.; - Optionally, some setup code to be run at the beginning of each task. These are used as inputs to a generic mapper function. Results from the; various mappers are then reduced and the final results are retrieved in; the local session. These are properly handled to perform extra checks,; depending on the data source. Finally, the local user-facing nodes are; filled with the values that were computed distributedly so that they; can be accessed in the application like with local RDataFrame.; """"""; # Updates the number of partitions for this dataframe if the user did; # not specify one initially. This is done each time the computations are; # triggered, in case the user changed the resource configuration; # between runs (e.g. changing the number of available cores).; # List of action nodes in the same order as values; # Execute graph distributedly and return the aggregated results from all tasks; # using the appropriate backend method based on whether or not live visualization is enabled; # Cleanup the current execution artifacts from the caches on the workers; # Perform any extra checks that may be needed according to the; # type of the head node; # Set the value of every action node; """"""; A factory for different kinds of head nodes of the RDataFrame computati",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/HeadNode.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/HeadNode.py
Modifiability,config,configuration,"buted backend. The needed ingredients are:. - A collection of logical ranges in which the dataset is split. Each; range is going to be assigned to a distributed task.; - A representation of the computation graph that the task needs to; execute.; - A way to generate an RDataFrame instance starting from the logical; range of the task.; - Optionally, some setup code to be run at the beginning of each task. These are used as inputs to a generic mapper function. Results from the; various mappers are then reduced and the final results are retrieved in; the local session. These are properly handled to perform extra checks,; depending on the data source. Finally, the local user-facing nodes are; filled with the values that were computed distributedly so that they; can be accessed in the application like with local RDataFrame.; """"""; # Updates the number of partitions for this dataframe if the user did; # not specify one initially. This is done each time the computations are; # triggered, in case the user changed the resource configuration; # between runs (e.g. changing the number of available cores).; # List of action nodes in the same order as values; # Execute graph distributedly and return the aggregated results from all tasks; # using the appropriate backend method based on whether or not live visualization is enabled; # Cleanup the current execution artifacts from the caches on the workers; # Perform any extra checks that may be needed according to the; # type of the head node; # Set the value of every action node; """"""; A factory for different kinds of head nodes of the RDataFrame computation; graph, depending on the arguments to the RDataFrame constructor. Parses the; arguments and compares them against the possible RDataFrame constructors.; """"""; # Early check that arguments are accepted by RDataFrame; """"""; The head node of a computation graph where the RDataFrame data source is; empty and a number of sequential entries will be created at runtime. This; head node is re",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/HeadNode.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/HeadNode.py
Performance,queue,queue," in a distributed task.; Attributes:; rdf: The starting node of the RDataFrame computation graph. Only in a; TTree-based run, if the task has nothing to process then this; attribute is None.; entries_in_trees: A struct holding the amount of processed entries in; the task, as well as a dictionary where each key is an identifier; for a tree opened in the task and the value is the number of entries; in that tree. This attribute is not None only in a TTree-based run.; """"""; """"""; The head node of the computation graph. Keeps record of all nodes in the; graph, is then able to generate a flat representation to be sent to the; distributed workers. Attributes:; backend: A reference to the instance of distributed backend that will; execute this computation graph. graph_nodes: A deque of references to the nodes belonging to this graph. node_counter: A counter of how many nodes were created in the graph of; this head node, starting from zero.; """"""; # It is important to have a double-ended queue because we need to; # traverse the nodes in different ways w.r.t. their insertion order.; # While pruning the graph, we need to check leaf nodes before their; # parents, so if a child is pruned then it also decrements the counter; # of children of its parent. Thus, we need a bottom-up traversal.; # While executing the graph in a task, we need to create the RDF nodes; # in the order the user requested them, e.g. starting from the; # RDataFrame itself, then calling its direct children, their children; # and so on. Thus, we need a top-down traversal.; # Uniquely identify each computation graph execution of this RDataFrame; # The uuid can be set here; # The full identifier is created at the beginning of each execution; # Internal attribute to keep track of the number of partitions. We also; # check whether it was specified by the user when creating the dataframe.; # If so, this attribute will not be updated when triggering.; # Internal RDataFrame object, useful to expose information such as; #",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/HeadNode.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/HeadNode.py
Safety,avoid,avoiding,"uested them, e.g. starting from the; # RDataFrame itself, then calling its direct children, their children; # and so on. Thus, we need a top-down traversal.; # Uniquely identify each computation graph execution of this RDataFrame; # The uuid can be set here; # The full identifier is created at the beginning of each execution; # Internal attribute to keep track of the number of partitions. We also; # check whether it was specified by the user when creating the dataframe.; # If so, this attribute will not be updated when triggering.; # Internal RDataFrame object, useful to expose information such as; # column names.; # A dictionary where the keys are the IDs of the objects to live visualize; # and the values are the corresponding callback functions ; # This attribute only gets set in case the LiveVisualize() function is called; """"""; Remove the reference to the local RDataFrame object as soon as this; object is garbage collected. This helps avoiding conflicts between; the garbage collector, the cppyy memory regulator and the C++ object; destructor.; """"""; """"""; The number of partitions for this dataframe is updated only if the user; did not initially specify one when creating the dataframe.; """"""; """"""; Prunes nodes from the graph under certain conditions. A node is pruned; if it has no children and the user has no references to it. The internal; representation of the graph is traversed in such a way so that leaf; nodes are checked before their parents.; """"""; """"""Generates a list of nodes in the graph that are actions.""""""; # This function is called after distributed execution of the graph, no; # need to repeat the pruning; """"""; Generates a dictionary holding information about all nodes in the graph.; It is then given to the distributed scheduler.; """"""; # Need to prune the graph before sending it for distributed execution; # We need to store references of each node id separately from the node; # objects themselves. In a distributed task, we need to know for any; # node which ",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/HeadNode.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/HeadNode.py
Security,expose,expose,"ave a double-ended queue because we need to; # traverse the nodes in different ways w.r.t. their insertion order.; # While pruning the graph, we need to check leaf nodes before their; # parents, so if a child is pruned then it also decrements the counter; # of children of its parent. Thus, we need a bottom-up traversal.; # While executing the graph in a task, we need to create the RDF nodes; # in the order the user requested them, e.g. starting from the; # RDataFrame itself, then calling its direct children, their children; # and so on. Thus, we need a top-down traversal.; # Uniquely identify each computation graph execution of this RDataFrame; # The uuid can be set here; # The full identifier is created at the beginning of each execution; # Internal attribute to keep track of the number of partitions. We also; # check whether it was specified by the user when creating the dataframe.; # If so, this attribute will not be updated when triggering.; # Internal RDataFrame object, useful to expose information such as; # column names.; # A dictionary where the keys are the IDs of the objects to live visualize; # and the values are the corresponding callback functions ; # This attribute only gets set in case the LiveVisualize() function is called; """"""; Remove the reference to the local RDataFrame object as soon as this; object is garbage collected. This helps avoiding conflicts between; the garbage collector, the cppyy memory regulator and the C++ object; destructor.; """"""; """"""; The number of partitions for this dataframe is updated only if the user; did not initially specify one when creating the dataframe.; """"""; """"""; Prunes nodes from the graph under certain conditions. A node is pruned; if it has no children and the user has no references to it. The internal; representation of the graph is traversed in such a way so that leaf; nodes are checked before their parents.; """"""; """"""Generates a list of nodes in the graph that are actions.""""""; # This function is called after distri",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/HeadNode.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/HeadNode.py
Testability,log,logical,"ut all nodes in the graph.; It is then given to the distributed scheduler.; """"""; # Need to prune the graph before sending it for distributed execution; # We need to store references of each node id separately from the node; # objects themselves. In a distributed task, we need to know for any; # node which node is its parent (in order to execute an RDF operation; # on the right node). We can't serialize the reference to the parent; # node that each node object has, since that would trigger recursive; # serialization of the parent(s) of parent(s) nodes.; # Prepare a dictionary with additional information for live visualization; # Key: node_id; # Tuple containing:; # 1. Callback functions passed by the user; # 2. Index of the node in the local_nodes list; # 3. Name of the operation associated with the node; # Filter: Only include nodes requested by the user; """"""; Executes an RDataFrame computation graph on a distributed backend. The needed ingredients are:. - A collection of logical ranges in which the dataset is split. Each; range is going to be assigned to a distributed task.; - A representation of the computation graph that the task needs to; execute.; - A way to generate an RDataFrame instance starting from the logical; range of the task.; - Optionally, some setup code to be run at the beginning of each task. These are used as inputs to a generic mapper function. Results from the; various mappers are then reduced and the final results are retrieved in; the local session. These are properly handled to perform extra checks,; depending on the data source. Finally, the local user-facing nodes are; filled with the values that were computed distributedly so that they; can be accessed in the application like with local RDataFrame.; """"""; # Updates the number of partitions for this dataframe if the user did; # not specify one initially. This is done each time the computations are; # triggered, in case the user changed the resource configuration; # between runs (e.g. changin",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/HeadNode.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/HeadNode.py
Deployability,update,updated,"# @author Silia Taider; # @date 2023-08; ################################################################################; # Copyright (C) 1995-2023, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; """"""; Enables real-time data representation for the given drawable objects.; The objects are drawn and updated every time a partial result returns from distributed execution. Args:; drawable_callback_dict (dict): A dictionary where keys are drawable objects ; and values are optional corresponding callback functions. . global_callback (function): An optional global callback function that ; is applied to all drawable objects. Raises:; ValueError: If a passed drawable object is not valid.; """"""; # Check if the objects already have a value (the computation graph has already been triggered); # Check if all drawables share the same headnode; # Key: node_id of the drawable object's proxied_node; # Value: List of validated callback functions for the drawable object; # Filter: Only include valid drawable objects; """"""; Wrapper function to facilitate calling LiveVisualize with a list or a tuple of drawable objects. Args:; drawables (list | tuple): Drawable objects to visualize.; 		; callback (function): An optional callback function to be applied to the drawable objects. Notes:; This function constructs a dictionary of drawable objects and their associated callback functions,; and then calls the main LiveVisualize function with the constructed dictionary.; """"""; """"""; Process and validate a callback function. Args:		; callback: The callback function to be validated. Returns:; validated_callback: The validated callback function, or None if not valid.; """"""; """"""; Checks if the provided callback function has exactly one unfilled argument. Args:; callback (Callable): The callback fun",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/LiveVisualize.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/LiveVisualize.py
Safety,safe,safe,"on graph has already been triggered); # Check if all drawables share the same headnode; # Key: node_id of the drawable object's proxied_node; # Value: List of validated callback functions for the drawable object; # Filter: Only include valid drawable objects; """"""; Wrapper function to facilitate calling LiveVisualize with a list or a tuple of drawable objects. Args:; drawables (list | tuple): Drawable objects to visualize.; 		; callback (function): An optional callback function to be applied to the drawable objects. Notes:; This function constructs a dictionary of drawable objects and their associated callback functions,; and then calls the main LiveVisualize function with the constructed dictionary.; """"""; """"""; Process and validate a callback function. Args:		; callback: The callback function to be validated. Returns:; validated_callback: The validated callback function, or None if not valid.; """"""; """"""; Checks if the provided callback function has exactly one unfilled argument. Args:; callback (Callable): The callback function to check. Returns:; bool: True if the callback function has exactly one unfilled argument,; False otherwise.; """"""; # Get the values of the functions parameters; """"""; Checks if the provided callback function is safe for live visualization, ; (does not contain blocked actions). Args:; callback (function): The callback function to check. Returns:; bool: True if the callback function is safe, ; False otherwise.; """"""; # Parse the callback function's source code; """"""; Checks if the given Abstract Syntax Tree (AST) node corresponds to a blocked action. Args:; node (ast.AST): The AST node to check. Returns:; bool: True if the AST node corresponds to a blocked action,; False otherwise.; """"""; """"""; Checks if the object is a valid drawable object for live visualization. Args:; obj: The object to be checked. Returns:; bool: True if the object is a valid drawable object for live visualization ; according to the ALLOWED_OPERATIONS list , False otherwise.; """"""",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/LiveVisualize.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/LiveVisualize.py
Security,validat,validated,"########; # Copyright (C) 1995-2023, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; """"""; Enables real-time data representation for the given drawable objects.; The objects are drawn and updated every time a partial result returns from distributed execution. Args:; drawable_callback_dict (dict): A dictionary where keys are drawable objects ; and values are optional corresponding callback functions. . global_callback (function): An optional global callback function that ; is applied to all drawable objects. Raises:; ValueError: If a passed drawable object is not valid.; """"""; # Check if the objects already have a value (the computation graph has already been triggered); # Check if all drawables share the same headnode; # Key: node_id of the drawable object's proxied_node; # Value: List of validated callback functions for the drawable object; # Filter: Only include valid drawable objects; """"""; Wrapper function to facilitate calling LiveVisualize with a list or a tuple of drawable objects. Args:; drawables (list | tuple): Drawable objects to visualize.; 		; callback (function): An optional callback function to be applied to the drawable objects. Notes:; This function constructs a dictionary of drawable objects and their associated callback functions,; and then calls the main LiveVisualize function with the constructed dictionary.; """"""; """"""; Process and validate a callback function. Args:		; callback: The callback function to be validated. Returns:; validated_callback: The validated callback function, or None if not valid.; """"""; """"""; Checks if the provided callback function has exactly one unfilled argument. Args:; callback (Callable): The callback function to check. Returns:; bool: True if the callback function has exactly one unfilled argument,; False otherwis",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/LiveVisualize.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/LiveVisualize.py
Integrability,wrap,wraps,"s:; get_head: A function returning the head node in the graph. node_id: The id of this node, given sequentially in the order of; creation with respect to the head node of the graph. operation: The operation that this node represents. `None` if the node; is the head node. parent: A reference to the parent node of this node. `None` if the node; is the head node. nchildren: A counter of how many children this node has. _new_op_name (str): The name of the new incoming operation of the next; child, which is the last child node among the current node's; children. value: The computed value after executing the operation in the current; node for a particular DistRDF graph. This is permanently :obj:`None`; for transformation nodes and the action nodes get a; :obj:`ROOT.RResultPtr` after event-loop execution. has_user_references (bool): A flag to check whether the node has; direct user references, that is if it is assigned to a variable.; Default value is :obj:`True`, turns to :obj:`False` if the proxy; that wraps the node gets garbage collected by Python. rdf_node: A reference to the result of calling a function of the; RDataFrame API with the current operation. This is practically a; node of the true computation graph, which is being executed in some; distributed task. It is a transient attribute. On the client, it; is always None. The value is computed and stored only during a task; on a worker.; """"""; # This is the internal attribute for the 'parent_id' property. It is; # serialized from the local information then deserialized and set in a; # distributed task.; """"""Retrieves the id of the parent node.""""""; """"""; Sets the id of the parent node. Only present to enable setting this; attribute when deserializing the node in a distributed task.; """"""; """"""; Serialize the minimum amount of information needed in a distributed task; to execute the operation corresponding to this node of the graph.; """"""; """"""; Checks whether the current node can be pruned from the computational; graph. Ret",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Node.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Node.py
Modifiability,variab,variable," represents a node in RDataFrame operations graph. A Node; houses an operation and has references to children nodes. Attributes:; get_head: A function returning the head node in the graph. node_id: The id of this node, given sequentially in the order of; creation with respect to the head node of the graph. operation: The operation that this node represents. `None` if the node; is the head node. parent: A reference to the parent node of this node. `None` if the node; is the head node. nchildren: A counter of how many children this node has. _new_op_name (str): The name of the new incoming operation of the next; child, which is the last child node among the current node's; children. value: The computed value after executing the operation in the current; node for a particular DistRDF graph. This is permanently :obj:`None`; for transformation nodes and the action nodes get a; :obj:`ROOT.RResultPtr` after event-loop execution. has_user_references (bool): A flag to check whether the node has; direct user references, that is if it is assigned to a variable.; Default value is :obj:`True`, turns to :obj:`False` if the proxy; that wraps the node gets garbage collected by Python. rdf_node: A reference to the result of calling a function of the; RDataFrame API with the current operation. This is practically a; node of the true computation graph, which is being executed in some; distributed task. It is a transient attribute. On the client, it; is always None. The value is computed and stored only during a task; on a worker.; """"""; # This is the internal attribute for the 'parent_id' property. It is; # serialized from the local information then deserialized and set in a; # distributed task.; """"""Retrieves the id of the parent node.""""""; """"""; Sets the id of the parent node. Only present to enable setting this; attribute when deserializing the node in a distributed task.; """"""; """"""; Serialize the minimum amount of information needed in a distributed task; to execute the operation corres",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Node.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Node.py
Safety,safe,safely,"# @author Vincenzo Eduardo Padulano; # @author Enric Tejedor; # @date 2021-02; ################################################################################; # Copyright (C) 1995-2022, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; """"""An operation attached to a distributed RDataFrame graph node.""""""; """"""An action attached to a distributed RDataFrame graph node.""""""; """"""; Any type of Histo*D action. This distinct class is needed to check that the user passes a model for the; histogram action. Merging histograms coming from different distributed tasks; is not possible unless the model was previously specified, since the binning; of the different histograms would be incompatible.; """"""; # The histogram model can be passed as the keyword argument 'model'. All; # Histo*D specializations have the same name for this argument. If it is; # present, we know the execution can proceed safely.; # If the keyword argument was not passed, we need to check the first; # positional argument. In all Histo*D overload where it is present,; # it is always the first argument.; """"""; DistRDF.VariationsFor creates a specific node in the distributed; RDataFrame graph. This acts as an action node.; """"""; """"""An instant action attached to a distributed RDataFrame graph node.""""""; """"""An 'AsNumpy' instant action attached to a distributed RDataFrame graph node.""""""; """"""A 'Snapshot' instant action attached to a distributed RDataFrame graph node.""""""; """"""A trasformation attached to a distributed RDataFrame graph node.""""""",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Operation.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Operation.py
Availability,error,error," called right before the current Proxy gets deleted by; Python. Its purpose is to show that the wrapped node has no more; user references, which is one of the conditions for the node to be; pruned from the computational graph.; """"""; """"""; Instances of ResultMapProxy act as futures of the result produced; by a call to DistRDF.VariationsFor. The aim is to mimic the functionality of; ROOT::RDF::Experimental::RResultMap to provide the same API usage.; """"""; """"""; The __getattr__ of the Proxy base class is an abstract method. This; class has no attributes to present to the user.; """"""; """"""; Equivalent of 'operator[]' of the RResultMap. Triggers the computation; graph, then returns the varied value linked to the 'key' name.; """"""; """"""; Equivalent of 'GetKeys' of the RResultMap. Unlike its C++ counterpart,; at the moment we cannot retrieve the list of variation names for a; certain action without triggering the distributed computation graph. For; this reason, the function raises an error if the keys are accessed; before computations have been triggered. In the future the behaviour; should be aligned with the C++ counterpart.; """"""; # TODO:; # The event loop has not been triggered yet. Currently we can't retrieve; # the list of variation names without starting the distributed computations; """"""; A list of names of systematic variations was requested, but the corresponding map of variations is not; present. The variation names cannot be retrieved unless the computation graph has properly run and; finished. Something may have gone wrong in the distributed execution, or no variation values were; explicitly requested. In the future, it will be possible to get the variation names without triggering.; """"""; """"""; Instances of ResultPtrProxy act as futures of the result produced; by some action node. The aim is to mimic the functionality of; ROOT::RDF::RResultPtr to provide the same API usage.; """"""; """"""; Intercepts calls on the result of; the action node. Returns:; function: A method to ha",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Proxy.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Proxy.py
Energy Efficiency,charge,charge,"e wrong in the distributed execution, or no variation values were; explicitly requested. In the future, it will be possible to get the variation names without triggering.; """"""; """"""; Instances of ResultPtrProxy act as futures of the result produced; by some action node. The aim is to mimic the functionality of; ROOT::RDF::RResultPtr to provide the same API usage.; """"""; """"""; Intercepts calls on the result of; the action node. Returns:; function: A method to handle an operation call to the; current action node.; """"""; # Stores the name of operation call; """"""; Returns the result value of the current action node if it was executed; before, else triggers the execution of the distributed graph before; returning the value.; """"""; """"""; Handles an operation call to the current action node and returns; result of the current action node.; """"""; """"""; Creates a node responsible to signal the creation of variations in the; distributed computation graph, returning a specialized proxy to that; node. This function is usually called from DistRDF.VariationsFor.; """"""; """"""; A proxy object to an non-action node. It implements acces to attributes; and methods of the proxied node. It is also in charge of the creation of; a new operation node in the graph. The aim is to mimic the functionality of; ROOT::RDF::RNode to provide the same API usage.; """"""; """"""; Intercepts calls to attributes and methods of the proxied node and; returns the appropriate object(s). Args:; attr (str): The name of the attribute or method of the proxied; node the user wants to access.; """"""; # if attr is a supported operation, start; # operation and node creation; # Stores new operation name; """"""Forward call to the internal RDataFrame object""""""; """"""Forward call to the internal RDataFrame object""""""; """"""; Handles an operation call to the current node and returns the new node; built using the operation call.; """"""; """"""""Returns appropriate proxy for the input node""""""; # An RSnapshotOptions instance was passed as fourth argument",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Proxy.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Proxy.py
Integrability,wrap,wrap," """"""; """"""; Executes the distributed RDataFrame computation graph the input node; belongs to. If the node already has a value, this is a no-op.; """"""; # If event-loop not triggered; # Creating a ROOT.TDirectory.TContext in a context manager so that; # ROOT.gDirectory won't be changed by the event loop execution.; # All the information needed to reconstruct the computation graph on; # the workers is contained in the head node; """"""Propagate transform operations to the headnode internal RDataFrame""""""; """"""Creates a new node and inserts it in the computation graph""""""; """"""; Abstract class for proxies objects. These objects help to keep track of; nodes' variable assignment. That is, when a node is no longer assigned; to a variable by the user, the role of the proxy is to show that. This is; done via changing the value of the :obj:`has_user_references` of the; proxied node from :obj:`True` to :obj:`False`.; """"""; """"""; Creates a new `Proxy` object for a given node. Args:; proxied_node: The node that the current Proxy should wrap.; """"""; """"""; Proxies have to declare the way they intercept calls to attributes; and methods of the proxied node.; """"""; """"""; This function is called right before the current Proxy gets deleted by; Python. Its purpose is to show that the wrapped node has no more; user references, which is one of the conditions for the node to be; pruned from the computational graph.; """"""; """"""; Instances of ResultMapProxy act as futures of the result produced; by a call to DistRDF.VariationsFor. The aim is to mimic the functionality of; ROOT::RDF::Experimental::RResultMap to provide the same API usage.; """"""; """"""; The __getattr__ of the Proxy base class is an abstract method. This; class has no attributes to present to the user.; """"""; """"""; Equivalent of 'operator[]' of the RResultMap. Triggers the computation; graph, then returns the varied value linked to the 'key' name.; """"""; """"""; Equivalent of 'GetKeys' of the RResultMap. Unlike its C++ counterpart,; at the moment we can",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Proxy.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Proxy.py
Modifiability,variab,variable,"# @author Vincenzo Eduardo Padulano; # @author Enric Tejedor; # @date 2021-02; ################################################################################; # Copyright (C) 1995-2022, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; """"""; Factory function, decorated with `contextlib.contextmanager` to make it; work in a `with` context manager. It creates a `ROOT.TDirectory.TContext`; that will store the current `ROOT.gDirectory` variable. At the end of the; context, the C++ destructor of the `TContext` object will be explicitly; called, thanks to the `__destruct__` dunder method implemented in PyROOT.; This will restore the `gDirectory` variable to its initial value, allowing; changing it in the context manager without permanent effects.; """"""; """"""; Executes the distributed RDataFrame computation graph the input node; belongs to. If the node already has a value, this is a no-op.; """"""; # If event-loop not triggered; # Creating a ROOT.TDirectory.TContext in a context manager so that; # ROOT.gDirectory won't be changed by the event loop execution.; # All the information needed to reconstruct the computation graph on; # the workers is contained in the head node; """"""Propagate transform operations to the headnode internal RDataFrame""""""; """"""Creates a new node and inserts it in the computation graph""""""; """"""; Abstract class for proxies objects. These objects help to keep track of; nodes' variable assignment. That is, when a node is no longer assigned; to a variable by the user, the role of the proxy is to show that. This is; done via changing the value of the :obj:`has_user_references` of the; proxied node from :obj:`True` to :obj:`False`.; """"""; """"""; Creates a new `Proxy` object for a given node. Args:; proxied_node: The node that the current Proxy should wrap.; """"""",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Proxy.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Proxy.py
Security,access,accessed," called right before the current Proxy gets deleted by; Python. Its purpose is to show that the wrapped node has no more; user references, which is one of the conditions for the node to be; pruned from the computational graph.; """"""; """"""; Instances of ResultMapProxy act as futures of the result produced; by a call to DistRDF.VariationsFor. The aim is to mimic the functionality of; ROOT::RDF::Experimental::RResultMap to provide the same API usage.; """"""; """"""; The __getattr__ of the Proxy base class is an abstract method. This; class has no attributes to present to the user.; """"""; """"""; Equivalent of 'operator[]' of the RResultMap. Triggers the computation; graph, then returns the varied value linked to the 'key' name.; """"""; """"""; Equivalent of 'GetKeys' of the RResultMap. Unlike its C++ counterpart,; at the moment we cannot retrieve the list of variation names for a; certain action without triggering the distributed computation graph. For; this reason, the function raises an error if the keys are accessed; before computations have been triggered. In the future the behaviour; should be aligned with the C++ counterpart.; """"""; # TODO:; # The event loop has not been triggered yet. Currently we can't retrieve; # the list of variation names without starting the distributed computations; """"""; A list of names of systematic variations was requested, but the corresponding map of variations is not; present. The variation names cannot be retrieved unless the computation graph has properly run and; finished. Something may have gone wrong in the distributed execution, or no variation values were; explicitly requested. In the future, it will be possible to get the variation names without triggering.; """"""; """"""; Instances of ResultPtrProxy act as futures of the result produced; by some action node. The aim is to mimic the functionality of; ROOT::RDF::RResultPtr to provide the same API usage.; """"""; """"""; Intercepts calls on the result of; the action node. Returns:; function: A method to ha",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Proxy.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Proxy.py
Integrability,depend,depending,"""""""; Encapsulate information coming from a Snapshot operation and know how to; merge it with other objects of this type.; """"""; # Transient attribute, it will be discarded before the end of the mapper; # function (in `Utils.get_mergeablevalue`) so that we don't incur in; # serialization of the RResultPtr; """"""; When calling Snapshot on a distributed worker, a list with the path to; the snapshotted file on the worker is stored. This function extends the; list of the current object with the elements from the list of the other; object.; """"""; """"""; With local RDataFrame, Snapshot returns another RDataFrame object that; can be used to continue the application. The equivalent in the; distributed scenario is to create a distributed RDataFrame. This is done by constructing a TChain with the name and the list of; paths stored in this object. The chain is then passed to the; `make_dataframe` function that changes depending on the backend. For example, if the original RDataFrame that triggered the distributed; computation was created via a Spark backend, then this function will; return another distributed RDataFrame build from a Spark backend; instance. And so on for all other DistRDF backends.; """"""; # Add partial snapshot files to the chain; # Create a new rdf with the chain and return that to user; # A type alias to signify any type of result that can be returned from the RDataFrame API",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/PythonMergeables.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/PythonMergeables.py
Modifiability,extend,extends,"""""""; Encapsulate information coming from a Snapshot operation and know how to; merge it with other objects of this type.; """"""; # Transient attribute, it will be discarded before the end of the mapper; # function (in `Utils.get_mergeablevalue`) so that we don't incur in; # serialization of the RResultPtr; """"""; When calling Snapshot on a distributed worker, a list with the path to; the snapshotted file on the worker is stored. This function extends the; list of the current object with the elements from the list of the other; object.; """"""; """"""; With local RDataFrame, Snapshot returns another RDataFrame object that; can be used to continue the application. The equivalent in the; distributed scenario is to create a distributed RDataFrame. This is done by constructing a TChain with the name and the list of; paths stored in this object. The chain is then passed to the; `make_dataframe` function that changes depending on the backend. For example, if the original RDataFrame that triggered the distributed; computation was created via a Spark backend, then this function will; return another distributed RDataFrame build from a Spark backend; instance. And so on for all other DistRDF backends.; """"""; # Add partial snapshot files to the chain; # Create a new rdf with the chain and return that to user; # A type alias to signify any type of result that can be returned from the RDataFrame API",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/PythonMergeables.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/PythonMergeables.py
Integrability,depend,depend,"ees partitioning them; by percentages.; """"""; # Given a number of files, partition them in npartitions, considering each; # file as splittable in percentages [0, 1]. Gather:; # 1. A list of percentages according to how many partitions are required.; # 2. The corresponding list of file boundaries, as integers.; # 3. The difference between the two above, to know to which percentage of; # a specific file any element of the first list belongs.; # Example with nfiles = 10 and npartitions = 7; # percentages = [0., 1.428, 2.857, 4.285, 5.714, 7.142, 8.571, 10.]; # files_of_percentages = [0, 1, 2, 4, 5, 7, 8, 10]; # percentages_wrt_files = [0., 0.428, 0.857, 0.285, 0.714, 0.142, 0.571, 0.]; # Compute which files are to be considered for the various tasks; # The indexes of starting files in each task are simply the list of files; # from above, except for the last value which corresponds to the end of the; # last file. Also, they are inclusive.; # The indexes of ending files in each task depend on what is the percentage; # considered for that file. Also, they are exclusive. When the percentage is; # zero, i.e. we are at a file boundary, we want to consider the whole; # (previous) file, we just take the file index (shifting the list by one).; # When the percentage is above zero, we increase the index (shifted by one); # by one to be able to consider also the current file.; # Compute the starting percentage of the first tree and the ending percentage; # of the last tree in each task.; # When computing the ending percentages, if the percentage defined above is; # zero, i.e. we are at file boundary, we want to consider the whole tree,; # thus we set it to one.; # We need to transmit the full list of treenames and filenames to each; # task, in order to properly align the full dataset considering friends.; # With the indexes created above, we can partition the lists of names of; # files and trees. Each task will get a number of trees dictated by the; # starting index (inclusive) and",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Ranges.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Ranges.py
Safety,sanity check,sanity check,"; Range of entries in one of the trees in the chain of a single distributed task. The entries are local with respect to the list of files that are processed; in this range. These files are a subset of the global list of input files of; the original dataset. Attributes:. treenames: List of tree names. filenames: List of files to be processed with this range. globalstart: Starting entry relative to the TChain made with the trees in; this range. globalend: Ending entry relative to the TChain made with the trees in this; range. friendinfo: Information about friend trees of the chain built for this; range. Not None if the user provided a TTree or TChain in the; distributed RDataFrame constructor.; """"""; """"""; Entries corresponding to each tree assigned to a certain task, plus the; actual number of entries that task will be processing. This information will; be aggregated along with the main mergeable results in distributed; execution. It serves as a sanity check that exactly the total amount of; entries in the dataset is processed in the application.; """"""; """"""Function to split the list of filenames into exactly N chunks of approximately equal size.""""""; """"""; Builds range pairs from the given values of the number of entries in; the dataset and number of partitions required. The `nentries` are divided; uniformly among the `npartitions`. Args:; nentries (int): The number of entries in a dataset. npartitions (int): The number of partititions the sequence of entries; should be split in. Returns:; list[DistRDF.Ranges.EmptySourceRange]: Each element of the list contains; the start and end entry of the corresponding range.; """"""; # Iterator; # Keep track of the current range id; # Start value of current range; # If the modulo value is not; # exhausted, add '1' to the end; # of the current range; """"""; Create a list of tasks that will process the given trees partitioning them; by percentages.; """"""; # Given a number of files, partition them in npartitions, considering each; # file as s",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Ranges.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Ranges.py
Testability,log,logical,"""""""; A logical range of entries in which a dataset is split. Depending on the; input data source, this can have different attributes. Attributes:. exec_id: An identifier for the current execution. id: A sequential counter to identify this range.; """"""; """"""; Empty source range of entries. Attributes:. start (int): Starting entry of this range. end (int): Ending entry of this range.; """"""; """"""; Range of percentages to be considered for a list of trees. Building block; for an actual range of entries of a distributed task. Attributes:. treenames: List of tree names. filenames: List of files to be processed with this range. first_file_idx: Index of the first file that this range will consider. last_file_idx: Index of the last file that this range will consider. first_tree_start_perc: Percentage of the first tree from which this range will; begin reading entries. last_tree_end_perc: Percentage of the last tree at which this range will; end reading entries. friendinfo: Information about friend trees of the chain built for this; range. Not None if the user provided a TTree or TChain in the; distributed RDataFrame constructor.; """"""; """"""; Range of entries in one of the trees in the chain of a single distributed task. The entries are local with respect to the list of files that are processed; in this range. These files are a subset of the global list of input files of; the original dataset. Attributes:. treenames: List of tree names. filenames: List of files to be processed with this range. globalstart: Starting entry relative to the TChain made with the trees in; this range. globalend: Ending entry relative to the TChain made with the trees in this; range. friendinfo: Information about friend trees of the chain built for this; range. Not None if the user provided a TTree or TChain in the; distributed RDataFrame constructor.; """"""; """"""; Entries corresponding to each tree assigned to a certain task, plus the; actual number of entries that task will be processing. This information w",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Ranges.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Ranges.py
Usability,simpl,simply," current range id; # Start value of current range; # If the modulo value is not; # exhausted, add '1' to the end; # of the current range; """"""; Create a list of tasks that will process the given trees partitioning them; by percentages.; """"""; # Given a number of files, partition them in npartitions, considering each; # file as splittable in percentages [0, 1]. Gather:; # 1. A list of percentages according to how many partitions are required.; # 2. The corresponding list of file boundaries, as integers.; # 3. The difference between the two above, to know to which percentage of; # a specific file any element of the first list belongs.; # Example with nfiles = 10 and npartitions = 7; # percentages = [0., 1.428, 2.857, 4.285, 5.714, 7.142, 8.571, 10.]; # files_of_percentages = [0, 1, 2, 4, 5, 7, 8, 10]; # percentages_wrt_files = [0., 0.428, 0.857, 0.285, 0.714, 0.142, 0.571, 0.]; # Compute which files are to be considered for the various tasks; # The indexes of starting files in each task are simply the list of files; # from above, except for the last value which corresponds to the end of the; # last file. Also, they are inclusive.; # The indexes of ending files in each task depend on what is the percentage; # considered for that file. Also, they are exclusive. When the percentage is; # zero, i.e. we are at a file boundary, we want to consider the whole; # (previous) file, we just take the file index (shifting the list by one).; # When the percentage is above zero, we increase the index (shifted by one); # by one to be able to consider also the current file.; # Compute the starting percentage of the first tree and the ending percentage; # of the last tree in each task.; # When computing the ending percentages, if the percentage defined above is; # zero, i.e. we are at file boundary, we want to consider the whole tree,; # thus we set it to one.; # We need to transmit the full list of treenames and filenames to each; # task, in order to properly align the full dataset consid",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Ranges.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Ranges.py
Security,hash,hashable,"""""""; A unique identifier for the current execution of the computation graph of; a particular RDataFrame instance. The class is hashable so it can be used; as a key in dictionaries. Attributes:. rdf_uuid: An identifier for the specific RDataFrame instance.; graph_uuid: An identifier for the computation graph sent to the workers for; the current execution.; """"""",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/_graph_cache.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/_graph_cache.py
Integrability,inject,inject,"# @author Vincenzo Eduardo Padulano; # @author Enric Tejedor; # @date 2021-02; ################################################################################; # Copyright (C) 1995-2022, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; """"""; Set a function that will be executed as a first step on every backend before; any other operation. This method also executes the function on the current; user environment so changes are visible on the running session. This allows users to inject and execute custom code on the worker; environment without being part of the RDataFrame computational graph. Args:; fun (function): Function to be executed. *args (list): Variable length argument list used to execute the; function. **kwargs (dict): Keyword arguments used to execute the function.; """"""; """"""; Trigger the execution of multiple RDataFrame computation graphs on a certain; distributed backend. If the backend doesn't support multiple job; submissions concurrently, the distributed computation graphs will be; executed sequentially. Args:; proxies(list): List of action proxies that should be triggered. Only; actions belonging to different RDataFrame graphs will be; triggered to avoid useless calls. Return:; (int): The number of unique computation graphs executed by this call. Example:. @code{.py}; import ROOT; RDataFrame = ROOT.RDF.Experimental.Distributed.Dask.RDataFrame; RunGraphs = ROOT.RDF.Experimental.Distributed.RunGraphs. # Create 3 different dataframes and book an histogram on each one; histoproxies = [; RDataFrame(100); .Define(""x"", ""rdfentry_""); .Histo1D((""name"", ""title"", 10, 0, 100), ""x""); for _ in range(4); ]. # Execute the 3 computation graphs; n_graphs_run = RunGraphs(histoproxies); # Retrieve all the histograms in one go; histos = [histoproxy.GetValue() for hi",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/__init__.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/__init__.py
Performance,concurren,concurrently,") 1995-2022, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; """"""; Set a function that will be executed as a first step on every backend before; any other operation. This method also executes the function on the current; user environment so changes are visible on the running session. This allows users to inject and execute custom code on the worker; environment without being part of the RDataFrame computational graph. Args:; fun (function): Function to be executed. *args (list): Variable length argument list used to execute the; function. **kwargs (dict): Keyword arguments used to execute the function.; """"""; """"""; Trigger the execution of multiple RDataFrame computation graphs on a certain; distributed backend. If the backend doesn't support multiple job; submissions concurrently, the distributed computation graphs will be; executed sequentially. Args:; proxies(list): List of action proxies that should be triggered. Only; actions belonging to different RDataFrame graphs will be; triggered to avoid useless calls. Return:; (int): The number of unique computation graphs executed by this call. Example:. @code{.py}; import ROOT; RDataFrame = ROOT.RDF.Experimental.Distributed.Dask.RDataFrame; RunGraphs = ROOT.RDF.Experimental.Distributed.RunGraphs. # Create 3 different dataframes and book an histogram on each one; histoproxies = [; RDataFrame(100); .Define(""x"", ""rdfentry_""); .Histo1D((""name"", ""title"", 10, 0, 100), ""x""); for _ in range(4); ]. # Execute the 3 computation graphs; n_graphs_run = RunGraphs(histoproxies); # Retrieve all the histograms in one go; histos = [histoproxy.GetValue() for histoproxy in histoproxies]; @endcode. """"""; # Import here to avoid circular dependencies in main module; # Get proxies belonging to distinct computation graphs; # Submit all co",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/__init__.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/__init__.py
Safety,avoid,avoid,"#; ################################################################################; """"""; Set a function that will be executed as a first step on every backend before; any other operation. This method also executes the function on the current; user environment so changes are visible on the running session. This allows users to inject and execute custom code on the worker; environment without being part of the RDataFrame computational graph. Args:; fun (function): Function to be executed. *args (list): Variable length argument list used to execute the; function. **kwargs (dict): Keyword arguments used to execute the function.; """"""; """"""; Trigger the execution of multiple RDataFrame computation graphs on a certain; distributed backend. If the backend doesn't support multiple job; submissions concurrently, the distributed computation graphs will be; executed sequentially. Args:; proxies(list): List of action proxies that should be triggered. Only; actions belonging to different RDataFrame graphs will be; triggered to avoid useless calls. Return:; (int): The number of unique computation graphs executed by this call. Example:. @code{.py}; import ROOT; RDataFrame = ROOT.RDF.Experimental.Distributed.Dask.RDataFrame; RunGraphs = ROOT.RDF.Experimental.Distributed.RunGraphs. # Create 3 different dataframes and book an histogram on each one; histoproxies = [; RDataFrame(100); .Define(""x"", ""rdfentry_""); .Histo1D((""name"", ""title"", 10, 0, 100), ""x""); for _ in range(4); ]. # Execute the 3 computation graphs; n_graphs_run = RunGraphs(histoproxies); # Retrieve all the histograms in one go; histos = [histoproxy.GetValue() for histoproxy in histoproxies]; @endcode. """"""; # Import here to avoid circular dependencies in main module; # Get proxies belonging to distinct computation graphs; # Submit all computation graphs concurrently from multiple Python threads.; # The submission is not computationally intensive; """"""; Equivalent of ROOT.RDF.Experimental.VariationsFor in distributed mode.; ",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/__init__.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/__init__.py
Security,inject,inject,"# @author Vincenzo Eduardo Padulano; # @author Enric Tejedor; # @date 2021-02; ################################################################################; # Copyright (C) 1995-2022, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; """"""; Set a function that will be executed as a first step on every backend before; any other operation. This method also executes the function on the current; user environment so changes are visible on the running session. This allows users to inject and execute custom code on the worker; environment without being part of the RDataFrame computational graph. Args:; fun (function): Function to be executed. *args (list): Variable length argument list used to execute the; function. **kwargs (dict): Keyword arguments used to execute the function.; """"""; """"""; Trigger the execution of multiple RDataFrame computation graphs on a certain; distributed backend. If the backend doesn't support multiple job; submissions concurrently, the distributed computation graphs will be; executed sequentially. Args:; proxies(list): List of action proxies that should be triggered. Only; actions belonging to different RDataFrame graphs will be; triggered to avoid useless calls. Return:; (int): The number of unique computation graphs executed by this call. Example:. @code{.py}; import ROOT; RDataFrame = ROOT.RDF.Experimental.Distributed.Dask.RDataFrame; RunGraphs = ROOT.RDF.Experimental.Distributed.RunGraphs. # Create 3 different dataframes and book an histogram on each one; histoproxies = [; RDataFrame(100); .Define(""x"", ""rdfentry_""); .Histo1D((""name"", ""title"", 10, 0, 100), ""x""); for _ in range(4); ]. # Execute the 3 computation graphs; n_graphs_run = RunGraphs(histoproxies); # Retrieve all the histograms in one go; histos = [histoproxy.GetValue() for hi",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/__init__.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/__init__.py
Deployability,release,release,"# @author Vincenzo Eduardo Padulano; # @author Enric Tejedor; # @date 2021-02; ################################################################################; # Copyright (C) 1995-2021, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Type hints only; """"""; Perform initial setup steps common to every mapper function.; """"""; # Disable graphics functionality in ROOT. It is not needed inside a; # distributed task; # Enable thread safety for the whole mapper function. We need to do; # this since two tasks could be invoking the C++ interpreter; # simultaneously, given that this function will release the GIL; # before calling into C++ to run the event loop. Dask multi-threaded; # or even multi-process workers could trigger such a scenario.; # Run initialization method to prepare the worker runtime; # environment; """"""; Triggers the computation graph and returns a list of mergeable values.; """"""; """"""; Holds objects returned by a task in distributed execution.; Attributes:; mergeables: A list of the partial results of the mapper. Only in a; TTree-based run, if the task has nothing to process then this; attribute is None.; entries_in_trees: A struct holding the amount of processed entries in; the task, as well as a dictionary where each key is an identifier; for a tree opened in the task and the value is the number of entries; in that tree. This attribute is not None only in a TTree-based run.; """"""; """"""; Maps the computation graph to the input logical range of entries.; """"""; # Wrap code that may be calling into C++ in a try-except block in order; # to better propagate exceptions.; # Build an RDataFrame instance for the current mapper task, based; # on the type of the head node.; """"""; Merge values of second argument into values of first argument and return; first argum",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Backends/Base.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Backends/Base.py
Energy Efficiency,reduce,reduce,"t; # entries, so we sum them; # Wrap code that may be calling into C++ in a try-except block in order; # to better propagate exceptions.; """"""; Base class for RDataFrame distributed backends. Attributes:; supported_operations (list): List of operations supported by the; backend.; initialization (function): Store user's initialization method, if; defined.; headers (list): List of headers that need to be declared for the; analysis.; shared_libraries (list): List of shared libraries needed for the; analysis.; """"""; """"""; Convert the initialization function and its arguments into a callable; without arguments. This callable is saved on the backend parent class.; Therefore, changes on the runtime backend do not require users to set; the initialization function again. Args:; fun (function): Function to be executed. *args (list): Variable length argument list used to execute the; function. **kwargs (dict): Keyword arguments used to execute the function.; """"""; """"""; Subclasses must define how to run map-reduce functions on a given; backend.; """"""; """"""; Subclasses must define how to send all files needed for the analysis; (like headers and libraries) to the workers.; """"""; """"""; Return a default number of partitions to split the dataframe in,; depending on the backend.; """"""; """"""; Sends to the workers the generic files needed by the user. Args:; files_paths (str, iter): Paths to the files to be sent to the; distributed workers.; """"""; """"""; Includes the C++ headers to be declared before execution. Args:; headers_paths (str, iter): A string or an iterable (such as a; list, set...) containing the paths to all necessary C++ headers; as strings. This function accepts both paths to the headers; themselves and paths to directories containing the headers.; """"""; # Distribute header files to the workers; # Declare headers locally; # Finally, add everything to the includes set; """"""; Includes the C++ shared libraries to be declared before execution. If; any pcm file is present in the same folder ",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Backends/Base.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Backends/Base.py
Integrability,depend,depending,"ons (list): List of operations supported by the; backend.; initialization (function): Store user's initialization method, if; defined.; headers (list): List of headers that need to be declared for the; analysis.; shared_libraries (list): List of shared libraries needed for the; analysis.; """"""; """"""; Convert the initialization function and its arguments into a callable; without arguments. This callable is saved on the backend parent class.; Therefore, changes on the runtime backend do not require users to set; the initialization function again. Args:; fun (function): Function to be executed. *args (list): Variable length argument list used to execute the; function. **kwargs (dict): Keyword arguments used to execute the function.; """"""; """"""; Subclasses must define how to run map-reduce functions on a given; backend.; """"""; """"""; Subclasses must define how to send all files needed for the analysis; (like headers and libraries) to the workers.; """"""; """"""; Return a default number of partitions to split the dataframe in,; depending on the backend.; """"""; """"""; Sends to the workers the generic files needed by the user. Args:; files_paths (str, iter): Paths to the files to be sent to the; distributed workers.; """"""; """"""; Includes the C++ headers to be declared before execution. Args:; headers_paths (str, iter): A string or an iterable (such as a; list, set...) containing the paths to all necessary C++ headers; as strings. This function accepts both paths to the headers; themselves and paths to directories containing the headers.; """"""; # Distribute header files to the workers; # Declare headers locally; # Finally, add everything to the includes set; """"""; Includes the C++ shared libraries to be declared before execution. If; any pcm file is present in the same folder as the shared libraries, the; function will try to retrieve them and distribute them. Args:; shared_libraries_paths (str, iter): A string or an iterable (such as; a list, set...) containing the paths to all necessary C++ ",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Backends/Base.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Backends/Base.py
Performance,multi-thread,multi-threaded,"# @author Vincenzo Eduardo Padulano; # @author Enric Tejedor; # @date 2021-02; ################################################################################; # Copyright (C) 1995-2021, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Type hints only; """"""; Perform initial setup steps common to every mapper function.; """"""; # Disable graphics functionality in ROOT. It is not needed inside a; # distributed task; # Enable thread safety for the whole mapper function. We need to do; # this since two tasks could be invoking the C++ interpreter; # simultaneously, given that this function will release the GIL; # before calling into C++ to run the event loop. Dask multi-threaded; # or even multi-process workers could trigger such a scenario.; # Run initialization method to prepare the worker runtime; # environment; """"""; Triggers the computation graph and returns a list of mergeable values.; """"""; """"""; Holds objects returned by a task in distributed execution.; Attributes:; mergeables: A list of the partial results of the mapper. Only in a; TTree-based run, if the task has nothing to process then this; attribute is None.; entries_in_trees: A struct holding the amount of processed entries in; the task, as well as a dictionary where each key is an identifier; for a tree opened in the task and the value is the number of entries; in that tree. This attribute is not None only in a TTree-based run.; """"""; """"""; Maps the computation graph to the input logical range of entries.; """"""; # Wrap code that may be calling into C++ in a try-except block in order; # to better propagate exceptions.; # Build an RDataFrame instance for the current mapper task, based; # on the type of the head node.; """"""; Merge values of second argument into values of first argument and return; first argum",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Backends/Base.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Backends/Base.py
Safety,safe,safety,"# @author Vincenzo Eduardo Padulano; # @author Enric Tejedor; # @date 2021-02; ################################################################################; # Copyright (C) 1995-2021, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Type hints only; """"""; Perform initial setup steps common to every mapper function.; """"""; # Disable graphics functionality in ROOT. It is not needed inside a; # distributed task; # Enable thread safety for the whole mapper function. We need to do; # this since two tasks could be invoking the C++ interpreter; # simultaneously, given that this function will release the GIL; # before calling into C++ to run the event loop. Dask multi-threaded; # or even multi-process workers could trigger such a scenario.; # Run initialization method to prepare the worker runtime; # environment; """"""; Triggers the computation graph and returns a list of mergeable values.; """"""; """"""; Holds objects returned by a task in distributed execution.; Attributes:; mergeables: A list of the partial results of the mapper. Only in a; TTree-based run, if the task has nothing to process then this; attribute is None.; entries_in_trees: A struct holding the amount of processed entries in; the task, as well as a dictionary where each key is an identifier; for a tree opened in the task and the value is the number of entries; in that tree. This attribute is not None only in a TTree-based run.; """"""; """"""; Maps the computation graph to the input logical range of entries.; """"""; # Wrap code that may be calling into C++ in a try-except block in order; # to better propagate exceptions.; # Build an RDataFrame instance for the current mapper task, based; # on the type of the head node.; """"""; Merge values of second argument into values of first argument and return; first argum",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Backends/Base.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Backends/Base.py
Security,access,access," run.; """"""; """"""; Maps the computation graph to the input logical range of entries.; """"""; # Wrap code that may be calling into C++ in a try-except block in order; # to better propagate exceptions.; # Build an RDataFrame instance for the current mapper task, based; # on the type of the head node.; """"""; Merge values of second argument into values of first argument and return; first argument.; """"""; # This should treat the 4 possible cases:; # 1. both arguments are non-empty: first if statement; # 2. First argument is None and second is not empty: elif statement; # 3. First argument is not empty and second is None: return first; # list, no need to do anything; # 4. Both arguments are None: return first, it's None anyway.; """"""; Merges two given iterables of values that were returned by two mapper; function executions. Returns the first argument with its values updated from; the second.; """"""; # Merge dictionaries of trees and their entries. Different tasks; # might have to access the same tree, so we must not count its; # entries more than once.; # On the other hand, any two tasks will process different; # entries, so we sum them; # Wrap code that may be calling into C++ in a try-except block in order; # to better propagate exceptions.; """"""; Base class for RDataFrame distributed backends. Attributes:; supported_operations (list): List of operations supported by the; backend.; initialization (function): Store user's initialization method, if; defined.; headers (list): List of headers that need to be declared for the; analysis.; shared_libraries (list): List of shared libraries needed for the; analysis.; """"""; """"""; Convert the initialization function and its arguments into a callable; without arguments. This callable is saved on the backend parent class.; Therefore, changes on the runtime backend do not require users to set; the initialization function again. Args:; fun (function): Function to be executed. *args (list): Variable length argument list used to execute the; functi",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Backends/Base.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Backends/Base.py
Testability,log,logical,"he whole mapper function. We need to do; # this since two tasks could be invoking the C++ interpreter; # simultaneously, given that this function will release the GIL; # before calling into C++ to run the event loop. Dask multi-threaded; # or even multi-process workers could trigger such a scenario.; # Run initialization method to prepare the worker runtime; # environment; """"""; Triggers the computation graph and returns a list of mergeable values.; """"""; """"""; Holds objects returned by a task in distributed execution.; Attributes:; mergeables: A list of the partial results of the mapper. Only in a; TTree-based run, if the task has nothing to process then this; attribute is None.; entries_in_trees: A struct holding the amount of processed entries in; the task, as well as a dictionary where each key is an identifier; for a tree opened in the task and the value is the number of entries; in that tree. This attribute is not None only in a TTree-based run.; """"""; """"""; Maps the computation graph to the input logical range of entries.; """"""; # Wrap code that may be calling into C++ in a try-except block in order; # to better propagate exceptions.; # Build an RDataFrame instance for the current mapper task, based; # on the type of the head node.; """"""; Merge values of second argument into values of first argument and return; first argument.; """"""; # This should treat the 4 possible cases:; # 1. both arguments are non-empty: first if statement; # 2. First argument is None and second is not empty: elif statement; # 3. First argument is not empty and second is None: return first; # list, no need to do anything; # 4. Both arguments are None: return first, it's None anyway.; """"""; Merges two given iterables of values that were returned by two mapper; function executions. Returns the first argument with its values updated from; the second.; """"""; # Merge dictionaries of trees and their entries. Different tasks; # might have to access the same tree, so we must not count its; # entries more ",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Backends/Base.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Backends/Base.py
Availability,error,error,"can find them. Even if the same path; is added twice, ROOT keeps a collection of unique paths. Find more at; `TInterpreter<https://root.cern.ch/doc/master/classTInterpreter.html>`_. Args:; include_path (str): the path to the directory containing files; needed for the analysis.; """"""; # Retrieve ROOT internal list of include paths and add debug statement; """"""; Declares all required headers using the ROOT's C++ Interpreter. Args:; headers_to_include (list): This list should consist of all; necessary C++ headers as strings.; """"""; # Retrieve header directory; # Add directory to ROOT's include path; # Create C++ include code; """"""; Declares all required shared libraries using the ROOT's C++; Interpreter. Args:; libraries_to_include (list): This list should consist of all; necessary C++ shared libraries as strings.; """"""; # Get return value for loading the shared library.; # On succesful load the value will be 0.; # If the library does not exist or there was an error; # while loading, the value will be -1; """"""; Retrieves paths to files (directory or single file) from a string. Args:; path_string (str): The string to the path of the file or directory; to be recursively searched for files. Returns:; set: The set with all paths returned from the directory, or a set; with only the path of the string.; """"""; # Create a set with all the headers in the directory; # Convert to set if this is a string; """"""; Retrieves paths to shared libraries and pcm file(s) in a directory. Args:; shared_library_path (str): The string to the path of the file or; directory to be recursively searched for files. Returns:; list, list: Two lists, the first with all paths to pcm files, the; second with all paths to shared libraries.; """"""; """"""; Generally the input argument to this function is an RResultPtr, for which a; corresponding RMergeableValue type already exists. Call into the C++; function to handle this case.; """"""; """"""; Results coming from an `AsNumpy` operation can be merged with others, but; we nee",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Backends/Utils.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Backends/Utils.py
Performance,load,loading,"he list of paths in which ROOT looks for headers and; libraries. Every header directory is added to the internal include; path of ROOT so the interpreter can find them. Even if the same path; is added twice, ROOT keeps a collection of unique paths. Find more at; `TInterpreter<https://root.cern.ch/doc/master/classTInterpreter.html>`_. Args:; include_path (str): the path to the directory containing files; needed for the analysis.; """"""; # Retrieve ROOT internal list of include paths and add debug statement; """"""; Declares all required headers using the ROOT's C++ Interpreter. Args:; headers_to_include (list): This list should consist of all; necessary C++ headers as strings.; """"""; # Retrieve header directory; # Add directory to ROOT's include path; # Create C++ include code; """"""; Declares all required shared libraries using the ROOT's C++; Interpreter. Args:; libraries_to_include (list): This list should consist of all; necessary C++ shared libraries as strings.; """"""; # Get return value for loading the shared library.; # On succesful load the value will be 0.; # If the library does not exist or there was an error; # while loading, the value will be -1; """"""; Retrieves paths to files (directory or single file) from a string. Args:; path_string (str): The string to the path of the file or directory; to be recursively searched for files. Returns:; set: The set with all paths returned from the directory, or a set; with only the path of the string.; """"""; # Create a set with all the headers in the directory; # Convert to set if this is a string; """"""; Retrieves paths to shared libraries and pcm file(s) in a directory. Args:; shared_library_path (str): The string to the path of the file or; directory to be recursively searched for files. Returns:; list, list: Two lists, the first with all paths to pcm files, the; second with all paths to shared libraries.; """"""; """"""; Generally the input argument to this function is an RResultPtr, for which a; corresponding RMergeableValue type alr",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Backends/Utils.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Backends/Utils.py
Security,access,access," coming from an `AsNumpy` operation can be merged with others, but; we need to make sure to call its `GetValue` method since that will populate; the private attribute `_py_arrays` (which is the actual dictionary of; numpy arrays extracted from the RDataFrame columns). This extra call is an; insurance against backends that do not automatically serialize objects; returned by the mapper function (otherwise this would be taken care by the; `AsNumpyResult`'s `__getstate__` method).; """"""; """"""; When performing a distributed Snapshot we return an object holding the name; of the dataset and the path to the partial snapshot. We can directly return; the object, no extra work needed.; """"""; """"""; Generally the arguments are `RMergeableValue` instances that can be directly; passed to the C++ function responsible for merging them.; """"""; """"""; Mergeables coming from `Snapshot` or `AsNumpy` operations have their own; `Merge` method.; """"""; """"""; Connects the final value after distributed computation to the corresponding; DistRDF node.; By default, the `GetValue` method of the mergeable returns the final value.; """"""; """"""; Connects the final value after distributed computation to the corresponding; DistRDF node.; This overload calls the `GetValue` method of `SnapshotResult`. This method; accepts a 'backend' parameter because we need to recreate a distributed; RDataFrame with the same backend of the input one.; """"""; """"""; Connects the final value after distributed computation to the corresponding; DistRDF node.; In this overload, the node stores the reference to the mergeable variations; directly. It is then responsibility of the ResultMapProxy object to access; the specific varied object asked by the user, calling the right method of; the RMergeableVariations class.; """"""; """"""; Clone the action held by an RResultPtr or RResultMap, registering it with; its RLoopManager.; """"""; # Create output file name for the cloned Snapshot; # Actually clone the RDF C++ Snapshot node with the new file name",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Backends/Utils.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Backends/Utils.py
Integrability,inject,inject,"# @author Vincenzo Eduardo Padulano; # @author Enric Tejedor; # @date 2021-02; ################################################################################; # Copyright (C) 1995-2021, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; """"""; Helper function to create the submodules of the backends.; """"""; # The actual python package with the backend implementation; # A dummy module to inject in the parent module; # PEP302 attributes; # dummy.__name__ is the constructor argument; # this makes it a package; # dummy.__loader__ is not defined; # Attached functions",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Backends/__init__.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Backends/__init__.py
Security,inject,inject,"# @author Vincenzo Eduardo Padulano; # @author Enric Tejedor; # @date 2021-02; ################################################################################; # Copyright (C) 1995-2021, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; """"""; Helper function to create the submodules of the backends.; """"""; # The actual python package with the backend implementation; # A dummy module to inject in the parent module; # PEP302 attributes; # dummy.__name__ is the constructor argument; # this makes it a package; # dummy.__loader__ is not defined; # Attached functions",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Backends/__init__.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Backends/__init__.py
Availability,avail,available,"# @author Vincenzo Eduardo Padulano; # @author Enric Tejedor; # @date 2021-11; ################################################################################; # Copyright (C) 1995-2021, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; """"""; Retrieve the total number of cores known to the Dask scheduler through the; client connection.; """"""; """"""; Retrieve the total number of cores from a Dask cluster connected to some; kind of batch system (HTCondor, Slurm...).; """"""; # Wrapping in a try-block in case any of the dictionaries do not have the; # needed keys; # In some cases the Dask scheduler doesn't know about available workers; # at creation time. Most notably, when using batch systems like HTCondor; # through dask-jobqueue, creating the cluster object doesn't actually; # start the workers. The scheduler will know about available workers in; # the cluster only after cluster.scale has been called and the resource; # manager has granted the requested jobs. So at this point, we can only; # rely on the information that was passed by the user as a specification; # of the cluster object. This comes in the form:; # {'WORKER-NAME-1': {'cls': <class 'dask.WORKERCLASS'>,; # 'options': {'CORES_OR_NTHREADS': N, ...}},; # 'WORKER-NAME-2': {'cls': <class 'dask.WORKERCLASS'>,; # 'options': {'CORES_OR_NTHREADS': N, ...}}}; # This concept can vary between different types of clusters, but in the; # cluster types defined in dask-jobqueue the keys of the dictionary above; # refer to the name of a job submission, which can then involve multiple; # cores of a node.; # For each job, there is a sub-dictionary that contains the 'options'; # key, which value is another dictionary with all the information; # specified when creating the cluster object. This contains also the; # 'cores' ke",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Backends/Dask/Backend.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Backends/Dask/Backend.py
Energy Efficiency,schedul,scheduler,"# @author Vincenzo Eduardo Padulano; # @author Enric Tejedor; # @date 2021-11; ################################################################################; # Copyright (C) 1995-2021, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; """"""; Retrieve the total number of cores known to the Dask scheduler through the; client connection.; """"""; """"""; Retrieve the total number of cores from a Dask cluster connected to some; kind of batch system (HTCondor, Slurm...).; """"""; # Wrapping in a try-block in case any of the dictionaries do not have the; # needed keys; # In some cases the Dask scheduler doesn't know about available workers; # at creation time. Most notably, when using batch systems like HTCondor; # through dask-jobqueue, creating the cluster object doesn't actually; # start the workers. The scheduler will know about available workers in; # the cluster only after cluster.scale has been called and the resource; # manager has granted the requested jobs. So at this point, we can only; # rely on the information that was passed by the user as a specification; # of the cluster object. This comes in the form:; # {'WORKER-NAME-1': {'cls': <class 'dask.WORKERCLASS'>,; # 'options': {'CORES_OR_NTHREADS': N, ...}},; # 'WORKER-NAME-2': {'cls': <class 'dask.WORKERCLASS'>,; # 'options': {'CORES_OR_NTHREADS': N, ...}}}; # This concept can vary between different types of clusters, but in the; # cluster types defined in dask-jobqueue the keys of the dictionary above; # refer to the name of a job submission, which can then involve multiple; # cores of a node.; # For each job, there is a sub-dictionary that contains the 'options'; # key, which value is another dictionary with all the information; # specified when creating the cluster object. This contains also the; # 'cores' ke",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Backends/Dask/Backend.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Backends/Dask/Backend.py
Integrability,depend,dependency," # rely on the information that was passed by the user as a specification; # of the cluster object. This comes in the form:; # {'WORKER-NAME-1': {'cls': <class 'dask.WORKERCLASS'>,; # 'options': {'CORES_OR_NTHREADS': N, ...}},; # 'WORKER-NAME-2': {'cls': <class 'dask.WORKERCLASS'>,; # 'options': {'CORES_OR_NTHREADS': N, ...}}}; # This concept can vary between different types of clusters, but in the; # cluster types defined in dask-jobqueue the keys of the dictionary above; # refer to the name of a job submission, which can then involve multiple; # cores of a node.; # For each job, there is a sub-dictionary that contains the 'options'; # key, which value is another dictionary with all the information; # specified when creating the cluster object. This contains also the; # 'cores' key for any type of dask-jobqueue cluster.; """"""; Retrieve the total number of cores of the Dask cluster.; """"""; # It may happen that the user is connected to a batch system. We try; # to import the 'dask_jobqueue' module lazily to avoid a dependency.; # We are not using 'dask_jobqueue', fall through to generic case; """"""Dask backend for distributed RDataFrame.""""""; # If the user didn't explicitly pass a Client instance, the argument; # `daskclient` will be `None`. In this case, we create a default Dask; # client connected to a cluster instance with N worker processes, where; # N is the number of cores on the local machine.; """"""; Attempts to compute a clever number of partitions for the current; execution. Currently it is the number of cores of the Dask cluster,; either retrieved if known or inferred from the user-provided cluster; specification.; """"""; """"""; Gets the paths to the file(s) in the current executor, then; declares the headers found. Args:; current_range (tuple): The current range of the dataset being; processed on the executor. headers (list): List of header file paths. shared_libraries (list): List of shared library file paths. mapper (function): The map function to be executed on ea",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Backends/Dask/Backend.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Backends/Dask/Backend.py
Performance,cache,cache,"mpleted method; # Save the current canvas; # Set up live visualization canvas; # Process partial results and display plots; # Close the live visualization canvas canvas; """"""; Set up a TCanvas for live visualization with divided pads based on the number of plots. Args:; num_plots (int): Number of plots to be displayed. Returns:; c: The initialized TCanvas object.; """"""; # Define constants for canvas layout; """"""; Process partial results and display plots on the provided canvas. Args:; canvas: The TCanvas object for displaying plots.; 			; drawables_info_dict (dict): A dictionary where keys are plot object IDs ; and values are tuples containing optional callback functions, ; index of the plot object, and operation name.; 			; reducer (function): A function for reducing partial results.; 			; future_tasks: Dask future tasks representing partial results. Returns:; merged_results (TaskResult): The merged result of the computation.; """"""; # Collect all futures in batches that had arrived since the last iteration; """"""; Apply callbacks and draw plots on the provided pad. Args:; pad: The TPad object for drawing plots.; 			; cumulative_plots: A dictionary of the current merged partial results.; 			; callbacks_list: A list of callback functions to be applied.; 			; operation_name (str): Name of the operation associated with the plot.; 		; index (int): Index of the plot in cumulative_plots dictionary.; """"""; """"""; Dask supports sending files to the workers via the `Client.upload_file`; method. Its stated purpose is to send local Python packages to the; nodes, but in practice it uploads the file to the path stored in the; `local_directory` attribute of each worker.; """"""; """"""; Creates an instance of distributed RDataFrame that can send computations; to a Dask cluster.; """"""; # Set the number of partitions for this dataframe, one of the following:; # 1. User-supplied `npartitions` optional argument; """"""; Remove the computation graph identified by the input argument from the; cache.; """"""",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Backends/Dask/Backend.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Backends/Dask/Backend.py
Safety,avoid,avoid," # rely on the information that was passed by the user as a specification; # of the cluster object. This comes in the form:; # {'WORKER-NAME-1': {'cls': <class 'dask.WORKERCLASS'>,; # 'options': {'CORES_OR_NTHREADS': N, ...}},; # 'WORKER-NAME-2': {'cls': <class 'dask.WORKERCLASS'>,; # 'options': {'CORES_OR_NTHREADS': N, ...}}}; # This concept can vary between different types of clusters, but in the; # cluster types defined in dask-jobqueue the keys of the dictionary above; # refer to the name of a job submission, which can then involve multiple; # cores of a node.; # For each job, there is a sub-dictionary that contains the 'options'; # key, which value is another dictionary with all the information; # specified when creating the cluster object. This contains also the; # 'cores' key for any type of dask-jobqueue cluster.; """"""; Retrieve the total number of cores of the Dask cluster.; """"""; # It may happen that the user is connected to a batch system. We try; # to import the 'dask_jobqueue' module lazily to avoid a dependency.; # We are not using 'dask_jobqueue', fall through to generic case; """"""Dask backend for distributed RDataFrame.""""""; # If the user didn't explicitly pass a Client instance, the argument; # `daskclient` will be `None`. In this case, we create a default Dask; # client connected to a cluster instance with N worker processes, where; # N is the number of cores on the local machine.; """"""; Attempts to compute a clever number of partitions for the current; execution. Currently it is the number of cores of the Dask cluster,; either retrieved if known or inferred from the user-provided cluster; specification.; """"""; """"""; Gets the paths to the file(s) in the current executor, then; declares the headers found. Args:; current_range (tuple): The current range of the dataset being; processed on the executor. headers (list): List of header file paths. shared_libraries (list): List of shared library file paths. mapper (function): The map function to be executed on ea",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Backends/Dask/Backend.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Backends/Dask/Backend.py
Availability,avail,available,"; ################################################################################; # Filename from path (should be platform-independent); """"""; Backend that executes the computational graph using using `Spark` framework; for distributed execution. """"""; """"""; Creates an instance of the Spark backend class. Args:; config (dict, optional): The config options for Spark backend.; The default value is an empty Python dictionary :obj:`{}`.; :obj:`config` should be a dictionary of Spark configuration; options and their values with :obj:'npartitions' as the only; allowed extra parameter. Example::. config = {; 'npartitions':20,; 'spark.master':'myMasterURL',; 'spark.executor.instances':10,; 'spark.app.name':'mySparkAppName'; }. Note:; If a SparkContext is already set in the current environment, the; Spark configuration parameters from :obj:'config' will be ignored; and the already existing SparkContext would be used. """"""; """"""; The SparkContext.defaultParallelism property roughly translates to the; available amount of logical cores on the cluster. Some examples:; - spark.master = local[n]: returns n.; - spark.executor.instances = m and spark.executor.cores = n: returns `n*m`.; By default, the minimum number this returns is 2 if the context; doesn't know any better. For example, if dynamic allocation is enabled.; """"""; """"""; Performs map-reduce using Spark framework. Args:; mapper (function): A function that runs the computational graph; and returns a list of values. reducer (function): A function that merges two lists that were; returned by the mapper. Returns:; list: A list representing the values of action nodes returned; after computation (Map-Reduce).; """"""; # These need to be passed as variables and not as class attributes; # otherwise the `spark_mapper` function would be referencing this; # this instance of the Spark backend along with the referenced; # SparkContext. This would cause the errors described in SPARK-5063.; """"""; Gets the paths to the file(s) in the current exec",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Backends/Spark/Backend.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Backends/Spark/Backend.py
Deployability,configurat,configuration,"# @author Vincenzo Eduardo Padulano; # @author Enric Tejedor; # @date 2021-02; ################################################################################; # Copyright (C) 1995-2021, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Filename from path (should be platform-independent); """"""; Backend that executes the computational graph using using `Spark` framework; for distributed execution. """"""; """"""; Creates an instance of the Spark backend class. Args:; config (dict, optional): The config options for Spark backend.; The default value is an empty Python dictionary :obj:`{}`.; :obj:`config` should be a dictionary of Spark configuration; options and their values with :obj:'npartitions' as the only; allowed extra parameter. Example::. config = {; 'npartitions':20,; 'spark.master':'myMasterURL',; 'spark.executor.instances':10,; 'spark.app.name':'mySparkAppName'; }. Note:; If a SparkContext is already set in the current environment, the; Spark configuration parameters from :obj:'config' will be ignored; and the already existing SparkContext would be used. """"""; """"""; The SparkContext.defaultParallelism property roughly translates to the; available amount of logical cores on the cluster. Some examples:; - spark.master = local[n]: returns n.; - spark.executor.instances = m and spark.executor.cores = n: returns `n*m`.; By default, the minimum number this returns is 2 if the context; doesn't know any better. For example, if dynamic allocation is enabled.; """"""; """"""; Performs map-reduce using Spark framework. Args:; mapper (function): A function that runs the computational graph; and returns a list of values. reducer (function): A function that merges two lists that were; returned by the mapper. Returns:; list: A list representing the values of action nodes returne",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Backends/Spark/Backend.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Backends/Spark/Backend.py
Energy Efficiency,reduce,reduce," options for Spark backend.; The default value is an empty Python dictionary :obj:`{}`.; :obj:`config` should be a dictionary of Spark configuration; options and their values with :obj:'npartitions' as the only; allowed extra parameter. Example::. config = {; 'npartitions':20,; 'spark.master':'myMasterURL',; 'spark.executor.instances':10,; 'spark.app.name':'mySparkAppName'; }. Note:; If a SparkContext is already set in the current environment, the; Spark configuration parameters from :obj:'config' will be ignored; and the already existing SparkContext would be used. """"""; """"""; The SparkContext.defaultParallelism property roughly translates to the; available amount of logical cores on the cluster. Some examples:; - spark.master = local[n]: returns n.; - spark.executor.instances = m and spark.executor.cores = n: returns `n*m`.; By default, the minimum number this returns is 2 if the context; doesn't know any better. For example, if dynamic allocation is enabled.; """"""; """"""; Performs map-reduce using Spark framework. Args:; mapper (function): A function that runs the computational graph; and returns a list of values. reducer (function): A function that merges two lists that were; returned by the mapper. Returns:; list: A list representing the values of action nodes returned; after computation (Map-Reduce).; """"""; # These need to be passed as variables and not as class attributes; # otherwise the `spark_mapper` function would be referencing this; # this instance of the Spark backend along with the referenced; # SparkContext. This would cause the errors described in SPARK-5063.; """"""; Gets the paths to the file(s) in the current executor, then; declares the headers found. Args:; current_range (tuple): A pair that contains the starting and; ending values of the current range. Returns:; function: The map function to be executed on each executor,; complete with all headers needed for the analysis.; """"""; # Get and declare headers on each worker; # Get and declare shared librarie",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Backends/Spark/Backend.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Backends/Spark/Backend.py
Modifiability,config,config,"# @author Vincenzo Eduardo Padulano; # @author Enric Tejedor; # @date 2021-02; ################################################################################; # Copyright (C) 1995-2021, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Filename from path (should be platform-independent); """"""; Backend that executes the computational graph using using `Spark` framework; for distributed execution. """"""; """"""; Creates an instance of the Spark backend class. Args:; config (dict, optional): The config options for Spark backend.; The default value is an empty Python dictionary :obj:`{}`.; :obj:`config` should be a dictionary of Spark configuration; options and their values with :obj:'npartitions' as the only; allowed extra parameter. Example::. config = {; 'npartitions':20,; 'spark.master':'myMasterURL',; 'spark.executor.instances':10,; 'spark.app.name':'mySparkAppName'; }. Note:; If a SparkContext is already set in the current environment, the; Spark configuration parameters from :obj:'config' will be ignored; and the already existing SparkContext would be used. """"""; """"""; The SparkContext.defaultParallelism property roughly translates to the; available amount of logical cores on the cluster. Some examples:; - spark.master = local[n]: returns n.; - spark.executor.instances = m and spark.executor.cores = n: returns `n*m`.; By default, the minimum number this returns is 2 if the context; doesn't know any better. For example, if dynamic allocation is enabled.; """"""; """"""; Performs map-reduce using Spark framework. Args:; mapper (function): A function that runs the computational graph; and returns a list of values. reducer (function): A function that merges two lists that were; returned by the mapper. Returns:; list: A list representing the values of action nodes returne",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Backends/Spark/Backend.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Backends/Spark/Backend.py
Testability,log,logical,"; ################################################################################; # Filename from path (should be platform-independent); """"""; Backend that executes the computational graph using using `Spark` framework; for distributed execution. """"""; """"""; Creates an instance of the Spark backend class. Args:; config (dict, optional): The config options for Spark backend.; The default value is an empty Python dictionary :obj:`{}`.; :obj:`config` should be a dictionary of Spark configuration; options and their values with :obj:'npartitions' as the only; allowed extra parameter. Example::. config = {; 'npartitions':20,; 'spark.master':'myMasterURL',; 'spark.executor.instances':10,; 'spark.app.name':'mySparkAppName'; }. Note:; If a SparkContext is already set in the current environment, the; Spark configuration parameters from :obj:'config' will be ignored; and the already existing SparkContext would be used. """"""; """"""; The SparkContext.defaultParallelism property roughly translates to the; available amount of logical cores on the cluster. Some examples:; - spark.master = local[n]: returns n.; - spark.executor.instances = m and spark.executor.cores = n: returns `n*m`.; By default, the minimum number this returns is 2 if the context; doesn't know any better. For example, if dynamic allocation is enabled.; """"""; """"""; Performs map-reduce using Spark framework. Args:; mapper (function): A function that runs the computational graph; and returns a list of values. reducer (function): A function that merges two lists that were; returned by the mapper. Returns:; list: A list representing the values of action nodes returned; after computation (Map-Reduce).; """"""; # These need to be passed as variables and not as class attributes; # otherwise the `spark_mapper` function would be referencing this; # this instance of the Spark backend along with the referenced; # SparkContext. This would cause the errors described in SPARK-5063.; """"""; Gets the paths to the file(s) in the current exec",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/python/DistRDF/Backends/Spark/Backend.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/python/DistRDF/Backends/Spark/Backend.py
Safety,avoid,avoid,"""""""Create dummy head node instance needed in the test""""""; # Pass None as `npartitions`. The tests will modify this member; # according to needs; """"""; Check mechanism to create a callable function that returns a PyROOT object; per each DistRDF graph node. This callable takes care of the grape pruning.; """"""; """"""Dummy backend.""""""; """"""Dummy implementation of ProcessAndMerge.""""""; """"""; Dummy implementation of distribute_unique_paths. Does nothing.; """"""; """"""Dummy make_dataframe""""""; """"""A Class for mocking RDF CPP object.""""""; """"""; Creates a mock instance. Each mock method adds an unique number to; the `ord_list` so we can check the order in which they were called.; """"""; """"""Mock Define method""""""; """"""Mock Filter method""""""; """"""Mock Count method""""""; """"""A simple test case to check the working of mapper.""""""; # A mock RDF object; # Head node; # Set of operations to build the graph; # noqa: avoid PEP8 F841; # Generate and execute the mapper; # Required order in the list of returned values (the nodes are stored; # in DFS order the first time they are appended to the graph); """"""; A test case to check that the mapper works even in the case of; pruning.; """"""; # A mock RDF object; # Head node; # Set of operations to build the graph; # noqa: avoid PEP8 F841; # Until here the graph would be:; # [1, 2, 2, 3, 3, 2]; # Reason for pruning (change of reference); # noqa: avoid PEP8 F841; # After the change of reference, it becomes; # [1, 2, 2, 3, 2, 2]; # that is, the Filter is appended at the end of the list, it is fine; # because it holds a reference to the ID of the father.; # Generate and execute the mapper; # One occurrence of 't' per action node; """"""; Test case to check that transformation nodes with no children and; no user references get pruned. """"""; # A mock RDF object; # Head node; # Graph nodes; # noqa: avoid PEP8 F841; # noqa: avoid PEP8 F841; # noqa: avoid PEP8 F841; # Transformation pruning, n5 was earlier a transformation node; # noqa: avoid PEP8 F841; # Generate and execute the ma",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/test/test_callable_generator.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/test/test_callable_generator.py
Testability,test,test,"""""""Create dummy head node instance needed in the test""""""; # Pass None as `npartitions`. The tests will modify this member; # according to needs; """"""; Check mechanism to create a callable function that returns a PyROOT object; per each DistRDF graph node. This callable takes care of the grape pruning.; """"""; """"""Dummy backend.""""""; """"""Dummy implementation of ProcessAndMerge.""""""; """"""; Dummy implementation of distribute_unique_paths. Does nothing.; """"""; """"""Dummy make_dataframe""""""; """"""A Class for mocking RDF CPP object.""""""; """"""; Creates a mock instance. Each mock method adds an unique number to; the `ord_list` so we can check the order in which they were called.; """"""; """"""Mock Define method""""""; """"""Mock Filter method""""""; """"""Mock Count method""""""; """"""A simple test case to check the working of mapper.""""""; # A mock RDF object; # Head node; # Set of operations to build the graph; # noqa: avoid PEP8 F841; # Generate and execute the mapper; # Required order in the list of returned values (the nodes are stored; # in DFS order the first time they are appended to the graph); """"""; A test case to check that the mapper works even in the case of; pruning.; """"""; # A mock RDF object; # Head node; # Set of operations to build the graph; # noqa: avoid PEP8 F841; # Until here the graph would be:; # [1, 2, 2, 3, 3, 2]; # Reason for pruning (change of reference); # noqa: avoid PEP8 F841; # After the change of reference, it becomes; # [1, 2, 2, 3, 2, 2]; # that is, the Filter is appended at the end of the list, it is fine; # because it holds a reference to the ID of the father.; # Generate and execute the mapper; # One occurrence of 't' per action node; """"""; Test case to check that transformation nodes with no children and; no user references get pruned. """"""; # A mock RDF object; # Head node; # Graph nodes; # noqa: avoid PEP8 F841; # noqa: avoid PEP8 F841; # noqa: avoid PEP8 F841; # Transformation pruning, n5 was earlier a transformation node; # noqa: avoid PEP8 F841; # Generate and execute the ma",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/test/test_callable_generator.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/test/test_callable_generator.py
Usability,simpl,simple,"""""""Create dummy head node instance needed in the test""""""; # Pass None as `npartitions`. The tests will modify this member; # according to needs; """"""; Check mechanism to create a callable function that returns a PyROOT object; per each DistRDF graph node. This callable takes care of the grape pruning.; """"""; """"""Dummy backend.""""""; """"""Dummy implementation of ProcessAndMerge.""""""; """"""; Dummy implementation of distribute_unique_paths. Does nothing.; """"""; """"""Dummy make_dataframe""""""; """"""A Class for mocking RDF CPP object.""""""; """"""; Creates a mock instance. Each mock method adds an unique number to; the `ord_list` so we can check the order in which they were called.; """"""; """"""Mock Define method""""""; """"""Mock Filter method""""""; """"""Mock Count method""""""; """"""A simple test case to check the working of mapper.""""""; # A mock RDF object; # Head node; # Set of operations to build the graph; # noqa: avoid PEP8 F841; # Generate and execute the mapper; # Required order in the list of returned values (the nodes are stored; # in DFS order the first time they are appended to the graph); """"""; A test case to check that the mapper works even in the case of; pruning.; """"""; # A mock RDF object; # Head node; # Set of operations to build the graph; # noqa: avoid PEP8 F841; # Until here the graph would be:; # [1, 2, 2, 3, 3, 2]; # Reason for pruning (change of reference); # noqa: avoid PEP8 F841; # After the change of reference, it becomes; # [1, 2, 2, 3, 2, 2]; # that is, the Filter is appended at the end of the list, it is fine; # because it holds a reference to the ID of the father.; # Generate and execute the mapper; # One occurrence of 't' per action node; """"""; Test case to check that transformation nodes with no children and; no user references get pruned. """"""; # A mock RDF object; # Head node; # Graph nodes; # noqa: avoid PEP8 F841; # noqa: avoid PEP8 F841; # noqa: avoid PEP8 F841; # Transformation pruning, n5 was earlier a transformation node; # noqa: avoid PEP8 F841; # Generate and execute the ma",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/test/test_callable_generator.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/test/test_callable_generator.py
Testability,test,test,"""""""Create dummy head node instance needed in the test""""""; # Pass None as `npartitions`. The tests will modify this member; # according to needs; """"""Unit test for the FriendInfo class""""""; """"""Creates a .root file with the parent TTree""""""; """"""Creates a .root file with the friend TTree""""""; # The friend will have a gaussian distribution with mean 20 and; # standard deviation 1; """"""; Check that RFriendInfo correctly stores information about the friend; trees; """"""; # Parent Tree; # Friend Tree; # Add friendTree to the parent; # Instantiate head node of the graph with the base TTree; # Retrieve information about friends; # Convert to Python collections; # Check that the three lists with treenames, filenames and subnames are populated; # as expected.; # Remove unnecessary .root files; """"""; Check that RFriendInfo correctly stores information about the friend; trees; """"""; # Parent Tree; # Friend chain; # Add friendTree to the parent; # Instantiate head node of the graph with the base TTree; # Retrieve information about friends; # Convert to Python collections; # Check that the three lists with treenames, filenames and subnames are populated; # as expected.; # Remove unnecessary .root files",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/test/test_friendinfo.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/test/test_friendinfo.py
Testability,test,test,"""""""Create dummy head node instance needed in the test""""""; # Pass None as `npartitions`. The tests will modify this member; # according to needs; """"""Check various functionalities of the HeadNode class""""""; """"""Create a dummy file to use for the RDataFrame constructor.""""""; """"""Constructor with incorrect arguments""""""; # Incorrect first argument in 2-argument case; # Incorrect third argument in 3-argument case; # No argument case; """"""Constructor with an in-memory-only tree is not supported""""""; # See https://github.com/root-project/root/issues/7541 and; # https://bugs.llvm.org/show_bug.cgi?id=49692 :; # llvm JIT fails to catch exceptions on M1, so we disable their testing; # Trees with no associated files are not supported; """"""; Asserts the arguments from 2 given; arguments lists. Specifically for the cases :; * [str, list or vector or str]; * [str, list or vector or str, list or vector]. """"""; # Check if the types are equal; # (this has to be done because, vector and; # list are iterables, but not of same type); # Check the contents; """"""Constructor with number of entries""""""; """"""Constructor with list of input files""""""; # Convert RDF files list to ROOT CPP vector; # RDataFrame constructor with 2nd argument as string; # RDataFrame constructor with 2nd argument as Python list; # RDataFrame constructor with 2nd argument as ROOT CPP Vector; # hn_3 got file names as std::vector<std::string> but the TreeHeadNode; # instance stores it as list[str]; """"""Constructor with TTree, one input file and selected branches""""""; # Convert RDF branches list to ROOT CPP Vector; # RDataFrame constructor with 3rd argument as Python list; # RDataFrame constructor with 3rd argument as ROOT CPP Vector; """"""Constructor with TTree, list of input files and selected branches""""""; # Convert RDF files list to ROOT CPP Vector; # Convert RDF files list to ROOT CPP Vector; # RDataFrame constructor with 2nd argument as Python List; # and 3rd argument as Python List; # RDataFrame constructor with 2nd argument as Pyth",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/test/test_headnode.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/test/test_headnode.py
Availability,error,error,"""""""Create dummy head node instance needed in the test""""""; # Pass None as `npartitions`. The tests will modify this member; # according to needs; """"""Dummy backend.""""""; """"""Dummy implementation of ProcessAndMerge.""""""; """"""; Dummy implementation of distribute_files. Does nothing.; """"""; """"""Dummy make_dataframe""""""; """"""; A series of test cases to check that all new operations are created properly; inside a new node.; """"""; """"""Function names are read accurately.""""""; # noqa: avoid PEP8 F841; """"""Arguments (unnamed) are read accurately.""""""; """"""Named arguments are read accurately.""""""; """"""; A series of test cases to check that right objects are returned for a node; (Proxy.ResultPtrProxy, Proxy.NodeProxy or Node).; """"""; """"""Proxy objects are returned for action nodes.""""""; """"""Node objects are returned for transformation nodes.""""""; """"""; Test cases to check the response of the Node class for various dunder; method calls. """"""; """"""; Test cases to check the working of other dunder methods on; Node class. """"""; # Regular dunder method must not throw an error; # Unknown dunder method",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/test/test_node.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/test/test_node.py
Safety,avoid,avoid,"""""""Create dummy head node instance needed in the test""""""; # Pass None as `npartitions`. The tests will modify this member; # according to needs; """"""Dummy backend.""""""; """"""Dummy implementation of ProcessAndMerge.""""""; """"""; Dummy implementation of distribute_files. Does nothing.; """"""; """"""Dummy make_dataframe""""""; """"""; A series of test cases to check that all new operations are created properly; inside a new node.; """"""; """"""Function names are read accurately.""""""; # noqa: avoid PEP8 F841; """"""Arguments (unnamed) are read accurately.""""""; """"""Named arguments are read accurately.""""""; """"""; A series of test cases to check that right objects are returned for a node; (Proxy.ResultPtrProxy, Proxy.NodeProxy or Node).; """"""; """"""Proxy objects are returned for action nodes.""""""; """"""Node objects are returned for transformation nodes.""""""; """"""; Test cases to check the response of the Node class for various dunder; method calls. """"""; """"""; Test cases to check the working of other dunder methods on; Node class. """"""; # Regular dunder method must not throw an error; # Unknown dunder method",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/test/test_node.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/test/test_node.py
Testability,test,test,"""""""Create dummy head node instance needed in the test""""""; # Pass None as `npartitions`. The tests will modify this member; # according to needs; """"""Dummy backend.""""""; """"""Dummy implementation of ProcessAndMerge.""""""; """"""; Dummy implementation of distribute_files. Does nothing.; """"""; """"""Dummy make_dataframe""""""; """"""; A series of test cases to check that all new operations are created properly; inside a new node.; """"""; """"""Function names are read accurately.""""""; # noqa: avoid PEP8 F841; """"""Arguments (unnamed) are read accurately.""""""; """"""Named arguments are read accurately.""""""; """"""; A series of test cases to check that right objects are returned for a node; (Proxy.ResultPtrProxy, Proxy.NodeProxy or Node).; """"""; """"""Proxy objects are returned for action nodes.""""""; """"""Node objects are returned for transformation nodes.""""""; """"""; Test cases to check the response of the Node class for various dunder; method calls. """"""; """"""; Test cases to check the working of other dunder methods on; Node class. """"""; # Regular dunder method must not throw an error; # Unknown dunder method",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/test/test_node.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/test/test_node.py
Availability,avail,available," is of type `DistRDF.ResultPtrProxy` and; wraps a node object.; """"""; """"""Test Proxy class methods.""""""; """"""A mock action node result class.""""""; """"""A test method to check function call on the Temp class.""""""; # A simple operation to check; """"""Dummy backend to test the _get_friend_info method in Dist class.""""""; """"""Dummy implementation of ProcessAndMerge.""""""; """"""; Dummy implementation of distribute_files. Does nothing.; """"""; """"""Dummy make_dataframe""""""; """"""ResultPtrProxy object reads the right input attribute.""""""; """"""; NodeProxy object reads the right input attributes,; returning the methods of the proxied node.; """"""; """"""; When a node attribute is called on a NodeProxy object, it; correctly returns the attribute of the proxied node.; """"""; """"""; When a non-defined Node class attribute is called on a; NodeProxy object, it raises an AttributeError.; """"""; """"""; Check that the user reference holds until the proxy lives. When the; Python garbage collector attempts to remove the proxy object, its; `__del__` method switches the node attribute `has_user_references` from; `True` to `False`.; """"""; # noqa: avoid PEP8 F841; """"""; Proxy object computes and returns the right output based on the; function call.; """"""; """"""Check 'GetValue' instance method in Proxy.""""""; """"""; Test backend to verify the working of 'GetValue' instance method; in Proxy.; """"""; """"""; Test implementation of the execute method; for 'TestBackend'. This records the head; node of the input DistRDF graph from the; generator object.; """"""; """"""do nothing""""""; """"""Dummy make_dataframe""""""; """"""; Test case to check the working of 'GetValue'; method in Proxy when the current action node; already houses a value.; """"""; """"""The HeadNode stores an internal RDataFrame for certain information""""""; """"""Create a dummy file to use for the RDataFrame constructor.""""""; """"""; Check newly defined columns are available also locally.; """"""; """"""; Check column type of the column in the dataset.; """"""; """"""; Check column type of the newly defined columns.; """"""",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/test/test_proxy.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/test/test_proxy.py
Integrability,wrap,wraps,"""""""Create dummy head node instance needed in the test""""""; # Pass None as `npartitions`. The tests will modify this member; # according to needs; """"""Proxy abstract class cannot be instantiated.""""""; """"""; Any attempt to instantiate the `Proxy` abstract class results in; a `TypeError`.; """"""; """"""Tests that right types are returned""""""; """"""; NodeProxy object is of type `DistRDF.NodeProxy` and; wraps a node object.; """"""; """"""; ResultPtrProxy object is of type `DistRDF.ResultPtrProxy` and; wraps a node object.; """"""; """"""Test Proxy class methods.""""""; """"""A mock action node result class.""""""; """"""A test method to check function call on the Temp class.""""""; # A simple operation to check; """"""Dummy backend to test the _get_friend_info method in Dist class.""""""; """"""Dummy implementation of ProcessAndMerge.""""""; """"""; Dummy implementation of distribute_files. Does nothing.; """"""; """"""Dummy make_dataframe""""""; """"""ResultPtrProxy object reads the right input attribute.""""""; """"""; NodeProxy object reads the right input attributes,; returning the methods of the proxied node.; """"""; """"""; When a node attribute is called on a NodeProxy object, it; correctly returns the attribute of the proxied node.; """"""; """"""; When a non-defined Node class attribute is called on a; NodeProxy object, it raises an AttributeError.; """"""; """"""; Check that the user reference holds until the proxy lives. When the; Python garbage collector attempts to remove the proxy object, its; `__del__` method switches the node attribute `has_user_references` from; `True` to `False`.; """"""; # noqa: avoid PEP8 F841; """"""; Proxy object computes and returns the right output based on the; function call.; """"""; """"""Check 'GetValue' instance method in Proxy.""""""; """"""; Test backend to verify the working of 'GetValue' instance method; in Proxy.; """"""; """"""; Test implementation of the execute method; for 'TestBackend'. This records the head; node of the input DistRDF graph from the; generator object.; """"""; """"""do nothing""""""; """"""Dummy make_dataframe""""""; """"""; Tes",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/test/test_proxy.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/test/test_proxy.py
Safety,avoid,avoid," is of type `DistRDF.ResultPtrProxy` and; wraps a node object.; """"""; """"""Test Proxy class methods.""""""; """"""A mock action node result class.""""""; """"""A test method to check function call on the Temp class.""""""; # A simple operation to check; """"""Dummy backend to test the _get_friend_info method in Dist class.""""""; """"""Dummy implementation of ProcessAndMerge.""""""; """"""; Dummy implementation of distribute_files. Does nothing.; """"""; """"""Dummy make_dataframe""""""; """"""ResultPtrProxy object reads the right input attribute.""""""; """"""; NodeProxy object reads the right input attributes,; returning the methods of the proxied node.; """"""; """"""; When a node attribute is called on a NodeProxy object, it; correctly returns the attribute of the proxied node.; """"""; """"""; When a non-defined Node class attribute is called on a; NodeProxy object, it raises an AttributeError.; """"""; """"""; Check that the user reference holds until the proxy lives. When the; Python garbage collector attempts to remove the proxy object, its; `__del__` method switches the node attribute `has_user_references` from; `True` to `False`.; """"""; # noqa: avoid PEP8 F841; """"""; Proxy object computes and returns the right output based on the; function call.; """"""; """"""Check 'GetValue' instance method in Proxy.""""""; """"""; Test backend to verify the working of 'GetValue' instance method; in Proxy.; """"""; """"""; Test implementation of the execute method; for 'TestBackend'. This records the head; node of the input DistRDF graph from the; generator object.; """"""; """"""do nothing""""""; """"""Dummy make_dataframe""""""; """"""; Test case to check the working of 'GetValue'; method in Proxy when the current action node; already houses a value.; """"""; """"""The HeadNode stores an internal RDataFrame for certain information""""""; """"""Create a dummy file to use for the RDataFrame constructor.""""""; """"""; Check newly defined columns are available also locally.; """"""; """"""; Check column type of the column in the dataset.; """"""; """"""; Check column type of the newly defined columns.; """"""",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/test/test_proxy.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/test/test_proxy.py
Testability,test,test,"""""""Create dummy head node instance needed in the test""""""; # Pass None as `npartitions`. The tests will modify this member; # according to needs; """"""Proxy abstract class cannot be instantiated.""""""; """"""; Any attempt to instantiate the `Proxy` abstract class results in; a `TypeError`.; """"""; """"""Tests that right types are returned""""""; """"""; NodeProxy object is of type `DistRDF.NodeProxy` and; wraps a node object.; """"""; """"""; ResultPtrProxy object is of type `DistRDF.ResultPtrProxy` and; wraps a node object.; """"""; """"""Test Proxy class methods.""""""; """"""A mock action node result class.""""""; """"""A test method to check function call on the Temp class.""""""; # A simple operation to check; """"""Dummy backend to test the _get_friend_info method in Dist class.""""""; """"""Dummy implementation of ProcessAndMerge.""""""; """"""; Dummy implementation of distribute_files. Does nothing.; """"""; """"""Dummy make_dataframe""""""; """"""ResultPtrProxy object reads the right input attribute.""""""; """"""; NodeProxy object reads the right input attributes,; returning the methods of the proxied node.; """"""; """"""; When a node attribute is called on a NodeProxy object, it; correctly returns the attribute of the proxied node.; """"""; """"""; When a non-defined Node class attribute is called on a; NodeProxy object, it raises an AttributeError.; """"""; """"""; Check that the user reference holds until the proxy lives. When the; Python garbage collector attempts to remove the proxy object, its; `__del__` method switches the node attribute `has_user_references` from; `True` to `False`.; """"""; # noqa: avoid PEP8 F841; """"""; Proxy object computes and returns the right output based on the; function call.; """"""; """"""Check 'GetValue' instance method in Proxy.""""""; """"""; Test backend to verify the working of 'GetValue' instance method; in Proxy.; """"""; """"""; Test implementation of the execute method; for 'TestBackend'. This records the head; node of the input DistRDF graph from the; generator object.; """"""; """"""do nothing""""""; """"""Dummy make_dataframe""""""; """"""; Tes",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/test/test_proxy.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/test/test_proxy.py
Usability,simpl,simple,"""""""Create dummy head node instance needed in the test""""""; # Pass None as `npartitions`. The tests will modify this member; # according to needs; """"""Proxy abstract class cannot be instantiated.""""""; """"""; Any attempt to instantiate the `Proxy` abstract class results in; a `TypeError`.; """"""; """"""Tests that right types are returned""""""; """"""; NodeProxy object is of type `DistRDF.NodeProxy` and; wraps a node object.; """"""; """"""; ResultPtrProxy object is of type `DistRDF.ResultPtrProxy` and; wraps a node object.; """"""; """"""Test Proxy class methods.""""""; """"""A mock action node result class.""""""; """"""A test method to check function call on the Temp class.""""""; # A simple operation to check; """"""Dummy backend to test the _get_friend_info method in Dist class.""""""; """"""Dummy implementation of ProcessAndMerge.""""""; """"""; Dummy implementation of distribute_files. Does nothing.; """"""; """"""Dummy make_dataframe""""""; """"""ResultPtrProxy object reads the right input attribute.""""""; """"""; NodeProxy object reads the right input attributes,; returning the methods of the proxied node.; """"""; """"""; When a node attribute is called on a NodeProxy object, it; correctly returns the attribute of the proxied node.; """"""; """"""; When a non-defined Node class attribute is called on a; NodeProxy object, it raises an AttributeError.; """"""; """"""; Check that the user reference holds until the proxy lives. When the; Python garbage collector attempts to remove the proxy object, its; `__del__` method switches the node attribute `has_user_references` from; `True` to `False`.; """"""; # noqa: avoid PEP8 F841; """"""; Proxy object computes and returns the right output based on the; function call.; """"""; """"""Check 'GetValue' instance method in Proxy.""""""; """"""; Test backend to verify the working of 'GetValue' instance method; in Proxy.; """"""; """"""; Test implementation of the execute method; for 'TestBackend'. This records the head; node of the input DistRDF graph from the; generator object.; """"""; """"""do nothing""""""; """"""Dummy make_dataframe""""""; """"""; Tes",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/test/test_proxy.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/test/test_proxy.py
Testability,test,tests,"""""""Convert EmptySourceRange objects to tuples with the shape (start, end)""""""; """"""Convert TreeRange objects to tuples with the shape (start, end, filenames).""""""; """"""; Test cases with ranges when there is an empty data source.; """"""; """"""; Building balanced ranges when the number of entries is a multiple of the; number of partitions.; """"""; # First case; # Second case; """"""; Building balanced ranges when the number of entries is not a multiple of; the number of partitions.; """"""; # Example in which fractional part of; # (nentries/npartitions) >= 0.5; # Example in which fractional part of; # (nentries/npartitions) < 0.5; # Required output pairs; """"""; Building balanced ranges when the number of entries is smaller than the; number of partitions.; """"""; """"""; Check that _build_ranges produces balanced ranges when there are no; clusters involved.; """"""; """"""; Test cases with ranges when the data source is a TTree.; """"""; """"""; Create some files to be used in the tests. Each file has 100 entries and; 10 clusters.; """"""; """"""Destroy the previously created files.""""""; """"""; Exactly one range is created when user asks for one partition. The range; spans the whole input file.; """"""; # This tree has 10 entries and 1 cluster; """"""; Asking for 2 partitions with an input file that contains only 1 cluster; returns a list with two tasks. One spans the whole file, the other is; None.; """"""; # This tree has 10 entries and 1 cluster; # We return one task per partition; # But only one is non-empty; """"""; Create clustered ranges respecting the cluster boundaries, even if that; implies to have ranges with different numbers of entries.; """"""; """"""; Check globbing returns the proper file name to create ranges.; """"""; """"""; Check proper handling of a TChain with different subnames.; """"""; # Create two dummy files; """"""; When the cluster boundaries allow it, create ranges as equal as possible; in terms of how many entries they span.; """"""; """"""; Create ranges that spany many clusters.; """"""; """"""; Create as many partitions",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/test/test_ranges.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/test/test_ranges.py
Integrability,depend,depend,"""""""Dist abstract class cannot be instantiated.""""""; """"""; Any attempt to instantiate the `Dist` abstract class results in; a `TypeError`. """"""; """"""; Creation of a subclass without implementing `processAndMerge`; method throws a `TypeError`. """"""; """"""; The result of distributed execution should not depend on the number of; partitions assigned to the dataframe.; """"""; """"""Dummy backend to test the _build_ranges method in Dist class.""""""; """"""; Dummy implementation of ProcessAndMerge.; """"""; """"""; Dummy implementation of distribute_unique_paths. Does nothing.; """"""; """"""Dummy make_dataframe""""""; """"""; Tests that counting the entries in the dataset does not depend on the; number of partitions. This could have happened if we used TEntryList; to restrict processing on a certain range of entries of the TChain in a; distributed task, but the changes in; https://github.com/root-project/root/commit/77bd5aa82e9544811e0d5fce197ab87c739c2e23; were not implemented yet.; """"""",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/test/backend/test_dist.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/test/backend/test_dist.py
Testability,test,test,"""""""Dist abstract class cannot be instantiated.""""""; """"""; Any attempt to instantiate the `Dist` abstract class results in; a `TypeError`. """"""; """"""; Creation of a subclass without implementing `processAndMerge`; method throws a `TypeError`. """"""; """"""; The result of distributed execution should not depend on the number of; partitions assigned to the dataframe.; """"""; """"""Dummy backend to test the _build_ranges method in Dist class.""""""; """"""; Dummy implementation of ProcessAndMerge.; """"""; """"""; Dummy implementation of distribute_unique_paths. Does nothing.; """"""; """"""Dummy make_dataframe""""""; """"""; Tests that counting the entries in the dataset does not depend on the; number of partitions. This could have happened if we used TEntryList; to restrict processing on a certain range of entries of the TChain in a; distributed task, but the changes in; https://github.com/root-project/root/commit/77bd5aa82e9544811e0d5fce197ab87c739c2e23; were not implemented yet.; """"""",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/test/backend/test_dist.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/test/backend/test_dist.py
Performance,cache,caches,"""""""; Different tasks run in the same process should not need to re-JIT the; computation graph.; """"""; """"""This implementation is needed for the Snapshot tests.""""""; """"""Clear the caches in between tests.""""""; """"""The cache is used to count entries with an empty source.""""""; # Start from a fresh cache at each subtest iteration; # The count operation should always return the correct value; # There should be exactly one cached RDF and set of actions; # The RDataFrame should have run as many times as partitions; """"""The cache is used to count entries of a TTree.""""""; # The maximum number of partitions is the number of clusters; # Start from a fresh cache at each subtest iteration; # The count operation should always return the correct value; # There should be exactly one cached RDF and set of actions; # The RDataFrame should have run as many times as partitions; """"""The cache is used to count entries of a TChain.""""""; # The maximum number of partitions is the number of clusters; # Start from a fresh cache at each subtest iteration; # The count operation should always return the correct value; # There should be exactly one cached RDF and set of actions; # The RDataFrame should have run as many times as partitions; """"""The cache is used to Snapshot data.""""""; # Start from a fresh cache at each subtest iteration; # There should be exactly one cached RDF and set of actions; # The RDataFrame should have run as many times as partitions; # All the correct output files should be present; # The snapshotted dataframe should be usable; # Remove output files at each iteration; """"""The caches are used with multiple executions.""""""; # Start from a fresh cache at each subtest iteration",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/test/backend/test_graph_caching.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/test/backend/test_graph_caching.py
Testability,test,tests,"""""""; Different tasks run in the same process should not need to re-JIT the; computation graph.; """"""; """"""This implementation is needed for the Snapshot tests.""""""; """"""Clear the caches in between tests.""""""; """"""The cache is used to count entries with an empty source.""""""; # Start from a fresh cache at each subtest iteration; # The count operation should always return the correct value; # There should be exactly one cached RDF and set of actions; # The RDataFrame should have run as many times as partitions; """"""The cache is used to count entries of a TTree.""""""; # The maximum number of partitions is the number of clusters; # Start from a fresh cache at each subtest iteration; # The count operation should always return the correct value; # There should be exactly one cached RDF and set of actions; # The RDataFrame should have run as many times as partitions; """"""The cache is used to count entries of a TChain.""""""; # The maximum number of partitions is the number of clusters; # Start from a fresh cache at each subtest iteration; # The count operation should always return the correct value; # There should be exactly one cached RDF and set of actions; # The RDataFrame should have run as many times as partitions; """"""The cache is used to Snapshot data.""""""; # Start from a fresh cache at each subtest iteration; # There should be exactly one cached RDF and set of actions; # The RDataFrame should have run as many times as partitions; # All the correct output files should be present; # The snapshotted dataframe should be usable; # Remove output files at each iteration; """"""The caches are used with multiple executions.""""""; # Start from a fresh cache at each subtest iteration",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/test/backend/test_graph_caching.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/test/backend/test_graph_caching.py
Usability,usab,usable,"""""""; Different tasks run in the same process should not need to re-JIT the; computation graph.; """"""; """"""This implementation is needed for the Snapshot tests.""""""; """"""Clear the caches in between tests.""""""; """"""The cache is used to count entries with an empty source.""""""; # Start from a fresh cache at each subtest iteration; # The count operation should always return the correct value; # There should be exactly one cached RDF and set of actions; # The RDataFrame should have run as many times as partitions; """"""The cache is used to count entries of a TTree.""""""; # The maximum number of partitions is the number of clusters; # Start from a fresh cache at each subtest iteration; # The count operation should always return the correct value; # There should be exactly one cached RDF and set of actions; # The RDataFrame should have run as many times as partitions; """"""The cache is used to count entries of a TChain.""""""; # The maximum number of partitions is the number of clusters; # Start from a fresh cache at each subtest iteration; # The count operation should always return the correct value; # There should be exactly one cached RDF and set of actions; # The RDataFrame should have run as many times as partitions; """"""The cache is used to Snapshot data.""""""; # Start from a fresh cache at each subtest iteration; # There should be exactly one cached RDF and set of actions; # The RDataFrame should have run as many times as partitions; # All the correct output files should be present; # The snapshotted dataframe should be usable; # Remove output files at each iteration; """"""The caches are used with multiple executions.""""""; # Start from a fresh cache at each subtest iteration",MatchSource.CODE_COMMENT,bindings/experimental/distrdf/test/backend/test_graph_caching.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/experimental/distrdf/test/backend/test_graph_caching.py
Integrability,wrap,wrapper,"# -*- coding:utf-8 -*-; #-----------------------------------------------------------------------------; # Author: Danilo Piparo <Danilo.Piparo@cern.ch> CERN; # Author: Enric Tejedor <enric.tejedor.saavedra@cern.ch> CERN; #-----------------------------------------------------------------------------; ################################################################################; # Copyright (C) 1995-2020, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Jit a wrapper for the ttabcom; """"""; std::vector<std::string> _TTabComHook(const char* pattern){; static auto ttc = new TTabCom;; const size_t lineBufSize = 2*1024; // must be equal to/larger than BUF_SIZE in TTabCom.cxx; std::unique_ptr<char[]> completed(new char[lineBufSize]);; strncpy(completed.get(), pattern, lineBufSize);; completed[lineBufSize-1] = '\\0';; int pLoc = strlen(completed.get());; std::ostringstream oss;; Int_t firstChange = ttc->Hook(completed.get(), &pLoc, oss);; if (firstChange == -2) { // got some completions in oss; auto completions = oss.str();; vector<string> completions_v;; istringstream f(completions);; string s;; while (getline(f, s, '\\n')) {; completions_v.push_back(s);; }; return completions_v;; }; if (firstChange == -1) { // found no completions; return vector<string>();; }; // found exactly one completion; return vector<string>(1, completed.get());; }; """"""; '''; Completer which interfaces to the TTabCom of ROOT. It is activated; (deactivated) upon the load(unload) of the load of the extension. >>> comp = CppCompleter(); >>> comp.activate(); >>> for suggestion in comp._completeImpl(""TTreeF""):; ... print(suggestion); TTreeFormula; TTreeFormulaManager; TTreeFriendLeafIter; >>> garbage = ROOT.gInterpreter.ProcessLine(""TH1F* h""); >>> for suggestion in comp._completeImpl(""h->GetA"")",MatchSource.CODE_COMMENT,bindings/jupyroot/python/JupyROOT/helpers/cppcompleter.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/jupyroot/python/JupyROOT/helpers/cppcompleter.py
Modifiability,variab,variable,"uggestion); TTreeFormula; TTreeFormulaManager; TTreeFriendLeafIter; >>> garbage = ROOT.gInterpreter.ProcessLine(""TH1F* h""); >>> for suggestion in comp._completeImpl(""h->GetA""):; ... print(suggestion); h->GetArray; h->GetAsymmetry; h->GetAt; h->GetAxisColor; >>> garbage = ROOT.gInterpreter.ProcessLine(""TH1F aa""); >>> for suggestion in comp._completeImpl(""aa.Add(""):; ... print(suggestion.replace(""\\t"","" "")); <BLANKLINE>; Bool_t Add(TF1* h1, Double_t c1 = 1, Option_t* option = """"); Bool_t Add(const TH1* h, const TH1* h2, Double_t c1 = 1, Double_t c2 = 1) // *MENU*; Bool_t Add(const TH1* h1, Double_t c1 = 1); >>> for suggestion in comp._completeImpl(""TROOT::Is""):; ... print(suggestion); TROOT::IsA; TROOT::IsBatch; TROOT::IsBuilt; TROOT::IsDestructed; TROOT::IsEqual; TROOT::IsEscaped; TROOT::IsExecutingMacro; TROOT::IsFolder; TROOT::IsInterrupted; TROOT::IsLineProcessing; TROOT::IsModified; TROOT::IsOnHeap; TROOT::IsProofServ; TROOT::IsRootFile; TROOT::IsSortable; TROOT::IsWebDisplay; TROOT::IsWebDisplayBatch; TROOT::IsWritable; TROOT::IsZombie; >>> comp.deactivate(); >>> for suggestion in comp._completeImpl(""TG""):; ... print(suggestion); '''; # Remove combinations of opening and closing brackets and just opening; # brackets at the end of a line. Jupyter seems to expect functions; # without these brackets to work properly. The brackets of'operator()'; # must not be removed; # If a function signature is encountered, add an empty item to the; # suggestions. Try to guess a function signature by an opening bracket; # ignoring 'operator()'.; # Prepend variable name to suggestions. Do not prepend if the; # suggestion already contains the variable name, this can happen if; # e.g. there is only one valid completion; '''; Autocomplete interfacing to TTabCom. If an accessor of a scope is; present in the line, the suggestions are prepended with the line.; That's how completers work. For example:; myGraph.Set<tab> will return ""myGraph.Set+suggestion in the list of; suggestions.; '''",MatchSource.CODE_COMMENT,bindings/jupyroot/python/JupyROOT/helpers/cppcompleter.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/jupyroot/python/JupyROOT/helpers/cppcompleter.py
Performance,load,load,"# Jit a wrapper for the ttabcom; """"""; std::vector<std::string> _TTabComHook(const char* pattern){; static auto ttc = new TTabCom;; const size_t lineBufSize = 2*1024; // must be equal to/larger than BUF_SIZE in TTabCom.cxx; std::unique_ptr<char[]> completed(new char[lineBufSize]);; strncpy(completed.get(), pattern, lineBufSize);; completed[lineBufSize-1] = '\\0';; int pLoc = strlen(completed.get());; std::ostringstream oss;; Int_t firstChange = ttc->Hook(completed.get(), &pLoc, oss);; if (firstChange == -2) { // got some completions in oss; auto completions = oss.str();; vector<string> completions_v;; istringstream f(completions);; string s;; while (getline(f, s, '\\n')) {; completions_v.push_back(s);; }; return completions_v;; }; if (firstChange == -1) { // found no completions; return vector<string>();; }; // found exactly one completion; return vector<string>(1, completed.get());; }; """"""; '''; Completer which interfaces to the TTabCom of ROOT. It is activated; (deactivated) upon the load(unload) of the load of the extension. >>> comp = CppCompleter(); >>> comp.activate(); >>> for suggestion in comp._completeImpl(""TTreeF""):; ... print(suggestion); TTreeFormula; TTreeFormulaManager; TTreeFriendLeafIter; >>> garbage = ROOT.gInterpreter.ProcessLine(""TH1F* h""); >>> for suggestion in comp._completeImpl(""h->GetA""):; ... print(suggestion); h->GetArray; h->GetAsymmetry; h->GetAt; h->GetAxisColor; >>> garbage = ROOT.gInterpreter.ProcessLine(""TH1F aa""); >>> for suggestion in comp._completeImpl(""aa.Add(""):; ... print(suggestion.replace(""\\t"","" "")); <BLANKLINE>; Bool_t Add(TF1* h1, Double_t c1 = 1, Option_t* option = """"); Bool_t Add(const TH1* h, const TH1* h2, Double_t c1 = 1, Double_t c2 = 1) // *MENU*; Bool_t Add(const TH1* h1, Double_t c1 = 1); >>> for suggestion in comp._completeImpl(""TROOT::Is""):; ... print(suggestion); TROOT::IsA; TROOT::IsBatch; TROOT::IsBuilt; TROOT::IsDestructed; TROOT::IsEqual; TROOT::IsEscaped; TROOT::IsExecutingMacro; TROOT::IsFolder; TROOT::IsInte",MatchSource.CODE_COMMENT,bindings/jupyroot/python/JupyROOT/helpers/cppcompleter.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/jupyroot/python/JupyROOT/helpers/cppcompleter.py
Security,access,accessor,"uggestion); TTreeFormula; TTreeFormulaManager; TTreeFriendLeafIter; >>> garbage = ROOT.gInterpreter.ProcessLine(""TH1F* h""); >>> for suggestion in comp._completeImpl(""h->GetA""):; ... print(suggestion); h->GetArray; h->GetAsymmetry; h->GetAt; h->GetAxisColor; >>> garbage = ROOT.gInterpreter.ProcessLine(""TH1F aa""); >>> for suggestion in comp._completeImpl(""aa.Add(""):; ... print(suggestion.replace(""\\t"","" "")); <BLANKLINE>; Bool_t Add(TF1* h1, Double_t c1 = 1, Option_t* option = """"); Bool_t Add(const TH1* h, const TH1* h2, Double_t c1 = 1, Double_t c2 = 1) // *MENU*; Bool_t Add(const TH1* h1, Double_t c1 = 1); >>> for suggestion in comp._completeImpl(""TROOT::Is""):; ... print(suggestion); TROOT::IsA; TROOT::IsBatch; TROOT::IsBuilt; TROOT::IsDestructed; TROOT::IsEqual; TROOT::IsEscaped; TROOT::IsExecutingMacro; TROOT::IsFolder; TROOT::IsInterrupted; TROOT::IsLineProcessing; TROOT::IsModified; TROOT::IsOnHeap; TROOT::IsProofServ; TROOT::IsRootFile; TROOT::IsSortable; TROOT::IsWebDisplay; TROOT::IsWebDisplayBatch; TROOT::IsWritable; TROOT::IsZombie; >>> comp.deactivate(); >>> for suggestion in comp._completeImpl(""TG""):; ... print(suggestion); '''; # Remove combinations of opening and closing brackets and just opening; # brackets at the end of a line. Jupyter seems to expect functions; # without these brackets to work properly. The brackets of'operator()'; # must not be removed; # If a function signature is encountered, add an empty item to the; # suggestions. Try to guess a function signature by an opening bracket; # ignoring 'operator()'.; # Prepend variable name to suggestions. Do not prepend if the; # suggestion already contains the variable name, this can happen if; # e.g. there is only one valid completion; '''; Autocomplete interfacing to TTabCom. If an accessor of a scope is; present in the line, the suggestions are prepended with the line.; That's how completers work. For example:; myGraph.Set<tab> will return ""myGraph.Set+suggestion in the list of; suggestions.; '''",MatchSource.CODE_COMMENT,bindings/jupyroot/python/JupyROOT/helpers/cppcompleter.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/jupyroot/python/JupyROOT/helpers/cppcompleter.py
Availability,error,error,".appendChild(script);; }}. if (typeof requirejs !== 'undefined') {{. // We are in jupyter notebooks, use require.js which should be configured already; requirejs.config({{; paths: {{ 'JSRootCore' : [ 'build/jsroot', 'https://root.cern/js/7.7.4/build/jsroot', 'https://jsroot.gsi.de/7.7.4/build/jsroot' ] }}; }})(['JSRootCore'], function(Core) {{; display_{jsDivId}(Core);; }});. }} else if (typeof JSROOT !== 'undefined') {{. // JSROOT already loaded, just use it; display_{jsDivId}(JSROOT);. }} else {{. // We are in jupyterlab without require.js, directly loading jsroot; // Jupyterlab might be installed in a different base_url so we need to know it.; try {{; var base_url = JSON.parse(document.getElementById('jupyter-config-data').innerHTML).baseUrl;; }} catch(_) {{; var base_url = '/';; }}. // Try loading a local version of requirejs and fallback to cdn if not possible.; script_load_{jsDivId}(base_url + 'static/build/jsroot.js', function(){{; console.error('Fail to load JSROOT locally, please check your jupyter_notebook_config.py file');; script_load_{jsDivId}('https://root.cern/js/7.7.4/build/jsroot.js', function(){{; document.getElementById(""{jsDivId}"").innerHTML = ""Failed to load JSROOT"";; }});; }});; }}. </script>; """"""; '''Return appropriate file extension for a shared library; >>> _getLibExtension('darwin'); '.dylib'; >>> _getLibExtension('win32'); '.dll'; >>> _getLibExtension('OddPlatform'); '.so'; '''; '''; >>> s=""// hello""; >>> commentRemover(s); ''; >>> s=""int /** Test **/ main() {return 0;}""; >>> commentRemover(s); 'int main() {return 0;}'; '''; # Return a string containing only the newline chars contained in strIn; # Matched string is //...EOL or /*...*/ ==> Blot out all non-newline chars; # Matched string is '...' or ""..."" ==> Keep unchanged; # Here functions are defined to process C++ code; #code = commentRemover(code); #code = commentRemover(code); '''FIXME!; This function is a workaround. On osx, it is impossible to link against; libzmq.so, among the othe",MatchSource.CODE_COMMENT,bindings/jupyroot/python/JupyROOT/helpers/utils.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/jupyroot/python/JupyROOT/helpers/utils.py
Deployability,update,update,"# -*- coding:utf-8 -*-; #-----------------------------------------------------------------------------; # Author: Danilo Piparo <Danilo.Piparo@cern.ch> CERN; #-----------------------------------------------------------------------------; ################################################################################; # Copyright (C) 1995-2020, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # We want iPython to take over the graphics; # Keep display handle for canvases to be able update them; """"""; Jupyter.CodeCell.options_default.highlight_modes['magic_{cppMIME}'] = {{'reg':[/^%%cpp/]}};; console.log(""JupyROOT - %%cpp magic configured"");; """"""; """""". <div id=""{jsDivId}"" style=""width: {jsCanvasWidth}px; height: {jsCanvasHeight}px; position: relative"">; </div>. <script>. function display_{jsDivId}(Core) {{; let obj = Core.parse({jsonContent});; Core.settings.HandleKeys = false;; Core.draw(""{jsDivId}"", obj, ""{jsDrawOptions}"");; }}. function script_load_{jsDivId}(src, on_error) {{; let script = document.createElement('script');; script.src = src;; script.onload = function() {{ display_{jsDivId}(JSROOT); }};; script.onerror = function() {{ script.remove(); on_error(); }};; document.head.appendChild(script);; }}. if (typeof requirejs !== 'undefined') {{. // We are in jupyter notebooks, use require.js which should be configured already; requirejs.config({{; paths: {{ 'JSRootCore' : [ 'build/jsroot', 'https://root.cern/js/7.7.4/build/jsroot', 'https://jsroot.gsi.de/7.7.4/build/jsroot' ] }}; }})(['JSRootCore'], function(Core) {{; display_{jsDivId}(Core);; }});. }} else if (typeof JSROOT !== 'undefined') {{. // JSROOT already loaded, just use it; display_{jsDivId}(JSROOT);. }} else {{. // We are in jupyterlab without require.js, directly loading jsroot; // Jupyterlab m",MatchSource.CODE_COMMENT,bindings/jupyroot/python/JupyROOT/helpers/utils.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/jupyroot/python/JupyROOT/helpers/utils.py
Modifiability,config,configured,"# -*- coding:utf-8 -*-; #-----------------------------------------------------------------------------; # Author: Danilo Piparo <Danilo.Piparo@cern.ch> CERN; #-----------------------------------------------------------------------------; ################################################################################; # Copyright (C) 1995-2020, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # We want iPython to take over the graphics; # Keep display handle for canvases to be able update them; """"""; Jupyter.CodeCell.options_default.highlight_modes['magic_{cppMIME}'] = {{'reg':[/^%%cpp/]}};; console.log(""JupyROOT - %%cpp magic configured"");; """"""; """""". <div id=""{jsDivId}"" style=""width: {jsCanvasWidth}px; height: {jsCanvasHeight}px; position: relative"">; </div>. <script>. function display_{jsDivId}(Core) {{; let obj = Core.parse({jsonContent});; Core.settings.HandleKeys = false;; Core.draw(""{jsDivId}"", obj, ""{jsDrawOptions}"");; }}. function script_load_{jsDivId}(src, on_error) {{; let script = document.createElement('script');; script.src = src;; script.onload = function() {{ display_{jsDivId}(JSROOT); }};; script.onerror = function() {{ script.remove(); on_error(); }};; document.head.appendChild(script);; }}. if (typeof requirejs !== 'undefined') {{. // We are in jupyter notebooks, use require.js which should be configured already; requirejs.config({{; paths: {{ 'JSRootCore' : [ 'build/jsroot', 'https://root.cern/js/7.7.4/build/jsroot', 'https://jsroot.gsi.de/7.7.4/build/jsroot' ] }}; }})(['JSRootCore'], function(Core) {{; display_{jsDivId}(Core);; }});. }} else if (typeof JSROOT !== 'undefined') {{. // JSROOT already loaded, just use it; display_{jsDivId}(JSROOT);. }} else {{. // We are in jupyterlab without require.js, directly loading jsroot; // Jupyterlab m",MatchSource.CODE_COMMENT,bindings/jupyroot/python/JupyROOT/helpers/utils.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/jupyroot/python/JupyROOT/helpers/utils.py
Performance,load,loaded," <div id=""{jsDivId}"" style=""width: {jsCanvasWidth}px; height: {jsCanvasHeight}px; position: relative"">; </div>. <script>. function display_{jsDivId}(Core) {{; let obj = Core.parse({jsonContent});; Core.settings.HandleKeys = false;; Core.draw(""{jsDivId}"", obj, ""{jsDrawOptions}"");; }}. function script_load_{jsDivId}(src, on_error) {{; let script = document.createElement('script');; script.src = src;; script.onload = function() {{ display_{jsDivId}(JSROOT); }};; script.onerror = function() {{ script.remove(); on_error(); }};; document.head.appendChild(script);; }}. if (typeof requirejs !== 'undefined') {{. // We are in jupyter notebooks, use require.js which should be configured already; requirejs.config({{; paths: {{ 'JSRootCore' : [ 'build/jsroot', 'https://root.cern/js/7.7.4/build/jsroot', 'https://jsroot.gsi.de/7.7.4/build/jsroot' ] }}; }})(['JSRootCore'], function(Core) {{; display_{jsDivId}(Core);; }});. }} else if (typeof JSROOT !== 'undefined') {{. // JSROOT already loaded, just use it; display_{jsDivId}(JSROOT);. }} else {{. // We are in jupyterlab without require.js, directly loading jsroot; // Jupyterlab might be installed in a different base_url so we need to know it.; try {{; var base_url = JSON.parse(document.getElementById('jupyter-config-data').innerHTML).baseUrl;; }} catch(_) {{; var base_url = '/';; }}. // Try loading a local version of requirejs and fallback to cdn if not possible.; script_load_{jsDivId}(base_url + 'static/build/jsroot.js', function(){{; console.error('Fail to load JSROOT locally, please check your jupyter_notebook_config.py file');; script_load_{jsDivId}('https://root.cern/js/7.7.4/build/jsroot.js', function(){{; document.getElementById(""{jsDivId}"").innerHTML = ""Failed to load JSROOT"";; }});; }});; }}. </script>; """"""; '''Return appropriate file extension for a shared library; >>> _getLibExtension('darwin'); '.dylib'; >>> _getLibExtension('win32'); '.dll'; >>> _getLibExtension('OddPlatform'); '.so'; '''; '''; >>> s=""// hello""; >>> com",MatchSource.CODE_COMMENT,bindings/jupyroot/python/JupyROOT/helpers/utils.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/jupyroot/python/JupyROOT/helpers/utils.py
Testability,log,log,"# -*- coding:utf-8 -*-; #-----------------------------------------------------------------------------; # Author: Danilo Piparo <Danilo.Piparo@cern.ch> CERN; #-----------------------------------------------------------------------------; ################################################################################; # Copyright (C) 1995-2020, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # We want iPython to take over the graphics; # Keep display handle for canvases to be able update them; """"""; Jupyter.CodeCell.options_default.highlight_modes['magic_{cppMIME}'] = {{'reg':[/^%%cpp/]}};; console.log(""JupyROOT - %%cpp magic configured"");; """"""; """""". <div id=""{jsDivId}"" style=""width: {jsCanvasWidth}px; height: {jsCanvasHeight}px; position: relative"">; </div>. <script>. function display_{jsDivId}(Core) {{; let obj = Core.parse({jsonContent});; Core.settings.HandleKeys = false;; Core.draw(""{jsDivId}"", obj, ""{jsDrawOptions}"");; }}. function script_load_{jsDivId}(src, on_error) {{; let script = document.createElement('script');; script.src = src;; script.onload = function() {{ display_{jsDivId}(JSROOT); }};; script.onerror = function() {{ script.remove(); on_error(); }};; document.head.appendChild(script);; }}. if (typeof requirejs !== 'undefined') {{. // We are in jupyter notebooks, use require.js which should be configured already; requirejs.config({{; paths: {{ 'JSRootCore' : [ 'build/jsroot', 'https://root.cern/js/7.7.4/build/jsroot', 'https://jsroot.gsi.de/7.7.4/build/jsroot' ] }}; }})(['JSRootCore'], function(Core) {{; display_{jsDivId}(Core);; }});. }} else if (typeof JSROOT !== 'undefined') {{. // JSROOT already loaded, just use it; display_{jsDivId}(JSROOT);. }} else {{. // We are in jupyterlab without require.js, directly loading jsroot; // Jupyterlab m",MatchSource.CODE_COMMENT,bindings/jupyroot/python/JupyROOT/helpers/utils.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/jupyroot/python/JupyROOT/helpers/utils.py
Performance,load,load,"# -*- coding:utf-8 -*-; #-----------------------------------------------------------------------------; # Authors: Omar Zapata <Omar.Zapata@cern.ch> http://oproject.org; # Danilo Piparo <Danilo.Piparo@cern.ch> CERN; # Enric Tejedor enric.tejedor.saavedra@cern.ch> CERN; #-----------------------------------------------------------------------------; ################################################################################; # Copyright (C) 1995-2020, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; '''Class to load JupyROOT Magics'''",MatchSource.CODE_COMMENT,bindings/jupyroot/python/JupyROOT/kernel/utils.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/jupyroot/python/JupyROOT/kernel/utils.py
Availability,error,error,"# -*- coding:utf-8 -*-; #-----------------------------------------------------------------------------; # Authors: Omar Zapata <Omar.Zapata@cern.ch> http://oproject.org; # Danilo Piparo <Danilo.Piparo@cern.ch> CERN; # Enric Tejedor enric.tejedor.saavedra@cern.ch> CERN; #-----------------------------------------------------------------------------; ################################################################################; # Copyright (C) 1995-2020, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; #NOTE:actually JupyROOT is not capturing the error on %%cpp -d if the function is wrong; '''Executes the content of the cell as C++ code.'''; # normal flow",MatchSource.CODE_COMMENT,bindings/jupyroot/python/JupyROOT/kernel/magics/cppmagic.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/jupyroot/python/JupyROOT/kernel/magics/cppmagic.py
Availability,avail,available,"# CPython; # https://packaging.python.org/guides/single-sourcing-package-version/; #; # customized commands; #; # b/c _install is a classobj, not type; # base install; # force build of the .pch underneath the cppyy package if not available yet; # ImportError may occur with wrong pip requirements resolution (unlikely); # AttributeError will occur with (older) PyPy as it relies on older backends; # pre-emptively add allDict.cxx.pch, which may or may not be created; need full; # path to make sure the final relative path is correct; # Author details; # TODO: numba_extensions will load all extensions even if the package; # itself is not otherwise imported, just installed; in the case of cppyy,; # that is currently too heavy (and breaks on conda); #entry_points={; # 'numba_extensions': [; # 'init = cppyy.numba_ext:_init_extension',; # ],; #},",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/setup.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/setup.py
Deployability,install,install,"# CPython; # https://packaging.python.org/guides/single-sourcing-package-version/; #; # customized commands; #; # b/c _install is a classobj, not type; # base install; # force build of the .pch underneath the cppyy package if not available yet; # ImportError may occur with wrong pip requirements resolution (unlikely); # AttributeError will occur with (older) PyPy as it relies on older backends; # pre-emptively add allDict.cxx.pch, which may or may not be created; need full; # path to make sure the final relative path is correct; # Author details; # TODO: numba_extensions will load all extensions even if the package; # itself is not otherwise imported, just installed; in the case of cppyy,; # that is currently too heavy (and breaks on conda); #entry_points={; # 'numba_extensions': [; # 'init = cppyy.numba_ext:_init_extension',; # ],; #},",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/setup.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/setup.py
Performance,load,load,"# CPython; # https://packaging.python.org/guides/single-sourcing-package-version/; #; # customized commands; #; # b/c _install is a classobj, not type; # base install; # force build of the .pch underneath the cppyy package if not available yet; # ImportError may occur with wrong pip requirements resolution (unlikely); # AttributeError will occur with (older) PyPy as it relies on older backends; # pre-emptively add allDict.cxx.pch, which may or may not be created; need full; # path to make sure the final relative path is correct; # Author details; # TODO: numba_extensions will load all extensions even if the package; # itself is not otherwise imported, just installed; in the case of cppyy,; # that is currently too heavy (and breaks on conda); #entry_points={; # 'numba_extensions': [; # 'init = cppyy.numba_ext:_init_extension',; # ],; #},",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/setup.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/setup.py
Usability,guid,guides,"# CPython; # https://packaging.python.org/guides/single-sourcing-package-version/; #; # customized commands; #; # b/c _install is a classobj, not type; # base install; # force build of the .pch underneath the cppyy package if not available yet; # ImportError may occur with wrong pip requirements resolution (unlikely); # AttributeError will occur with (older) PyPy as it relies on older backends; # pre-emptively add allDict.cxx.pch, which may or may not be created; need full; # path to make sure the final relative path is correct; # Author details; # TODO: numba_extensions will load all extensions even if the package; # itself is not otherwise imported, just installed; in the case of cppyy,; # that is currently too heavy (and breaks on conda); #entry_points={; # 'numba_extensions': [; # 'init = cppyy.numba_ext:_init_extension',; # ],; #},",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/setup.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/setup.py
Testability,benchmark,benchmark,"#- group: empty-free ---------------------------------------------------------; """"""; def test_{0}_free_empty_call(benchmark):; benchmark({1}.empty_call); """"""; #- group: empty-inst ---------------------------------------------------------; """"""; def test_{0}_inst_empty_call(benchmark):; inst = {1}.EmptyCall(); benchmark(call_instance_empty, inst); """"""; #- group: builtin-args-free --------------------------------------------------; """"""; def test_{0}_free_take_an_int(benchmark):; benchmark({1}.take_an_int, 1); """"""; """"""; def test_{0}_free_take_a_double(benchmark):; benchmark({1}.take_a_double, 1.); """"""; """"""; def test_{0}_free_take_a_struct(benchmark):; benchmark({1}.take_a_struct, {1}.Value()); """"""; #- group: builtin-args-inst --------------------------------------------------; """"""; def test_{0}_inst_take_an_int(benchmark):; inst = {1}.TakeAValue(); benchmark(call_instance_take_an_int, inst, 1); """"""; """"""; def test_{0}_inst_take_a_double(benchmark):; inst = {1}.TakeAValue(); benchmark(call_instance_take_a_double, inst, 1.); """"""; """"""; def test_{0}_inst_take_a_struct(benchmark):; inst = {1}.TakeAValue(); benchmark(call_instance_take_a_struct, inst, {1}.Value()); """"""; #- group: builtin-args-pass --------------------------------------------------; """"""; def test_{0}_inst_pass_int(benchmark):; inst = {1}.TakeAValue(); benchmark(call_instance_pass_int, inst, 1); """"""; #- group: do_work-free -------------------------------------------------------; """"""; def test_{0}_free_do_work(benchmark):; benchmark({1}.do_work, 1.); """"""; #- group: do_work-inst -------------------------------------------------------; """"""; def test_{0}_inst_do_work(benchmark):; inst = {1}.DoWork(); benchmark(call_instance_do_work, inst); """"""; #- group: overload-inst ------------------------------------------------------; """"""; def test_{0}_inst_overload(benchmark):; inst = {1}.OverloadedCall(); benchmark(call_instance_overload, inst); """"""; #- actual creation of all benches -------------------------------------------",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/bench/bench_functioncalls.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/bench/bench_functioncalls.py
Testability,benchmark,benchmark,"# too slow to run on CPython; #- group: stl-vector ---------------------------------------------------------; """"""; def test_{0}_stl_vector(benchmark):; benchmark(sum, {1}.global_vector); """"""; #- actual creation of all benches --------------------------------------------",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/bench/bench_runvector.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/bench/bench_runvector.py
Availability,avail,available,"oss-reference text.; #add_function_parentheses = True; # If true, the current module name will be prepended to all description; # unit titles (such as .. function::).; #add_module_names = True; # If true, sectionauthor and moduleauthor directives will be shown in the; # output. They are ignored by default.; #show_authors = False; # The name of the Pygments (syntax highlighting) style to use.; #pygments_style = 'sphinx'; # A list of ignored prefixes for module index sorting.; #modindex_common_prefix = []; # If true, keep warnings as ""system message"" paragraphs in the built documents.; #keep_warnings = False; # If true, `todo` and `todoList` produce output, else they produce nothing.; # -- Options for HTML output ----------------------------------------------; # The theme to use for HTML and HTML Help pages. See the documentation for; # a list of builtin themes.; # Theme options are theme-specific and customize the look and feel of a theme; # further. For a list of options available for each theme, see the; # documentation.; #html_theme_options = {}; # Add any paths that contain custom themes here, relative to this directory.; # The name for this set of Sphinx documents. If None, it defaults to; # ""<project> v<release> documentation"".; #html_title = None; # A shorter title for the navigation bar. Default is the same as html_title.; #html_short_title = None; # The name of an image file (relative to this directory) to place at the top; # of the sidebar.; #html_logo = None; # The name of an image file (within the static path) to use as favicon of the; # docs. This file should be a Windows icon file (.ico) being 16x16 or 32x32; # pixels large.; #html_favicon = None; # Add any paths that contain custom static files (such as style sheets) here,; # relative to this directory. They are copied after the builtin static files,; # so a file named ""default.css"" will overwrite the builtin ""default.css"".; # Add any extra paths that contain custom files (such as robots.txt or; # .hta",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/doc/source/conf.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/conf.py
Deployability,configurat,configuration,"# -*- coding: utf-8 -*-; #; # cppyy documentation build configuration file, created by; # sphinx-quickstart on Wed Jul 12 14:35:45 2017.; #; # This file is execfile()d with the current directory set to its; # containing dir.; #; # Note that not all possible configuration values are present in this; # autogenerated file.; #; # All configuration values have a default; values that are commented out; # serve to show the default.; # If extensions (or modules to document with autodoc) are in another directory,; # add these directories to sys.path here. If the directory is relative to the; # documentation root, use os.path.abspath to make it absolute, like shown here.; #sys.path.insert(0, os.path.abspath('.')); # -- General configuration ------------------------------------------------; # If your documentation needs a minimal Sphinx version, state it here.; #needs_sphinx = '1.0'; # Add any Sphinx extension module names here, as strings. They can be; # extensions coming with Sphinx (named 'sphinx.ext.*') or your custom; # ones.; # Add any paths that contain templates here, relative to this directory.; # The suffix(es) of source filenames.; # You can specify multiple suffix as a list of string:; # source_suffix = ['.rst', '.md']; # The encoding of source files.; #source_encoding = 'utf-8-sig'; # The master toctree document.; # General information about the project.; # The version info for the project you're documenting, acts as replacement for; # |version| and |release|, also used in various other places throughout the; # built documents.; #; # The short X.Y version.; # The full version, including alpha/beta/rc tags.; # The language for content autogenerated by Sphinx. Refer to documentation; # for a list of supported languages.; #; # This is also used if you do content translation via gettext catalogs.; # Usually you set ""language"" from the command line for these cases.; # There are two options for replacing |today|: either, you set today to some; # non-false value, then it ",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/doc/source/conf.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/conf.py
Integrability,message,message,"ing |today|: either, you set today to some; # non-false value, then it is used:; #today = ''; # Else, today_fmt is used as the format for a strftime call.; #today_fmt = '%B %d, %Y'; # List of patterns, relative to source directory, that match files and; # directories to ignore when looking for source files.; # The reST default role (used for this markup: `text`) to use for all; # documents.; #default_role = None; # If true, '()' will be appended to :func: etc. cross-reference text.; #add_function_parentheses = True; # If true, the current module name will be prepended to all description; # unit titles (such as .. function::).; #add_module_names = True; # If true, sectionauthor and moduleauthor directives will be shown in the; # output. They are ignored by default.; #show_authors = False; # The name of the Pygments (syntax highlighting) style to use.; #pygments_style = 'sphinx'; # A list of ignored prefixes for module index sorting.; #modindex_common_prefix = []; # If true, keep warnings as ""system message"" paragraphs in the built documents.; #keep_warnings = False; # If true, `todo` and `todoList` produce output, else they produce nothing.; # -- Options for HTML output ----------------------------------------------; # The theme to use for HTML and HTML Help pages. See the documentation for; # a list of builtin themes.; # Theme options are theme-specific and customize the look and feel of a theme; # further. For a list of options available for each theme, see the; # documentation.; #html_theme_options = {}; # Add any paths that contain custom themes here, relative to this directory.; # The name for this set of Sphinx documents. If None, it defaults to; # ""<project> v<release> documentation"".; #html_title = None; # A shorter title for the navigation bar. Default is the same as html_title.; #html_short_title = None; # The name of an image file (relative to this directory) to place at the top; # of the sidebar.; #html_logo = None; # The name of an image file (within the ",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/doc/source/conf.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/conf.py
Modifiability,config,configuration,"# -*- coding: utf-8 -*-; #; # cppyy documentation build configuration file, created by; # sphinx-quickstart on Wed Jul 12 14:35:45 2017.; #; # This file is execfile()d with the current directory set to its; # containing dir.; #; # Note that not all possible configuration values are present in this; # autogenerated file.; #; # All configuration values have a default; values that are commented out; # serve to show the default.; # If extensions (or modules to document with autodoc) are in another directory,; # add these directories to sys.path here. If the directory is relative to the; # documentation root, use os.path.abspath to make it absolute, like shown here.; #sys.path.insert(0, os.path.abspath('.')); # -- General configuration ------------------------------------------------; # If your documentation needs a minimal Sphinx version, state it here.; #needs_sphinx = '1.0'; # Add any Sphinx extension module names here, as strings. They can be; # extensions coming with Sphinx (named 'sphinx.ext.*') or your custom; # ones.; # Add any paths that contain templates here, relative to this directory.; # The suffix(es) of source filenames.; # You can specify multiple suffix as a list of string:; # source_suffix = ['.rst', '.md']; # The encoding of source files.; #source_encoding = 'utf-8-sig'; # The master toctree document.; # General information about the project.; # The version info for the project you're documenting, acts as replacement for; # |version| and |release|, also used in various other places throughout the; # built documents.; #; # The short X.Y version.; # The full version, including alpha/beta/rc tags.; # The language for content autogenerated by Sphinx. Refer to documentation; # for a list of supported languages.; #; # This is also used if you do content translation via gettext catalogs.; # Usually you set ""language"" from the command line for these cases.; # There are two options for replacing |today|: either, you set today to some; # non-false value, then it ",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/doc/source/conf.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/conf.py
Performance,optimiz,optimized,"ors, manual section).; # If true, show URL addresses after external links.; #man_show_urls = False; # -- Options for Texinfo output -------------------------------------------; # Grouping the document tree into Texinfo files. List of tuples; # (source start file, target name, title, author,; # dir menu entry, description, category); # Documents to append as an appendix to all manuals.; #texinfo_appendices = []; # If false, no module index is generated.; #texinfo_domain_indices = True; # How to display URL addresses: 'footnote', 'no', or 'inline'.; #texinfo_show_urls = 'footnote'; # If true, do not generate a @detailmenu in the ""Top"" node's menu.; #texinfo_no_detailmenu = False; # -- Options for Epub output ----------------------------------------------; # Bibliographic Dublin Core info.; # The basename for the epub file. It defaults to the project name.; #epub_basename = project; # The HTML theme for the epub output. Since the default themes are not optimized; # for small screen space, using the same theme for HTML and epub output is; # usually not wise. This defaults to 'epub', a theme designed to save visual; # space.; #epub_theme = 'epub'; # The language of the text. It defaults to the language option; # or 'en' if the language is not set.; #epub_language = ''; # The scheme of the identifier. Typical schemes are ISBN or URL.; #epub_scheme = ''; # The unique identifier of the text. This can be a ISBN number; # or the project homepage.; #epub_identifier = ''; # A unique identification for the text.; #epub_uid = ''; # A tuple containing the cover image and cover page html template filenames.; #epub_cover = (); # A sequence of (type, uri, title) tuples for the guide element of content.opf.; #epub_guide = (); # HTML files that should be inserted before the pages created by sphinx.; # The format is a list of tuples containing the path and title.; #epub_pre_files = []; # HTML files shat should be inserted after the pages created by sphinx.; # The format is a list of tup",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/doc/source/conf.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/conf.py
Usability,guid,guide," = 'footnote'; # If true, do not generate a @detailmenu in the ""Top"" node's menu.; #texinfo_no_detailmenu = False; # -- Options for Epub output ----------------------------------------------; # Bibliographic Dublin Core info.; # The basename for the epub file. It defaults to the project name.; #epub_basename = project; # The HTML theme for the epub output. Since the default themes are not optimized; # for small screen space, using the same theme for HTML and epub output is; # usually not wise. This defaults to 'epub', a theme designed to save visual; # space.; #epub_theme = 'epub'; # The language of the text. It defaults to the language option; # or 'en' if the language is not set.; #epub_language = ''; # The scheme of the identifier. Typical schemes are ISBN or URL.; #epub_scheme = ''; # The unique identifier of the text. This can be a ISBN number; # or the project homepage.; #epub_identifier = ''; # A unique identification for the text.; #epub_uid = ''; # A tuple containing the cover image and cover page html template filenames.; #epub_cover = (); # A sequence of (type, uri, title) tuples for the guide element of content.opf.; #epub_guide = (); # HTML files that should be inserted before the pages created by sphinx.; # The format is a list of tuples containing the path and title.; #epub_pre_files = []; # HTML files shat should be inserted after the pages created by sphinx.; # The format is a list of tuples containing the path and title.; #epub_post_files = []; # A list of files that should not be packed into the epub file.; # The depth of the table of contents in toc.ncx.; #epub_tocdepth = 3; # Allow duplicate toc entries.; #epub_tocdup = True; # Choose between 'default' and 'includehidden'.; # Fix unsupported image types using the Pillow.; #epub_fix_images = False; # Scale large images.; #epub_max_image_width = 0; # How to display URL addresses: 'footnote', 'no', or 'inline'.; #epub_show_urls = 'inline'; # If false, no index is generated.; #epub_use_index = True",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/doc/source/conf.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/conf.py
Deployability,patch,patch,"# monkey patch to be able to select a specific backend based on PyPy's version,; # which is not possible in the pyproject.toml file as there is currently no; # marker for it (this may change, after which this file can be removed); # _BACKEND is the primary, __legacy__ the backwards compatible backend; # fallback as the name __legacy__ is actually documented (and part of __all__); # the following ensures proper build/installation order, after which the normal; # install through setup.py picks up their wheels from the cache (TODO: note the; # duplication here with setup.py; find a better way); # CPython",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/installer/cppyy_monkey_patch.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/installer/cppyy_monkey_patch.py
Performance,cache,cache,"# monkey patch to be able to select a specific backend based on PyPy's version,; # which is not possible in the pyproject.toml file as there is currently no; # marker for it (this may change, after which this file can be removed); # _BACKEND is the primary, __legacy__ the backwards compatible backend; # fallback as the name __legacy__ is actually documented (and part of __all__); # the following ensures proper build/installation order, after which the normal; # install through setup.py picks up their wheels from the cache (TODO: note the; # duplication here with setup.py; find a better way); # CPython",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/installer/cppyy_monkey_patch.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/installer/cppyy_monkey_patch.py
Integrability,wrap,wrapping,"d not be the same. Therefore, the; # struct is split in a series of byte members to get the total size right; # and to allow addressing at the correct offsets.; # return: representation used for return argument.; # declare data members to Numba; # TODO: this refresh is needed b/c the scope type is registered as a; # callable after the tracing started; no idea of the side-effects ...; # create a model description for Numba; # TODO: eventually we need not derive from StructModel; # proxies are always accessed by pointer, which are not composites; # from StructModel; # data: representation used when storing into containers (e.g. arrays).; # TODO ...; # value: representation inside function body. Maybe stored in stack.; # The representation here are flexible.; # the C++ object, b/c through a proxy, is always accessed by pointer; it is represented; # as a pointer to POD to allow indexing by Numba for data member type checking, but the; # address offsetting for loading data member values is independent (see get(), below),; # so the exact layout need not match a POD; # TODO: this doesn't work for real PODs, b/c those are unpacked into their elements and; # passed through registers; # argument: representation used for function argument. Needs to be builtin type,; # but unlike other Numba composites, C++ proxies are not flattened.; # return: representation used for return argument.; # TODO ...; # access to public data members; """"""Get a field at the given position/field name""""""; # TODO: this code exists purely to be able to use the indexing and hierarchy; # of the base class StructModel, which isn't much of a reason; # Python proxy unwrapping for arguments into the Numba trace; # C++ object to Python proxy wrapping for returns from Numba trace; # Required for nopython mode, numba nrt requres each member box call to decref since it steals the reference; #; # C++ instance -> Numba; #; # Pass the val itself to obtain Cling address of the CPPInstance for reference to C++ objects",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/python/cppyy/numba_ext.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/python/cppyy/numba_ext.py
Modifiability,flexible,flexible,"ss given the matched overload; # the function pointer of this overload can not be exactly typed, but; # only the storage size is relevant, so simply use a void*; # TODO: needs to exist for the proper flow, but why? The lowering of the; # actual overload is handled dynamically.; #; # C++ method / data member -> Numba; #; #; # C++ class -> Numba; #; # overrides value in Type; # TODO: the following relies on the fact that numba will first lower the; # field access, then immediately lower the call; and that the `val` loads; # the struct representing the C++ object. Neither need be stable.; # TODO: easier with inttoptr and ptrtoint (cgutils.pointer_add)?; # assume this is a method; """"""Get a field at the given position/field name""""""; # Use the offsets for direct addressing, rather than getting the elements; # from the struct type.; # TODO : Should the address have to be passed here and stored in meminfo; # value: representation inside function body. Maybe stored in stack.; # The representation here are flexible.; # data: representation used when storing into containers (e.g. arrays).; # The struct model relies on data being a POD, but for C++ objects, there; # can be hidden data (e.g. vtable, thunks, or simply private members), and; # the alignment of Cling and Numba also need not be the same. Therefore, the; # struct is split in a series of byte members to get the total size right; # and to allow addressing at the correct offsets.; # return: representation used for return argument.; # declare data members to Numba; # TODO: this refresh is needed b/c the scope type is registered as a; # callable after the tracing started; no idea of the side-effects ...; # create a model description for Numba; # TODO: eventually we need not derive from StructModel; # proxies are always accessed by pointer, which are not composites; # from StructModel; # data: representation used when storing into containers (e.g. arrays).; # TODO ...; # value: representation inside function body. Maybe sto",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/python/cppyy/numba_ext.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/python/cppyy/numba_ext.py
Performance,load,loads," # more likely match candidate; # If the user explicitly passes an argument using numba CPointer, the regex match is used; # to detect the pass by reference since the dispatcher always returns typeref[val*]; # TODO: looks like Numba treats unsigned types as signed when lowering,; # which seems to work as they're just reinterpret_casts; ## TODO should be possible to obtain the vector length from the CPPDataMember val; #; # C++ function pointer -> Numba; #; # by definition; #TODO : Remove the redundancy of __overload__ matching and use this function to only obtain the address given the matched overload; # the function pointer of this overload can not be exactly typed, but; # only the storage size is relevant, so simply use a void*; # TODO: needs to exist for the proper flow, but why? The lowering of the; # actual overload is handled dynamically.; #; # C++ method / data member -> Numba; #; #; # C++ class -> Numba; #; # overrides value in Type; # TODO: the following relies on the fact that numba will first lower the; # field access, then immediately lower the call; and that the `val` loads; # the struct representing the C++ object. Neither need be stable.; # TODO: easier with inttoptr and ptrtoint (cgutils.pointer_add)?; # assume this is a method; """"""Get a field at the given position/field name""""""; # Use the offsets for direct addressing, rather than getting the elements; # from the struct type.; # TODO : Should the address have to be passed here and stored in meminfo; # value: representation inside function body. Maybe stored in stack.; # The representation here are flexible.; # data: representation used when storing into containers (e.g. arrays).; # The struct model relies on data being a POD, but for C++ objects, there; # can be hidden data (e.g. vtable, thunks, or simply private members), and; # the alignment of Cling and Numba also need not be the same. Therefore, the; # struct is split in a series of byte members to get the total size right; # and to allow address",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/python/cppyy/numba_ext.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/python/cppyy/numba_ext.py
Safety,detect,detect,""""""" cppyy extensions for numba; """"""; # setuptools entry point for Numba; # by convention; # for clarity; # special case access to unboxing/boxing APIs; # TODO: distinguish ptr/ref/byval; # TODO: Only metaclasses/proxies end up here since; # ref cases makes the RETURN_TYPE from reflex a string; # prefer ""int"" in the case of intc over ""int32_t""; # Python int; # TODO: this is only necessary until ""best matching"" is in place; # more likely match candidate; # If the user explicitly passes an argument using numba CPointer, the regex match is used; # to detect the pass by reference since the dispatcher always returns typeref[val*]; # TODO: looks like Numba treats unsigned types as signed when lowering,; # which seems to work as they're just reinterpret_casts; ## TODO should be possible to obtain the vector length from the CPPDataMember val; #; # C++ function pointer -> Numba; #; # by definition; #TODO : Remove the redundancy of __overload__ matching and use this function to only obtain the address given the matched overload; # the function pointer of this overload can not be exactly typed, but; # only the storage size is relevant, so simply use a void*; # TODO: needs to exist for the proper flow, but why? The lowering of the; # actual overload is handled dynamically.; #; # C++ method / data member -> Numba; #; #; # C++ class -> Numba; #; # overrides value in Type; # TODO: the following relies on the fact that numba will first lower the; # field access, then immediately lower the call; and that the `val` loads; # the struct representing the C++ object. Neither need be stable.; # TODO: easier with inttoptr and ptrtoint (cgutils.pointer_add)?; # assume this is a method; """"""Get a field at the given position/field name""""""; # Use the offsets for direct addressing, rather than getting the elements; # from the struct type.; # TODO : Should the address have to be passed here and stored in meminfo; # value: representation inside function body. Maybe stored in stack.; # The representa",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/python/cppyy/numba_ext.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/python/cppyy/numba_ext.py
Security,access,access,""""""" cppyy extensions for numba; """"""; # setuptools entry point for Numba; # by convention; # for clarity; # special case access to unboxing/boxing APIs; # TODO: distinguish ptr/ref/byval; # TODO: Only metaclasses/proxies end up here since; # ref cases makes the RETURN_TYPE from reflex a string; # prefer ""int"" in the case of intc over ""int32_t""; # Python int; # TODO: this is only necessary until ""best matching"" is in place; # more likely match candidate; # If the user explicitly passes an argument using numba CPointer, the regex match is used; # to detect the pass by reference since the dispatcher always returns typeref[val*]; # TODO: looks like Numba treats unsigned types as signed when lowering,; # which seems to work as they're just reinterpret_casts; ## TODO should be possible to obtain the vector length from the CPPDataMember val; #; # C++ function pointer -> Numba; #; # by definition; #TODO : Remove the redundancy of __overload__ matching and use this function to only obtain the address given the matched overload; # the function pointer of this overload can not be exactly typed, but; # only the storage size is relevant, so simply use a void*; # TODO: needs to exist for the proper flow, but why? The lowering of the; # actual overload is handled dynamically.; #; # C++ method / data member -> Numba; #; #; # C++ class -> Numba; #; # overrides value in Type; # TODO: the following relies on the fact that numba will first lower the; # field access, then immediately lower the call; and that the `val` loads; # the struct representing the C++ object. Neither need be stable.; # TODO: easier with inttoptr and ptrtoint (cgutils.pointer_add)?; # assume this is a method; """"""Get a field at the given position/field name""""""; # Use the offsets for direct addressing, rather than getting the elements; # from the struct type.; # TODO : Should the address have to be passed here and stored in meminfo; # value: representation inside function body. Maybe stored in stack.; # The representa",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/python/cppyy/numba_ext.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/python/cppyy/numba_ext.py
Usability,simpl,simply,""""""" cppyy extensions for numba; """"""; # setuptools entry point for Numba; # by convention; # for clarity; # special case access to unboxing/boxing APIs; # TODO: distinguish ptr/ref/byval; # TODO: Only metaclasses/proxies end up here since; # ref cases makes the RETURN_TYPE from reflex a string; # prefer ""int"" in the case of intc over ""int32_t""; # Python int; # TODO: this is only necessary until ""best matching"" is in place; # more likely match candidate; # If the user explicitly passes an argument using numba CPointer, the regex match is used; # to detect the pass by reference since the dispatcher always returns typeref[val*]; # TODO: looks like Numba treats unsigned types as signed when lowering,; # which seems to work as they're just reinterpret_casts; ## TODO should be possible to obtain the vector length from the CPPDataMember val; #; # C++ function pointer -> Numba; #; # by definition; #TODO : Remove the redundancy of __overload__ matching and use this function to only obtain the address given the matched overload; # the function pointer of this overload can not be exactly typed, but; # only the storage size is relevant, so simply use a void*; # TODO: needs to exist for the proper flow, but why? The lowering of the; # actual overload is handled dynamically.; #; # C++ method / data member -> Numba; #; #; # C++ class -> Numba; #; # overrides value in Type; # TODO: the following relies on the fact that numba will first lower the; # field access, then immediately lower the call; and that the `val` loads; # the struct representing the C++ object. Neither need be stable.; # TODO: easier with inttoptr and ptrtoint (cgutils.pointer_add)?; # assume this is a method; """"""Get a field at the given position/field name""""""; # Use the offsets for direct addressing, rather than getting the elements; # from the struct type.; # TODO : Should the address have to be passed here and stored in meminfo; # value: representation inside function body. Maybe stored in stack.; # The representa",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/python/cppyy/numba_ext.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/python/cppyy/numba_ext.py
Availability,avail,available,""""""" CPython-specific touch-ups; """"""; # first load the dependency libraries of the backend, then pull in the; # libcppyy extension module; # explicitly expose APIs from libcppyy; # some beautification for inspect (only on p2); # TODO: this reliese on CPPOverload cooking up a func_code object, which atm; # is simply not implemented for p3 :/; # convince inspect that cppyy method proxies are possible drop-ins for python; # methods and classes for pydoc; ### template support ---------------------------------------------------------; # expected/used by ProxyWrappers.cxx in CPyCppyy; # multi-argument to [] becomes a single tuple argument; # if already instantiated, return the existing class; # construct the type name from the types or their string representation; # memoize the class to prevent spurious lookups/re-pythonizations; # special case pythonization (builtin_map is not available from the C-API); # back-pointer for reflection; # for C++17, we're required to derive the type when using initializer syntax; # (i.e. a tuple or list); not sure how to do that in general, but below the; # most common cases are covered; # old 'metaclass-style' template instantiations; #- :: and std:: namespaces ---------------------------------------------------; # for move, we want our ""pythonized"" one, not the C++ template; #- add to the dynamic path as needed -----------------------------------------; # MacOS, Linux; # Windows; # assuming that we are in PREFIX/lib/python/site-packages/cppyy, add PREFIX/lib to the search path; #- exports -------------------------------------------------------------------; # not guaranteed, but common",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/python/cppyy/_cpython_cppyy.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/python/cppyy/_cpython_cppyy.py
Integrability,depend,dependency,""""""" CPython-specific touch-ups; """"""; # first load the dependency libraries of the backend, then pull in the; # libcppyy extension module; # explicitly expose APIs from libcppyy; # some beautification for inspect (only on p2); # TODO: this reliese on CPPOverload cooking up a func_code object, which atm; # is simply not implemented for p3 :/; # convince inspect that cppyy method proxies are possible drop-ins for python; # methods and classes for pydoc; ### template support ---------------------------------------------------------; # expected/used by ProxyWrappers.cxx in CPyCppyy; # multi-argument to [] becomes a single tuple argument; # if already instantiated, return the existing class; # construct the type name from the types or their string representation; # memoize the class to prevent spurious lookups/re-pythonizations; # special case pythonization (builtin_map is not available from the C-API); # back-pointer for reflection; # for C++17, we're required to derive the type when using initializer syntax; # (i.e. a tuple or list); not sure how to do that in general, but below the; # most common cases are covered; # old 'metaclass-style' template instantiations; #- :: and std:: namespaces ---------------------------------------------------; # for move, we want our ""pythonized"" one, not the C++ template; #- add to the dynamic path as needed -----------------------------------------; # MacOS, Linux; # Windows; # assuming that we are in PREFIX/lib/python/site-packages/cppyy, add PREFIX/lib to the search path; #- exports -------------------------------------------------------------------; # not guaranteed, but common",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/python/cppyy/_cpython_cppyy.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/python/cppyy/_cpython_cppyy.py
Performance,load,load,""""""" CPython-specific touch-ups; """"""; # first load the dependency libraries of the backend, then pull in the; # libcppyy extension module; # explicitly expose APIs from libcppyy; # some beautification for inspect (only on p2); # TODO: this reliese on CPPOverload cooking up a func_code object, which atm; # is simply not implemented for p3 :/; # convince inspect that cppyy method proxies are possible drop-ins for python; # methods and classes for pydoc; ### template support ---------------------------------------------------------; # expected/used by ProxyWrappers.cxx in CPyCppyy; # multi-argument to [] becomes a single tuple argument; # if already instantiated, return the existing class; # construct the type name from the types or their string representation; # memoize the class to prevent spurious lookups/re-pythonizations; # special case pythonization (builtin_map is not available from the C-API); # back-pointer for reflection; # for C++17, we're required to derive the type when using initializer syntax; # (i.e. a tuple or list); not sure how to do that in general, but below the; # most common cases are covered; # old 'metaclass-style' template instantiations; #- :: and std:: namespaces ---------------------------------------------------; # for move, we want our ""pythonized"" one, not the C++ template; #- add to the dynamic path as needed -----------------------------------------; # MacOS, Linux; # Windows; # assuming that we are in PREFIX/lib/python/site-packages/cppyy, add PREFIX/lib to the search path; #- exports -------------------------------------------------------------------; # not guaranteed, but common",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/python/cppyy/_cpython_cppyy.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/python/cppyy/_cpython_cppyy.py
Security,expose,expose,""""""" CPython-specific touch-ups; """"""; # first load the dependency libraries of the backend, then pull in the; # libcppyy extension module; # explicitly expose APIs from libcppyy; # some beautification for inspect (only on p2); # TODO: this reliese on CPPOverload cooking up a func_code object, which atm; # is simply not implemented for p3 :/; # convince inspect that cppyy method proxies are possible drop-ins for python; # methods and classes for pydoc; ### template support ---------------------------------------------------------; # expected/used by ProxyWrappers.cxx in CPyCppyy; # multi-argument to [] becomes a single tuple argument; # if already instantiated, return the existing class; # construct the type name from the types or their string representation; # memoize the class to prevent spurious lookups/re-pythonizations; # special case pythonization (builtin_map is not available from the C-API); # back-pointer for reflection; # for C++17, we're required to derive the type when using initializer syntax; # (i.e. a tuple or list); not sure how to do that in general, but below the; # most common cases are covered; # old 'metaclass-style' template instantiations; #- :: and std:: namespaces ---------------------------------------------------; # for move, we want our ""pythonized"" one, not the C++ template; #- add to the dynamic path as needed -----------------------------------------; # MacOS, Linux; # Windows; # assuming that we are in PREFIX/lib/python/site-packages/cppyy, add PREFIX/lib to the search path; #- exports -------------------------------------------------------------------; # not guaranteed, but common",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/python/cppyy/_cpython_cppyy.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/python/cppyy/_cpython_cppyy.py
Usability,simpl,simply,""""""" CPython-specific touch-ups; """"""; # first load the dependency libraries of the backend, then pull in the; # libcppyy extension module; # explicitly expose APIs from libcppyy; # some beautification for inspect (only on p2); # TODO: this reliese on CPPOverload cooking up a func_code object, which atm; # is simply not implemented for p3 :/; # convince inspect that cppyy method proxies are possible drop-ins for python; # methods and classes for pydoc; ### template support ---------------------------------------------------------; # expected/used by ProxyWrappers.cxx in CPyCppyy; # multi-argument to [] becomes a single tuple argument; # if already instantiated, return the existing class; # construct the type name from the types or their string representation; # memoize the class to prevent spurious lookups/re-pythonizations; # special case pythonization (builtin_map is not available from the C-API); # back-pointer for reflection; # for C++17, we're required to derive the type when using initializer syntax; # (i.e. a tuple or list); not sure how to do that in general, but below the; # most common cases are covered; # old 'metaclass-style' template instantiations; #- :: and std:: namespaces ---------------------------------------------------; # for move, we want our ""pythonized"" one, not the C++ template; #- add to the dynamic path as needed -----------------------------------------; # MacOS, Linux; # Windows; # assuming that we are in PREFIX/lib/python/site-packages/cppyy, add PREFIX/lib to the search path; #- exports -------------------------------------------------------------------; # not guaranteed, but common",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/python/cppyy/_cpython_cppyy.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/python/cppyy/_cpython_cppyy.py
Availability,avail,available,""""""" PyPy-specific touch-ups; """"""; # first load the dependency libraries of the backend, then; # pull in the built-in low-level cppyy; # some older versions can be fixed up through a compatibility; # module on the python side; load it, if available; # built-in module; #- exports -------------------------------------------------------------------; # add other exports to all",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/python/cppyy/_pypy_cppyy.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/python/cppyy/_pypy_cppyy.py
Integrability,depend,dependency,""""""" PyPy-specific touch-ups; """"""; # first load the dependency libraries of the backend, then; # pull in the built-in low-level cppyy; # some older versions can be fixed up through a compatibility; # module on the python side; load it, if available; # built-in module; #- exports -------------------------------------------------------------------; # add other exports to all",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/python/cppyy/_pypy_cppyy.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/python/cppyy/_pypy_cppyy.py
Performance,load,load,""""""" PyPy-specific touch-ups; """"""; # first load the dependency libraries of the backend, then; # pull in the built-in low-level cppyy; # some older versions can be fixed up through a compatibility; # module on the python side; load it, if available; # built-in module; #- exports -------------------------------------------------------------------; # add other exports to all",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/python/cppyy/_pypy_cppyy.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/python/cppyy/_pypy_cppyy.py
Energy Efficiency,reduce,reduce,""""""" Pythonization API.; """"""; # user-provided, general pythonizations; """"""<pythonizor> should be a callable taking two arguments: a class proxy,; and its C++ name. It is called each time a named class from <scope> (the; global one by default, but a relevant C++ namespace is recommended) is bound.; """"""; """"""Remove previously registered <pythonizor> from <scope>.; """"""; # prevent auto-casting (e.g. for interfaces); # mapper to reduce template expression trees; """"""Reduce <reducable> to <reduced> type on returns from function calls.; """"""; # exception pythonizations; #--- Pythonization factories --------------------------------------------; # NB: Ideally, we'd use the version commented out below, but for now, we; # make do with the hackier version here.; #.__dict__:; #if not self.keep_orig: delattr(obj, k); # def rename_attribute(match_class, orig_attribute, new_attribute, keep_orig=False):; # class method_pythonizor:; # def __init__(self, match_class, orig_attribute, new_attribute, keep_orig):; # import re; # self.match_class = re.compile(match_class); # self.match_attr = re.compile(orig_attribute); # self.new_attr = new_attribute; # self.keep_orig = keep_orig; # def __call__(self, obj, name):; # import sys; # if not self.match_class.match(name):; # return; # sys.stderr.write(""%s %s %s %s"" % (""!!!"", obj, name, ""\n"")); # for k in dir(obj): #obj.__dict__:; # if not self.match_attr.match(k): continue; # try:; # tmp = getattr(obj, k); # except Exception as e:; # continue; # setattr(obj, self.new_attr, tmp); # if not self.keep_orig: delattr(obj, k); # return method_pythonizor(match_class, orig_attribute, new_attribute, keep_orig); # Shared with PyPy:; #.__dict__:; #.__dict__:; #.__dict__:; #.__dict__:; #.__dict__:",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/python/cppyy/_pythonization.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/python/cppyy/_pythonization.py
Integrability,interface,interfaces,""""""" Pythonization API.; """"""; # user-provided, general pythonizations; """"""<pythonizor> should be a callable taking two arguments: a class proxy,; and its C++ name. It is called each time a named class from <scope> (the; global one by default, but a relevant C++ namespace is recommended) is bound.; """"""; """"""Remove previously registered <pythonizor> from <scope>.; """"""; # prevent auto-casting (e.g. for interfaces); # mapper to reduce template expression trees; """"""Reduce <reducable> to <reduced> type on returns from function calls.; """"""; # exception pythonizations; #--- Pythonization factories --------------------------------------------; # NB: Ideally, we'd use the version commented out below, but for now, we; # make do with the hackier version here.; #.__dict__:; #if not self.keep_orig: delattr(obj, k); # def rename_attribute(match_class, orig_attribute, new_attribute, keep_orig=False):; # class method_pythonizor:; # def __init__(self, match_class, orig_attribute, new_attribute, keep_orig):; # import re; # self.match_class = re.compile(match_class); # self.match_attr = re.compile(orig_attribute); # self.new_attr = new_attribute; # self.keep_orig = keep_orig; # def __call__(self, obj, name):; # import sys; # if not self.match_class.match(name):; # return; # sys.stderr.write(""%s %s %s %s"" % (""!!!"", obj, name, ""\n"")); # for k in dir(obj): #obj.__dict__:; # if not self.match_attr.match(k): continue; # try:; # tmp = getattr(obj, k); # except Exception as e:; # continue; # setattr(obj, self.new_attr, tmp); # if not self.keep_orig: delattr(obj, k); # return method_pythonizor(match_class, orig_attribute, new_attribute, keep_orig); # Shared with PyPy:; #.__dict__:; #.__dict__:; #.__dict__:; #.__dict__:; #.__dict__:",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/python/cppyy/_pythonization.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/python/cppyy/_pythonization.py
Performance,load,loading,"# It may be that the interpreter (wether python or pypy-c) was not linked; # with C++; force its loading before doing anything else (note that not; # linking with C++ spells trouble anyway for any C++ libraries ...); # TODO: check executable to see whether linking indeed didn't happen; # TODO: what if Linux/clang and what if Mac?",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/python/cppyy/_stdcpp_fix.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/python/cppyy/_stdcpp_fix.py
Energy Efficiency,charge,charge,""""""" Externally provided types: get looked up if all else fails, e.g.; for typedef-ed C++ builtin types.; """"""; # from six.py ---; # Copyright (c) 2010-2017 Benjamin Peterson; #; # Permission is hereby granted, free of charge, to any person obtaining a copy; # of this software and associated documentation files (the ""Software""), to deal; # in the Software without restriction, including without limitation the rights; # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell; # copies of the Software, and to permit persons to whom the Software is; # furnished to do so, subject to the following conditions:; #; # The above copyright notice and this permission notice shall be included in all; # copies or substantial portions of the Software.; #; # THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR; # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,; # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE; # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER; # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,; # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE; # SOFTWARE.; """"""Create a base class with a metaclass.""""""; # This requires a bit of explanation: the basic idea is to make a dummy; # metaclass for one level of class instantiation that replaces itself with; # the actual metaclass.; # --- end from six.py; # boolean type (builtin type bool can nog be subclassed); # char types; # integer types; # floating point types; # void*",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/python/cppyy/_typemap.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/python/cppyy/_typemap.py
Availability,error,error,"write the; # custom ""npos"" object (to allow easy result checking of find/rfind) in Python; # drop b/c is const data; # TODO: PyPy still has the old-style pythonizations, which require the full; # class name (not possible for std::tuple ...); # std::make_shared/unique create needless templates: rely on Python's introspection; # instead. This also allows Python derived classes to be handled correctly.; # C++ takes ownership; #--- interface to Cling ------------------------------------------------------; """"""Declare C++ source <src> to Cling.""""""; """"""Execute C++ statement <stmt> in Cling's global scope.""""""; # capture stderr, but note that ProcessLine could legitimately be writing to; # std::cerr, in which case the captured output needs to be printed as normal; """"""Attempt to evalute a C/C++ pre-processor macro as a constant""""""; """"""Explicitly load a shared library.""""""; # special case for Windows as of python3.8: use winmode=0, otherwise the default; # will not consider regular search paths (such as $PATH); # raises on error; """"""Load (and JIT) header file <header> into Cling.""""""; """"""Load (and JIT) header file <header> into Cling.""""""; """"""extern ""C"" {; #include ""%s""; }""""""; """"""Add a path to the include paths available to Cling.""""""; """"""Add a path to the library search paths available to Cling.""""""; # add access to Python C-API headers; # possibly structured without 'pythonx.y' in path; # add access to extra headers for dispatcher (CPyCppyy only (?)); # a ""normal"" structure finds the include directory up to 3 levels up,; # ie. dropping lib/pythonx.y[md]/site-packages; # add back pythonx.y or site/pythonx.y if present; # MacOS, Linux; # Windows; # assuming that we are in PREFIX/lib/python/site-packages/cppyy, add PREFIX/include to the search path; """"""Add the entries from a autoload (.rootmap) file to Cling.""""""; """"""Enable/disable debug output.""""""; """"""Returns the storage size (in chars) of C++ type <tt>.""""""; """"""Returns the C++ runtime type information for type <tt>.""""""; # after six,",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/python/cppyy/__init__.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/python/cppyy/__init__.py
Integrability,interface,interface,"backs; #- allow importing from gbl --------------------------------------------------; #- external typemap ----------------------------------------------------------; # also creates (u)int8_t mapper; # ensures same _integer_ type; #- pythonization factories ---------------------------------------------------; # pythonization of tuple; TODO: placed here for convenience, but a custom case; # for tuples on each platform can be made much more performant ...; # pythonization of std::string; placed here because it's simpler to write the; # custom ""npos"" object (to allow easy result checking of find/rfind) in Python; # drop b/c is const data; # TODO: PyPy still has the old-style pythonizations, which require the full; # class name (not possible for std::tuple ...); # std::make_shared/unique create needless templates: rely on Python's introspection; # instead. This also allows Python derived classes to be handled correctly.; # C++ takes ownership; #--- interface to Cling ------------------------------------------------------; """"""Declare C++ source <src> to Cling.""""""; """"""Execute C++ statement <stmt> in Cling's global scope.""""""; # capture stderr, but note that ProcessLine could legitimately be writing to; # std::cerr, in which case the captured output needs to be printed as normal; """"""Attempt to evalute a C/C++ pre-processor macro as a constant""""""; """"""Explicitly load a shared library.""""""; # special case for Windows as of python3.8: use winmode=0, otherwise the default; # will not consider regular search paths (such as $PATH); # raises on error; """"""Load (and JIT) header file <header> into Cling.""""""; """"""Load (and JIT) header file <header> into Cling.""""""; """"""extern ""C"" {; #include ""%s""; }""""""; """"""Add a path to the include paths available to Cling.""""""; """"""Add a path to the library search paths available to Cling.""""""; # add access to Python C-API headers; # possibly structured without 'pythonx.y' in path; # add access to extra headers for dispatcher (CPyCppyy only (?)); # a ""normal",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/python/cppyy/__init__.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/python/cppyy/__init__.py
Modifiability,inherit,inheritance,"blic:; ... MyClass(int i) : m_data(i) {}; ... int m_data;; ... };\""\""\""); True; >>> from cppyy.gbl import MyClass; >>> m = MyClass(42); >>> cppyy.cppdef(\""\""\""; ... void say_hello(MyClass* m) {; ... std::cout << ""Hello, the number is: "" << m->m_data << std::endl;; ... }\""\""\""); True; >>> MyClass.say_hello = cppyy.gbl.say_hello; >>> m.say_hello(); Hello, the number is: 42; >>> m.m_data = 13; >>> m.say_hello(); Hello, the number is: 13; >>>. For full documentation, see:; https://cppyy.readthedocs.io/. """"""; # declare C++ source to Cling; # execute a C++ statement; # attempt to evaluate a cpp macro; # load and jit a header file; # load and jit a C header file; # load a shared library; # unique pointer representing NULL; # size of a C++ type; # typeid of a C++ type; # helper for multiple inheritance; # add a path to search for headers; # add a path to search for headers; # explicitly include an autoload map; # enable/disable debug output; # import separately instead of in the above try/except block for easier to; # understand tracebacks; #- allow importing from gbl --------------------------------------------------; #- external typemap ----------------------------------------------------------; # also creates (u)int8_t mapper; # ensures same _integer_ type; #- pythonization factories ---------------------------------------------------; # pythonization of tuple; TODO: placed here for convenience, but a custom case; # for tuples on each platform can be made much more performant ...; # pythonization of std::string; placed here because it's simpler to write the; # custom ""npos"" object (to allow easy result checking of find/rfind) in Python; # drop b/c is const data; # TODO: PyPy still has the old-style pythonizations, which require the full; # class name (not possible for std::tuple ...); # std::make_shared/unique create needless templates: rely on Python's introspection; # instead. This also allows Python derived classes to be handled correctly.; # C++ takes ownership; #--- ",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/python/cppyy/__init__.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/python/cppyy/__init__.py
Performance,load,load,"blic:; ... MyClass(int i) : m_data(i) {}; ... int m_data;; ... };\""\""\""); True; >>> from cppyy.gbl import MyClass; >>> m = MyClass(42); >>> cppyy.cppdef(\""\""\""; ... void say_hello(MyClass* m) {; ... std::cout << ""Hello, the number is: "" << m->m_data << std::endl;; ... }\""\""\""); True; >>> MyClass.say_hello = cppyy.gbl.say_hello; >>> m.say_hello(); Hello, the number is: 42; >>> m.m_data = 13; >>> m.say_hello(); Hello, the number is: 13; >>>. For full documentation, see:; https://cppyy.readthedocs.io/. """"""; # declare C++ source to Cling; # execute a C++ statement; # attempt to evaluate a cpp macro; # load and jit a header file; # load and jit a C header file; # load a shared library; # unique pointer representing NULL; # size of a C++ type; # typeid of a C++ type; # helper for multiple inheritance; # add a path to search for headers; # add a path to search for headers; # explicitly include an autoload map; # enable/disable debug output; # import separately instead of in the above try/except block for easier to; # understand tracebacks; #- allow importing from gbl --------------------------------------------------; #- external typemap ----------------------------------------------------------; # also creates (u)int8_t mapper; # ensures same _integer_ type; #- pythonization factories ---------------------------------------------------; # pythonization of tuple; TODO: placed here for convenience, but a custom case; # for tuples on each platform can be made much more performant ...; # pythonization of std::string; placed here because it's simpler to write the; # custom ""npos"" object (to allow easy result checking of find/rfind) in Python; # drop b/c is const data; # TODO: PyPy still has the old-style pythonizations, which require the full; # class name (not possible for std::tuple ...); # std::make_shared/unique create needless templates: rely on Python's introspection; # instead. This also allows Python derived classes to be handled correctly.; # C++ takes ownership; #--- ",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/python/cppyy/__init__.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/python/cppyy/__init__.py
Security,access,access,"le pythonizations, which require the full; # class name (not possible for std::tuple ...); # std::make_shared/unique create needless templates: rely on Python's introspection; # instead. This also allows Python derived classes to be handled correctly.; # C++ takes ownership; #--- interface to Cling ------------------------------------------------------; """"""Declare C++ source <src> to Cling.""""""; """"""Execute C++ statement <stmt> in Cling's global scope.""""""; # capture stderr, but note that ProcessLine could legitimately be writing to; # std::cerr, in which case the captured output needs to be printed as normal; """"""Attempt to evalute a C/C++ pre-processor macro as a constant""""""; """"""Explicitly load a shared library.""""""; # special case for Windows as of python3.8: use winmode=0, otherwise the default; # will not consider regular search paths (such as $PATH); # raises on error; """"""Load (and JIT) header file <header> into Cling.""""""; """"""Load (and JIT) header file <header> into Cling.""""""; """"""extern ""C"" {; #include ""%s""; }""""""; """"""Add a path to the include paths available to Cling.""""""; """"""Add a path to the library search paths available to Cling.""""""; # add access to Python C-API headers; # possibly structured without 'pythonx.y' in path; # add access to extra headers for dispatcher (CPyCppyy only (?)); # a ""normal"" structure finds the include directory up to 3 levels up,; # ie. dropping lib/pythonx.y[md]/site-packages; # add back pythonx.y or site/pythonx.y if present; # MacOS, Linux; # Windows; # assuming that we are in PREFIX/lib/python/site-packages/cppyy, add PREFIX/include to the search path; """"""Add the entries from a autoload (.rootmap) file to Cling.""""""; """"""Enable/disable debug output.""""""; """"""Returns the storage size (in chars) of C++ type <tt>.""""""; """"""Returns the C++ runtime type information for type <tt>.""""""; # after six, see also _typemap.py; """"""Resolve metaclasses for multiple inheritance.""""""; # contruct a ""no conflict"" meta class; the '_meta' is needed by convention",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/python/cppyy/__init__.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/python/cppyy/__init__.py
Usability,simpl,simpler,"header file; # load a shared library; # unique pointer representing NULL; # size of a C++ type; # typeid of a C++ type; # helper for multiple inheritance; # add a path to search for headers; # add a path to search for headers; # explicitly include an autoload map; # enable/disable debug output; # import separately instead of in the above try/except block for easier to; # understand tracebacks; #- allow importing from gbl --------------------------------------------------; #- external typemap ----------------------------------------------------------; # also creates (u)int8_t mapper; # ensures same _integer_ type; #- pythonization factories ---------------------------------------------------; # pythonization of tuple; TODO: placed here for convenience, but a custom case; # for tuples on each platform can be made much more performant ...; # pythonization of std::string; placed here because it's simpler to write the; # custom ""npos"" object (to allow easy result checking of find/rfind) in Python; # drop b/c is const data; # TODO: PyPy still has the old-style pythonizations, which require the full; # class name (not possible for std::tuple ...); # std::make_shared/unique create needless templates: rely on Python's introspection; # instead. This also allows Python derived classes to be handled correctly.; # C++ takes ownership; #--- interface to Cling ------------------------------------------------------; """"""Declare C++ source <src> to Cling.""""""; """"""Execute C++ statement <stmt> in Cling's global scope.""""""; # capture stderr, but note that ProcessLine could legitimately be writing to; # std::cerr, in which case the captured output needs to be printed as normal; """"""Attempt to evalute a C/C++ pre-processor macro as a constant""""""; """"""Explicitly load a shared library.""""""; # special case for Windows as of python3.8: use winmode=0, otherwise the default; # will not consider regular search paths (such as $PATH); # raises on error; """"""Load (and JIT) header file <header> into Cling.",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/python/cppyy/__init__.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/python/cppyy/__init__.py
Performance,load,load,"""""""Compatibility layer for PyPy 5.7-9; """"""; # first load and move the builtin cppyy module; # move to the location of the backend, just in case '.' is; # in the dynloader's path; # now locate and load the pip cppyy module; # walk over sys.path skips builtins; # copy over the _cppyy functions into cppyy; # for pypy5.9 we may need to move to the location of the backend, if '.' happens; # to be in LD_LIBRARY_PATH, but not the full directory",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/python/cppyy_compat/__init__.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/python/cppyy_compat/__init__.py
Availability,avail,available,"# namespace at the global level; # cppyy functions; # 'cppyy.gbl' bound to 'g'; # full lazy lookup available",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/assert_interactive.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/assert_interactive.py
Availability,down,downcast,"data member access when using virtual inheritence""""""; #-----; #-----; #-----; """"""Test reference passing when using virtual inheritance""""""; #-----; #-----; #-----; """"""Test passing of variants of void pointer arguments""""""; # there is no 8-byte integer type array on Windows 64b; # alternate path through static method handling, which does NOT; # provide 'pp' as argument (and thus need no removing); """"""Test passing around of opaque pointers""""""; # TODO: figure out the PyPy equivalent of CObject (may have to do this; # through the C-API from C++); #cobj = cppyy.as_cobject(o); #assert o == cppyy.bind_object(cobj, some_concrete_class); #assert o == cppyy.bind_object(cobj, type(o)); #assert o == cppyy.bind_object(cobj, o.__class__); #assert o == cppyy.bind_object(cobj, ""some_concrete_class""); """"""Test object identity""""""; """"""Test calling of methods from multiple inheritance""""""; """"""Test that a pointer to base return does an auto-downcast""""""; # special case when round-tripping through a void* ptr; """"""Test auto-downcast in adverse inheritance situation""""""; """"""Verify that class-level overloaded new/delete are called""""""; """"""Test template instantiation with a std::vector<float>""""""; # the following will simply fail if there is a naming problem (e.g.; # std::, allocator<int>, etc., etc.); note the parsing required ...; """"""Test template global function lookup and calls""""""; """"""Test assignment to an instance returned by reference""""""; """"""Test operator int/long/double incl. typedef""""""; """"""Check that the global operator!=/== is picked up""""""; """"""Test return type against proper overload w/ const and covariance""""""; """"""Access global_variables_and_pointers""""""; """"""Catching of C++ exceptions""""""; """"""Accessibility of using declarations""""""; """"""Typedefs to private classes should not resolve""""""; """"""Mapping of __str__ through operator<<(ostream&)""""""; # only cached for global functions and in principle should; # not be needed anymore ...; # print through base class (used to fail with compilation error); #",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_advancedcpp.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_advancedcpp.py
Modifiability,inherit,inheritance,"""""""Test usage of default arguments""""""; """"""Test binding of a basic inheritance structure""""""; """"""Test access to namespaces and inner classes""""""; """"""Test whether namespaces can be shared across dictionaries.""""""; """"""Test bindings of templated types""""""; #----- mix in some of the alternative syntax; #-----; #-----; #-----; """"""Test non-instatiatability of abstract classes""""""; """"""Test data member access when using virtual inheritence""""""; #-----; #-----; #-----; """"""Test reference passing when using virtual inheritance""""""; #-----; #-----; #-----; """"""Test passing of variants of void pointer arguments""""""; # there is no 8-byte integer type array on Windows 64b; # alternate path through static method handling, which does NOT; # provide 'pp' as argument (and thus need no removing); """"""Test passing around of opaque pointers""""""; # TODO: figure out the PyPy equivalent of CObject (may have to do this; # through the C-API from C++); #cobj = cppyy.as_cobject(o); #assert o == cppyy.bind_object(cobj, some_concrete_class); #assert o == cppyy.bind_object(cobj, type(o)); #assert o == cppyy.bind_object(cobj, o.__class__); #assert o == cppyy.bind_object(cobj, ""some_concrete_class""); """"""Test object identity""""""; """"""Test calling of methods from multiple inheritance""""""; """"""Test that a pointer to base return does an auto-downcast""""""; # special case when round-tripping through a void* ptr; """"""Test auto-downcast in adverse inheritance situation""""""; """"""Verify that class-level overloaded new/delete are called""""""; """"""Test template instantiation with a std::vector<float>""""""; # the following will simply fail if there is a naming problem (e.g.; # std::, allocator<int>, etc., etc.); note the parsing required ...; """"""Test template global function lookup and calls""""""; """"""Test assignment to an instance returned by reference""""""; """"""Test operator int/long/double incl. typedef""""""; """"""Check that the global operator!=/== is picked up""""""; """"""Test return type against proper overload w/ const and covariance""""""; """"""Acce",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_advancedcpp.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_advancedcpp.py
Performance,cache,cached,"bind_object(cobj, o.__class__); #assert o == cppyy.bind_object(cobj, ""some_concrete_class""); """"""Test object identity""""""; """"""Test calling of methods from multiple inheritance""""""; """"""Test that a pointer to base return does an auto-downcast""""""; # special case when round-tripping through a void* ptr; """"""Test auto-downcast in adverse inheritance situation""""""; """"""Verify that class-level overloaded new/delete are called""""""; """"""Test template instantiation with a std::vector<float>""""""; # the following will simply fail if there is a naming problem (e.g.; # std::, allocator<int>, etc., etc.); note the parsing required ...; """"""Test template global function lookup and calls""""""; """"""Test assignment to an instance returned by reference""""""; """"""Test operator int/long/double incl. typedef""""""; """"""Check that the global operator!=/== is picked up""""""; """"""Test return type against proper overload w/ const and covariance""""""; """"""Access global_variables_and_pointers""""""; """"""Catching of C++ exceptions""""""; """"""Accessibility of using declarations""""""; """"""Typedefs to private classes should not resolve""""""; """"""Mapping of __str__ through operator<<(ostream&)""""""; # only cached for global functions and in principle should; # not be needed anymore ...; # print through base class (used to fail with compilation error); # print through friend; """"""\; namespace PrintingNS {; class X {; friend std::ostream& operator<<(std::ostream& os, const X&) { return os << ""X""; }; };. class Y {; };. std::ostream& operator<<(std::ostream& os, const Y&) { return os << ""Y""; }; } """"""; """"""Test using directive in namespaces""""""; """"""Test that typedefs are not shadowed""""""; """"""; namespace ShadowedTypedef {; struct A {; typedef std::shared_ptr<A> Ptr;; typedef int Val;; };. struct B : public A {; typedef std::shared_ptr<B> Ptr;; typedef double Val;; };. struct C : public A {; /* empty */; }; }""""""; # pull A::Ptr first; # id. A::Val; # used to be A.Ptr through python-side dict lookup; # id. B::Val; # is A.Ptr; # is A.Val; # TODO: current",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_advancedcpp.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_advancedcpp.py
Security,access,access,"""""""Test usage of default arguments""""""; """"""Test binding of a basic inheritance structure""""""; """"""Test access to namespaces and inner classes""""""; """"""Test whether namespaces can be shared across dictionaries.""""""; """"""Test bindings of templated types""""""; #----- mix in some of the alternative syntax; #-----; #-----; #-----; """"""Test non-instatiatability of abstract classes""""""; """"""Test data member access when using virtual inheritence""""""; #-----; #-----; #-----; """"""Test reference passing when using virtual inheritance""""""; #-----; #-----; #-----; """"""Test passing of variants of void pointer arguments""""""; # there is no 8-byte integer type array on Windows 64b; # alternate path through static method handling, which does NOT; # provide 'pp' as argument (and thus need no removing); """"""Test passing around of opaque pointers""""""; # TODO: figure out the PyPy equivalent of CObject (may have to do this; # through the C-API from C++); #cobj = cppyy.as_cobject(o); #assert o == cppyy.bind_object(cobj, some_concrete_class); #assert o == cppyy.bind_object(cobj, type(o)); #assert o == cppyy.bind_object(cobj, o.__class__); #assert o == cppyy.bind_object(cobj, ""some_concrete_class""); """"""Test object identity""""""; """"""Test calling of methods from multiple inheritance""""""; """"""Test that a pointer to base return does an auto-downcast""""""; # special case when round-tripping through a void* ptr; """"""Test auto-downcast in adverse inheritance situation""""""; """"""Verify that class-level overloaded new/delete are called""""""; """"""Test template instantiation with a std::vector<float>""""""; # the following will simply fail if there is a naming problem (e.g.; # std::, allocator<int>, etc., etc.); note the parsing required ...; """"""Test template global function lookup and calls""""""; """"""Test assignment to an instance returned by reference""""""; """"""Test operator int/long/double incl. typedef""""""; """"""Check that the global operator!=/== is picked up""""""; """"""Test return type against proper overload w/ const and covariance""""""; """"""Acce",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_advancedcpp.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_advancedcpp.py
Testability,assert,assert,"""""""Test usage of default arguments""""""; """"""Test binding of a basic inheritance structure""""""; """"""Test access to namespaces and inner classes""""""; """"""Test whether namespaces can be shared across dictionaries.""""""; """"""Test bindings of templated types""""""; #----- mix in some of the alternative syntax; #-----; #-----; #-----; """"""Test non-instatiatability of abstract classes""""""; """"""Test data member access when using virtual inheritence""""""; #-----; #-----; #-----; """"""Test reference passing when using virtual inheritance""""""; #-----; #-----; #-----; """"""Test passing of variants of void pointer arguments""""""; # there is no 8-byte integer type array on Windows 64b; # alternate path through static method handling, which does NOT; # provide 'pp' as argument (and thus need no removing); """"""Test passing around of opaque pointers""""""; # TODO: figure out the PyPy equivalent of CObject (may have to do this; # through the C-API from C++); #cobj = cppyy.as_cobject(o); #assert o == cppyy.bind_object(cobj, some_concrete_class); #assert o == cppyy.bind_object(cobj, type(o)); #assert o == cppyy.bind_object(cobj, o.__class__); #assert o == cppyy.bind_object(cobj, ""some_concrete_class""); """"""Test object identity""""""; """"""Test calling of methods from multiple inheritance""""""; """"""Test that a pointer to base return does an auto-downcast""""""; # special case when round-tripping through a void* ptr; """"""Test auto-downcast in adverse inheritance situation""""""; """"""Verify that class-level overloaded new/delete are called""""""; """"""Test template instantiation with a std::vector<float>""""""; # the following will simply fail if there is a naming problem (e.g.; # std::, allocator<int>, etc., etc.); note the parsing required ...; """"""Test template global function lookup and calls""""""; """"""Test assignment to an instance returned by reference""""""; """"""Test operator int/long/double incl. typedef""""""; """"""Check that the global operator!=/== is picked up""""""; """"""Test return type against proper overload w/ const and covariance""""""; """"""Acce",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_advancedcpp.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_advancedcpp.py
Usability,simpl,simply,"data member access when using virtual inheritence""""""; #-----; #-----; #-----; """"""Test reference passing when using virtual inheritance""""""; #-----; #-----; #-----; """"""Test passing of variants of void pointer arguments""""""; # there is no 8-byte integer type array on Windows 64b; # alternate path through static method handling, which does NOT; # provide 'pp' as argument (and thus need no removing); """"""Test passing around of opaque pointers""""""; # TODO: figure out the PyPy equivalent of CObject (may have to do this; # through the C-API from C++); #cobj = cppyy.as_cobject(o); #assert o == cppyy.bind_object(cobj, some_concrete_class); #assert o == cppyy.bind_object(cobj, type(o)); #assert o == cppyy.bind_object(cobj, o.__class__); #assert o == cppyy.bind_object(cobj, ""some_concrete_class""); """"""Test object identity""""""; """"""Test calling of methods from multiple inheritance""""""; """"""Test that a pointer to base return does an auto-downcast""""""; # special case when round-tripping through a void* ptr; """"""Test auto-downcast in adverse inheritance situation""""""; """"""Verify that class-level overloaded new/delete are called""""""; """"""Test template instantiation with a std::vector<float>""""""; # the following will simply fail if there is a naming problem (e.g.; # std::, allocator<int>, etc., etc.); note the parsing required ...; """"""Test template global function lookup and calls""""""; """"""Test assignment to an instance returned by reference""""""; """"""Test operator int/long/double incl. typedef""""""; """"""Check that the global operator!=/== is picked up""""""; """"""Test return type against proper overload w/ const and covariance""""""; """"""Access global_variables_and_pointers""""""; """"""Catching of C++ exceptions""""""; """"""Accessibility of using declarations""""""; """"""Typedefs to private classes should not resolve""""""; """"""Mapping of __str__ through operator<<(ostream&)""""""; # only cached for global functions and in principle should; # not be needed anymore ...; # print through base class (used to fail with compilation error); #",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_advancedcpp.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_advancedcpp.py
Testability,test,test,"""""""Availability of boost::any""""""; """"""boost::any assignment and casting""""""; # test both by-ref and by rvalue; # move forced; #assert len(extract) == 0 # not guaranteed by the standard; # TODO: we hit boost::any_cast<int>(boost::any* operand) instead; # of the reference version which raises; # raises(Exception, boost.any_cast[int], val); # getting here is good, too ...; """"""ordered_field_operators as base used to crash""""""; """"""; namespace boost_test {; class Derived : boost::ordered_field_operators<Derived>, boost::ordered_field_operators<Derived, mpq_class> {};; }; """"""; """"""boost::variant usage""""""; # as posted on stackoverflow as example; """"""namespace BV {; class A { };; class B { };; class C { }; } """"""; """"""boost::type_erasure usage""""""; """"""; BOOST_TYPE_ERASURE_MEMBER((has_member_f), f, 0). using LengthsInterface = boost::mpl::vector<; boost::type_erasure::copy_constructible<>,; has_member_f<std::vector<int>() const>>;. using Lengths = boost::type_erasure::any<LengthsInterface>;. struct Unerased {; std::vector<int> f() const { return std::vector<int>{}; }; };. Lengths lengths() {; return Unerased{};; }; """"""",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_boost.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_boost.py
Safety,timeout,timeout,"""""""\; namespace Workers {; double calc(double d) { return d*42.; }; }""""""; """"""Run basic Python threads""""""; """"""Run with Python futures""""""; """"""Time-out with threads""""""; """"""\; namespace test12_timeout {; bool _islive = false; volatile bool* islive = &_islive;; bool _stopit = false; volatile bool* stopit = &_stopit;; }""""""; # have to give ProcessLine() time to actually start doing work; # in seconds; # join the thread with a timeout after 0.1s; # id.; # was timed-out; """"""Threads and Python exceptions""""""; """"""namespace thread_test {; #include <thread>. struct consumer {; virtual ~consumer() {}; virtual void process(int) = 0;; };. struct worker {; worker(consumer* c) : cons(c) { }; ~worker() { wait(); }. void start() {; t = std::thread([this] {; int counter = 0;; while (counter++ < 10); try {; cons->process(counter);; } catch (CPyCppyy::PyException& e) {; err_msg = e.what();; return;; }; });; }. void wait() {; if (t.joinable()); t.join();; }. std::thread t;; consumer* cons = nullptr;; std::string err_msg;; }; }""""""; """"""Passing of 2-dim float arguments""""""; """"""\; namespace FloatDim2 {; #include <thread>. struct Buffer {; Buffer() = default;. void setData(float** newData) {; data = newData;; }. void setSample(int newChannel, int newSample, float value) {; data[newChannel][newSample] = value;; }. float** data = nullptr;; };. struct Processor {; virtual ~Processor() = default;; virtual void process(float** data, int channels, int samples) = 0;; };. void callback(Processor& p) {; std::thread t([&p] {; int channels = 2;; int samples = 32;. float** data = new float*[channels];; for (int i = 0; i < channels; ++i); data[i] = new float[samples];. p.process(data, channels, samples);. for (int i = 0; i < channels; ++i); delete[] data[i];. delete[] data;; });. t.join();; } }""""""; # < used to crash here; """"""Threads reuse overload objects; check for clashes""""""; """"""\; namespace CPPOverloadReuse {; class Simulation1 {; public:; virtual void set_something(std::map<std::string, std::string>, std::",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_concurrent.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_concurrent.py
Availability,error,error,"""""""Test implicit conversions of std::vector""""""; """"""Verify that memory of temporaries is properly cleaned up""""""; """"""Verify error handling""""""; """"""Allow implicit conversions from tuples as arguments {}-like""""""; # Note: fails on windows b/c the assignment operator for strings is; # template, which (""operator=(std::string)"") doesn't instantiate; # implicit conversion to std::pair; """"""Test operator bool() and null pointer behavior""""""; """"""\; namespace BoolConversions {; struct Test1 {};; struct Test2 {; Test2(bool b) : m_b(b) {}; explicit operator bool() const { return m_b; }; bool m_b;; };. Test1* CreateNullTest1() { return nullptr; }; Test2* CreateNullTest2() { return nullptr; }; }""""""",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_conversions.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_conversions.py
Availability,error,error,"ype taken from pointer; """"""Ability to pass unique_ptr<Derived> through unique_ptr<Base>""""""; # move matching unique_ptr; # move with conversion; # TODO: why does the following fail, but succeed for shared_ptr??; # assert move_unique_ptr(std.move(dd)) == 100; # ability to take over by-value python-owned objects; # alternative make_unique with type taken from pointer; """"""Allow the programmer to pass NULL in certain cases""""""; # test existence; # assert not hasattr(cppyy.gbl, 'nullptr'); # further usage is tested in datatypes.py:test15_nullptr_passing; """"""Move construction, assignment, and methods""""""; # move constructor; # cctor; # can't check ref-count; # should call move, not memoized cctor; # both move and ref-count; # move assignment; # can't check ref-count; # order of moving and normal functions are reversed in 1, 2, for; # overload resolution testing; # implicit conversion and move; """"""Initializer list construction""""""; """"""Call (global) lambdas""""""; """"""Use of optional and nullopt""""""; """"""; enum Enum { A = -1 };; bool callopt(std::optional<Enum>) { return true; }; """"""; """"""Use of chrono and overloaded operator+""""""; # following used to fail with compilation error; """"""Use of std::function with arguments in a namespace""""""; # and for good measure, inline; """"""namespace FunctionNS2 {; struct FNTestStruct { FNTestStruct(int i) : t(i) {} int t; };; std::function<int(const FNTestStruct& t)> FNCreateTestStructFunc() { return [](const FNTestStruct& t) { return t.t; }; }; }""""""; """"""Use of std::hash""""""; # to test effect of caching; """"""Ability to pass normal pointers through shared_ptr by value""""""; # pass was by shared copy; """"""Argument type deduction with std::unique_ptr""""""; """"""namespace UniqueTempl {; template <typename T>; std::unique_ptr<T> returnptr(std::unique_ptr<T>&& a) {; return std::move(a);; } }""""""; # not an RValue; """"""std::unique_ptr requires moves""""""; """"""namespace unique_ptr_moves {; template <typename T>; std::unique_ptr<T> returnptr_value(std::unique_ptr<T> a) {; retur",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_cpp11features.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_cpp11features.py
Modifiability,inherit,inherited,"in certain cases""""""; # test existence; # assert not hasattr(cppyy.gbl, 'nullptr'); # further usage is tested in datatypes.py:test15_nullptr_passing; """"""Move construction, assignment, and methods""""""; # move constructor; # cctor; # can't check ref-count; # should call move, not memoized cctor; # both move and ref-count; # move assignment; # can't check ref-count; # order of moving and normal functions are reversed in 1, 2, for; # overload resolution testing; # implicit conversion and move; """"""Initializer list construction""""""; """"""Call (global) lambdas""""""; """"""Use of optional and nullopt""""""; """"""; enum Enum { A = -1 };; bool callopt(std::optional<Enum>) { return true; }; """"""; """"""Use of chrono and overloaded operator+""""""; # following used to fail with compilation error; """"""Use of std::function with arguments in a namespace""""""; # and for good measure, inline; """"""namespace FunctionNS2 {; struct FNTestStruct { FNTestStruct(int i) : t(i) {} int t; };; std::function<int(const FNTestStruct& t)> FNCreateTestStructFunc() { return [](const FNTestStruct& t) { return t.t; }; }; }""""""; """"""Use of std::hash""""""; # to test effect of caching; """"""Ability to pass normal pointers through shared_ptr by value""""""; # pass was by shared copy; """"""Argument type deduction with std::unique_ptr""""""; """"""namespace UniqueTempl {; template <typename T>; std::unique_ptr<T> returnptr(std::unique_ptr<T>&& a) {; return std::move(a);; } }""""""; # not an RValue; """"""std::unique_ptr requires moves""""""; """"""namespace unique_ptr_moves {; template <typename T>; std::unique_ptr<T> returnptr_value(std::unique_ptr<T> a) {; return std::move(a);; }; template <typename T>; std::unique_ptr<T> returnptr_move(std::unique_ptr<T>&& a) {; return std::move(a);; } }""""""; """"""std::unique_ptr as data means implicitly no copy ctor""""""; """"""namespace unique_ptr_data{; class Example {; private:; std::unique_ptr<double> x;; public:; Example() {}; virtual ~Example() = default;; double y = 66.;; }; }""""""; # Test whether this attribute was inherited",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_cpp11features.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_cpp11features.py
Security,access,access,"""""""Usage and access of std::shared/unique_ptr<>""""""; """"""Shared/Unique pointer ctor is templated, requiring special care""""""; """"""Test shared/unique pointer memory ownership""""""; """"""Ability to pass shared_ptr<Derived> through shared_ptr<Base>""""""; # ability to take over by-value python-owned objects; # alternative make_shared with type taken from pointer; """"""Ability to pass unique_ptr<Derived> through unique_ptr<Base>""""""; # move matching unique_ptr; # move with conversion; # TODO: why does the following fail, but succeed for shared_ptr??; # assert move_unique_ptr(std.move(dd)) == 100; # ability to take over by-value python-owned objects; # alternative make_unique with type taken from pointer; """"""Allow the programmer to pass NULL in certain cases""""""; # test existence; # assert not hasattr(cppyy.gbl, 'nullptr'); # further usage is tested in datatypes.py:test15_nullptr_passing; """"""Move construction, assignment, and methods""""""; # move constructor; # cctor; # can't check ref-count; # should call move, not memoized cctor; # both move and ref-count; # move assignment; # can't check ref-count; # order of moving and normal functions are reversed in 1, 2, for; # overload resolution testing; # implicit conversion and move; """"""Initializer list construction""""""; """"""Call (global) lambdas""""""; """"""Use of optional and nullopt""""""; """"""; enum Enum { A = -1 };; bool callopt(std::optional<Enum>) { return true; }; """"""; """"""Use of chrono and overloaded operator+""""""; # following used to fail with compilation error; """"""Use of std::function with arguments in a namespace""""""; # and for good measure, inline; """"""namespace FunctionNS2 {; struct FNTestStruct { FNTestStruct(int i) : t(i) {} int t; };; std::function<int(const FNTestStruct& t)> FNCreateTestStructFunc() { return [](const FNTestStruct& t) { return t.t; }; }; }""""""; """"""Use of std::hash""""""; # to test effect of caching; """"""Ability to pass normal pointers through shared_ptr by value""""""; # pass was by shared copy; """"""Argument type deduction with std::u",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_cpp11features.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_cpp11features.py
Testability,assert,assert,"""""""Usage and access of std::shared/unique_ptr<>""""""; """"""Shared/Unique pointer ctor is templated, requiring special care""""""; """"""Test shared/unique pointer memory ownership""""""; """"""Ability to pass shared_ptr<Derived> through shared_ptr<Base>""""""; # ability to take over by-value python-owned objects; # alternative make_shared with type taken from pointer; """"""Ability to pass unique_ptr<Derived> through unique_ptr<Base>""""""; # move matching unique_ptr; # move with conversion; # TODO: why does the following fail, but succeed for shared_ptr??; # assert move_unique_ptr(std.move(dd)) == 100; # ability to take over by-value python-owned objects; # alternative make_unique with type taken from pointer; """"""Allow the programmer to pass NULL in certain cases""""""; # test existence; # assert not hasattr(cppyy.gbl, 'nullptr'); # further usage is tested in datatypes.py:test15_nullptr_passing; """"""Move construction, assignment, and methods""""""; # move constructor; # cctor; # can't check ref-count; # should call move, not memoized cctor; # both move and ref-count; # move assignment; # can't check ref-count; # order of moving and normal functions are reversed in 1, 2, for; # overload resolution testing; # implicit conversion and move; """"""Initializer list construction""""""; """"""Call (global) lambdas""""""; """"""Use of optional and nullopt""""""; """"""; enum Enum { A = -1 };; bool callopt(std::optional<Enum>) { return true; }; """"""; """"""Use of chrono and overloaded operator+""""""; # following used to fail with compilation error; """"""Use of std::function with arguments in a namespace""""""; # and for good measure, inline; """"""namespace FunctionNS2 {; struct FNTestStruct { FNTestStruct(int i) : t(i) {} int t; };; std::function<int(const FNTestStruct& t)> FNCreateTestStructFunc() { return [](const FNTestStruct& t) { return t.t; }; }; }""""""; """"""Use of std::hash""""""; # to test effect of caching; """"""Ability to pass normal pointers through shared_ptr by value""""""; # pass was by shared copy; """"""Argument type deduction with std::u",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_cpp11features.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_cpp11features.py
Availability,error,errors,"""""""Test ability to override a simple function""""""; """"""Test constructor usage for derived classes""""""; """"""Test ability to override a simple function with an abstract base""""""; # direct call to init can not work; # clarifying message; # now with abstract constructor that takes an argument; """"""Test ability to override functions that take arguments""""""; """"""Test ability to override overloaded functions""""""; """"""Declared const methods should keep that qualifier""""""; """"""Derive from a base class that is instantiated from a template""""""; """"""Python errors should propagate through wrapper""""""; """"""\; namespace CrossInheritance {; std::string call_base1(Base1* b) {; try {; b->sum_value(-7);; } catch (CPyCppyy::PyException& e) {; e.clear();; return e.what();; }; return """";; } }""""""; """"""Conversion errors should be Python exceptions""""""; # missing return; """"""Usage of Python derived objects in std::vector""""""; # uses copy ctor; # copy ctor copies python state; # uses copy ctor; """"""Usage of Python derived objects with std::make_shared""""""; """"""namespace MakeSharedTest {; class Abstract {; public:; virtual ~Abstract() = 0;; virtual int some_imp() = 0;; };. Abstract::~Abstract() {}. int call_shared(std::shared_ptr<Abstract>& ptr) {; return ptr->some_imp();; } }""""""; """"""Test countable base counting""""""; # test counter; """"""Usage of Python derived objects with std::shared_ptr""""""; """"""Usage of virtual destructors and Python-side del.""""""; """"""namespace VirtualDtor {; class MyClass1 {}; // no virtual dtor ... class MyClass2 {; public:; virtual ~MyClass2() {}; };. class MyClass3 : public MyClass2 {};. template<class T>; class MyClass4 {; public:; virtual ~MyClass4() {}; }; }""""""; # rethought this: just issue a warning if there is no virtual destructor; # as the C++ side now carries the type of the dispatcher, not the type of; # the direct base class; # TODO: verify warning is given; # used to crash; # used to crash; # check a few more derivations that should not fail; """"""Derived classes should have access to pro",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_crossinheritance.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_crossinheritance.py
Integrability,message,message,"""""""Test ability to override a simple function""""""; """"""Test constructor usage for derived classes""""""; """"""Test ability to override a simple function with an abstract base""""""; # direct call to init can not work; # clarifying message; # now with abstract constructor that takes an argument; """"""Test ability to override functions that take arguments""""""; """"""Test ability to override overloaded functions""""""; """"""Declared const methods should keep that qualifier""""""; """"""Derive from a base class that is instantiated from a template""""""; """"""Python errors should propagate through wrapper""""""; """"""\; namespace CrossInheritance {; std::string call_base1(Base1* b) {; try {; b->sum_value(-7);; } catch (CPyCppyy::PyException& e) {; e.clear();; return e.what();; }; return """";; } }""""""; """"""Conversion errors should be Python exceptions""""""; # missing return; """"""Usage of Python derived objects in std::vector""""""; # uses copy ctor; # copy ctor copies python state; # uses copy ctor; """"""Usage of Python derived objects with std::make_shared""""""; """"""namespace MakeSharedTest {; class Abstract {; public:; virtual ~Abstract() = 0;; virtual int some_imp() = 0;; };. Abstract::~Abstract() {}. int call_shared(std::shared_ptr<Abstract>& ptr) {; return ptr->some_imp();; } }""""""; """"""Test countable base counting""""""; # test counter; """"""Usage of Python derived objects with std::shared_ptr""""""; """"""Usage of virtual destructors and Python-side del.""""""; """"""namespace VirtualDtor {; class MyClass1 {}; // no virtual dtor ... class MyClass2 {; public:; virtual ~MyClass2() {}; };. class MyClass3 : public MyClass2 {};. template<class T>; class MyClass4 {; public:; virtual ~MyClass4() {}; }; }""""""; # rethought this: just issue a warning if there is no virtual destructor; # as the C++ side now carries the type of the dispatcher, not the type of; # the direct base class; # TODO: verify warning is given; # used to crash; # used to crash; # check a few more derivations that should not fail; """"""Derived classes should have access to pro",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_crossinheritance.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_crossinheritance.py
Modifiability,inherit,inheritance,"mmonBase {; Base1(const Base1&) {}; public:; Base1() {}; virtual ~Base1() {}; virtual std::string whoami() { return ""Base1""; }; };. class Base2 : public CommonBase {; protected:; Base2(const Base2&) {}; public:; Base2() {}; virtual ~Base2() {}; virtual std::string whoami() { return ""Base2""; }; };. std::string callit(CommonBase& obj) { return obj.whoami(); } }""""""; """"""Test a deep Python hierarchy with pure virtual functions""""""; """"""namespace deep_hierarchy {; class Base {; public:; virtual ~Base() {}; virtual std::string whoami() = 0;; };. std::string callit(Base& obj) { return obj.whoami(); } }""""""; """"""Hierarchy with abstract classes""""""; """"""namespace abstract_classes {; class Base {; public:; virtual ~Base() {}; virtual std::string whoami() = 0;; virtual std::string message() = 0;; };. std::string whois(Base& obj) { return obj.whoami(); }; std::string saywot(Base& obj) { return obj.message(); } }""""""; """"""Hierarchy with multiple inheritance on the C++ side""""""; """""" namespace cpp_side_multiple_inheritance {; struct Result {; Result() : result(1337) {}; Result(int r) : result(r) {}; int result;; };. class Base1 {; public:; virtual ~Base1() {}; virtual Result abstract1() = 0;; };. class Base2 {; public:; virtual ~Base2() {}; virtual Result abstract2() = 0;; };. class Base : public Base1, public Base2 {; public:; Result abstract2() override { return Result(999); }; }; } """"""; """"""Basic multiple inheritance""""""; """"""namespace basic_multiple_inheritance {; class MyClass1 {; public:; MyClass1() : m_1(13) {}; virtual ~MyClass1() {}; virtual int x() = 0;. public:; int m_1;; };; int callx(MyClass1& m) { return m.x(); }. class MyClass2 {; public:; MyClass2() : m_2(42) {}; virtual ~MyClass2() {}; virtual int y() = 0;. public:; int m_2;; };; int cally(MyClass2& m) { return m.y(); }. class MyClass3 {; public:; MyClass3() : m_3(67) {}; virtual ~MyClass3() {}; virtual int z() = 0;. public:; int m_3;; };; int callz(MyClass3& m) { return m.z(); } }""""""; """"""Multiple inheritance with constructors",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_crossinheritance.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_crossinheritance.py
Security,access,access,"ython state; # uses copy ctor; """"""Usage of Python derived objects with std::make_shared""""""; """"""namespace MakeSharedTest {; class Abstract {; public:; virtual ~Abstract() = 0;; virtual int some_imp() = 0;; };. Abstract::~Abstract() {}. int call_shared(std::shared_ptr<Abstract>& ptr) {; return ptr->some_imp();; } }""""""; """"""Test countable base counting""""""; # test counter; """"""Usage of Python derived objects with std::shared_ptr""""""; """"""Usage of virtual destructors and Python-side del.""""""; """"""namespace VirtualDtor {; class MyClass1 {}; // no virtual dtor ... class MyClass2 {; public:; virtual ~MyClass2() {}; };. class MyClass3 : public MyClass2 {};. template<class T>; class MyClass4 {; public:; virtual ~MyClass4() {}; }; }""""""; # rethought this: just issue a warning if there is no virtual destructor; # as the C++ side now carries the type of the dispatcher, not the type of; # the direct base class; # TODO: verify warning is given; # used to crash; # used to crash; # check a few more derivations that should not fail; """"""Derived classes should have access to protected members""""""; """"""Return of C++ objects from overridden functions""""""; # Part 1: return of a new C++ object; """"""namespace object_returns {; class Base {; public:; virtual Base* foo() { return new Base(); }; virtual ~Base() {}; virtual std::string whoami() { return ""Base""; }; };. class CppDerived : public Base {; CppDerived* foo() { return new CppDerived(); }; ~CppDerived() {}; virtual std::string whoami() { return ""CppDerived""; }; };. Base* call_foo(Base& obj) { return obj.foo(); } }""""""; # Part 2: return of a new Python derived object; """"""Python derived class of C++ class with access controlled cctor""""""; """"""namespace cctor_access_controlled {; class CommonBase {; public:; virtual ~CommonBase() {}; virtual std::string whoami() = 0;; };. class Base1 : public CommonBase {; Base1(const Base1&) {}; public:; Base1() {}; virtual ~Base1() {}; virtual std::string whoami() { return ""Base1""; }; };. class Base2 : public CommonB",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_crossinheritance.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_crossinheritance.py
Testability,test,test,"st ability to override functions that take arguments""""""; """"""Test ability to override overloaded functions""""""; """"""Declared const methods should keep that qualifier""""""; """"""Derive from a base class that is instantiated from a template""""""; """"""Python errors should propagate through wrapper""""""; """"""\; namespace CrossInheritance {; std::string call_base1(Base1* b) {; try {; b->sum_value(-7);; } catch (CPyCppyy::PyException& e) {; e.clear();; return e.what();; }; return """";; } }""""""; """"""Conversion errors should be Python exceptions""""""; # missing return; """"""Usage of Python derived objects in std::vector""""""; # uses copy ctor; # copy ctor copies python state; # uses copy ctor; """"""Usage of Python derived objects with std::make_shared""""""; """"""namespace MakeSharedTest {; class Abstract {; public:; virtual ~Abstract() = 0;; virtual int some_imp() = 0;; };. Abstract::~Abstract() {}. int call_shared(std::shared_ptr<Abstract>& ptr) {; return ptr->some_imp();; } }""""""; """"""Test countable base counting""""""; # test counter; """"""Usage of Python derived objects with std::shared_ptr""""""; """"""Usage of virtual destructors and Python-side del.""""""; """"""namespace VirtualDtor {; class MyClass1 {}; // no virtual dtor ... class MyClass2 {; public:; virtual ~MyClass2() {}; };. class MyClass3 : public MyClass2 {};. template<class T>; class MyClass4 {; public:; virtual ~MyClass4() {}; }; }""""""; # rethought this: just issue a warning if there is no virtual destructor; # as the C++ side now carries the type of the dispatcher, not the type of; # the direct base class; # TODO: verify warning is given; # used to crash; # used to crash; # check a few more derivations that should not fail; """"""Derived classes should have access to protected members""""""; """"""Return of C++ objects from overridden functions""""""; # Part 1: return of a new C++ object; """"""namespace object_returns {; class Base {; public:; virtual Base* foo() { return new Base(); }; virtual ~Base() {}; virtual std::string whoami() { return ""Base""; }; };. class Cp",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_crossinheritance.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_crossinheritance.py
Usability,simpl,simple,"""""""Test ability to override a simple function""""""; """"""Test constructor usage for derived classes""""""; """"""Test ability to override a simple function with an abstract base""""""; # direct call to init can not work; # clarifying message; # now with abstract constructor that takes an argument; """"""Test ability to override functions that take arguments""""""; """"""Test ability to override overloaded functions""""""; """"""Declared const methods should keep that qualifier""""""; """"""Derive from a base class that is instantiated from a template""""""; """"""Python errors should propagate through wrapper""""""; """"""\; namespace CrossInheritance {; std::string call_base1(Base1* b) {; try {; b->sum_value(-7);; } catch (CPyCppyy::PyException& e) {; e.clear();; return e.what();; }; return """";; } }""""""; """"""Conversion errors should be Python exceptions""""""; # missing return; """"""Usage of Python derived objects in std::vector""""""; # uses copy ctor; # copy ctor copies python state; # uses copy ctor; """"""Usage of Python derived objects with std::make_shared""""""; """"""namespace MakeSharedTest {; class Abstract {; public:; virtual ~Abstract() = 0;; virtual int some_imp() = 0;; };. Abstract::~Abstract() {}. int call_shared(std::shared_ptr<Abstract>& ptr) {; return ptr->some_imp();; } }""""""; """"""Test countable base counting""""""; # test counter; """"""Usage of Python derived objects with std::shared_ptr""""""; """"""Usage of virtual destructors and Python-side del.""""""; """"""namespace VirtualDtor {; class MyClass1 {}; // no virtual dtor ... class MyClass2 {; public:; virtual ~MyClass2() {}; };. class MyClass3 : public MyClass2 {};. template<class T>; class MyClass4 {; public:; virtual ~MyClass4() {}; }; }""""""; # rethought this: just issue a warning if there is no virtual destructor; # as the C++ side now carries the type of the dispatcher, not the type of; # the direct base class; # TODO: verify warning is given; # used to crash; # used to crash; # check a few more derivations that should not fail; """"""Derived classes should have access to pro",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_crossinheritance.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_crossinheritance.py
Availability,error,error,"buffer sizing""""""; """"""Test usage of void* data""""""; # memory regulator recycles; # likewise; """"""\; namespace VoidP {; void* vvv[3][5][7];; struct Init {; Init() {; for (size_t i = 0; i < 3; ++i) {; for (size_t j = 0; j < 5; ++j) {; for (size_t k = 0; k < 7; ++k); vvv[i][j][k] = (void*)(i+j+k);; }; }; }; } _init; }""""""; """"""Usage of unsigned char* as byte array and std::byte*""""""; # p2; # p3; # The following create a unique type for fixed-size C arrays: ctypes.c_char_Array_9; # and neither inherits from a non-sized type nor implements the buffer interface.; # As such, it can't be handled. TODO?; #pbuf = ctypes.create_string_buffer(len(buf), buf); #assert f(pbuf, len(buf)) == total; """"""Function pointer passing""""""; # incorrect spelling; # b/c cached; """"""Passing callables through function pointers""""""; # call of void function; # call of function with reference argument; # callable that does not accept weak-ref; # error testing; # life line protected; # destroys life lines; # lambda gone out of scope; """"""Passing callables through std::function""""""; # call of void function; # call of function with reference argument; # callable that does not accept weak-ref; # error testing; # life line protected; # destroys life lines; # lambda gone out of scope; """"""Life lines to std::function data members""""""; """"""\; namespace BoundMethod2StdFunction {; class Base {; public:; virtual ~Base() {}. std::function<std::string()> execute;; std::string do_execute() {; return execute();; }; }; } """"""; """"""Multi-dim arrays of builtins""""""; """"""; template<class T, int nlayers>; struct MultiDimTest {; T* layers[nlayers];. MultiDimTest(int width, int height) {; for (int i=0; i<nlayers; ++i) {; layers[i] = new T[width*height];; for (int j=0; j<width*height; ++j); layers[i][j] = j*2;; }; }; ~MultiDimTest() { for (int i=0; i<nlayers; ++i) delete[] layers[i]; }; };; """"""; """"""Anonymous unions place there fields in the parent scope""""""; """"""\; namespace AnonUnion {; struct Event1 {; Event1() : num(1) { shrd.a = 5.; }; i",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_datatypes.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_datatypes.py
Energy Efficiency,schedul,schedule," ('NULL') and nullptr allowed to pass through instance*""""""; """"""Test that privacy settings are respected""""""; """"""Verify object and pointer comparisons""""""; # FourVector overrides operator==; # like this to ensure __ne__ is called; # id.; """"""Comparisons with C++ providing __eq__/__ne__""""""; """"""; namespace MoreComparisons {; struct Comparable1 {; Comparable1(int i) : fInt(i) {}; int fInt;; static bool __eq__(const Comparable1& self, const Comparable1& other){; return self.fInt == other.fInt;; }; static bool __ne__(const Comparable1& self, const Comparable1& other){; return self.fInt != other.fInt;; }; };. struct Comparable2 {; Comparable2(int i) : fInt(i) {}; int fInt;; bool __eq__(const Comparable2& other){; return fInt == other.fInt;; }; bool __ne__(const Comparable2& other){; return fInt != other.fInt;; }; }; }""""""; # the following works as a side-effect of a workaround for vector calls and; # it is probably preferable to have it working, so leave the discrepancy for; # now: python's aggressive end-of-life schedule will catch up soon enough; """"""Test object validity checking""""""; """"""Correctness of declared buffer shapes""""""; """"""\; namespace ShapeTester {; enum Enum{One, Two, Three};. template<typename T>; struct Foo {; T a[5];; T aa[5][4];; T aaa[5][4][3];; }; }""""""; # TODO: verify the following is for historic reasons and should be modified; # once bug #344 (bitbucket) is fixed; """"""Test usage of buffer sizing""""""; """"""Test usage of void* data""""""; # memory regulator recycles; # likewise; """"""\; namespace VoidP {; void* vvv[3][5][7];; struct Init {; Init() {; for (size_t i = 0; i < 3; ++i) {; for (size_t j = 0; j < 5; ++j) {; for (size_t k = 0; k < 7; ++k); vvv[i][j][k] = (void*)(i+j+k);; }; }; }; } _init; }""""""; """"""Usage of unsigned char* as byte array and std::byte*""""""; # p2; # p3; # The following create a unique type for fixed-size C arrays: ctypes.c_char_Array_9; # and neither inherits from a non-sized type nor implements the buffer interface.; # As such, it can't be handled",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_datatypes.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_datatypes.py
Integrability,interface,interface,"referable to have it working, so leave the discrepancy for; # now: python's aggressive end-of-life schedule will catch up soon enough; """"""Test object validity checking""""""; """"""Correctness of declared buffer shapes""""""; """"""\; namespace ShapeTester {; enum Enum{One, Two, Three};. template<typename T>; struct Foo {; T a[5];; T aa[5][4];; T aaa[5][4][3];; }; }""""""; # TODO: verify the following is for historic reasons and should be modified; # once bug #344 (bitbucket) is fixed; """"""Test usage of buffer sizing""""""; """"""Test usage of void* data""""""; # memory regulator recycles; # likewise; """"""\; namespace VoidP {; void* vvv[3][5][7];; struct Init {; Init() {; for (size_t i = 0; i < 3; ++i) {; for (size_t j = 0; j < 5; ++j) {; for (size_t k = 0; k < 7; ++k); vvv[i][j][k] = (void*)(i+j+k);; }; }; }; } _init; }""""""; """"""Usage of unsigned char* as byte array and std::byte*""""""; # p2; # p3; # The following create a unique type for fixed-size C arrays: ctypes.c_char_Array_9; # and neither inherits from a non-sized type nor implements the buffer interface.; # As such, it can't be handled. TODO?; #pbuf = ctypes.create_string_buffer(len(buf), buf); #assert f(pbuf, len(buf)) == total; """"""Function pointer passing""""""; # incorrect spelling; # b/c cached; """"""Passing callables through function pointers""""""; # call of void function; # call of function with reference argument; # callable that does not accept weak-ref; # error testing; # life line protected; # destroys life lines; # lambda gone out of scope; """"""Passing callables through std::function""""""; # call of void function; # call of function with reference argument; # callable that does not accept weak-ref; # error testing; # life line protected; # destroys life lines; # lambda gone out of scope; """"""Life lines to std::function data members""""""; """"""\; namespace BoundMethod2StdFunction {; class Base {; public:; virtual ~Base() {}. std::function<std::string()> execute;; std::string do_execute() {; return execute();; }; }; } """"""; """"""Multi-dim arrays ",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_datatypes.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_datatypes.py
Modifiability,inherit,inherits,"referable to have it working, so leave the discrepancy for; # now: python's aggressive end-of-life schedule will catch up soon enough; """"""Test object validity checking""""""; """"""Correctness of declared buffer shapes""""""; """"""\; namespace ShapeTester {; enum Enum{One, Two, Three};. template<typename T>; struct Foo {; T a[5];; T aa[5][4];; T aaa[5][4][3];; }; }""""""; # TODO: verify the following is for historic reasons and should be modified; # once bug #344 (bitbucket) is fixed; """"""Test usage of buffer sizing""""""; """"""Test usage of void* data""""""; # memory regulator recycles; # likewise; """"""\; namespace VoidP {; void* vvv[3][5][7];; struct Init {; Init() {; for (size_t i = 0; i < 3; ++i) {; for (size_t j = 0; j < 5; ++j) {; for (size_t k = 0; k < 7; ++k); vvv[i][j][k] = (void*)(i+j+k);; }; }; }; } _init; }""""""; """"""Usage of unsigned char* as byte array and std::byte*""""""; # p2; # p3; # The following create a unique type for fixed-size C arrays: ctypes.c_char_Array_9; # and neither inherits from a non-sized type nor implements the buffer interface.; # As such, it can't be handled. TODO?; #pbuf = ctypes.create_string_buffer(len(buf), buf); #assert f(pbuf, len(buf)) == total; """"""Function pointer passing""""""; # incorrect spelling; # b/c cached; """"""Passing callables through function pointers""""""; # call of void function; # call of function with reference argument; # callable that does not accept weak-ref; # error testing; # life line protected; # destroys life lines; # lambda gone out of scope; """"""Passing callables through std::function""""""; # call of void function; # call of function with reference argument; # callable that does not accept weak-ref; # error testing; # life line protected; # destroys life lines; # lambda gone out of scope; """"""Life lines to std::function data members""""""; """"""\; namespace BoundMethod2StdFunction {; class Base {; public:; virtual ~Base() {}. std::function<std::string()> execute;; std::string do_execute() {; return execute();; }; }; } """"""; """"""Multi-dim arrays ",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_datatypes.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_datatypes.py
Performance,cache,cached,"buffer sizing""""""; """"""Test usage of void* data""""""; # memory regulator recycles; # likewise; """"""\; namespace VoidP {; void* vvv[3][5][7];; struct Init {; Init() {; for (size_t i = 0; i < 3; ++i) {; for (size_t j = 0; j < 5; ++j) {; for (size_t k = 0; k < 7; ++k); vvv[i][j][k] = (void*)(i+j+k);; }; }; }; } _init; }""""""; """"""Usage of unsigned char* as byte array and std::byte*""""""; # p2; # p3; # The following create a unique type for fixed-size C arrays: ctypes.c_char_Array_9; # and neither inherits from a non-sized type nor implements the buffer interface.; # As such, it can't be handled. TODO?; #pbuf = ctypes.create_string_buffer(len(buf), buf); #assert f(pbuf, len(buf)) == total; """"""Function pointer passing""""""; # incorrect spelling; # b/c cached; """"""Passing callables through function pointers""""""; # call of void function; # call of function with reference argument; # callable that does not accept weak-ref; # error testing; # life line protected; # destroys life lines; # lambda gone out of scope; """"""Passing callables through std::function""""""; # call of void function; # call of function with reference argument; # callable that does not accept weak-ref; # error testing; # life line protected; # destroys life lines; # lambda gone out of scope; """"""Life lines to std::function data members""""""; """"""\; namespace BoundMethod2StdFunction {; class Base {; public:; virtual ~Base() {}. std::function<std::string()> execute;; std::string do_execute() {; return execute();; }; }; } """"""; """"""Multi-dim arrays of builtins""""""; """"""; template<class T, int nlayers>; struct MultiDimTest {; T* layers[nlayers];. MultiDimTest(int width, int height) {; for (int i=0; i<nlayers; ++i) {; layers[i] = new T[width*height];; for (int j=0; j<width*height; ++j); layers[i][j] = j*2;; }; }; ~MultiDimTest() { for (int i=0; i<nlayers; ++i) delete[] layers[i]; }; };; """"""; """"""Anonymous unions place there fields in the parent scope""""""; """"""\; namespace AnonUnion {; struct Event1 {; Event1() : num(1) { shrd.a = 5.; }; i",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_datatypes.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_datatypes.py
Security,access,access,"""""""Read access to instance public data and verify values""""""; # reading boolean type; # reading char types; # reading integer types; # reading floating point types; # complex<double> type; # complex<int> retains C++ type in all cases (but includes pythonization to; # resemble Python's complex more closely; # _Complex double type; # complex overloads; """"""; namespace ComplexOverload {; template<typename T>; struct CO {; CO(std::size_t sz) : m_size(sz), m_cplx(std::complex<T>(7,42)) {}; CO(std::complex<T> cplx) : m_size(42), m_cplx(cplx) {}. std::size_t m_size;; std::complex<T> m_cplx;; };; }""""""; # reading of enum types; # reading of boolean array; # reading of integer array types; # reading of floating point array types; # out-of-bounds checks; # can not access an instance member on the class; """"""Test write access to instance public data and verify values""""""; # boolean types through functions; # boolean types through data members; # char types through functions; # char types through data members; # integer types; # float types through functions; # float types through data members; # (non-)writing of enum types; # arrays; there will be pointer copies, so destroy the current ones; # integer arrays (skip int8_t and uint8_t as these are presented as (unsigned) char still); # buffer copies; # pointer copies; # can not write to constant data; """"""Test passing of array arguments""""""; # test arrays in mixed order, to give overload resolution a workout; # typed passing; # 'l' returns PyInt for small values in p2; # void* passing; # 'l' returns PyInt for small values in p2; # NULL/nullptr passing (will use short*); # raises SegfaultException; # id. id.; """"""Test read access to class public data and verify values""""""; # char types; # integer types; # floating point types; """"""Test write access to class public data and verify values""""""; # char types; # integer types; # floating point types; """"""Test the ranges of integer types""""""; # TODO: should these be TypeErrors, or should char/bool r",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_datatypes.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_datatypes.py
Testability,test,test,"pes; # reading integer types; # reading floating point types; # complex<double> type; # complex<int> retains C++ type in all cases (but includes pythonization to; # resemble Python's complex more closely; # _Complex double type; # complex overloads; """"""; namespace ComplexOverload {; template<typename T>; struct CO {; CO(std::size_t sz) : m_size(sz), m_cplx(std::complex<T>(7,42)) {}; CO(std::complex<T> cplx) : m_size(42), m_cplx(cplx) {}. std::size_t m_size;; std::complex<T> m_cplx;; };; }""""""; # reading of enum types; # reading of boolean array; # reading of integer array types; # reading of floating point array types; # out-of-bounds checks; # can not access an instance member on the class; """"""Test write access to instance public data and verify values""""""; # boolean types through functions; # boolean types through data members; # char types through functions; # char types through data members; # integer types; # float types through functions; # float types through data members; # (non-)writing of enum types; # arrays; there will be pointer copies, so destroy the current ones; # integer arrays (skip int8_t and uint8_t as these are presented as (unsigned) char still); # buffer copies; # pointer copies; # can not write to constant data; """"""Test passing of array arguments""""""; # test arrays in mixed order, to give overload resolution a workout; # typed passing; # 'l' returns PyInt for small values in p2; # void* passing; # 'l' returns PyInt for small values in p2; # NULL/nullptr passing (will use short*); # raises SegfaultException; # id. id.; """"""Test read access to class public data and verify values""""""; # char types; # integer types; # floating point types; """"""Test write access to class public data and verify values""""""; # char types; # integer types; # floating point types; """"""Test the ranges of integer types""""""; # TODO: should these be TypeErrors, or should char/bool raise; # ValueErrors? In any case, consistency is needed ...; """"""Test conversions between builtin type",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_datatypes.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_datatypes.py
Availability,error,error,"al is a lion"";; }. std::string identify_animal(Mouse*) {; return ""the animal is a mouse"";; }. }; """"""; # pythonize the animal release function to take ownership on return; """"""Existence and importability of created class""""""; """"""Introspection of newly created class/instances""""""; """"""Instantiate STL contaienrs with new class""""""; """"""Create a pretty repr for the new class""""""; """"""Implement and test a pythonizor""""""; # first time a new namespace is used, it can not be imported from; """"""Add operator+""""""; """"""; namespace Math {; Integer2 operator+(const Integer2& left, const Integer1& right) {; return left.m_data + right.m_data;; }; }""""""; """"""Bunch of zoo animals running around""""""; """"""Shared pointer transparency""""""; """"""; namespace Zoo {; std::shared_ptr<Lion> free_lion{new Lion{}};. std::string identify_animal_smart(std::shared_ptr<Lion>& smart) {; return ""the animal is a lion"";; }; }; """"""; """"""Templated free function""""""; # make sure cached values are actually looked up; # TODO: the following error message is rather confusing :(; """"""STL algorithm on std::string""""""; """"""Reduce available overloads to 1""""""; """"""namespace Advert01 {; class A {; public:; A(int) {}; A(double) {}; }; }""""""; """"""Use of opaque handles and ctypes.c_void_p""""""; ### void pointer as opaque handle; """"""namespace Advert02 {; typedef void* PicamHandle;; void Picam_OpenFirstCamera(PicamHandle* cam) {; *cam = new int(42);; }. bool Picam_CloseCamera(PicamHandle cam) {; bool ret = false;; if (*((int*)cam) == 42) ret = true;; delete (int*)cam;; return ret;; }; }""""""; # first approach; # second approch; """"""Use of (opaque) enum through ctypes.c_void_p""""""; """"""namespace Advert03 {; enum SomeEnum1 { AA = -1, BB = 42 };; void build_enum_array1(SomeEnum1** ptr, int* sz) {; *ptr = (SomeEnum1*)malloc(sizeof(SomeEnum1)*4);; *sz = 4;; (*ptr)[0] = AA; (*ptr)[1] = BB; (*ptr)[2] = AA; (*ptr)[3] = BB;; }. enum SomeEnum2 { CC = 1, DD = 42 };; void build_enum_array2(SomeEnum2** ptr, int* sz) {; *ptr = (SomeEnum2*)malloc(sizeof(SomeEnum2)*4);",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_doc_features.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_doc_features.py
Deployability,release,release,"{ return m_data; }; };; }""""""; """"""; namespace Zoo {. enum EAnimal { eLion, eMouse };. class Animal {; public:; virtual ~Animal() {}; virtual std::string make_sound() = 0;; };. class Lion : public Animal {; public:; virtual std::string make_sound() { return s_lion_sound; }; static std::string s_lion_sound;; };; std::string Lion::s_lion_sound = ""growl!"";. class Mouse : public Animal {; public:; virtual std::string make_sound() { return ""peep!""; }; };. Animal* release_animal(EAnimal animal) {; if (animal == eLion) return new Lion{};; if (animal == eMouse) return new Mouse{};; return nullptr;; }. std::string identify_animal(Lion*) {; return ""the animal is a lion"";; }. std::string identify_animal(Mouse*) {; return ""the animal is a mouse"";; }. }; """"""; # pythonize the animal release function to take ownership on return; """"""Existence and importability of created class""""""; """"""Introspection of newly created class/instances""""""; """"""Instantiate STL contaienrs with new class""""""; """"""Create a pretty repr for the new class""""""; """"""Implement and test a pythonizor""""""; # first time a new namespace is used, it can not be imported from; """"""Add operator+""""""; """"""; namespace Math {; Integer2 operator+(const Integer2& left, const Integer1& right) {; return left.m_data + right.m_data;; }; }""""""; """"""Bunch of zoo animals running around""""""; """"""Shared pointer transparency""""""; """"""; namespace Zoo {; std::shared_ptr<Lion> free_lion{new Lion{}};. std::string identify_animal_smart(std::shared_ptr<Lion>& smart) {; return ""the animal is a lion"";; }; }; """"""; """"""Templated free function""""""; # make sure cached values are actually looked up; # TODO: the following error message is rather confusing :(; """"""STL algorithm on std::string""""""; """"""Reduce available overloads to 1""""""; """"""namespace Advert01 {; class A {; public:; A(int) {}; A(double) {}; }; }""""""; """"""Use of opaque handles and ctypes.c_void_p""""""; ### void pointer as opaque handle; """"""namespace Advert02 {; typedef void* PicamHandle;; void Picam_OpenFirstCamer",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_doc_features.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_doc_features.py
Integrability,message,message,"al is a lion"";; }. std::string identify_animal(Mouse*) {; return ""the animal is a mouse"";; }. }; """"""; # pythonize the animal release function to take ownership on return; """"""Existence and importability of created class""""""; """"""Introspection of newly created class/instances""""""; """"""Instantiate STL contaienrs with new class""""""; """"""Create a pretty repr for the new class""""""; """"""Implement and test a pythonizor""""""; # first time a new namespace is used, it can not be imported from; """"""Add operator+""""""; """"""; namespace Math {; Integer2 operator+(const Integer2& left, const Integer1& right) {; return left.m_data + right.m_data;; }; }""""""; """"""Bunch of zoo animals running around""""""; """"""Shared pointer transparency""""""; """"""; namespace Zoo {; std::shared_ptr<Lion> free_lion{new Lion{}};. std::string identify_animal_smart(std::shared_ptr<Lion>& smart) {; return ""the animal is a lion"";; }; }; """"""; """"""Templated free function""""""; # make sure cached values are actually looked up; # TODO: the following error message is rather confusing :(; """"""STL algorithm on std::string""""""; """"""Reduce available overloads to 1""""""; """"""namespace Advert01 {; class A {; public:; A(int) {}; A(double) {}; }; }""""""; """"""Use of opaque handles and ctypes.c_void_p""""""; ### void pointer as opaque handle; """"""namespace Advert02 {; typedef void* PicamHandle;; void Picam_OpenFirstCamera(PicamHandle* cam) {; *cam = new int(42);; }. bool Picam_CloseCamera(PicamHandle cam) {; bool ret = false;; if (*((int*)cam) == 42) ret = true;; delete (int*)cam;; return ret;; }; }""""""; # first approach; # second approch; """"""Use of (opaque) enum through ctypes.c_void_p""""""; """"""namespace Advert03 {; enum SomeEnum1 { AA = -1, BB = 42 };; void build_enum_array1(SomeEnum1** ptr, int* sz) {; *ptr = (SomeEnum1*)malloc(sizeof(SomeEnum1)*4);; *sz = 4;; (*ptr)[0] = AA; (*ptr)[1] = BB; (*ptr)[2] = AA; (*ptr)[3] = BB;; }. enum SomeEnum2 { CC = 1, DD = 42 };; void build_enum_array2(SomeEnum2** ptr, int* sz) {; *ptr = (SomeEnum2*)malloc(sizeof(SomeEnum2)*4);",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_doc_features.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_doc_features.py
Modifiability,inherit,inheritance,"d::string call_abstract_method1(Abstract1* a) {; return a->abstract_method1();; }. std::string call_abstract_method2(Abstract2* a) {; return a->abstract_method2();; }. //-----; int global_function(int) {; return 42;; }. double global_function(double) {; return std::exp(1);; }. int call_int_int_function(int (*f)(int, int), int i1, int i2) {; return f(i1, i2);; }. template<class A, class B, class C = A>; C multiply(A a, B b) {; return C{a*b};; }. //-----; namespace Namespace {. class Concrete {; public:; class NestedClass {; public:; std::vector<int> m_v;; };. };. int global_function(int i) {; return 2*::global_function(i);; }. double global_function(double d) {; return 2*::global_function(d);; }. //-----; enum EFruit {kApple=78, kBanana=29, kCitrus=34};; enum class NamedClassEnum { E1 = 42 };. } // namespace Namespace. """"""; # unknown keyword; """"""Multiple cross-inheritance""""""; """"""Exception throwing and catching""""""; """"""; class Integer1 {; public:; Integer1(int i) : m_data(i) {}; int m_data;; };""""""; """"""; namespace Math {; class Integer2 : public Integer1 {; public:; using Integer1::Integer1;; operator int() { return m_data; }; };; }""""""; """"""; namespace Zoo {. enum EAnimal { eLion, eMouse };. class Animal {; public:; virtual ~Animal() {}; virtual std::string make_sound() = 0;; };. class Lion : public Animal {; public:; virtual std::string make_sound() { return s_lion_sound; }; static std::string s_lion_sound;; };; std::string Lion::s_lion_sound = ""growl!"";. class Mouse : public Animal {; public:; virtual std::string make_sound() { return ""peep!""; }; };. Animal* release_animal(EAnimal animal) {; if (animal == eLion) return new Lion{};; if (animal == eMouse) return new Mouse{};; return nullptr;; }. std::string identify_animal(Lion*) {; return ""the animal is a lion"";; }. std::string identify_animal(Mouse*) {; return ""the animal is a mouse"";; }. }; """"""; # pythonize the animal release function to take ownership on return; """"""Existence and importability of created class""""""; """"""I",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_doc_features.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_doc_features.py
Performance,cache,cached,"al is a lion"";; }. std::string identify_animal(Mouse*) {; return ""the animal is a mouse"";; }. }; """"""; # pythonize the animal release function to take ownership on return; """"""Existence and importability of created class""""""; """"""Introspection of newly created class/instances""""""; """"""Instantiate STL contaienrs with new class""""""; """"""Create a pretty repr for the new class""""""; """"""Implement and test a pythonizor""""""; # first time a new namespace is used, it can not be imported from; """"""Add operator+""""""; """"""; namespace Math {; Integer2 operator+(const Integer2& left, const Integer1& right) {; return left.m_data + right.m_data;; }; }""""""; """"""Bunch of zoo animals running around""""""; """"""Shared pointer transparency""""""; """"""; namespace Zoo {; std::shared_ptr<Lion> free_lion{new Lion{}};. std::string identify_animal_smart(std::shared_ptr<Lion>& smart) {; return ""the animal is a lion"";; }; }; """"""; """"""Templated free function""""""; # make sure cached values are actually looked up; # TODO: the following error message is rather confusing :(; """"""STL algorithm on std::string""""""; """"""Reduce available overloads to 1""""""; """"""namespace Advert01 {; class A {; public:; A(int) {}; A(double) {}; }; }""""""; """"""Use of opaque handles and ctypes.c_void_p""""""; ### void pointer as opaque handle; """"""namespace Advert02 {; typedef void* PicamHandle;; void Picam_OpenFirstCamera(PicamHandle* cam) {; *cam = new int(42);; }. bool Picam_CloseCamera(PicamHandle cam) {; bool ret = false;; if (*((int*)cam) == 42) ret = true;; delete (int*)cam;; return ret;; }; }""""""; # first approach; # second approch; """"""Use of (opaque) enum through ctypes.c_void_p""""""; """"""namespace Advert03 {; enum SomeEnum1 { AA = -1, BB = 42 };; void build_enum_array1(SomeEnum1** ptr, int* sz) {; *ptr = (SomeEnum1*)malloc(sizeof(SomeEnum1)*4);; *sz = 4;; (*ptr)[0] = AA; (*ptr)[1] = BB; (*ptr)[2] = AA; (*ptr)[3] = BB;; }. enum SomeEnum2 { CC = 1, DD = 42 };; void build_enum_array2(SomeEnum2** ptr, int* sz) {; *ptr = (SomeEnum2*)malloc(sizeof(SomeEnum2)*4);",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_doc_features.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_doc_features.py
Security,access,access,"; int total = 0;; for (int i = 0; i < sz; ++i) total += them[i]->i;; return total;; } }""""""; # initialization on python side; # initialization on C++ side; """"""namespace Advert04 {; void ptr2ptr_init(SomeStruct** ref) {; *ref = new SomeStruct(42);; } }""""""; """"""Example of ptr-ptr with array""""""; """"""namespace Advert05 {; struct SomeStruct { int i; };. void create_them(SomeStruct** them, int* sz) {; *sz = 4;; *them = new SomeStruct[*sz];; for (int i = 0; i < *sz; ++i) (*them)[i].i = i*i;; } }""""""; """"""Example of ctypes.c_char_p usage""""""; """"""namespace Advert06 {; intptr_t createit(const char** out) {; *out = (char*)malloc(4);; return (intptr_t)*out;; }; intptr_t destroyit(const char* in) {; intptr_t out = (intptr_t)in;; free((void*)in);; return out;; } }""""""; """"""Example of array of array usage""""""; """"""; #define NREADOUTS %d; #define NPIXELS %d; namespace Advert07 {; struct S {; S() {; uint16_t** readout = new uint16_t*[NREADOUTS];; for (int i=0; i<4; ++i) {; readout[i] = new uint16_t[NPIXELS];; for (int j=0; j<NPIXELS; ++j) readout[i][j] = i*NPIXELS+j;; }; fField = (void*)readout;; }; ~S() {; for (int i = 0; i < 4; ++i) delete[] ((uint16_t**)fField)[i];; delete [] (uint16_t**)fField;; }; void* fField;; }; }""""""; """"""Example of access to array of void ptrs""""""; """"""; namespace VoidPtrArray {; typedef struct _name {; _name() { p[0] = (void*)0x1; p[1] = (void*)0x2; p[2] = (void*)0x3; }; void* p[3];; } name;; }""""""; """"""Example of customized str""""""; """"""\; namespace TopologicCore {. class Shell {; public:; virtual std::string GetTypeAsString() const {; return ""hi there!"";; }; }; }""""""; """"""Test code posted in the LLVM blog posting""""""; # The series of tests below mostly exists already in other places, but these; # were used as examples for the CaaS' cppyy presentation and are preserved here.; """"""\; namespace talk_examples {; struct MyClass {; MyClass(int i) : fData(i) {}; virtual ~MyClass() {}; virtual int add(int i) {; return fData + i;; }; int fData;; };}""""""; """"""Run-time template instantia",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_doc_features.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_doc_features.py
Testability,test,test,"# touch __version__ as a test; """"""; #include <cmath>; #include <iostream>; #include <vector>. //-----; unsigned int gUint = 0;. //-----; class Abstract {; public:; virtual ~Abstract() {}; virtual std::string abstract_method() = 0;; virtual void concrete_method() = 0;; };. void Abstract::concrete_method() {; std::cout << ""called Abstract::concrete_method"" << std::endl;; }. //-----; class Concrete : Abstract {; public:; Concrete(int n=42) : m_int(n), m_const_int(17) {}; ~Concrete() {}. virtual std::string abstract_method() {; return ""called Concrete::abstract_method"";; }. virtual void concrete_method() {; std::cout << ""called Concrete::concrete_method"" << std::endl;; }. void array_method(int* ad, int size) {; for (int i=0; i < size; ++i); std::cerr << ad[i] << ' ';; std::cerr << std::endl;; }. void array_method(double* ad, int size) {; for (int i=0; i < size; ++i); std::cerr << ad[i] << ' ';; std::cerr << std::endl;; }. void uint_ref_assign(unsigned int& target, unsigned int value) {; target = value;; }. Abstract* show_autocast() {; return this;; }. operator const char*() {; return ""Hello operator const char*!"";; }. public:; double m_data[4];; int m_int;; const int m_const_int;. static int s_int;; };. typedef Concrete Concrete_t;. int Concrete::s_int = 321;. std::string call_abstract_method(Abstract* a) {; return a->abstract_method();; }. //-----; class Abstract1 {; public:; virtual ~Abstract1() {}; virtual std::string abstract_method1() = 0;; };. class Abstract2 {; public:; virtual ~Abstract2() {}; virtual std::string abstract_method2() = 0;; };. std::string call_abstract_method1(Abstract1* a) {; return a->abstract_method1();; }. std::string call_abstract_method2(Abstract2* a) {; return a->abstract_method2();; }. //-----; int global_function(int) {; return 42;; }. double global_function(double) {; return std::exp(1);; }. int call_int_int_function(int (*f)(int, int), int i1, int i2) {; return f(i1, i2);; }. template<class A, class B, class C = A>; C multiply(A a, B b) ",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_doc_features.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_doc_features.py
Testability,assert,assert,"""""""Basic creation of an Eigen::Matrix and Eigen::Vector""""""; """"""Comma insertion overload""""""; # TODO: this calls a conversion to int ...; #m.resize(cppyy.gbl.Eigen.NoChange_t(), 3); #assert m.rows() == 2; #assert m.cols() == 3; # equivalent of 'm << 12, 11, ..., 1' in C++; # TODO: the insertion operator is a template that expect only the base class; #(matB << matA).__comma__(matA/10).__comma__(matA/10).__comma__(matA); # the following is equivalent to:; # (v << 1).__comma__(2).__comma__(3).__comma__(4).__comma__(5); """"""Matrices and vectors""""""; # 'dynamic' matrices/vectors; # 'static' matrices/vectors; """"""Resize on assignment""""""; """"""Use of Map (used to crash)""""""; # used to crash",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_eigen.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_eigen.py
Availability,failure,failure,"""""""Test failure to load dictionary""""""; """"""Test (non-)access to missing classes""""""; """"""Test reporting when providing wrong arguments""""""; """"""Test arguments that are yet unsupported""""""; # allowing access to e.m_pp_no_such is debatable, but it allows a typed address; # to be passed back into C++, which may be useful ...; """"""Test addressof() error reporting""""""; # regression (m_int is 0 by default, but its address is not); # see also test08_void_pointer_passing in test_advancedcpp.py; """"""Test that using an incorrect self argument raises""""""; """"""Test that an unnamed enum does not cause infinite recursion""""""; """"""Test that an unhandled scoped data member does not cause infinite recursion""""""; """"""Access to global vars with an operator bool() returning False""""""; """"""Check contents of documentation""""""; # raises TypeError; # raises TypeError; # TODO: pypy-c does not indicate which argument failed to convert, CPython does; # likewise there are still minor differences in descriptiveness of messages; #assert ""no converter available for 'fragile::no_such_class*'"" in str(e); #assert ""char or small int type expected"" in str(e); #assert ""int/long conversion expects an integer object"" in str(e); # raises TypeError; """"""Test __dir__ method""""""; # classes; # namespace; # TODO: think this through ... probably want this, but interferes with; # the (new) policy of lazy lookups; #assert 'fglobal' in members # function; # variable; # GetAllCppNames() behaves differently from python dir() but providing the full; # set, which is then filtered in dir(); check both; """"""\; #ifdef _MSC_VER; #define CPPYY_IMPORT extern __declspec(dllimport); #else; #define CPPYY_IMPORT extern; #endif. namespace Cppyy {. typedef size_t TCppScope_t;. CPPYY_IMPORT TCppScope_t GetScope(const std::string& scope_name);; CPPYY_IMPORT void GetAllCppNames(TCppScope_t scope, std::set<std::string>& cppnames);. }""""""; """"""\; namespace GG {; struct S {; int _a;; int _c;; S(int a, int c): _a{a}, _c{c} { }; S(): _a{0}, _c{0} { }; bool oper",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_fragile.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_fragile.py
Integrability,message,messages,"""""""Test failure to load dictionary""""""; """"""Test (non-)access to missing classes""""""; """"""Test reporting when providing wrong arguments""""""; """"""Test arguments that are yet unsupported""""""; # allowing access to e.m_pp_no_such is debatable, but it allows a typed address; # to be passed back into C++, which may be useful ...; """"""Test addressof() error reporting""""""; # regression (m_int is 0 by default, but its address is not); # see also test08_void_pointer_passing in test_advancedcpp.py; """"""Test that using an incorrect self argument raises""""""; """"""Test that an unnamed enum does not cause infinite recursion""""""; """"""Test that an unhandled scoped data member does not cause infinite recursion""""""; """"""Access to global vars with an operator bool() returning False""""""; """"""Check contents of documentation""""""; # raises TypeError; # raises TypeError; # TODO: pypy-c does not indicate which argument failed to convert, CPython does; # likewise there are still minor differences in descriptiveness of messages; #assert ""no converter available for 'fragile::no_such_class*'"" in str(e); #assert ""char or small int type expected"" in str(e); #assert ""int/long conversion expects an integer object"" in str(e); # raises TypeError; """"""Test __dir__ method""""""; # classes; # namespace; # TODO: think this through ... probably want this, but interferes with; # the (new) policy of lazy lookups; #assert 'fglobal' in members # function; # variable; # GetAllCppNames() behaves differently from python dir() but providing the full; # set, which is then filtered in dir(); check both; """"""\; #ifdef _MSC_VER; #define CPPYY_IMPORT extern __declspec(dllimport); #else; #define CPPYY_IMPORT extern; #endif. namespace Cppyy {. typedef size_t TCppScope_t;. CPPYY_IMPORT TCppScope_t GetScope(const std::string& scope_name);; CPPYY_IMPORT void GetAllCppNames(TCppScope_t scope, std::set<std::string>& cppnames);. }""""""; """"""\; namespace GG {; struct S {; int _a;; int _c;; S(int a, int c): _a{a}, _c{c} { }; S(): _a{0}, _c{0} { }; bool oper",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_fragile.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_fragile.py
Modifiability,variab,variable,"; """"""Test that using an incorrect self argument raises""""""; """"""Test that an unnamed enum does not cause infinite recursion""""""; """"""Test that an unhandled scoped data member does not cause infinite recursion""""""; """"""Access to global vars with an operator bool() returning False""""""; """"""Check contents of documentation""""""; # raises TypeError; # raises TypeError; # TODO: pypy-c does not indicate which argument failed to convert, CPython does; # likewise there are still minor differences in descriptiveness of messages; #assert ""no converter available for 'fragile::no_such_class*'"" in str(e); #assert ""char or small int type expected"" in str(e); #assert ""int/long conversion expects an integer object"" in str(e); # raises TypeError; """"""Test __dir__ method""""""; # classes; # namespace; # TODO: think this through ... probably want this, but interferes with; # the (new) policy of lazy lookups; #assert 'fglobal' in members # function; # variable; # GetAllCppNames() behaves differently from python dir() but providing the full; # set, which is then filtered in dir(); check both; """"""\; #ifdef _MSC_VER; #define CPPYY_IMPORT extern __declspec(dllimport); #else; #define CPPYY_IMPORT extern; #endif. namespace Cppyy {. typedef size_t TCppScope_t;. CPPYY_IMPORT TCppScope_t GetScope(const std::string& scope_name);; CPPYY_IMPORT void GetAllCppNames(TCppScope_t scope, std::set<std::string>& cppnames);. }""""""; """"""\; namespace GG {; struct S {; int _a;; int _c;; S(int a, int c): _a{a}, _c{c} { }; S(): _a{0}, _c{0} { }; bool operator<(int i) { return i < (_a+_c); }; }; }""""""; # is __init__; """"""Test ability to import from namespace (or fail with ImportError)""""""; # TODO: namespaces aren't loaded (and thus not added to sys.modules); # with just the from ... import statement; actual use is needed; # according to warnings, can't test ""import *"" ...; # test writability of __module__; # classes in namespace should inherit; # as should objects; """"""Test proper handling when a hierarchy is not fully available"""""";",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_fragile.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_fragile.py
Performance,load,load,"""""""Test failure to load dictionary""""""; """"""Test (non-)access to missing classes""""""; """"""Test reporting when providing wrong arguments""""""; """"""Test arguments that are yet unsupported""""""; # allowing access to e.m_pp_no_such is debatable, but it allows a typed address; # to be passed back into C++, which may be useful ...; """"""Test addressof() error reporting""""""; # regression (m_int is 0 by default, but its address is not); # see also test08_void_pointer_passing in test_advancedcpp.py; """"""Test that using an incorrect self argument raises""""""; """"""Test that an unnamed enum does not cause infinite recursion""""""; """"""Test that an unhandled scoped data member does not cause infinite recursion""""""; """"""Access to global vars with an operator bool() returning False""""""; """"""Check contents of documentation""""""; # raises TypeError; # raises TypeError; # TODO: pypy-c does not indicate which argument failed to convert, CPython does; # likewise there are still minor differences in descriptiveness of messages; #assert ""no converter available for 'fragile::no_such_class*'"" in str(e); #assert ""char or small int type expected"" in str(e); #assert ""int/long conversion expects an integer object"" in str(e); # raises TypeError; """"""Test __dir__ method""""""; # classes; # namespace; # TODO: think this through ... probably want this, but interferes with; # the (new) policy of lazy lookups; #assert 'fglobal' in members # function; # variable; # GetAllCppNames() behaves differently from python dir() but providing the full; # set, which is then filtered in dir(); check both; """"""\; #ifdef _MSC_VER; #define CPPYY_IMPORT extern __declspec(dllimport); #else; #define CPPYY_IMPORT extern; #endif. namespace Cppyy {. typedef size_t TCppScope_t;. CPPYY_IMPORT TCppScope_t GetScope(const std::string& scope_name);; CPPYY_IMPORT void GetAllCppNames(TCppScope_t scope, std::set<std::string>& cppnames);. }""""""; """"""\; namespace GG {; struct S {; int _a;; int _c;; S(int a, int c): _a{a}, _c{c} { }; S(): _a{0}, _c{0} { }; bool oper",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_fragile.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_fragile.py
Safety,abort,abortive,"active access to the Cling global scope""""""; """"""Setting of global gDebug variable""""""; """"""Check availability of ASAN with gcc""""""; """"""Check error reporting of cppyy.cppdef""""""; # brings in symbol from library; # redefine symbol, leading to duplicate; """"""\; namespace fragile {; int add42(int i) { return i + 42; }; }""""""; # missing return statement; """"""\; namespace fragile {; double add42d(double d) { d + 42.; return d; }; }""""""; # mix of error and warning; # redefine symbol, leading to duplicate; """"""\; namespace fragile {; float add42f(float d) { d + 42.f; }; int add42(int i) { return i + 42; }; }""""""; """"""Test access to C++ pre-processor macro's""""""; """"""Pickling of enum types""""""; """"""; enum MyPickleEnum { PickleFoo, PickleBar };; namespace MyPickleNamespace {; enum MyPickleEnum { PickleFoo, PickleBar };; }""""""; """"""memoryview of an empty array""""""; # used to crash in PyObject_CheckBuffer on Linux; """"""Offset calculation of vector datamember""""""; # used to crash on Mac arm64; """"""Conversion from abortive signals to Python exceptions""""""; # can only recover once from each error on Windows, which is functionally; # enough, but precludes further testing here (change: now drop all, see above,; # as on some MSVC builds, no signals are caught ??); """"""STL classes should live in std:: only""""""; # inject a vector in the global namespace; """"""Standard int types live in both global and std::""""""; # TODO: get the types to match exactly as well; """"""Redefines of std:: typedefs should be possible in global""""""; """"""; using uint = unsigned int;; using ushort = unsigned short;; using uchar = unsigned char;; using byte = unsigned char;; """"""; """"""Test some functions that previously crashed""""""; """"""; enum ELogLevel {; kLogEmerg = 0,; kLogAlert = 1,; kLogCrit = 2,; kLogErr = 3,; kLogWarning = 4,; kLogNotice = 5,; kLogInfo = 6,; kLogDebug = 7; };""""""; """"""Test compatibility of span under C++2a compilers that support it""""""; """"""\; #if __has_include(<span>); #include <span>; std::span<int> my_test_span1;; #endif; """"""",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_fragile.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_fragile.py
Security,access,access,"""""""Test failure to load dictionary""""""; """"""Test (non-)access to missing classes""""""; """"""Test reporting when providing wrong arguments""""""; """"""Test arguments that are yet unsupported""""""; # allowing access to e.m_pp_no_such is debatable, but it allows a typed address; # to be passed back into C++, which may be useful ...; """"""Test addressof() error reporting""""""; # regression (m_int is 0 by default, but its address is not); # see also test08_void_pointer_passing in test_advancedcpp.py; """"""Test that using an incorrect self argument raises""""""; """"""Test that an unnamed enum does not cause infinite recursion""""""; """"""Test that an unhandled scoped data member does not cause infinite recursion""""""; """"""Access to global vars with an operator bool() returning False""""""; """"""Check contents of documentation""""""; # raises TypeError; # raises TypeError; # TODO: pypy-c does not indicate which argument failed to convert, CPython does; # likewise there are still minor differences in descriptiveness of messages; #assert ""no converter available for 'fragile::no_such_class*'"" in str(e); #assert ""char or small int type expected"" in str(e); #assert ""int/long conversion expects an integer object"" in str(e); # raises TypeError; """"""Test __dir__ method""""""; # classes; # namespace; # TODO: think this through ... probably want this, but interferes with; # the (new) policy of lazy lookups; #assert 'fglobal' in members # function; # variable; # GetAllCppNames() behaves differently from python dir() but providing the full; # set, which is then filtered in dir(); check both; """"""\; #ifdef _MSC_VER; #define CPPYY_IMPORT extern __declspec(dllimport); #else; #define CPPYY_IMPORT extern; #endif. namespace Cppyy {. typedef size_t TCppScope_t;. CPPYY_IMPORT TCppScope_t GetScope(const std::string& scope_name);; CPPYY_IMPORT void GetAllCppNames(TCppScope_t scope, std::set<std::string>& cppnames);. }""""""; """"""\; namespace GG {; struct S {; int _a;; int _c;; S(int a, int c): _a{a}, _c{c} { }; S(): _a{0}, _c{0} { }; bool oper",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_fragile.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_fragile.py
Testability,assert,assert,"""""""Test failure to load dictionary""""""; """"""Test (non-)access to missing classes""""""; """"""Test reporting when providing wrong arguments""""""; """"""Test arguments that are yet unsupported""""""; # allowing access to e.m_pp_no_such is debatable, but it allows a typed address; # to be passed back into C++, which may be useful ...; """"""Test addressof() error reporting""""""; # regression (m_int is 0 by default, but its address is not); # see also test08_void_pointer_passing in test_advancedcpp.py; """"""Test that using an incorrect self argument raises""""""; """"""Test that an unnamed enum does not cause infinite recursion""""""; """"""Test that an unhandled scoped data member does not cause infinite recursion""""""; """"""Access to global vars with an operator bool() returning False""""""; """"""Check contents of documentation""""""; # raises TypeError; # raises TypeError; # TODO: pypy-c does not indicate which argument failed to convert, CPython does; # likewise there are still minor differences in descriptiveness of messages; #assert ""no converter available for 'fragile::no_such_class*'"" in str(e); #assert ""char or small int type expected"" in str(e); #assert ""int/long conversion expects an integer object"" in str(e); # raises TypeError; """"""Test __dir__ method""""""; # classes; # namespace; # TODO: think this through ... probably want this, but interferes with; # the (new) policy of lazy lookups; #assert 'fglobal' in members # function; # variable; # GetAllCppNames() behaves differently from python dir() but providing the full; # set, which is then filtered in dir(); check both; """"""\; #ifdef _MSC_VER; #define CPPYY_IMPORT extern __declspec(dllimport); #else; #define CPPYY_IMPORT extern; #endif. namespace Cppyy {. typedef size_t TCppScope_t;. CPPYY_IMPORT TCppScope_t GetScope(const std::string& scope_name);; CPPYY_IMPORT void GetAllCppNames(TCppScope_t scope, std::set<std::string>& cppnames);. }""""""; """"""\; namespace GG {; struct S {; int _a;; int _c;; S(int a, int c): _a{a}, _c{c} { }; S(): _a{0}, _c{0} { }; bool oper",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_fragile.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_fragile.py
Testability,test,test,"""""""Leak-check 'func', given args and kwds""""""; # if tmpl_args is provided as a keyword, then this is a templated; # function that is to be found on each call python-side; # warmup function (TOOD: why doesn't once suffice?); # actually, 2 seems to be enough; # number of iterations; # leak check; """"""Leak test of free functions""""""; """"""\; namespace LeakCheck {; void free_f() {}; void free_f_ol(int) {}; void free_f_ol(std::string s) {}; template<class T> void free_f_ol(T) {}; int free_f_ret1() { return 27; }; std::string free_f_ret2() { return ""aap""; }; }""""""; # template; """"""Leak test of static methods""""""; """"""\; namespace LeakCheck {; class MyClass02 {; public:; static void static_method() {}; static void static_method_ol(int) {}; static void static_method_ol(std::string s) {}; template<class T> static void static_method_ol(T) {}; static std::string static_method_ret() { return ""aap""; }; }; }""""""; # template; """"""Leak test of methods""""""; """"""\; namespace LeakCheck {; class MyClass03 {; public:; void method() {}; void method_ol(int) {}; void method_ol(std::string s) {}; std::string method_ret() { return ""aap""; }; template<class T> void method_ol(T) {}; }; }""""""; # template; """"""Leak test for functions with default arguments""""""; """"""\; namespace LeakCheck {; double free_default(int a=11, float b=22.f, double c=33.) {; return a*b*c;; }. class MyClass04 {; public:; static double static_default(int a=11, float b=22.f, double c=33.) {; return free_default(a, b, c);; }; double method_default(int a=11, float b=22.f, double c=33.) {; return free_default(a, b, c);; }; }; }""""""; # TODO: no keyword arguments for static methods yet; #for m in [ns.MyClass04, ns.MyClass04()]:; # self.check_func(m, 'static_default'); # self.check_func(m, 'static_default', a=-99); # self.check_func(m, 'static_default', b=-99); # self.check_func(m, 'static_default', c=-99); """"""Leak test of aggregate creation""""""; """"""\; namespace LeakCheck {; typedef enum _TYPE { DATA=0, SHAPE } TYPE;. struct SomePOD {; int fInt;; do",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_leakcheck.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_leakcheck.py
Modifiability,variab,variable,"y/ctypes.html#fundamental-data-types; #; # ctypes type C type Python type; # ------------------------------------------------------------------------------; # c_char_p char* (NULL terminated) string or None; # c_wchar_p wchar_t* (NULL terminated) unicode or None; # c_void_p void* int/long or None; """"""If types don't match with ctypes, expect exceptions""""""; """"""Test passing of numpy bool array""""""; """"""Test passting of const char*[]""""""; # debatable, but the following works:; """"""Test passting of const char**&""""""; # IN parameter case; """"""\; namespace ConstCharStarStarRef {; int initialize(int& argc, char**& argv) {; argv[0][0] = 'H';; argv[1][0] = 'W';; return argc;; } }""""""; # OUT parameter case; """"""\; namespace ConstCharStarStarRef {; void fill(int& argc, char**& argv) {; argc = 2;; argv = new char*[argc];; argv[0] = new char[6]; strcpy(argv[0], ""Hello"");; argv[1] = new char[6]; strcpy(argv[1], ""World"");; } }""""""; """"""Null low level view as empty list""""""; """"""\; namespace NullArray {; double* gime_null() { return nullptr; }; }""""""; """"""Test usage of __array__ from numpy""""""; """"""\; namespace ArrayConversions {; int ivals[] = {1, 2, 3};; }""""""; # default behavior; """"""Use of arrays in template types""""""; """"""Use of gmpxx array types in templates""""""; """"""\; namespace test15_templated_arrays_gmpxx::vector {; template <typename T>; using value_type = typename T::value_type;; }""""""; """"""Access and use of 2D data members""""""; """"""Direct assignment of 2D arrays""""""; # copy assignment; # size checking for copy assignment; # pointer assignment; """"""Access and use of 3D data members""""""; """"""Use of malloc to create multi-dim arrays""""""; """"""\; namespace MallocChecker {; template<typename T>; struct Foo {; T* bar;. Foo() {}; Foo(T* other) : bar(other) {}. bool eq(T* other) { return bar == other; }; };. template<typename T>; auto create(T* other) {; return Foo<T>(other);; } }""""""; # variable assignment; # pointer passed to the constructor; # pointer passed to a function; """"""Multi-dimensional char arrays""""""",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_lowlevel.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_lowlevel.py
Testability,test,tests,"ress(SomeObject* ptr) { return (intptr_t)ptr; }; uintptr_t get_deref(void* ptr) { return (uintptr_t)(*(void**)ptr); }; }""""""; """"""Use arrays for pass-by-ref""""""; # boolean type; # char types (as data); # integer types; # floating point types; """"""Use ctypes for pass-by-ref/ptr""""""; # See:; # https://docs.python.org/2/library/ctypes.html#fundamental-data-types; #; # ctypes type C type Python type; # ------------------------------------------------------------------------------; # c_bool _Bool bool (1); #; # c_char char 1-character string; # c_wchar wchar_t 1-character unicode string; # c_byte char int; # c_ubyte unsigned char int; #; # c_int8 signed char int; # c_uint8 unsigned char int; # c_short short int; # c_ushort unsigned short int; # c_int int int; # c_uint unsigned int int/long; # c_long long int/long; # c_ulong unsigned long int/long; # c_longlong __int64 or long long int/long; # c_ulonglong unsigned __int64 or unsigned long long int/long; #; # c_float float float; # c_double double float; # c_longdouble long double float; ### pass by reference/pointer and set value back; # boolean type; # char types; # integer types; # floating point types; ### pass by pointer and set value back, now using byref (not recommended); # boolean type; # char types; # integer types; # floating point types; ### pass by ptr/ptr with allocation (ptr/ptr is ambiguous in it's task, so many; # types are allowed to pass; this tests allocation into the pointer); # boolean type; # char types; # integer types; # template resolves as signed char*; # template resolves as unsigned char*; # floating point types; """"""Use ctypes for pass-by-ptr/ptr-ptr""""""; # See:; # https://docs.python.org/2/library/ctypes.html#fundamental-data-types; #; # ctypes type C type Python type; # ------------------------------------------------------------------------------; # c_char_p char* (NULL terminated) string or None; # c_wchar_p wchar_t* (NULL terminated) unicode or None; # c_void_p void* int/long or None; """"""If types",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_lowlevel.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_lowlevel.py
Modifiability,variab,variables,"""""; """"""\; namespace ReflexTest {; int free1() { return 42; }; double free2() { return 42.; }. class MyData_m1 {};; }""""""; """"""Data member reflection tooling""""""; """"""\; namespace ReflexTest {; class MyData_d1 {; public:; int m_int;; double m_double;; }; }""""""; """"""Numba-JITing of a compiled free function""""""; """"""Numba-JITing of Cling-JITed templated free function""""""; """"""Numba-JITing of a free function taking a proxy argument for field access""""""; # note: need a sizable array to outperform given the unboxing overhead; """"""Numba-JITing of a free function taking a proxy argument for method access""""""; # note: need a sizable array to outperform given the unboxing overhead; """"""Numba-JITing of functions with multiple arguments""""""; """"""; double add_double(double a, double b, double c) {; double d = a + b + c;; return d;; }; """"""; """"""Numba-JITing of a free template function that recieves more than one template arg""""""; """"""; namespace NumbaSupportExample {; template<typename T1>; T1 add(T1 a, T1 b) { return a + b; }; }""""""; """"""Numba-JITing of various data types""""""; """"""\; namespace NumbaDTT {; struct M%d { M%d(%s f) : fField(f) {};; %s buf, fField;; }; }""""""; # 'int8_t', 'uint8_t', # TODO b/c check using return type fails; """"""Numba-JITing of a function that returns an object""""""; """"""Numba-JITing of a free template function that recieves multiple template args with non types""""""; """"""; namespace NumbaSupportExample {; template<typename T1, typename T2>; double add(double a, T1 b, T2 c) { return a + b + c; }; }""""""; """"""; int64_t& ref_add(int64_t x, int64_t y) {; int64_t c = x + y;; static int64_t result = c;; return c;; }; """"""; """"""Numba-JITing of a increment method belonging to a class, and also swaps the pointers and reflects the change on the python ctypes variables""""""; """"""; namespace RefTest {; class Box{; public:; long a;; long *b;; long *c;; Box(long i, long& j, long& k){; a = i;; b = &j;; c = &k;; }. void swap_ref(long &a, long &b) {; long temp = a;; a = b;; b = temp;; }. void inc(long* valu",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_numba.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_numba.py
Performance,perform,performs,";; %s buf, fField;; }; }""""""; # 'int8_t', 'uint8_t', # TODO b/c check using return type fails; """"""Numba-JITing of a function that returns an object""""""; """"""Numba-JITing of a free template function that recieves multiple template args with non types""""""; """"""; namespace NumbaSupportExample {; template<typename T1, typename T2>; double add(double a, T1 b, T2 c) { return a + b + c; }; }""""""; """"""; int64_t& ref_add(int64_t x, int64_t y) {; int64_t c = x + y;; static int64_t result = c;; return c;; }; """"""; """"""Numba-JITing of a increment method belonging to a class, and also swaps the pointers and reflects the change on the python ctypes variables""""""; """"""; namespace RefTest {; class Box{; public:; long a;; long *b;; long *c;; Box(long i, long& j, long& k){; a = i;; b = &j;; c = &k;; }. void swap_ref(long &a, long &b) {; long temp = a;; a = b;; b = temp;; }. void inc(long* value) {; (*value)++;; }; };; }; """"""; """"""Numba-JITing of a method that performs scalar addition to a std::vector initialised through pointers """"""; """"""; template<typename T>; std::vector<T> make_vector(const std::vector<T>& v, std::vector<T> l) {; std::vector<T> u(l);; u.insert(u.end(), v.begin(), v.end());; return u;; }; namespace RefTest {; class BoxVector{; public:; std::vector<long>* a;. BoxVector() : a(new std::vector<long>()) {}; BoxVector(std::vector<long>* i) : a(i){}. void square_vec(){; for (auto& num : *a) {; num = num * num;; }; }. void add_2_vec(long k){; for (auto& num : *a) {; num = num + k;; }; }. void append_vector(const std::vector<long>& value) {; *a = make_vector(value, *a);; }; };; }; """"""; # We use b to run square_vec where the values must be < 4 to avoid exceeding longs max value; """"""Numba-JITing of a dot_product method of a class that stores pointers to std::vectors on the python side""""""; """"""; namespace RefTest {; class DotVector{; private:; std::vector<long>* a;; std::vector<long>* b;. public:; long g = 0;; long *res = &g;; DotVector(std::vector<long>* i, std::vector<long>* j) : a(i), b(j",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_numba.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_numba.py
Safety,avoid,avoid," long a;; long *b;; long *c;; Box(long i, long& j, long& k){; a = i;; b = &j;; c = &k;; }. void swap_ref(long &a, long &b) {; long temp = a;; a = b;; b = temp;; }. void inc(long* value) {; (*value)++;; }; };; }; """"""; """"""Numba-JITing of a method that performs scalar addition to a std::vector initialised through pointers """"""; """"""; template<typename T>; std::vector<T> make_vector(const std::vector<T>& v, std::vector<T> l) {; std::vector<T> u(l);; u.insert(u.end(), v.begin(), v.end());; return u;; }; namespace RefTest {; class BoxVector{; public:; std::vector<long>* a;. BoxVector() : a(new std::vector<long>()) {}; BoxVector(std::vector<long>* i) : a(i){}. void square_vec(){; for (auto& num : *a) {; num = num * num;; }; }. void add_2_vec(long k){; for (auto& num : *a) {; num = num + k;; }; }. void append_vector(const std::vector<long>& value) {; *a = make_vector(value, *a);; }; };; }; """"""; # We use b to run square_vec where the values must be < 4 to avoid exceeding longs max value; """"""Numba-JITing of a dot_product method of a class that stores pointers to std::vectors on the python side""""""; """"""; namespace RefTest {; class DotVector{; private:; std::vector<long>* a;; std::vector<long>* b;. public:; long g = 0;; long *res = &g;; DotVector(std::vector<long>* i, std::vector<long>* j) : a(i), b(j) {}. long self_dot_product() {; long result = 0;; size_t size = a->size(); // Cache the vector size; const long* data_a = a->data();; const long* data_b = b->data();. for (size_t i = 0; i < size; ++i) {; result += data_a[i] * data_b[i];; }; return result;; }. long dot_product(const std::vector<long>& vec1, const std::vector<long>& vec2) {; long result = 0;; for (size_t i = 0; i < vec1.size(); ++i) {; result += vec1[i] * vec2[i];; }; return result;; }; };; }""""""; # TODO : Interestingly njit fails while passing a list of std.vectors because it; # cannot reflect element of reflected container: reflected list(reflected list(CppClass(std::vector<long>))<iv=None>)<iv=None>; # vec_list = [];",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_numba.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_numba.py
Security,access,access,"""""; """"""\; namespace ReflexTest {; int free1() { return 42; }; double free2() { return 42.; }. class MyData_m1 {};; }""""""; """"""Data member reflection tooling""""""; """"""\; namespace ReflexTest {; class MyData_d1 {; public:; int m_int;; double m_double;; }; }""""""; """"""Numba-JITing of a compiled free function""""""; """"""Numba-JITing of Cling-JITed templated free function""""""; """"""Numba-JITing of a free function taking a proxy argument for field access""""""; # note: need a sizable array to outperform given the unboxing overhead; """"""Numba-JITing of a free function taking a proxy argument for method access""""""; # note: need a sizable array to outperform given the unboxing overhead; """"""Numba-JITing of functions with multiple arguments""""""; """"""; double add_double(double a, double b, double c) {; double d = a + b + c;; return d;; }; """"""; """"""Numba-JITing of a free template function that recieves more than one template arg""""""; """"""; namespace NumbaSupportExample {; template<typename T1>; T1 add(T1 a, T1 b) { return a + b; }; }""""""; """"""Numba-JITing of various data types""""""; """"""\; namespace NumbaDTT {; struct M%d { M%d(%s f) : fField(f) {};; %s buf, fField;; }; }""""""; # 'int8_t', 'uint8_t', # TODO b/c check using return type fails; """"""Numba-JITing of a function that returns an object""""""; """"""Numba-JITing of a free template function that recieves multiple template args with non types""""""; """"""; namespace NumbaSupportExample {; template<typename T1, typename T2>; double add(double a, T1 b, T2 c) { return a + b + c; }; }""""""; """"""; int64_t& ref_add(int64_t x, int64_t y) {; int64_t c = x + y;; static int64_t result = c;; return c;; }; """"""; """"""Numba-JITing of a increment method belonging to a class, and also swaps the pointers and reflects the change on the python ctypes variables""""""; """"""; namespace RefTest {; class Box{; public:; long a;; long *b;; long *c;; Box(long i, long& j, long& k){; a = i;; b = &j;; c = &k;; }. void swap_ref(long &a, long &b) {; long temp = a;; a = b;; b = temp;; }. void inc(long* valu",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_numba.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_numba.py
Modifiability,inherit,inheritance,"""""""Test overloading of math operators""""""; """"""Test overloading of unary math operators""""""; """"""Test overloading of comparison operators""""""; """"""Test implementation of operator bool""""""; """"""Test converter operators of exact types""""""; """"""Test converter operators of approximate types""""""; """"""Test use of virtual bool operator==""""""; # derived operator== returns opposite; # the following is a wee bit interesting due to python resolution; # rules on the one hand, and C++ inheritance on the other: python; # will never select the derived comparison b/c the call will fail; # to pass a base through a const derived&; """"""Map () to []""""""; """"""Templated operator<()""""""; """"""Use of radd/rmul with non-associative types""""""; # Note: calls are repeated to test caching, if any; """"""Overloaded operator*/+-""""""; """"""Unary operator-+~""""""; #assert (~n).i == ~42; """"""Comma operator""""""; """"""Non-reference, single-argument, call not mapped to getitem""""""; """"""\; namespace IndexingOperators {; struct Foo {; float operator[] (float x) { return x; }; };. struct Bar : public Foo {; float operator() (float x) { return 5.f; }; }; }""""""; """"""Iterator methods have both class and global overloads""""""; """"""Globally defined ordered oeprators""""""; """"""\; namespace FriendOperator {. struct ALt { ALt(int d) : data(d) {} int data; };; bool operator< (const ALt& a1, const ALt& a2) { return a1.data < a2.data; }. struct ALe { ALe(int d) : data(d) {} int data; };; bool operator<=(const ALe& a1, const ALe& a2) { return a1.data <= a2.data; }. struct AGt { AGt(int d) : data(d) {} int data; };; bool operator> (const AGt& a1, const AGt& a2) { return a1.data > a2.data; }. struct AGe { AGe(int d) : data(d) {} int data; };; bool operator>=(const AGe& a1, const AGe& a2) { return a1.data >= a2.data; }. }""""""",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_operators.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_operators.py
Testability,test,test,"""""""Test overloading of math operators""""""; """"""Test overloading of unary math operators""""""; """"""Test overloading of comparison operators""""""; """"""Test implementation of operator bool""""""; """"""Test converter operators of exact types""""""; """"""Test converter operators of approximate types""""""; """"""Test use of virtual bool operator==""""""; # derived operator== returns opposite; # the following is a wee bit interesting due to python resolution; # rules on the one hand, and C++ inheritance on the other: python; # will never select the derived comparison b/c the call will fail; # to pass a base through a const derived&; """"""Map () to []""""""; """"""Templated operator<()""""""; """"""Use of radd/rmul with non-associative types""""""; # Note: calls are repeated to test caching, if any; """"""Overloaded operator*/+-""""""; """"""Unary operator-+~""""""; #assert (~n).i == ~42; """"""Comma operator""""""; """"""Non-reference, single-argument, call not mapped to getitem""""""; """"""\; namespace IndexingOperators {; struct Foo {; float operator[] (float x) { return x; }; };. struct Bar : public Foo {; float operator() (float x) { return 5.f; }; }; }""""""; """"""Iterator methods have both class and global overloads""""""; """"""Globally defined ordered oeprators""""""; """"""\; namespace FriendOperator {. struct ALt { ALt(int d) : data(d) {} int data; };; bool operator< (const ALt& a1, const ALt& a2) { return a1.data < a2.data; }. struct ALe { ALe(int d) : data(d) {} int data; };; bool operator<=(const ALe& a1, const ALe& a2) { return a1.data <= a2.data; }. struct AGt { AGt(int d) : data(d) {} int data; };; bool operator> (const AGt& a1, const AGt& a2) { return a1.data > a2.data; }. struct AGe { AGe(int d) : data(d) {} int data; };; bool operator>=(const AGe& a1, const AGe& a2) { return a1.data >= a2.data; }. }""""""",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_operators.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_operators.py
Availability,error,error,"gfilename) {; throw ConfigFileNotFoundError{configfilename};; }; };. class MyClass2 {; public:; MyClass2(const std::string& configfilename) {; throw ConfigFileNotFoundError{configfilename};; }; MyClass2(const char* configfilename) {; throw ConfigFileNotFoundError{configfilename};; }; };. class MyClass2a {; public:; MyClass2a() {}; void initialize(const std::string& configfilename) {; throw ConfigFileNotFoundError{configfilename};; }; void initialize(const char* configfilename) {; throw ConfigFileNotFoundError{configfilename};; }; };. class MyClass3 {; public:; MyClass3(const std::string& configfilename) {; throw ConfigFileNotFoundError{configfilename};; }; MyClass3(const char* configfilename) {; throw ConfigFileNotFoundError{configfilename};; }; MyClass3(const MyClass3& other) = delete;; MyClass3(const MyClass3&& other) = delete;; };. class MyClass4 {; public:; MyClass4(int) {}; MyClass4(const MyClass3& other) {}; };. class MyClass4a {; public:; MyClass4a() {}; void initialize(const std::string& configfilename) {; throw ConfigFileNotFoundError{configfilename};; }; void initialize(const char* configfilename) {; throw ConfigFileNotFoundError{configfilename};; }; void initialize(int) {}; }; }""""""; # single C++ exception during overload selection: assumes this is a logic; # error and prioritizes the C++ exception; # special case b/c of copy constructor; # multiple C++ exceptions are considered argument conversion errors and; # only result in the same exception type if they are all the same; # special case b/c of copy constructor; # copy constructor deleted; # a mix of exceptions becomes a TypeError; """"""Prioritize expected most derived class""""""; """"""\; namespace DeepInheritance {; class A {};; class B: public A {};; class C: public B {};. class D: public A {};; class E: public D {};. std::string myfunc1(const B&) { return ""B""; }; std::string myfunc1(const C&) { return ""C""; }; std::string myfunc2(const E&) { return ""E""; }; std::string myfunc2(const D&) { return ""D""; }; }""""""",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_overloads.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_overloads.py
Modifiability,config,configfilename,"""""""Test functions overloaded on different C++ clases""""""; """"""Test explicitly resolved function overloads""""""; # TODO: #assert c_overload.__dispatch__('get_int', 'b_overload*')(c, b_overload()) == 13; """"""Test functions overloaded on void* and non-existing classes""""""; """"""Test that unknown* is preferred over unknown&""""""; """"""Test functions overloaded on different arrays""""""; """"""Test overloads on int/doubles""""""; """"""Adapted test for array overloading""""""; """"""Check selectability of const/non-const overloads""""""; """"""Check bool/int overloaded calls""""""; """"""Prioritize reporting C++ exceptions from callee""""""; """"""\; namespace ExceptionTypeTest {. class ConfigFileNotFoundError : public std::exception {; std::string fMsg;; public:; ConfigFileNotFoundError(const std::string& msg) : fMsg(msg) {}; const char* what() const throw() { return fMsg.c_str(); }; };. class MyClass1 {; public:; MyClass1(const std::string& configfilename) {; throw ConfigFileNotFoundError{configfilename};; }; MyClass1(const MyClass1& other) {}; };. class MyClass1a {; public:; MyClass1a() {}; void initialize(const std::string& configfilename) {; throw ConfigFileNotFoundError{configfilename};; }; };. class MyClass2 {; public:; MyClass2(const std::string& configfilename) {; throw ConfigFileNotFoundError{configfilename};; }; MyClass2(const char* configfilename) {; throw ConfigFileNotFoundError{configfilename};; }; };. class MyClass2a {; public:; MyClass2a() {}; void initialize(const std::string& configfilename) {; throw ConfigFileNotFoundError{configfilename};; }; void initialize(const char* configfilename) {; throw ConfigFileNotFoundError{configfilename};; }; };. class MyClass3 {; public:; MyClass3(const std::string& configfilename) {; throw ConfigFileNotFoundError{configfilename};; }; MyClass3(const char* configfilename) {; throw ConfigFileNotFoundError{configfilename};; }; MyClass3(const MyClass3& other) = delete;; MyClass3(const MyClass3&& other) = delete;; };. class MyClass4 {; public:; MyClass4(int) {}; MyClass4(co",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_overloads.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_overloads.py
Testability,assert,assert,"""""""Test functions overloaded on different C++ clases""""""; """"""Test explicitly resolved function overloads""""""; # TODO: #assert c_overload.__dispatch__('get_int', 'b_overload*')(c, b_overload()) == 13; """"""Test functions overloaded on void* and non-existing classes""""""; """"""Test that unknown* is preferred over unknown&""""""; """"""Test functions overloaded on different arrays""""""; """"""Test overloads on int/doubles""""""; """"""Adapted test for array overloading""""""; """"""Check selectability of const/non-const overloads""""""; """"""Check bool/int overloaded calls""""""; """"""Prioritize reporting C++ exceptions from callee""""""; """"""\; namespace ExceptionTypeTest {. class ConfigFileNotFoundError : public std::exception {; std::string fMsg;; public:; ConfigFileNotFoundError(const std::string& msg) : fMsg(msg) {}; const char* what() const throw() { return fMsg.c_str(); }; };. class MyClass1 {; public:; MyClass1(const std::string& configfilename) {; throw ConfigFileNotFoundError{configfilename};; }; MyClass1(const MyClass1& other) {}; };. class MyClass1a {; public:; MyClass1a() {}; void initialize(const std::string& configfilename) {; throw ConfigFileNotFoundError{configfilename};; }; };. class MyClass2 {; public:; MyClass2(const std::string& configfilename) {; throw ConfigFileNotFoundError{configfilename};; }; MyClass2(const char* configfilename) {; throw ConfigFileNotFoundError{configfilename};; }; };. class MyClass2a {; public:; MyClass2a() {}; void initialize(const std::string& configfilename) {; throw ConfigFileNotFoundError{configfilename};; }; void initialize(const char* configfilename) {; throw ConfigFileNotFoundError{configfilename};; }; };. class MyClass3 {; public:; MyClass3(const std::string& configfilename) {; throw ConfigFileNotFoundError{configfilename};; }; MyClass3(const char* configfilename) {; throw ConfigFileNotFoundError{configfilename};; }; MyClass3(const MyClass3& other) = delete;; MyClass3(const MyClass3&& other) = delete;; };. class MyClass4 {; public:; MyClass4(int) {}; MyClass4(co",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_overloads.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_overloads.py
Availability,error,error,"""; """"""Test (un)bound method calls""""""; """"""Test installing and calling global C++ function as python method""""""; """"""A sub-class on the python side should have that class as type""""""; """"""Respective return values of temporaries should not go away""""""; """"""namespace Lifeline {; struct A1 { A1(int x) : x(x) {} int x; };; struct A2 { A2(int x) { v.emplace_back(x); } std::vector<A1> v; std::vector<A1>& get() { return v; } };; struct A3 { A3(int x) { v.emplace_back(x); } std::vector<A2> v; std::vector<A2>& get() { return v; } };; struct A4 { A4(int x) { v.emplace_back(x); } std::vector<A3> v; std::vector<A3>& get() { return v; } };; struct A5 { A5(int x) { v.emplace_back(x); } std::vector<A4> v; std::vector<A4>& get() { return v; } };. A5 gime(int i) { return A5(i); }; }""""""; """"""Use of keyword arguments""""""; """"""namespace KeyWords {; struct A {; A(std::initializer_list<int> vals) : fVals(vals) {}; std::vector<int> fVals;; };. struct B {; B() = delete;; B(const A& in_A, const A& out_A) : fVal(42), fIn(in_A), fOut(out_A) {}; B(int val, const A& in_A, const A& out_A) : fVal(val), fIn(in_A), fOut(out_A) {}; int fVal;; A fIn, fOut;; };. int callme(int choice, int a, int b, int c) {; if (choice == 0) return a;; if (choice == 1) return b;; return c;; }. struct C {; int fChoice;; };. int callme_c(const C& o, int a, int b, int c) {; return callme(o.fChoice, a, b, c);; } }""""""; # constructor and implicit conversion with keywords; # global function with keywords; # global function as method with keywords; """"""Use of keyword arguments mixed with defaults""""""; """"""namespace KeyWordsAndDefaults {; int foo(int a=10, int b=20, int c=5, int d=4) {; return a-b/c*d;; }. std::string bar(const std::string& a = ""a"", const std::string& b = ""b"") {; return a+b;; }. class MyClass {};. void foobar(const MyClass& m1 = MyClass(), const MyClass& m2 = MyClass()) {; /* empty */; } }""""""; """"""Test addition of user-defined pythonizations""""""; """"""Test pythonizations error reporting""""""; """"""Test overwritability of globals""""""",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_pythonify.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_pythonify.py
Deployability,install,installing,"""""""Test whether loading a dictionary twice results in the same object""""""; """"""Test the lookup of a class, and its caching""""""; """"""Test calling of static methods""""""; # TODO: this leaks; # TODO: id.; # TODO: id.; """"""Test object and method calls""""""; # TODO: this leaks; # TODO: this leaks; """"""Pass object by pointer""""""; """"""Return an object py pointer""""""; """"""Return an object by value""""""; """"""Call a global function""""""; # creation lookup; # cached lookup; """"""Test proper C++ destruction by the garbage collector""""""; # TODO: need ReferenceError on touching pl_a; """"""Test propagation of default function arguments""""""; # NOTE: when called through the stub, default args are fine; """"""Test functions overloaded on arguments""""""; """"""Test access and use of typedefs""""""; """"""Test recognition of '_' as part of a valid class name""""""; """"""Test (un)bound method calls""""""; """"""Test installing and calling global C++ function as python method""""""; """"""A sub-class on the python side should have that class as type""""""; """"""Respective return values of temporaries should not go away""""""; """"""namespace Lifeline {; struct A1 { A1(int x) : x(x) {} int x; };; struct A2 { A2(int x) { v.emplace_back(x); } std::vector<A1> v; std::vector<A1>& get() { return v; } };; struct A3 { A3(int x) { v.emplace_back(x); } std::vector<A2> v; std::vector<A2>& get() { return v; } };; struct A4 { A4(int x) { v.emplace_back(x); } std::vector<A3> v; std::vector<A3>& get() { return v; } };; struct A5 { A5(int x) { v.emplace_back(x); } std::vector<A4> v; std::vector<A4>& get() { return v; } };. A5 gime(int i) { return A5(i); }; }""""""; """"""Use of keyword arguments""""""; """"""namespace KeyWords {; struct A {; A(std::initializer_list<int> vals) : fVals(vals) {}; std::vector<int> fVals;; };. struct B {; B() = delete;; B(const A& in_A, const A& out_A) : fVal(42), fIn(in_A), fOut(out_A) {}; B(int val, const A& in_A, const A& out_A) : fVal(val), fIn(in_A), fOut(out_A) {}; int fVal;; A fIn, fOut;; };. int callme(int choice, int a, int b, int c) {; if (cho",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_pythonify.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_pythonify.py
Performance,load,loading,"""""""Test whether loading a dictionary twice results in the same object""""""; """"""Test the lookup of a class, and its caching""""""; """"""Test calling of static methods""""""; # TODO: this leaks; # TODO: id.; # TODO: id.; """"""Test object and method calls""""""; # TODO: this leaks; # TODO: this leaks; """"""Pass object by pointer""""""; """"""Return an object py pointer""""""; """"""Return an object by value""""""; """"""Call a global function""""""; # creation lookup; # cached lookup; """"""Test proper C++ destruction by the garbage collector""""""; # TODO: need ReferenceError on touching pl_a; """"""Test propagation of default function arguments""""""; # NOTE: when called through the stub, default args are fine; """"""Test functions overloaded on arguments""""""; """"""Test access and use of typedefs""""""; """"""Test recognition of '_' as part of a valid class name""""""; """"""Test (un)bound method calls""""""; """"""Test installing and calling global C++ function as python method""""""; """"""A sub-class on the python side should have that class as type""""""; """"""Respective return values of temporaries should not go away""""""; """"""namespace Lifeline {; struct A1 { A1(int x) : x(x) {} int x; };; struct A2 { A2(int x) { v.emplace_back(x); } std::vector<A1> v; std::vector<A1>& get() { return v; } };; struct A3 { A3(int x) { v.emplace_back(x); } std::vector<A2> v; std::vector<A2>& get() { return v; } };; struct A4 { A4(int x) { v.emplace_back(x); } std::vector<A3> v; std::vector<A3>& get() { return v; } };; struct A5 { A5(int x) { v.emplace_back(x); } std::vector<A4> v; std::vector<A4>& get() { return v; } };. A5 gime(int i) { return A5(i); }; }""""""; """"""Use of keyword arguments""""""; """"""namespace KeyWords {; struct A {; A(std::initializer_list<int> vals) : fVals(vals) {}; std::vector<int> fVals;; };. struct B {; B() = delete;; B(const A& in_A, const A& out_A) : fVal(42), fIn(in_A), fOut(out_A) {}; B(int val, const A& in_A, const A& out_A) : fVal(val), fIn(in_A), fOut(out_A) {}; int fVal;; A fIn, fOut;; };. int callme(int choice, int a, int b, int c) {; if (cho",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_pythonify.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_pythonify.py
Security,access,access,"""""""Test whether loading a dictionary twice results in the same object""""""; """"""Test the lookup of a class, and its caching""""""; """"""Test calling of static methods""""""; # TODO: this leaks; # TODO: id.; # TODO: id.; """"""Test object and method calls""""""; # TODO: this leaks; # TODO: this leaks; """"""Pass object by pointer""""""; """"""Return an object py pointer""""""; """"""Return an object by value""""""; """"""Call a global function""""""; # creation lookup; # cached lookup; """"""Test proper C++ destruction by the garbage collector""""""; # TODO: need ReferenceError on touching pl_a; """"""Test propagation of default function arguments""""""; # NOTE: when called through the stub, default args are fine; """"""Test functions overloaded on arguments""""""; """"""Test access and use of typedefs""""""; """"""Test recognition of '_' as part of a valid class name""""""; """"""Test (un)bound method calls""""""; """"""Test installing and calling global C++ function as python method""""""; """"""A sub-class on the python side should have that class as type""""""; """"""Respective return values of temporaries should not go away""""""; """"""namespace Lifeline {; struct A1 { A1(int x) : x(x) {} int x; };; struct A2 { A2(int x) { v.emplace_back(x); } std::vector<A1> v; std::vector<A1>& get() { return v; } };; struct A3 { A3(int x) { v.emplace_back(x); } std::vector<A2> v; std::vector<A2>& get() { return v; } };; struct A4 { A4(int x) { v.emplace_back(x); } std::vector<A3> v; std::vector<A3>& get() { return v; } };; struct A5 { A5(int x) { v.emplace_back(x); } std::vector<A4> v; std::vector<A4>& get() { return v; } };. A5 gime(int i) { return A5(i); }; }""""""; """"""Use of keyword arguments""""""; """"""namespace KeyWords {; struct A {; A(std::initializer_list<int> vals) : fVals(vals) {}; std::vector<int> fVals;; };. struct B {; B() = delete;; B(const A& in_A, const A& out_A) : fVal(42), fIn(in_A), fOut(out_A) {}; B(int val, const A& in_A, const A& out_A) : fVal(val), fIn(in_A), fOut(out_A) {}; int fVal;; A fIn, fOut;; };. int callme(int choice, int a, int b, int c) {; if (cho",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_pythonify.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_pythonify.py
Testability,stub,stub,"""""""Test whether loading a dictionary twice results in the same object""""""; """"""Test the lookup of a class, and its caching""""""; """"""Test calling of static methods""""""; # TODO: this leaks; # TODO: id.; # TODO: id.; """"""Test object and method calls""""""; # TODO: this leaks; # TODO: this leaks; """"""Pass object by pointer""""""; """"""Return an object py pointer""""""; """"""Return an object by value""""""; """"""Call a global function""""""; # creation lookup; # cached lookup; """"""Test proper C++ destruction by the garbage collector""""""; # TODO: need ReferenceError on touching pl_a; """"""Test propagation of default function arguments""""""; # NOTE: when called through the stub, default args are fine; """"""Test functions overloaded on arguments""""""; """"""Test access and use of typedefs""""""; """"""Test recognition of '_' as part of a valid class name""""""; """"""Test (un)bound method calls""""""; """"""Test installing and calling global C++ function as python method""""""; """"""A sub-class on the python side should have that class as type""""""; """"""Respective return values of temporaries should not go away""""""; """"""namespace Lifeline {; struct A1 { A1(int x) : x(x) {} int x; };; struct A2 { A2(int x) { v.emplace_back(x); } std::vector<A1> v; std::vector<A1>& get() { return v; } };; struct A3 { A3(int x) { v.emplace_back(x); } std::vector<A2> v; std::vector<A2>& get() { return v; } };; struct A4 { A4(int x) { v.emplace_back(x); } std::vector<A3> v; std::vector<A3>& get() { return v; } };; struct A5 { A5(int x) { v.emplace_back(x); } std::vector<A4> v; std::vector<A4>& get() { return v; } };. A5 gime(int i) { return A5(i); }; }""""""; """"""Use of keyword arguments""""""; """"""namespace KeyWords {; struct A {; A(std::initializer_list<int> vals) : fVals(vals) {}; std::vector<int> fVals;; };. struct B {; B() = delete;; B(const A& in_A, const A& out_A) : fVal(42), fIn(in_A), fOut(out_A) {}; B(int val, const A& in_A, const A& out_A) : fVal(val), fIn(in_A), fOut(out_A) {}; int fVal;; A fIn, fOut;; };. int callme(int choice, int a, int b, int c) {; if (cho",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_pythonify.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_pythonify.py
Availability,avail,available,"""""""Test basic semantics of the pythonization API""""""; # global pythonizors are still run even if namespaced ones available; # include '\0'; """"""Use composites to map GetSize() onto buffer returns""""""; """"""Use composites to map GetSize() onto buffer returns""""""; """"""Verify pinnability of returns""""""; """"""Transparent use of smart pointers""""""; """"""Smart pointer argument passing""""""; # TODO:; # cppyy.gbl.mine = mine; """"""Smart pointer return types""""""; """"""Effect of creates flag on return type""""""; # there's eg. one global variable; """"""Derived class should not re-pythonize base class pythonization""""""; # skip the IndexErorr test: pythonization for __getitem__[index] < size(); # can not be applied strict enough (instead of an index, this could be an; # associative container, with 'index' a key, not a counter; #raises(IndexError, d.__getitem__, 1); """"""Use of C++ side pythonizations""""""; # explicit pythonization; # up-the-hierarchy pythonization; ## actual test run",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_pythonization.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_pythonization.py
Modifiability,variab,variable,"""""""Test basic semantics of the pythonization API""""""; # global pythonizors are still run even if namespaced ones available; # include '\0'; """"""Use composites to map GetSize() onto buffer returns""""""; """"""Use composites to map GetSize() onto buffer returns""""""; """"""Verify pinnability of returns""""""; """"""Transparent use of smart pointers""""""; """"""Smart pointer argument passing""""""; # TODO:; # cppyy.gbl.mine = mine; """"""Smart pointer return types""""""; """"""Effect of creates flag on return type""""""; # there's eg. one global variable; """"""Derived class should not re-pythonize base class pythonization""""""; # skip the IndexErorr test: pythonization for __getitem__[index] < size(); # can not be applied strict enough (instead of an index, this could be an; # associative container, with 'index' a key, not a counter; #raises(IndexError, d.__getitem__, 1); """"""Use of C++ side pythonizations""""""; # explicit pythonization; # up-the-hierarchy pythonization; ## actual test run",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_pythonization.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_pythonization.py
Testability,test,test,"""""""Test basic semantics of the pythonization API""""""; # global pythonizors are still run even if namespaced ones available; # include '\0'; """"""Use composites to map GetSize() onto buffer returns""""""; """"""Use composites to map GetSize() onto buffer returns""""""; """"""Verify pinnability of returns""""""; """"""Transparent use of smart pointers""""""; """"""Smart pointer argument passing""""""; # TODO:; # cppyy.gbl.mine = mine; """"""Smart pointer return types""""""; """"""Effect of creates flag on return type""""""; # there's eg. one global variable; """"""Derived class should not re-pythonize base class pythonization""""""; # skip the IndexErorr test: pythonization for __getitem__[index] < size(); # can not be applied strict enough (instead of an index, this could be an; # associative container, with 'index' a key, not a counter; #raises(IndexError, d.__getitem__, 1); """"""Use of C++ side pythonizations""""""; # explicit pythonization; # up-the-hierarchy pythonization; ## actual test run",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_pythonization.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_pythonization.py
Availability,error,error,"""""""Doc strings for KDcrawIface (used to crash).""""""; # TODO: run a find for these paths; # need to resolve qt_version_tag for the incremental compiler; since; # it's not otherwise used, just make something up; # bring in some symbols to resolve the class; """"""For the same reasons as test01_kdcraw, this used to crash.""""""; # TODO: it's deeply silly that namespaces inherit from CPPInstance (in CPyCppyy); """"""Help on a generated pyfunc used to crash.""""""; # potentially pulled in by Qt/xapian.h; """"""#include ""Python.h""; long py2long(PyObject* obj) { return PyLong_AsLong(obj); }""""""; """"""Test usability of AVX by default.""""""; # attribute error if compilation failed; """"""Calling a templated method on a templated class with all defaults used to crash.""""""; """"""\; template<typename T>; class AllDefault {; public:; AllDefault(int val) : m_t(val) {}; template<int aap=1, int noot=2>; int do_stuff() { return m_t+aap+noot; }. public:; T m_t;; };""""""; """"""Calling with default argument for float or unsigned, which not parse""""""; """"""\; namespace Defaulters {; float take_float(float a=5.f, int b=2) { return a*b; }; double take_double(double a=5.f, int b=2) { return a*b; }; long take_long(long a=5l, int b=2) { return a*b; }; unsigned long take_ulong(unsigned long a=5ul, int b=2) { return a*b; }; }""""""; # the following default argument used to fail to parse; """"""The memory regulator would leave an additional refcount on classes""""""; """"""Nested typedefs should retain identity""""""; """"""; namespace PyABC {; struct S1 {};; struct S2 {; typedef std::vector<const PyABC::S1*> S1_coll;; }; }""""""; """"""GIL was released by accident for by-value returns""""""; """"""; #include ""Python.h"". std::vector<float> some_foo_calling_python() {; auto pyobj = reinterpret_cast<PyObject*>(ADDRESS);; float f = (float)PyFloat_AsDouble(pyobj);; std::vector<float> v;; v.push_back(f);; return v;; }; """"""; """"""Enum declared in search.h did not appear in global space""""""; # no such enum in MSVC's search.h; """"""AsCObject (now as_cobject) had a deref ",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_regression.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_regression.py
Deployability,release,released,"CPyCppyy); """"""Help on a generated pyfunc used to crash.""""""; # potentially pulled in by Qt/xapian.h; """"""#include ""Python.h""; long py2long(PyObject* obj) { return PyLong_AsLong(obj); }""""""; """"""Test usability of AVX by default.""""""; # attribute error if compilation failed; """"""Calling a templated method on a templated class with all defaults used to crash.""""""; """"""\; template<typename T>; class AllDefault {; public:; AllDefault(int val) : m_t(val) {}; template<int aap=1, int noot=2>; int do_stuff() { return m_t+aap+noot; }. public:; T m_t;; };""""""; """"""Calling with default argument for float or unsigned, which not parse""""""; """"""\; namespace Defaulters {; float take_float(float a=5.f, int b=2) { return a*b; }; double take_double(double a=5.f, int b=2) { return a*b; }; long take_long(long a=5l, int b=2) { return a*b; }; unsigned long take_ulong(unsigned long a=5ul, int b=2) { return a*b; }; }""""""; # the following default argument used to fail to parse; """"""The memory regulator would leave an additional refcount on classes""""""; """"""Nested typedefs should retain identity""""""; """"""; namespace PyABC {; struct S1 {};; struct S2 {; typedef std::vector<const PyABC::S1*> S1_coll;; }; }""""""; """"""GIL was released by accident for by-value returns""""""; """"""; #include ""Python.h"". std::vector<float> some_foo_calling_python() {; auto pyobj = reinterpret_cast<PyObject*>(ADDRESS);; float f = (float)PyFloat_AsDouble(pyobj);; std::vector<float> v;; v.push_back(f);; return v;; }; """"""; """"""Enum declared in search.h did not appear in global space""""""; # no such enum in MSVC's search.h; """"""AsCObject (now as_cobject) had a deref too many""""""; """"""Exception from SetDetailedException during exception handling used to crash""""""; """"""Map str to const char* over char""""""; # This is debatable, but although a single character string passes through char,; # it is more consistent to prefer const char* or std::string in all cases. The; # original bug report is here:; # https://bitbucket.org/wlav/cppyy/issues/127/string-argument-",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_regression.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_regression.py
Modifiability,inherit,inherit,"""""""Doc strings for KDcrawIface (used to crash).""""""; # TODO: run a find for these paths; # need to resolve qt_version_tag for the incremental compiler; since; # it's not otherwise used, just make something up; # bring in some symbols to resolve the class; """"""For the same reasons as test01_kdcraw, this used to crash.""""""; # TODO: it's deeply silly that namespaces inherit from CPPInstance (in CPyCppyy); """"""Help on a generated pyfunc used to crash.""""""; # potentially pulled in by Qt/xapian.h; """"""#include ""Python.h""; long py2long(PyObject* obj) { return PyLong_AsLong(obj); }""""""; """"""Test usability of AVX by default.""""""; # attribute error if compilation failed; """"""Calling a templated method on a templated class with all defaults used to crash.""""""; """"""\; template<typename T>; class AllDefault {; public:; AllDefault(int val) : m_t(val) {}; template<int aap=1, int noot=2>; int do_stuff() { return m_t+aap+noot; }. public:; T m_t;; };""""""; """"""Calling with default argument for float or unsigned, which not parse""""""; """"""\; namespace Defaulters {; float take_float(float a=5.f, int b=2) { return a*b; }; double take_double(double a=5.f, int b=2) { return a*b; }; long take_long(long a=5l, int b=2) { return a*b; }; unsigned long take_ulong(unsigned long a=5ul, int b=2) { return a*b; }; }""""""; # the following default argument used to fail to parse; """"""The memory regulator would leave an additional refcount on classes""""""; """"""Nested typedefs should retain identity""""""; """"""; namespace PyABC {; struct S1 {};; struct S2 {; typedef std::vector<const PyABC::S1*> S1_coll;; }; }""""""; """"""GIL was released by accident for by-value returns""""""; """"""; #include ""Python.h"". std::vector<float> some_foo_calling_python() {; auto pyobj = reinterpret_cast<PyObject*>(ADDRESS);; float f = (float)PyFloat_AsDouble(pyobj);; std::vector<float> v;; v.push_back(f);; return v;; }; """"""; """"""Enum declared in search.h did not appear in global space""""""; # no such enum in MSVC's search.h; """"""AsCObject (now as_cobject) had a deref ",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_regression.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_regression.py
Performance,optimiz,optimization,"ame();; }; template<class T>; size_t sizeit(T&& t) {; return t.size();; }; }""""""; """"""Use template to iterate over an enum""""""; # from: https://stackoverflow.com/questions/52459530/pybind11-emulate-python-enum-behaviour; """"""; template <typename Enum>; struct my_iter_enum {; struct iterator {; using value_type = Enum;; using difference_type = ptrdiff_t;; using reference = const Enum&;; using pointer = const Enum*;; using iterator_category = std::input_iterator_tag;. iterator(Enum value) : cur(value) {}. reference operator*() { return cur; }; pointer operator->() { return &cur; }; bool operator==(const iterator& other) { return cur == other.cur; }; bool operator!=(const iterator& other) { return !(*this == other); }; iterator& operator++() { if (cur != Enum::Unknown) cur = static_cast<Enum>(static_cast<std::underlying_type_t<Enum>>(cur) + 1); return *this; }; iterator operator++(int) { iterator other = *this; ++(*this); return other; }. private:; Enum cur;; int TODO_why_is_this_placeholder_needed; // JIT error? Too aggressive optimization?; };. iterator begin() {; return iterator(Enum::Black);; }. iterator end() {; return iterator(Enum::Unknown);; }; };. enum class MyColorEnum : char {; Black = 1,; Blue,; Red,; Yellow,; Unknown; };""""""; """"""Base class python-side operator== interered with derived one""""""; """"""; namespace SelectOpEq {; class Base {};. class Derived1 : public Base {; public:; bool operator==(Derived1&) { return true; }; };. class Derived2 : public Base {; public:; bool operator!=(Derived2&) { return true; }; }; }""""""; # derived class' C++ operator== called; # derived class' C++ operator!= called; """"""operator+(string, string) should return a string""""""; """"""Hashing of std::string""""""; # hashes of std::string larger than 2**31 would fail; run a couple of; # strings to check although it may still succeed by accident (and never; # was an issue on p3 anyway); """"""Signed char executor was self-referencing""""""; """"""; class SignedCharRefGetter {; public:; void setter(signed ",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_regression.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_regression.py
Security,hash,hashes,"operator++(int) { iterator other = *this; ++(*this); return other; }. private:; Enum cur;; int TODO_why_is_this_placeholder_needed; // JIT error? Too aggressive optimization?; };. iterator begin() {; return iterator(Enum::Black);; }. iterator end() {; return iterator(Enum::Unknown);; }; };. enum class MyColorEnum : char {; Black = 1,; Blue,; Red,; Yellow,; Unknown; };""""""; """"""Base class python-side operator== interered with derived one""""""; """"""; namespace SelectOpEq {; class Base {};. class Derived1 : public Base {; public:; bool operator==(Derived1&) { return true; }; };. class Derived2 : public Base {; public:; bool operator!=(Derived2&) { return true; }; }; }""""""; # derived class' C++ operator== called; # derived class' C++ operator!= called; """"""operator+(string, string) should return a string""""""; """"""Hashing of std::string""""""; # hashes of std::string larger than 2**31 would fail; run a couple of; # strings to check although it may still succeed by accident (and never; # was an issue on p3 anyway); """"""Signed char executor was self-referencing""""""; """"""; class SignedCharRefGetter {; public:; void setter(signed char sc) { m_c = sc; }; signed char& getter() { return m_c; }; signed char m_c;; };""""""; """"""Extend a life line to references into a vector if needed""""""; """"""; std::vector<std::string> get_some_temporary_vector() { return { ""x"", ""y"", ""z"" }; }; """"""; """"""Conversion rules when selecting intializer_list v.s. temporary""""""; """"""\; namespace regression_test21 {; std::string what_called = """";; class Foo {; public:; Foo() = default;; Foo(int i) {; what_called += ""Foo(int)"";; }; Foo(std::initializer_list<uint8_t> il) {; std::ostringstream os;; os << ""Foo(il<size="" << il.size() << "">)"";; what_called += os.str();; }; };. class Bar {; public:; Bar() = default;; Bar(int i) {; what_called = ""Bar(int)"";; }; Bar(std::initializer_list<uint8_t> il) {; std::ostringstream os;; os << ""Bar(il<size="" << il.size() << "">)"";; what_called += os.str();; }; Bar(Foo x) {; what_called += ""Bar(Foo)"";;",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_regression.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_regression.py
Testability,test,test,". template class ReferenceWavefunction<double>;. template<class T>; auto run_as() {; return std::make_tuple(20., T{});; } } // namespace type, property_types; """"""; """"""Print empty collection through Cling""""""; # printing an empty collection used to have a missing symbol on 64b Windows; """"""Static path object used to crash on destruction""""""; # TODO: this is b/c of the mangling: it's looking for '_std', but name is '__'; """"""\; #include <filesystem>; std::string stack_std_path() {; std::filesystem::path p = ""/usr"";; std::ostringstream os;; os << p;; return os.str();; }""""""; """"""cppyy.sizeof forwards to ctypes.sizeof where necessary""""""; """"""\; namespace test36_ctypes_sizeof {; void func(uint32_t* param) {; *param = 42;; }; }""""""; """"""Passing an array of pointers used to crash""""""; """"""\; namespace ArrayOfPointers {; void* test(int *arr[8], bool is_int=true) { return is_int ? (void*)arr : nullptr; }; void* test(uint8_t *arr[8], bool is_int=false) { return is_int ? nullptr : (void*)arr; }; }""""""; """"""Access to fixed-size char16 arrays as data members""""""; # vector of objects; # array of objects; # low-level array of objects; """"""vector<T*>'s const T*& used to be T**, now T*""""""; """"""Construct and pass an explicit initializer list""""""; """"""Typedef-ed enums do not have enum tag in declarations""""""; """"""\; namespace TypedefedEnum {; typedef enum {; MONDAY = 0,; TUESDAY = 1,; WEDNESDAY = 2; } Day;. int func(const Day day) { return (int)day; }; }""""""; """"""Consistent usage of char[] arrays""""""; """"""Call a static method with default args on an instance""""""; """"""\; namespace StaticWithDefault {; struct MyClass {; void static smethod(const std::string& s1, const std::string& s2="""") {}; }; }""""""; # used to fail with vectorcall; """"""Ownership of arguments with heuristic memory policy""""""; """"""\; namespace MemTester {; void CallRef( std::string&) {}; void CallConstRef( const std::string&) {}; void CallPtr( std::string*) {}; void CallConstPtr( const std::string*) {}; };; """"""; # The scope with the heuristic memory ",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_regression.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_regression.py
Usability,usab,usability,"""""""Doc strings for KDcrawIface (used to crash).""""""; # TODO: run a find for these paths; # need to resolve qt_version_tag for the incremental compiler; since; # it's not otherwise used, just make something up; # bring in some symbols to resolve the class; """"""For the same reasons as test01_kdcraw, this used to crash.""""""; # TODO: it's deeply silly that namespaces inherit from CPPInstance (in CPyCppyy); """"""Help on a generated pyfunc used to crash.""""""; # potentially pulled in by Qt/xapian.h; """"""#include ""Python.h""; long py2long(PyObject* obj) { return PyLong_AsLong(obj); }""""""; """"""Test usability of AVX by default.""""""; # attribute error if compilation failed; """"""Calling a templated method on a templated class with all defaults used to crash.""""""; """"""\; template<typename T>; class AllDefault {; public:; AllDefault(int val) : m_t(val) {}; template<int aap=1, int noot=2>; int do_stuff() { return m_t+aap+noot; }. public:; T m_t;; };""""""; """"""Calling with default argument for float or unsigned, which not parse""""""; """"""\; namespace Defaulters {; float take_float(float a=5.f, int b=2) { return a*b; }; double take_double(double a=5.f, int b=2) { return a*b; }; long take_long(long a=5l, int b=2) { return a*b; }; unsigned long take_ulong(unsigned long a=5ul, int b=2) { return a*b; }; }""""""; # the following default argument used to fail to parse; """"""The memory regulator would leave an additional refcount on classes""""""; """"""Nested typedefs should retain identity""""""; """"""; namespace PyABC {; struct S1 {};; struct S2 {; typedef std::vector<const PyABC::S1*> S1_coll;; }; }""""""; """"""GIL was released by accident for by-value returns""""""; """"""; #include ""Python.h"". std::vector<float> some_foo_calling_python() {; auto pyobj = reinterpret_cast<PyObject*>(ADDRESS);; float f = (float)PyFloat_AsDouble(pyobj);; std::vector<float> v;; v.push_back(f);; return v;; }; """"""; """"""Enum declared in search.h did not appear in global space""""""; # no such enum in MSVC's search.h; """"""AsCObject (now as_cobject) had a deref ",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_regression.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_regression.py
Availability,error,error,"s); # Create from various iteratables; # as above, can not put strings in type-checked containers; #for s in (""123"", """", range(1000), ('do', 1.2), range(2000,2200,5)):; # as above, no strings; #assert type2test(c for c in ""123"") == type2test(""123""); # Issue #23757 (in CPython); #assert type2test(LyingTuple((2,))) == type2test((1,)); #assert type2test(LyingList([2])) == type2test([1]); """"""Detailed slicing tests from CPython""""""; # Extended slices; # Test extreme cases with long ints; # the following two fail b/c PySlice_GetIndices succeeds w/o error, while; # returning an overflown value (list object uses different internal APIs); #assert a[ -pow(2,128): 3 ] == type2test([0,1,2]); #assert a[ 3: pow(2,145) ] == type2test([3,4]); """"""Test access to std::vector<int>/std::vector<double>""""""; #-----; #-----; #-----; """"""Test access to an std::vector<just_a_class>""""""; """"""Test behavior of empty std::vector<int>""""""; """"""Test iteration over an std::vector<int>""""""; """"""Test usage of += of iterable on push_back-able container""""""; # string shouldn't pass; # TODO: decide whether this should roll-back; """"""Test python-style indexing to an std::vector<int>""""""; # 2 off from start, 1 from end; """"""Usability of std::vector<bool> which can be a specialization""""""; """"""Usability of std::vector<> of some enums""""""; """"""Adverse effect of implicit conversion on vector<string>""""""; """"""Use of std::distance with vector""""""; """"""Use of std::vector<std::pair>""""""; # after the original bug report; """"""; class PairVector {; public:; std::vector<std::pair<double, double>> vector_pair(const std::vector<std::pair<double, double>>& a) {; return a;; }; };; """"""; # TODO: nicer error handling for the following (current: template compilation failure trying; # to assign a pair with <double, string> to <double, double>); # ll2 = ll[:]; # ll2[2] = ll[2][:]; # ll2[2][1] = 'a'; # v = a.vector_pair(ll2); """"""Check lifeline setting on vectors of objects""""""; """"""namespace Lifeline {; static int count = 0;; template <typename T>; st",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_stltypes.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_stltypes.py
Integrability,protocol,protocol,"# -*- coding: UTF-8 -*-; # after CPython's Lib/test/seq_tests.py; """"""Regular generator""""""; """"""Sequence using __getitem__""""""; """"""Sequence using iterator protocol""""""; # p2.7; """"""Sequence using iterator protocol defined with a generator""""""; """"""Missing __getitem__ and __iter__""""""; # p2.7; """"""Iterator missing __next__()""""""; """"""Test propagation of exceptions""""""; # p2.7; """"""Test immediate stop""""""; # p2.7; """"""Test multiple tiers of iterators""""""; # the following does not work for type-checked containers; #s = ""this is also a sequence""; #vv = type2test(s); #assert len(vv) == len(s); # Create from various iteratables; # as above, can not put strings in type-checked containers; #for s in (""123"", """", range(1000), ('do', 1.2), range(2000,2200,5)):; # as above, no strings; #assert type2test(c for c in ""123"") == type2test(""123""); # Issue #23757 (in CPython); #assert type2test(LyingTuple((2,))) == type2test((1,)); #assert type2test(LyingList([2])) == type2test([1]); """"""Detailed slicing tests from CPython""""""; # Extended slices; # Test extreme cases with long ints; # the following two fail b/c PySlice_GetIndices succeeds w/o error, while; # returning an overflown value (list object uses different internal APIs); #assert a[ -pow(2,128): 3 ] == type2test([0,1,2]); #assert a[ 3: pow(2,145) ] == type2test([3,4]); """"""Test access to std::vector<int>/std::vector<double>""""""; #-----; #-----; #-----; """"""Test access to an std::vector<just_a_class>""""""; """"""Test behavior of empty std::vector<int>""""""; """"""Test iteration over an std::vector<int>""""""; """"""Test usage of += of iterable on push_back-able container""""""; # string shouldn't pass; # TODO: decide whether this should roll-back; """"""Test python-style indexing to an std::vector<int>""""""; # 2 off from start, 1 from end; """"""Usability of std::vector<bool> which can be a specialization""""""; """"""Usability of std::vector<> of some enums""""""; """"""Adverse effect of implicit conversion on vector<string>""""""; """"""Use of std::distance with vector""""""; """"""Use of std::vec",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_stltypes.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_stltypes.py
Modifiability,polymorphi,polymorphic,") : y(y) { }; std::vector<std::shared_ptr<X>> gimeVec() {; std::vector<std::shared_ptr<X>> result;; for (int i = 0; i < 10; ++i) {; result.push_back(std::make_shared<X>(i));; }; return result;; }; }; }""""""; """"""Nested vectors""""""; """"""Advanced test of vector slicing""""""; # additional test from CPython's test suite; """"""Vector construction following CPython's sequence""""""; """"""C++17 style initialization of std::vector""""""; """"""Test usage of __array__ from numpy""""""; """"""\; namespace ImplicitVector {; int func(std::vector<int> v) {; return std::accumulate(v.begin(), v.end(), 0);; } }""""""; """"""Iteration over a vector of by-value objects""""""; """"""namespace vector_point3d {; class Point3D {; double x, y, z;. public:; Point3D(double x, double y, double z) : x(x), y(y), z(z) {}; double square() { return x*x+y*y+z*z; }; }; }""""""; """"""Usage of a vector of const char*""""""; """"""\; namespace VectorConstCharStar {; std::vector<const char*> test = {""hello""};; }""""""; """"""Vector of structs data() should return array-like""""""; """"""\; namespace ArrayLike {; struct __attribute__((__packed__)) Vector3f {; float x, y, z;; }; }""""""; # the following should not raise; # length of the view is in bytes; """"""Vector of polymorphic types should auto-cast""""""; """"""\; namespace Polymorphic {; class vertex {; public:; virtual ~vertex() {}; };. class Mvertex : public vertex {};. class vCont {; public:; virtual ~vCont() { for (auto& v: verts) delete v; }; std::vector<vertex*> verts { new vertex(), new Mvertex() };; const std::vector<vertex*>& vertices() { return verts; }; }; }""""""; """"""Vector given an array of different type should copy convert""""""; """"""Test mapping of python strings and std::[w]string""""""; # pass through const std::[w]string&; # pass through std::string (by value); # getting through std::[w]string&; """"""Test access to std::string object data members""""""; """"""Test that strings with NULL do not get truncated""""""; """"""Access to global arrays of strings""""""; # fix up the size; # multi-dimensional; """"""Mixing unicode and std::",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_stltypes.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_stltypes.py
Security,access,access,"s); # Create from various iteratables; # as above, can not put strings in type-checked containers; #for s in (""123"", """", range(1000), ('do', 1.2), range(2000,2200,5)):; # as above, no strings; #assert type2test(c for c in ""123"") == type2test(""123""); # Issue #23757 (in CPython); #assert type2test(LyingTuple((2,))) == type2test((1,)); #assert type2test(LyingList([2])) == type2test([1]); """"""Detailed slicing tests from CPython""""""; # Extended slices; # Test extreme cases with long ints; # the following two fail b/c PySlice_GetIndices succeeds w/o error, while; # returning an overflown value (list object uses different internal APIs); #assert a[ -pow(2,128): 3 ] == type2test([0,1,2]); #assert a[ 3: pow(2,145) ] == type2test([3,4]); """"""Test access to std::vector<int>/std::vector<double>""""""; #-----; #-----; #-----; """"""Test access to an std::vector<just_a_class>""""""; """"""Test behavior of empty std::vector<int>""""""; """"""Test iteration over an std::vector<int>""""""; """"""Test usage of += of iterable on push_back-able container""""""; # string shouldn't pass; # TODO: decide whether this should roll-back; """"""Test python-style indexing to an std::vector<int>""""""; # 2 off from start, 1 from end; """"""Usability of std::vector<bool> which can be a specialization""""""; """"""Usability of std::vector<> of some enums""""""; """"""Adverse effect of implicit conversion on vector<string>""""""; """"""Use of std::distance with vector""""""; """"""Use of std::vector<std::pair>""""""; # after the original bug report; """"""; class PairVector {; public:; std::vector<std::pair<double, double>> vector_pair(const std::vector<std::pair<double, double>>& a) {; return a;; }; };; """"""; # TODO: nicer error handling for the following (current: template compilation failure trying; # to assign a pair with <double, string> to <double, double>); # ll2 = ll[:]; # ll2[2] = ll[2][:]; # ll2[2][1] = 'a'; # v = a.vector_pair(ll2); """"""Check lifeline setting on vectors of objects""""""; """"""namespace Lifeline {; static int count = 0;; template <typename T>; st",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_stltypes.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_stltypes.py
Testability,test,test,"# -*- coding: UTF-8 -*-; # after CPython's Lib/test/seq_tests.py; """"""Regular generator""""""; """"""Sequence using __getitem__""""""; """"""Sequence using iterator protocol""""""; # p2.7; """"""Sequence using iterator protocol defined with a generator""""""; """"""Missing __getitem__ and __iter__""""""; # p2.7; """"""Iterator missing __next__()""""""; """"""Test propagation of exceptions""""""; # p2.7; """"""Test immediate stop""""""; # p2.7; """"""Test multiple tiers of iterators""""""; # the following does not work for type-checked containers; #s = ""this is also a sequence""; #vv = type2test(s); #assert len(vv) == len(s); # Create from various iteratables; # as above, can not put strings in type-checked containers; #for s in (""123"", """", range(1000), ('do', 1.2), range(2000,2200,5)):; # as above, no strings; #assert type2test(c for c in ""123"") == type2test(""123""); # Issue #23757 (in CPython); #assert type2test(LyingTuple((2,))) == type2test((1,)); #assert type2test(LyingList([2])) == type2test([1]); """"""Detailed slicing tests from CPython""""""; # Extended slices; # Test extreme cases with long ints; # the following two fail b/c PySlice_GetIndices succeeds w/o error, while; # returning an overflown value (list object uses different internal APIs); #assert a[ -pow(2,128): 3 ] == type2test([0,1,2]); #assert a[ 3: pow(2,145) ] == type2test([3,4]); """"""Test access to std::vector<int>/std::vector<double>""""""; #-----; #-----; #-----; """"""Test access to an std::vector<just_a_class>""""""; """"""Test behavior of empty std::vector<int>""""""; """"""Test iteration over an std::vector<int>""""""; """"""Test usage of += of iterable on push_back-able container""""""; # string shouldn't pass; # TODO: decide whether this should roll-back; """"""Test python-style indexing to an std::vector<int>""""""; # 2 off from start, 1 from end; """"""Usability of std::vector<bool> which can be a specialization""""""; """"""Usability of std::vector<> of some enums""""""; """"""Adverse effect of implicit conversion on vector<string>""""""; """"""Use of std::distance with vector""""""; """"""Use of std::vec",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_stltypes.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_stltypes.py
Availability,avail,availability,"""""""Test availability of std::ostream""""""; """"""Test access to std::cout""""""; """"""Naming consistency if char_traits""""""; """"""\; namespace stringstream_base {; void pass_through_base(std::ostream& o) {; o << ""TEST STRING"";; } }""""""; # base class used to fail to match; """"""Naming consistency of ostringstream""""""",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_streams.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_streams.py
Security,access,access,"""""""Test availability of std::ostream""""""; """"""Test access to std::cout""""""; """"""Naming consistency if char_traits""""""; """"""\; namespace stringstream_base {; void pass_through_base(std::ostream& o) {; o << ""TEST STRING"";; } }""""""; # base class used to fail to match; """"""Naming consistency of ostringstream""""""",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_streams.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_streams.py
Availability,avail,available,"alue = 42;. template <typename T>; struct DerivedClassUsingStatic : public BaseClassWithStatic<T> {; using BaseClassWithStatic<T>::ref_value;. explicit DerivedClassUsingStatic(T x) : BaseClassWithStatic<T>() {; m_value = x > ref_value ? ref_value : x;; }. T m_value;; };""""""; # assert b1.ref_value == 42; # assert b2.ref_value == 42; """"""Test that templated operator() translates to __call__""""""; """"""Test that base class methods are not considered when hidden""""""; """"""Test templated constructors""""""; """"""\; template <typename T>; class RTTest_SomeClassWithTCtor {; public:; template<typename R>; RTTest_SomeClassWithTCtor(int n, R val) : m_double(n+val) {}; double m_double;; };. namespace RTTest_SomeNamespace {; template <typename T>; class RTTest_SomeClassWithTCtor {; public:; RTTest_SomeClassWithTCtor() : m_double(-1.) {}; template<typename R>; RTTest_SomeClassWithTCtor(int n, R val) : m_double(n+val) {}; double m_double;; };; } """"""; """"""Access to templates made available with 'using'""""""; # through dictionary; # through interpreter; # with variadic template; # with inner types using; # used to fail; """"""Access to base class templated methods through 'using'""""""; #assert type(d.get1['double'](5)) == float; #assert d.get1['double'](5) == 10.; """"""Use of a templated return type""""""; """"""\; struct RTTest_SomeStruct1 {};; template<class ...T> struct RTTest_TemplatedList {};; template<class ...T> auto rttest_make_tlist(T ... args) {; return RTTest_TemplatedList<T...>{};; }. namespace RTTest_SomeNamespace {; struct RTTest_SomeStruct2 {};; template<class ...T> struct RTTest_TemplatedList2 {};; }. template<class ...T> auto rttest_make_tlist2(T ... args) {; return RTTest_SomeNamespace::RTTest_TemplatedList2<T...>{};; } """"""; """"""Use of a template with r-values; should accept builtin types""""""; # bit of regression testing; # used to crash; # actual method calls; """"""Range of variadic templates""""""; # helper to make all platforms look the same; # templated class; # static functions; # member functio",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_templates.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_templates.py
Energy Efficiency,reduce,reducer,"ass>; class TNaFn;. template<class R, class... Args>; class TNaFn<R(Args...)>;. template<class T>; class TNaV;. template<class T>; class TNaA;. template<class T, class TNaA=TNaA<T>>; class TNaVA;. template<class T, class U=void>; class TNaVU;; }""""""; """"""\; namespace TNaRun_{n} {{; template<class T>; using f = {f};. template<class T>; using v = {v};. using fi = f<void ({i})>;; using fv = f<void (v<{i}>)>;. using fii = f<void ({i},{i})>;; using fiv = f<void ({i},v<{i}>)>;; using fvi = f<void (v<{i}>,{i})>;; using fvv = f<void (v<{i}>,v<{i}>)>;. using fiii = f<void ({i},{i},{i})>;; using fivi = f<void ({i},v<{i}>,{i})>;; using fvii = f<void (v<{i}>,{i},{i})>;; using fvvi = f<void (v<{i}>,v<{i}>,{i})>;. using fiiv = f<void ({i},{i},v<{i}>)>;; using fivv = f<void ({i},v<{i}>,v<{i}>)>;; using fviv = f<void (v<{i}>,{i},v<{i}>)>;; using fvvv = f<void (v<{i}>,v<{i}>,v<{i}>)>;; }}""""""; """"""`using` type as template argument""""""; """"""; namespace UsingPtr {; struct Test {};; using testptr = Test*;. template<typename T>; bool testfun(T const& x) { return !(bool)x; }; }""""""; # TODO: raises TypeError; the problem is that the type is resolved; # from UsingPtr::Test*const& to UsingPtr::Test*& (ie. `const` is lost); # assert ns.testfun[""UsingPtr::testptr""](cppyy.nullptr); """"""`const char*` use over std::string""""""; """"""Test presence and validity of using typedefs""""""; """"""Test that mapped types can be used as builtin""""""; # TODO: 'long long', 'unsigned long long'; """"""Test that mapped types can be used as template arguments""""""; """"""Usage of type reducer""""""; """"""; template <typename T> struct DeductTest_Wrap {; static auto whatis(T t) { return t; }; };; """"""; """"""Usage of type reducer with extern template""""""; """"""\; namespace FailedTypeDeducer {; template<class T>; class A {; public:; T result() { return T{5}; }; };. extern template class A<int>;; }""""""; # feature disabled; # FailedTypeDeducer::B is defined in the templates.h header; """"""Squash template expressions for binary operations (like in gmpxx)""""""",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_templates.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_templates.py
Modifiability,inherit,inheritance,". Us, typename... Ts>; static EventId Schedule2 (Time const &delay, EventId (*f)(Us...), Ts&&... args) {; return f(args...);; }; template <typename... Us, typename... Ts>; static EventId Schedule3 (Time const &delay, EventId (*f)(Us...), Ts&&... args) {; return f(args...);; }; template <typename... Us, typename... Ts>; static EventId Schedule4 (Time const &delay, EventId (*f)(Us...), Ts&&... args) {; return f(args...);; }; template <typename... Us, typename... Ts>; static EventId Schedule5 (Time const &delay, EventId (*f)(Us...), Ts&&... args) {; return f(args...);; }; template <typename... Us, typename... Ts>; static EventId Schedule6 (Time const &delay, EventId (*f)(Us...), Ts&&... args) {; return f(args...);; }; };. EventId cpp_adapt(Node& n) {; return EventId{n.fData};; } }""""""; # based on reflected __cpp_name__; # based on explicit __cpp_name__; # based on __annotations__ (p3.5 and later); # verify that the node is correctly modified; """"""Mix of (non-)templated across inheritance""""""; """"""namespace MixNMatch {; class NonTemplated {; public:; double& operator[](int idx) { return fPayLoad; }. protected:; double fPayLoad = 0;; };. class Templated: public NonTemplated {; public:; double& operator[](int idx) { return fPayLoad; }; template <typename T> double& operator[](int idx) { return fPayLoad; }; }; }""""""; # used to crash; """"""Verify lookup of template names with << in the name""""""; """"""\; namespace TestSomeLut {; template<class T, uint8_t X, uint8_t Y>; struct Lut {; Lut() { }; constexpr size_t size() const noexcept { return (1<<X)+1; }. std::array<T, 3> data1;; std::array<T, X> data2;; std::array<T, 2*X> data3;; std::array<T, 16385> data4;; std::array<T, (1UL<<(std::size_t)3)+1UL> data5;; std::array<T, ((1<<3)+1)> data6;; std::array<T, ((1<<X)+1)> data7;; static int constexpr array_size = X<<2;; std::array<T, array_size> data8;; };. template<class T, uint8_t X, uint8_t Y, uint32_t asize=((1<<X)+1)>; struct Lut2 {; Lut2() { }; constexpr size_t size() const noexcept { re",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_templates.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_templates.py
Performance,load,loaded,"""""""Template reflection""""""; """"""Template member functions lookup and calls""""""; # implicit (called before other tests to check caching); # pre-instantiated; # specialized; # auto-instantiation; # auto through typedef; """"""Use of non-types as template arguments""""""; """"""Templated global and static functions lookup and calls""""""; # TODO: the following only works if something else has already; # loaded the headers associated with this template; # test forced creation of subsequent overloads; # float in, float out; # int in, float out; # float in, int out; # int in, int out; """"""Call a variadic function""""""; # Fails; wrong overload on PyPy, none on CPython; #s << ""(""; """"""; template<typename... myTypes>; int test04_variadic_func() { return sizeof...(myTypes); }; """"""; """"""Call an overloaded variadic function""""""; """"""Attribute testing through SFINAE""""""; # load; """"""Traits/type deduction""""""; # load; # TODO: the following crashes deep inside cling/clang ...; #raises(TypeError, getattr, select_template_arg[2, Obj1, Obj2], 'argument'); # This is a bit subtle: to be able to use typedefs in templates, builtin; # types are present as subclasses that carry __cpp_name__, hence the result; # is not 'int' or 'float', but such custom subtypes; """"""Derived class using static data of base""""""; """"""; template <typename T> struct BaseClassWithStatic {; static T const ref_value;; };. template <typename T>; T const BaseClassWithStatic<T>::ref_value = 42;. template <typename T>; struct DerivedClassUsingStatic : public BaseClassWithStatic<T> {; using BaseClassWithStatic<T>::ref_value;. explicit DerivedClassUsingStatic(T x) : BaseClassWithStatic<T>() {; m_value = x > ref_value ? ref_value : x;; }. T m_value;; };""""""; # assert b1.ref_value == 42; # assert b2.ref_value == 42; """"""Test that templated operator() translates to __call__""""""; """"""Test that base class methods are not considered when hidden""""""; """"""Test templated constructors""""""; """"""\; template <typename T>; class RTTest_SomeClassWithTCtor {; public:; temp",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_templates.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_templates.py
Security,access,access,"ce or pointer args""""""; # used to fail b/c type trimming threw away end ')' together with '*' or '&'; """"""\; namespace LambdaAndTemplates {; template <typename T>; struct S {};. template <typename T>; bool f(const std::function<bool(const S<T>&)>& callback) {; return callback({});; }. template <typename T>; bool f_noref(const std::function<bool(const S<T>)>& callback) {; return callback({});; }. struct S0 {};. bool f_notemplate(const std::function<bool(const S0&)>& callback) {; return callback({});; } }""""""; # similar/same problem as above; """"""\; namespace LambdaAndTemplates {; template <typename T>; bool f_nofun(bool (*callback)(const S<T>&)) {; return callback({});; } }""""""; # following used to fail argument conversion; """"""\; namespace FuncPtrArrays {; typedef struct {; double* a0, *a1, *a2, *a3;; } Arrays;. typedef struct {; void (*fnc) (Arrays* const, Arrays* const);; } Foo;. void bar(Arrays* const, Arrays* const) {; return;; } }""""""; # <- this access used to fail; """"""Deduction of types with partial templates""""""; """"""\; template <typename A, typename B>; B partial_template_foo1(B b) { return b; }. template <typename A, typename B>; B partial_template_foo2(B b) { return b; }. namespace partial_template {; template <typename A, typename B>; B foo1(B b) { return b; }. template <typename A, typename B>; B foo2(B b) { return b; }; } """"""; """"""\; template <typename A, typename... Other, typename B>; B partial_template_bar1(B b) { return b; }. template <typename A, typename... Other, typename B>; B partial_template_bar2(B b) { return b; }. namespace partial_template {; template <typename A, typename... Other, typename B>; B bar1(B b) { return b; }. template <typename A, typename... Other, typename B>; B bar2(B b) { return b; }; }""""""; """"""Use of variadic template function as contructor""""""; """"""\; namespace VadiadicConstructor {; class Atom {; public:; using mass_type = double;. Atom() {}. template<typename... Args>; explicit Atom(const mass_type& mass_in, Args&&... args) :; Atom(",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_templates.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_templates.py
Testability,test,tests,"""""""Template reflection""""""; """"""Template member functions lookup and calls""""""; # implicit (called before other tests to check caching); # pre-instantiated; # specialized; # auto-instantiation; # auto through typedef; """"""Use of non-types as template arguments""""""; """"""Templated global and static functions lookup and calls""""""; # TODO: the following only works if something else has already; # loaded the headers associated with this template; # test forced creation of subsequent overloads; # float in, float out; # int in, float out; # float in, int out; # int in, int out; """"""Call a variadic function""""""; # Fails; wrong overload on PyPy, none on CPython; #s << ""(""; """"""; template<typename... myTypes>; int test04_variadic_func() { return sizeof...(myTypes); }; """"""; """"""Call an overloaded variadic function""""""; """"""Attribute testing through SFINAE""""""; # load; """"""Traits/type deduction""""""; # load; # TODO: the following crashes deep inside cling/clang ...; #raises(TypeError, getattr, select_template_arg[2, Obj1, Obj2], 'argument'); # This is a bit subtle: to be able to use typedefs in templates, builtin; # types are present as subclasses that carry __cpp_name__, hence the result; # is not 'int' or 'float', but such custom subtypes; """"""Derived class using static data of base""""""; """"""; template <typename T> struct BaseClassWithStatic {; static T const ref_value;; };. template <typename T>; T const BaseClassWithStatic<T>::ref_value = 42;. template <typename T>; struct DerivedClassUsingStatic : public BaseClassWithStatic<T> {; using BaseClassWithStatic<T>::ref_value;. explicit DerivedClassUsingStatic(T x) : BaseClassWithStatic<T>() {; m_value = x > ref_value ? ref_value : x;; }. T m_value;; };""""""; # assert b1.ref_value == 42; # assert b2.ref_value == 42; """"""Test that templated operator() translates to __call__""""""; """"""Test that base class methods are not considered when hidden""""""; """"""Test templated constructors""""""; """"""\; template <typename T>; class RTTest_SomeClassWithTCtor {; public:; temp",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy/test/test_templates.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/test/test_templates.py
Availability,alive,alive,"#!/usr/bin/env python; # p3; # main only needed in more recent root b/c of rootcling; #; #; ## ROOT source pull and cleansing; #; # now take the removed entries out of the CMakeLists.txt; # remove everything except for the listed set of libraries; # trim main (only need rootcling); # trim core (remove lz4 and lzma, and gGLManager which otherwise crashes on call); # get rid of v7 stuff; # do not copy wchar.h & friends b/c the pch should be generated at install time,; # so preventing conflict; # remove extraneous external software explicitly; # openui5 (doesn't follow convention); # remove testing, examples, and notebook; # some more explicit removes:; # special fixes; """"""//#include ""TSocket.h""; enum ESockOptions {; kSendBuffer, // size of send buffer; kRecvBuffer, // size of receive buffer; kOobInline, // OOB message inline; kKeepAlive, // keep socket alive; kReuseAddr, // allow reuse of local portion of address 5-tuple; kNoDelay, // send without delay; kNoBlock, // non-blocking I/O; kProcessGroup, // socket process group (used for SIGURG and SIGIO); kAtMark, // are we at out-of-band mark (read only); kBytesToRead // get number of bytes to read, FIONREAD (read only); };. enum ESendRecvOptions {; kDefault, // default option (= 0); kOob, // send or receive out-of-band data; kPeek, // peek at incoming message (receive only); kDontBlock // send/recv as much data as possible without blocking; };; """"""; # done; # debugging: run a test build; #; ## package creation; #; #; ## apply patches (in order); #; #; ## manylinux1 specific patch, as there a different, older, compiler is used; #; #; ## finally, remove the ROOT source directory, as it can not be reused; #; # done!",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy-backend/cling/create_src_directory.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/cling/create_src_directory.py
Deployability,install,install,"#!/usr/bin/env python; # p3; # main only needed in more recent root b/c of rootcling; #; #; ## ROOT source pull and cleansing; #; # now take the removed entries out of the CMakeLists.txt; # remove everything except for the listed set of libraries; # trim main (only need rootcling); # trim core (remove lz4 and lzma, and gGLManager which otherwise crashes on call); # get rid of v7 stuff; # do not copy wchar.h & friends b/c the pch should be generated at install time,; # so preventing conflict; # remove extraneous external software explicitly; # openui5 (doesn't follow convention); # remove testing, examples, and notebook; # some more explicit removes:; # special fixes; """"""//#include ""TSocket.h""; enum ESockOptions {; kSendBuffer, // size of send buffer; kRecvBuffer, // size of receive buffer; kOobInline, // OOB message inline; kKeepAlive, // keep socket alive; kReuseAddr, // allow reuse of local portion of address 5-tuple; kNoDelay, // send without delay; kNoBlock, // non-blocking I/O; kProcessGroup, // socket process group (used for SIGURG and SIGIO); kAtMark, // are we at out-of-band mark (read only); kBytesToRead // get number of bytes to read, FIONREAD (read only); };. enum ESendRecvOptions {; kDefault, // default option (= 0); kOob, // send or receive out-of-band data; kPeek, // peek at incoming message (receive only); kDontBlock // send/recv as much data as possible without blocking; };; """"""; # done; # debugging: run a test build; #; ## package creation; #; #; ## apply patches (in order); #; #; ## manylinux1 specific patch, as there a different, older, compiler is used; #; #; ## finally, remove the ROOT source directory, as it can not be reused; #; # done!",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy-backend/cling/create_src_directory.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/cling/create_src_directory.py
Integrability,message,message,"#!/usr/bin/env python; # p3; # main only needed in more recent root b/c of rootcling; #; #; ## ROOT source pull and cleansing; #; # now take the removed entries out of the CMakeLists.txt; # remove everything except for the listed set of libraries; # trim main (only need rootcling); # trim core (remove lz4 and lzma, and gGLManager which otherwise crashes on call); # get rid of v7 stuff; # do not copy wchar.h & friends b/c the pch should be generated at install time,; # so preventing conflict; # remove extraneous external software explicitly; # openui5 (doesn't follow convention); # remove testing, examples, and notebook; # some more explicit removes:; # special fixes; """"""//#include ""TSocket.h""; enum ESockOptions {; kSendBuffer, // size of send buffer; kRecvBuffer, // size of receive buffer; kOobInline, // OOB message inline; kKeepAlive, // keep socket alive; kReuseAddr, // allow reuse of local portion of address 5-tuple; kNoDelay, // send without delay; kNoBlock, // non-blocking I/O; kProcessGroup, // socket process group (used for SIGURG and SIGIO); kAtMark, // are we at out-of-band mark (read only); kBytesToRead // get number of bytes to read, FIONREAD (read only); };. enum ESendRecvOptions {; kDefault, // default option (= 0); kOob, // send or receive out-of-band data; kPeek, // peek at incoming message (receive only); kDontBlock // send/recv as much data as possible without blocking; };; """"""; # done; # debugging: run a test build; #; ## package creation; #; #; ## apply patches (in order); #; #; ## manylinux1 specific patch, as there a different, older, compiler is used; #; #; ## finally, remove the ROOT source directory, as it can not be reused; #; # done!",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy-backend/cling/create_src_directory.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/cling/create_src_directory.py
Testability,test,testing,"#!/usr/bin/env python; # p3; # main only needed in more recent root b/c of rootcling; #; #; ## ROOT source pull and cleansing; #; # now take the removed entries out of the CMakeLists.txt; # remove everything except for the listed set of libraries; # trim main (only need rootcling); # trim core (remove lz4 and lzma, and gGLManager which otherwise crashes on call); # get rid of v7 stuff; # do not copy wchar.h & friends b/c the pch should be generated at install time,; # so preventing conflict; # remove extraneous external software explicitly; # openui5 (doesn't follow convention); # remove testing, examples, and notebook; # some more explicit removes:; # special fixes; """"""//#include ""TSocket.h""; enum ESockOptions {; kSendBuffer, // size of send buffer; kRecvBuffer, // size of receive buffer; kOobInline, // OOB message inline; kKeepAlive, // keep socket alive; kReuseAddr, // allow reuse of local portion of address 5-tuple; kNoDelay, // send without delay; kNoBlock, // non-blocking I/O; kProcessGroup, // socket process group (used for SIGURG and SIGIO); kAtMark, // are we at out-of-band mark (read only); kBytesToRead // get number of bytes to read, FIONREAD (read only); };. enum ESendRecvOptions {; kDefault, // default option (= 0); kOob, // send or receive out-of-band data; kPeek, // peek at incoming message (receive only); kDontBlock // send/recv as much data as possible without blocking; };; """"""; # done; # debugging: run a test build; #; ## package creation; #; #; ## apply patches (in order); #; #; ## manylinux1 specific patch, as there a different, older, compiler is used; #; #; ## finally, remove the ROOT source directory, as it can not be reused; #; # done!",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy-backend/cling/create_src_directory.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/cling/create_src_directory.py
Availability,avail,available,"#; # platform-dependent helpers; #; # debug info too large for wheels; """"""cppyy-cling build.""""""; """"""cppyy-cling source.""""""; """"""cppyy-cling installation.""""""; #; # customized commands; #; # base run; # custom run; # get C++ standard to use, if set; # current cmake claims MSVC'17 does not support C++17 yet; # extra optimization flags for Cling; # use $MAKE to build if it is defined; # default to using all available cores (x2 if hyperthreading enabled); # Custom clean. Clean everything except that which the base clean; # (see below) or create_src_directory.py is responsible for.; # remove build directories; # Base clean.; # depending on goal, copy over pre-installed tree; # base install; # custom install of backend; # use $MAKE to install if it is defined; # remove allDict.cxx.pch as it's not portable (rebuild on first run, see cppyy); # for manylinux, reset the default cxxversion to 17 if no user override; # this is a universal, but platform-specific package; a combination; # that wheel does not recognize, thus simply fool it; # add allDict.cxx.pch to record; #; # customized distribition to disable binaries; #; # disable bdist_egg as it only packages the python code, skipping the build; # Author details",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy-backend/cling/setup.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/cling/setup.py
Deployability,install,installation,"#; # platform-dependent helpers; #; # debug info too large for wheels; """"""cppyy-cling build.""""""; """"""cppyy-cling source.""""""; """"""cppyy-cling installation.""""""; #; # customized commands; #; # base run; # custom run; # get C++ standard to use, if set; # current cmake claims MSVC'17 does not support C++17 yet; # extra optimization flags for Cling; # use $MAKE to build if it is defined; # default to using all available cores (x2 if hyperthreading enabled); # Custom clean. Clean everything except that which the base clean; # (see below) or create_src_directory.py is responsible for.; # remove build directories; # Base clean.; # depending on goal, copy over pre-installed tree; # base install; # custom install of backend; # use $MAKE to install if it is defined; # remove allDict.cxx.pch as it's not portable (rebuild on first run, see cppyy); # for manylinux, reset the default cxxversion to 17 if no user override; # this is a universal, but platform-specific package; a combination; # that wheel does not recognize, thus simply fool it; # add allDict.cxx.pch to record; #; # customized distribition to disable binaries; #; # disable bdist_egg as it only packages the python code, skipping the build; # Author details",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy-backend/cling/setup.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/cling/setup.py
Integrability,depend,dependent,"#; # platform-dependent helpers; #; # debug info too large for wheels; """"""cppyy-cling build.""""""; """"""cppyy-cling source.""""""; """"""cppyy-cling installation.""""""; #; # customized commands; #; # base run; # custom run; # get C++ standard to use, if set; # current cmake claims MSVC'17 does not support C++17 yet; # extra optimization flags for Cling; # use $MAKE to build if it is defined; # default to using all available cores (x2 if hyperthreading enabled); # Custom clean. Clean everything except that which the base clean; # (see below) or create_src_directory.py is responsible for.; # remove build directories; # Base clean.; # depending on goal, copy over pre-installed tree; # base install; # custom install of backend; # use $MAKE to install if it is defined; # remove allDict.cxx.pch as it's not portable (rebuild on first run, see cppyy); # for manylinux, reset the default cxxversion to 17 if no user override; # this is a universal, but platform-specific package; a combination; # that wheel does not recognize, thus simply fool it; # add allDict.cxx.pch to record; #; # customized distribition to disable binaries; #; # disable bdist_egg as it only packages the python code, skipping the build; # Author details",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy-backend/cling/setup.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/cling/setup.py
Modifiability,portab,portable,"#; # platform-dependent helpers; #; # debug info too large for wheels; """"""cppyy-cling build.""""""; """"""cppyy-cling source.""""""; """"""cppyy-cling installation.""""""; #; # customized commands; #; # base run; # custom run; # get C++ standard to use, if set; # current cmake claims MSVC'17 does not support C++17 yet; # extra optimization flags for Cling; # use $MAKE to build if it is defined; # default to using all available cores (x2 if hyperthreading enabled); # Custom clean. Clean everything except that which the base clean; # (see below) or create_src_directory.py is responsible for.; # remove build directories; # Base clean.; # depending on goal, copy over pre-installed tree; # base install; # custom install of backend; # use $MAKE to install if it is defined; # remove allDict.cxx.pch as it's not portable (rebuild on first run, see cppyy); # for manylinux, reset the default cxxversion to 17 if no user override; # this is a universal, but platform-specific package; a combination; # that wheel does not recognize, thus simply fool it; # add allDict.cxx.pch to record; #; # customized distribition to disable binaries; #; # disable bdist_egg as it only packages the python code, skipping the build; # Author details",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy-backend/cling/setup.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/cling/setup.py
Performance,optimiz,optimization,"#; # platform-dependent helpers; #; # debug info too large for wheels; """"""cppyy-cling build.""""""; """"""cppyy-cling source.""""""; """"""cppyy-cling installation.""""""; #; # customized commands; #; # base run; # custom run; # get C++ standard to use, if set; # current cmake claims MSVC'17 does not support C++17 yet; # extra optimization flags for Cling; # use $MAKE to build if it is defined; # default to using all available cores (x2 if hyperthreading enabled); # Custom clean. Clean everything except that which the base clean; # (see below) or create_src_directory.py is responsible for.; # remove build directories; # Base clean.; # depending on goal, copy over pre-installed tree; # base install; # custom install of backend; # use $MAKE to install if it is defined; # remove allDict.cxx.pch as it's not portable (rebuild on first run, see cppyy); # for manylinux, reset the default cxxversion to 17 if no user override; # this is a universal, but platform-specific package; a combination; # that wheel does not recognize, thus simply fool it; # add allDict.cxx.pch to record; #; # customized distribition to disable binaries; #; # disable bdist_egg as it only packages the python code, skipping the build; # Author details",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy-backend/cling/setup.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/cling/setup.py
Usability,simpl,simply,"#; # platform-dependent helpers; #; # debug info too large for wheels; """"""cppyy-cling build.""""""; """"""cppyy-cling source.""""""; """"""cppyy-cling installation.""""""; #; # customized commands; #; # base run; # custom run; # get C++ standard to use, if set; # current cmake claims MSVC'17 does not support C++17 yet; # extra optimization flags for Cling; # use $MAKE to build if it is defined; # default to using all available cores (x2 if hyperthreading enabled); # Custom clean. Clean everything except that which the base clean; # (see below) or create_src_directory.py is responsible for.; # remove build directories; # Base clean.; # depending on goal, copy over pre-installed tree; # base install; # custom install of backend; # use $MAKE to install if it is defined; # remove allDict.cxx.pch as it's not portable (rebuild on first run, see cppyy); # for manylinux, reset the default cxxversion to 17 if no user override; # this is a universal, but platform-specific package; a combination; # that wheel does not recognize, thus simply fool it; # add allDict.cxx.pch to record; #; # customized distribition to disable binaries; #; # disable bdist_egg as it only packages the python code, skipping the build; # Author details",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy-backend/cling/setup.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/cling/setup.py
Availability,avail,available,"ng, such as ctypes.c_int.; #; #; # Valid customisations are routines named ""c13n_<something>"".; #; """"""; Wrap setuptools.setup for some bindings. :param pkg: Name of the bindings.; :param setup_py: Base __init__.py file of the bindings.; :param cmake_shared_library_prefix:; ${cmake_shared_library_prefix}; :param cmake_shared_library_suffix:; ${cmake_shared_library_suffix}; :param extra_pythons: Semicolon-separated list of customisation code.; :param pkg_version: The version of the bindings.; :param author: The name of the library author.; :param author_email: The email address of the library author.; :param url: The home page for the library.; :param license: The license.; """"""; """"""Bindings for {}.; These bindings are based on https://cppyy.readthedocs.io/en/latest/, and can be; used as per the documentation provided via the cppyy.cgl namespace. The environment; variable LD_LIBRARY_PATH must contain the path of the {}.rootmap file. Use; ""import cppyy; from cppyy.gbl import <some-C++-entity>"". Alternatively, use ""import {}"". This convenience wrapper supports ""discovery"" of the; available C++ entities using, for example Python 3's command line completion support.; """"""; #; # Base build.; #; #; # Custom build.; #; #; # Move CMake output to self.build_lib.; #; #; # Implement a pkgutil-style namespace package as per the guidance on; # https://packaging.python.org/guides/packaging-namespace-packages.; #; #; # Custom clean.; # TODO: There is no way to reliably clean the ""dist"" directory.; #; #; # Base clean.; #; #; # This is a universal (Python2/Python3), but platform-specific (has; # compiled parts) package; a combination that wheel does not recognize,; # thus simply fool it.; #; """"""; What pip versions do we have?. :return: [pip_program]; """"""; #; # The command 'pip -V' returns a string of the form:; #; # pip 9.0.1 from /usr/lib/python2.7/dist-packages (python 2.7); #; #; # All pip variants that map onto a given Python version are de-duped.; #; #; # We want the pip names.; #",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/bindings_utils.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/bindings_utils.py
Integrability,rout,routines,"_prefix}; :param cmake_shared_library_suffix:; ${cmake_shared_library_suffix}; """"""; """"""; Map the given C++ operator name on the python equivalent.; """"""; # dereference v.s. multiplication of two instances; # unary positive v.s. addition of two instances; # unary negative v.s. subtraction of two instances; # prefix v.s. postfix increment; # prefix v.s. postfix decrement; # might get here, as not all operator methods are handled (new, delete, etc.); #; # Add level 1 objects to the pkg namespace.; #; #; # Ignore some names based on heuristics.; #; #; # Don't attempt to look up numbers (i.e. non-type template parameters).; #; #; # Classes, variables etc.; #; #raise; #; # Load the library.; #; #; # Parse the map file.; #; #; # Iterate over all the items at the top level of each file, and add them; # to the pkg.; #; #; # Load any customisations.; #; #; # Deleting the modules after use runs the risk of GC running on; # stuff we are using, such as ctypes.c_int.; #; #; # Valid customisations are routines named ""c13n_<something>"".; #; """"""; Wrap setuptools.setup for some bindings. :param pkg: Name of the bindings.; :param setup_py: Base __init__.py file of the bindings.; :param cmake_shared_library_prefix:; ${cmake_shared_library_prefix}; :param cmake_shared_library_suffix:; ${cmake_shared_library_suffix}; :param extra_pythons: Semicolon-separated list of customisation code.; :param pkg_version: The version of the bindings.; :param author: The name of the library author.; :param author_email: The email address of the library author.; :param url: The home page for the library.; :param license: The license.; """"""; """"""Bindings for {}.; These bindings are based on https://cppyy.readthedocs.io/en/latest/, and can be; used as per the documentation provided via the cppyy.cgl namespace. The environment; variable LD_LIBRARY_PATH must contain the path of the {}.rootmap file. Use; ""import cppyy; from cppyy.gbl import <some-C++-entity>"". Alternatively, use ""import {}"". This convenience wrapp",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/bindings_utils.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/bindings_utils.py
Modifiability,variab,variables," for bindings.; """"""; #; # Python2.; #; #; # Python3.; #; # Optional; only necessary if you want to be able to import the module; # by name later.; # Keep PyCharm happy.; """"""; Initialise the bindings module. :param pkg: The bindings package.; :param __init__py: Base __init__.py file of the bindings.; :param cmake_shared_library_prefix:; ${cmake_shared_library_prefix}; :param cmake_shared_library_suffix:; ${cmake_shared_library_suffix}; """"""; """"""; Map the given C++ operator name on the python equivalent.; """"""; # dereference v.s. multiplication of two instances; # unary positive v.s. addition of two instances; # unary negative v.s. subtraction of two instances; # prefix v.s. postfix increment; # prefix v.s. postfix decrement; # might get here, as not all operator methods are handled (new, delete, etc.); #; # Add level 1 objects to the pkg namespace.; #; #; # Ignore some names based on heuristics.; #; #; # Don't attempt to look up numbers (i.e. non-type template parameters).; #; #; # Classes, variables etc.; #; #raise; #; # Load the library.; #; #; # Parse the map file.; #; #; # Iterate over all the items at the top level of each file, and add them; # to the pkg.; #; #; # Load any customisations.; #; #; # Deleting the modules after use runs the risk of GC running on; # stuff we are using, such as ctypes.c_int.; #; #; # Valid customisations are routines named ""c13n_<something>"".; #; """"""; Wrap setuptools.setup for some bindings. :param pkg: Name of the bindings.; :param setup_py: Base __init__.py file of the bindings.; :param cmake_shared_library_prefix:; ${cmake_shared_library_prefix}; :param cmake_shared_library_suffix:; ${cmake_shared_library_suffix}; :param extra_pythons: Semicolon-separated list of customisation code.; :param pkg_version: The version of the bindings.; :param author: The name of the library author.; :param author_email: The email address of the library author.; :param url: The home page for the library.; :param license: The license.; """"""; """"""Bindings fo",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/bindings_utils.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/bindings_utils.py
Safety,risk,risk," __init__.py file of the bindings.; :param cmake_shared_library_prefix:; ${cmake_shared_library_prefix}; :param cmake_shared_library_suffix:; ${cmake_shared_library_suffix}; """"""; """"""; Map the given C++ operator name on the python equivalent.; """"""; # dereference v.s. multiplication of two instances; # unary positive v.s. addition of two instances; # unary negative v.s. subtraction of two instances; # prefix v.s. postfix increment; # prefix v.s. postfix decrement; # might get here, as not all operator methods are handled (new, delete, etc.); #; # Add level 1 objects to the pkg namespace.; #; #; # Ignore some names based on heuristics.; #; #; # Don't attempt to look up numbers (i.e. non-type template parameters).; #; #; # Classes, variables etc.; #; #raise; #; # Load the library.; #; #; # Parse the map file.; #; #; # Iterate over all the items at the top level of each file, and add them; # to the pkg.; #; #; # Load any customisations.; #; #; # Deleting the modules after use runs the risk of GC running on; # stuff we are using, such as ctypes.c_int.; #; #; # Valid customisations are routines named ""c13n_<something>"".; #; """"""; Wrap setuptools.setup for some bindings. :param pkg: Name of the bindings.; :param setup_py: Base __init__.py file of the bindings.; :param cmake_shared_library_prefix:; ${cmake_shared_library_prefix}; :param cmake_shared_library_suffix:; ${cmake_shared_library_suffix}; :param extra_pythons: Semicolon-separated list of customisation code.; :param pkg_version: The version of the bindings.; :param author: The name of the library author.; :param author_email: The email address of the library author.; :param url: The home page for the library.; :param license: The license.; """"""; """"""Bindings for {}.; These bindings are based on https://cppyy.readthedocs.io/en/latest/, and can be; used as per the documentation provided via the cppyy.cgl namespace. The environment; variable LD_LIBRARY_PATH must contain the path of the {}.rootmap file. Use; ""import cppyy; ",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/bindings_utils.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/bindings_utils.py
Usability,guid,guidance,"ng, such as ctypes.c_int.; #; #; # Valid customisations are routines named ""c13n_<something>"".; #; """"""; Wrap setuptools.setup for some bindings. :param pkg: Name of the bindings.; :param setup_py: Base __init__.py file of the bindings.; :param cmake_shared_library_prefix:; ${cmake_shared_library_prefix}; :param cmake_shared_library_suffix:; ${cmake_shared_library_suffix}; :param extra_pythons: Semicolon-separated list of customisation code.; :param pkg_version: The version of the bindings.; :param author: The name of the library author.; :param author_email: The email address of the library author.; :param url: The home page for the library.; :param license: The license.; """"""; """"""Bindings for {}.; These bindings are based on https://cppyy.readthedocs.io/en/latest/, and can be; used as per the documentation provided via the cppyy.cgl namespace. The environment; variable LD_LIBRARY_PATH must contain the path of the {}.rootmap file. Use; ""import cppyy; from cppyy.gbl import <some-C++-entity>"". Alternatively, use ""import {}"". This convenience wrapper supports ""discovery"" of the; available C++ entities using, for example Python 3's command line completion support.; """"""; #; # Base build.; #; #; # Custom build.; #; #; # Move CMake output to self.build_lib.; #; #; # Implement a pkgutil-style namespace package as per the guidance on; # https://packaging.python.org/guides/packaging-namespace-packages.; #; #; # Custom clean.; # TODO: There is no way to reliably clean the ""dist"" directory.; #; #; # Base clean.; #; #; # This is a universal (Python2/Python3), but platform-specific (has; # compiled parts) package; a combination that wheel does not recognize,; # thus simply fool it.; #; """"""; What pip versions do we have?. :return: [pip_program]; """"""; #; # The command 'pip -V' returns a string of the form:; #; # pip 9.0.1 from /usr/lib/python2.7/dist-packages (python 2.7); #; #; # All pip variants that map onto a given Python version are de-duped.; #; #; # We want the pip names.; #",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/bindings_utils.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/bindings_utils.py
Integrability,depend,dependencies,""""""" cppyy_backend loader; """"""; # load libcppyy_backend; # build precompiled header as necessary; # normal load, allowing for user overrides of LD_LIBRARY_PATH; # failed ... try absolute path; # needed on MacOS12 with soversion; # failed ... load dependencies explicitly; # the precompiled header of standard and system headers is not part of the; # distribution as there are too many varieties; create it now if needed; # test whether the pch is older than the include directory; # no point in updating as it will fail; # the precompiled header of standard and system headers is not part of the; # distribution as there are too many varieties; create it now if needed; # only ever call once, even if failed; # magic keyword to disable pch; # quiet",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/loader.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/loader.py
Performance,load,loader,""""""" cppyy_backend loader; """"""; # load libcppyy_backend; # build precompiled header as necessary; # normal load, allowing for user overrides of LD_LIBRARY_PATH; # failed ... try absolute path; # needed on MacOS12 with soversion; # failed ... load dependencies explicitly; # the precompiled header of standard and system headers is not part of the; # distribution as there are too many varieties; create it now if needed; # test whether the pch is older than the include directory; # no point in updating as it will fail; # the precompiled header of standard and system headers is not part of the; # distribution as there are too many varieties; create it now if needed; # only ever call once, even if failed; # magic keyword to disable pch; # quiet",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/loader.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/loader.py
Testability,test,test,""""""" cppyy_backend loader; """"""; # load libcppyy_backend; # build precompiled header as necessary; # normal load, allowing for user overrides of LD_LIBRARY_PATH; # failed ... try absolute path; # needed on MacOS12 with soversion; # failed ... load dependencies explicitly; # the precompiled header of standard and system headers is not part of the; # distribution as there are too many varieties; create it now if needed; # test whether the pch is older than the include directory; # no point in updating as it will fail; # the precompiled header of standard and system headers is not part of the; # distribution as there are too many varieties; create it now if needed; # only ever call once, even if failed; # magic keyword to disable pch; # quiet",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/loader.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/loader.py
Modifiability,config,config,"# happens on Windows b/c root-config is a bash script; the; # following covers the most important options until that; # gets fixed upstream; # return flags+'/std:c++latest'; # most important is get the C++ version flag right; # most important are C++ flag and include directory",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/_cling_config.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/_cling_config.py
Availability,error,error,"itialisers.; # TEMPLATE_KINDS: The result type.; #; """"""; The parser does not seem to provide access to the complete text of a parameter.; This makes it hard to find any default values, so we:. 1. Run the lexer from ""here"" to the end of the file, bailing out when we see the "",""; or a "")"" marking the end.; 2. Watch for the assignment.; """"""; #; # Get the text after the ""="". Macro expansion can make relying on tokens fraught...and; # member.get_tokens() simply does not always return anything.; #; #; # Now count balanced anything-which-can-contain-a-comma till we get to the end.; #; """"""; Generate the translation for a typedef. :param container: A class or namespace.; :param typedef: The typedef object.; :param level: Recursion level controls indentation.; :param h_file: The source header file of interest.; :return: Info().; """"""; #; # Create an entry to collect information for this level of template arguments.; #; #; # Happens e.g. if the template is a dependent type; instead, try to parse; # its definition (brittle, but the original code just had '1' as a guess,; # which is even worse ...).; #; # logger.error(_(""Unexpected template_arg_count={} for {}"").format(tmp, typedef.type.get_typedef_name())); #; # Non-first template_infos are just parameters.; #; #; # This is a template parameter.; #; #; # Not a template.; #; #; # This must be a function type. TODO: what if there are no PARM_DECLs?; #; #; # TODO: this is actually the signature:; #; # ""int (Object::*)(QMetaObject::Call, int, void **)""; #; """"""; Generate the translation for a type. :param tag: ""typedef"", ""variable"" etc.; :param parent: The typed object.; :param level: Recursion level controls indentation.; :return: Info().; """"""; # https://github.com/Rip-Rip/clang_complete/issues/238; """"""; Locate clang's headers relative to its library (which is given on the; command line in --flags.; """"""; # default value; # gentoo; # opensuse; # Google; # x86_64 (openSUSE, Fedora); #; # Load the given libclang.; #; #; # Generate!; #",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/_cppyy_generator.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/_cppyy_generator.py
Integrability,depend,dependent,"itialisers.; # TEMPLATE_KINDS: The result type.; #; """"""; The parser does not seem to provide access to the complete text of a parameter.; This makes it hard to find any default values, so we:. 1. Run the lexer from ""here"" to the end of the file, bailing out when we see the "",""; or a "")"" marking the end.; 2. Watch for the assignment.; """"""; #; # Get the text after the ""="". Macro expansion can make relying on tokens fraught...and; # member.get_tokens() simply does not always return anything.; #; #; # Now count balanced anything-which-can-contain-a-comma till we get to the end.; #; """"""; Generate the translation for a typedef. :param container: A class or namespace.; :param typedef: The typedef object.; :param level: Recursion level controls indentation.; :param h_file: The source header file of interest.; :return: Info().; """"""; #; # Create an entry to collect information for this level of template arguments.; #; #; # Happens e.g. if the template is a dependent type; instead, try to parse; # its definition (brittle, but the original code just had '1' as a guess,; # which is even worse ...).; #; # logger.error(_(""Unexpected template_arg_count={} for {}"").format(tmp, typedef.type.get_typedef_name())); #; # Non-first template_infos are just parameters.; #; #; # This is a template parameter.; #; #; # Not a template.; #; #; # This must be a function type. TODO: what if there are no PARM_DECLs?; #; #; # TODO: this is actually the signature:; #; # ""int (Object::*)(QMetaObject::Call, int, void **)""; #; """"""; Generate the translation for a type. :param tag: ""typedef"", ""variable"" etc.; :param parent: The typed object.; :param level: Recursion level controls indentation.; :return: Info().; """"""; # https://github.com/Rip-Rip/clang_complete/issues/238; """"""; Locate clang's headers relative to its library (which is given on the; command line in --flags.; """"""; # default value; # gentoo; # opensuse; # Google; # x86_64 (openSUSE, Fedora); #; # Load the given libclang.; #; #; # Generate!; #",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/_cppyy_generator.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/_cppyy_generator.py
Modifiability,variab,variable,"itialisers.; # TEMPLATE_KINDS: The result type.; #; """"""; The parser does not seem to provide access to the complete text of a parameter.; This makes it hard to find any default values, so we:. 1. Run the lexer from ""here"" to the end of the file, bailing out when we see the "",""; or a "")"" marking the end.; 2. Watch for the assignment.; """"""; #; # Get the text after the ""="". Macro expansion can make relying on tokens fraught...and; # member.get_tokens() simply does not always return anything.; #; #; # Now count balanced anything-which-can-contain-a-comma till we get to the end.; #; """"""; Generate the translation for a typedef. :param container: A class or namespace.; :param typedef: The typedef object.; :param level: Recursion level controls indentation.; :param h_file: The source header file of interest.; :return: Info().; """"""; #; # Create an entry to collect information for this level of template arguments.; #; #; # Happens e.g. if the template is a dependent type; instead, try to parse; # its definition (brittle, but the original code just had '1' as a guess,; # which is even worse ...).; #; # logger.error(_(""Unexpected template_arg_count={} for {}"").format(tmp, typedef.type.get_typedef_name())); #; # Non-first template_infos are just parameters.; #; #; # This is a template parameter.; #; #; # Not a template.; #; #; # This must be a function type. TODO: what if there are no PARM_DECLs?; #; #; # TODO: this is actually the signature:; #; # ""int (Object::*)(QMetaObject::Call, int, void **)""; #; """"""; Generate the translation for a type. :param tag: ""typedef"", ""variable"" etc.; :param parent: The typed object.; :param level: Recursion level controls indentation.; :return: Info().; """"""; # https://github.com/Rip-Rip/clang_complete/issues/238; """"""; Locate clang's headers relative to its library (which is given on the; command line in --flags.; """"""; # default value; # gentoo; # opensuse; # Google; # x86_64 (openSUSE, Fedora); #; # Load the given libclang.; #; #; # Generate!; #",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/_cppyy_generator.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/_cppyy_generator.py
Safety,avoid,avoid,"g level.; :param sip: The sip.; :param key: The key in the sip which may need; fixing up.; :return:; """"""; #; # Depending on the type of the SIP entry, replace the Clang; # version of the value with the actual version.; #; """"""; Constructor. :param compile_flags: The compile flags for the file.; :param dump_modules: Turn on tracing for modules.; :param dump_items: Turn on tracing for container members.; :param dump_includes: Turn on diagnostics for include files.; :param dump_privates: Turn on diagnostics for omitted private items.; :param verbose: Turn on diagnostics for command lines.; """"""; """"""; Generate a dict describing the given source header file. This is the; main entry point for this class. :param h_file: The source header file of interest.; :returns: A dict corresponding to the h_file.; """"""; #; # Use Clang to parse the source and return its AST.; #; #; # We expect to be run over hundreds of files. Any parsing issues are likely to be very repetitive.; # So, to avoid bothering the user, we suppress duplicates.; #; #; # Run through the top level children in the translation unit.; #; """"""; Generate the (recursive) translation for a class or namespace. :param container: A class or namespace.; :param level: Recursion level controls indentation.; :param h_file: The source header file of interest.; :return: Info().; """"""; #; # Could this be a forward declaration?; #; #; # What level of template parameter is on this container?; #; #; # Only emit items in the translation unit.; #; #; # Structs and unions are emitted twice:; #; # - first as just the bare fields; # - then as children of the typedef; #; #; # Create an entry to collect information for this level of template arguments.; #; #; # Non-first template_infos are just parameters.; #; #; # This is a template parameter.; #; #; # Not a template.; #; """"""; In principle, we just want member.access_specifier.name.lower(), except that we need to handle:. Q_SIGNALS:|signals:. which are converted by the preprocessor...so read",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/_cppyy_generator.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/_cppyy_generator.py
Security,access,access,"#!/usr/bin/env python; """"""Cppyy binding description generator.""""""; # Keep PyCharm happy.; #; # All Qt-specific logic is driven from these identifiers. Setting them to; # nonsense values would effectively disable all Qt-specific logic.; #; """"""; Centralise all processing of the source.; Ideally, we'd use Clang for everything, but on occasion, we'll need access; to the source, without pre-processing.; """"""; """"""; Use Clang to parse the source and return its AST.; :param source: The source file.; """"""; #; # Stash ourselves on the tu for later use.; #; """"""; Read the given range from the raw source. :param extent: The range of text required.; :param nl: What \n should be replaced by.; """"""; #; # Return a single buffer of text.; #; """"""; A helper function which returns the parents of a cursor in the forms:; - A::B::C::...N for non-top level entities.; - filename.h for top level entities.; - """" in exceptional cases of having no parents.; """"""; """"""; A helper function providing a standardised description for an item,; which may be a cursor.; """"""; """"""; Clang seems to replace template parameter N of the form ""T"" with; ""type-parameter-<depth>-N""...so we need to put ""T"" back. :param level: Template nesting level.; :param sip: The sip.; :param key: The key in the sip which may need; fixing up.; :return:; """"""; #; # Depending on the type of the SIP entry, replace the Clang; # version of the value with the actual version.; #; """"""; Constructor. :param compile_flags: The compile flags for the file.; :param dump_modules: Turn on tracing for modules.; :param dump_items: Turn on tracing for container members.; :param dump_includes: Turn on diagnostics for include files.; :param dump_privates: Turn on diagnostics for omitted private items.; :param verbose: Turn on diagnostics for command lines.; """"""; """"""; Generate a dict describing the given source header file. This is the; main entry point for this class. :param h_file: The source header file of interest.; :returns: A dict corresponding to the h",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/_cppyy_generator.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/_cppyy_generator.py
Testability,log,logic,"#!/usr/bin/env python; """"""Cppyy binding description generator.""""""; # Keep PyCharm happy.; #; # All Qt-specific logic is driven from these identifiers. Setting them to; # nonsense values would effectively disable all Qt-specific logic.; #; """"""; Centralise all processing of the source.; Ideally, we'd use Clang for everything, but on occasion, we'll need access; to the source, without pre-processing.; """"""; """"""; Use Clang to parse the source and return its AST.; :param source: The source file.; """"""; #; # Stash ourselves on the tu for later use.; #; """"""; Read the given range from the raw source. :param extent: The range of text required.; :param nl: What \n should be replaced by.; """"""; #; # Return a single buffer of text.; #; """"""; A helper function which returns the parents of a cursor in the forms:; - A::B::C::...N for non-top level entities.; - filename.h for top level entities.; - """" in exceptional cases of having no parents.; """"""; """"""; A helper function providing a standardised description for an item,; which may be a cursor.; """"""; """"""; Clang seems to replace template parameter N of the form ""T"" with; ""type-parameter-<depth>-N""...so we need to put ""T"" back. :param level: Template nesting level.; :param sip: The sip.; :param key: The key in the sip which may need; fixing up.; :return:; """"""; #; # Depending on the type of the SIP entry, replace the Clang; # version of the value with the actual version.; #; """"""; Constructor. :param compile_flags: The compile flags for the file.; :param dump_modules: Turn on tracing for modules.; :param dump_items: Turn on tracing for container members.; :param dump_includes: Turn on diagnostics for include files.; :param dump_privates: Turn on diagnostics for omitted private items.; :param verbose: Turn on diagnostics for command lines.; """"""; """"""; Generate a dict describing the given source header file. This is the; main entry point for this class. :param h_file: The source header file of interest.; :returns: A dict corresponding to the h",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/_cppyy_generator.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/_cppyy_generator.py
Usability,simpl,simply,"lity attributes and the like.; #; """"""; Generate the translation for a function. :param container: A class or namespace.; :param fn: The function object.; :param level: Recursion level controls indentation.; :param is_signal: Is this a Qt signal?; :return: A string.; """"""; #; # So far so good, but we need any default value.; #; #; # Ignore:; #; # CursorKind.COMPOUND_STMT: Function body.; # CursorKind.CXX_OVERRIDE_ATTR: The ""override"" keyword.; # CursorKind.MEMBER_REF, CursorKind.DECL_REF_EXPR, CursorKind.CALL_EXPR: Constructor initialisers.; # TEMPLATE_KINDS: The result type.; #; """"""; The parser does not seem to provide access to the complete text of a parameter.; This makes it hard to find any default values, so we:. 1. Run the lexer from ""here"" to the end of the file, bailing out when we see the "",""; or a "")"" marking the end.; 2. Watch for the assignment.; """"""; #; # Get the text after the ""="". Macro expansion can make relying on tokens fraught...and; # member.get_tokens() simply does not always return anything.; #; #; # Now count balanced anything-which-can-contain-a-comma till we get to the end.; #; """"""; Generate the translation for a typedef. :param container: A class or namespace.; :param typedef: The typedef object.; :param level: Recursion level controls indentation.; :param h_file: The source header file of interest.; :return: Info().; """"""; #; # Create an entry to collect information for this level of template arguments.; #; #; # Happens e.g. if the template is a dependent type; instead, try to parse; # its definition (brittle, but the original code just had '1' as a guess,; # which is even worse ...).; #; # logger.error(_(""Unexpected template_arg_count={} for {}"").format(tmp, typedef.type.get_typedef_name())); #; # Non-first template_infos are just parameters.; #; #; # This is a template parameter.; #; #; # Not a template.; #; #; # This must be a function type. TODO: what if there are no PARM_DECLs?; #; #; # TODO: this is actually the signature:; #; # ""int (O",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/_cppyy_generator.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/_cppyy_generator.py
Modifiability,variab,variables,"""""""; Support utilities for bindings.; """"""; #; # Python2.; #; #; # Python3.; #; # Optional; only necessary if you want to be able to import the module; # by name later.; # Keep PyCharm happy.; """"""; Initialise the bindings module. :param pkg: The bindings package.; :param __init__py: Base __init__.py file of the bindings.; :param cmake_shared_library_prefix:; ${cmake_shared_library_prefix}; :param cmake_shared_library_suffix:; ${cmake_shared_library_suffix}; """"""; """"""; Map the given C++ operator name on the python equivalent.; """"""; # dereference v.s. multiplication of two instances; # unary positive v.s. addition of two instances; # unary negative v.s. subtraction of two instances; # prefix v.s. postfix increment; # prefix v.s. postfix decrement; # might get here, as not all operator methods are handled (new, delete, etc.); #; # Add level 1 objects to the pkg namespace.; #; #; # Ignore some names based on heuristics.; #; #; # Don't attempt to look up numbers (i.e. non-type template parameters).; #; #; # Classes, variables etc.; #; #raise; #; # Load the library.; #; #; # Load pythonizations; #; # versions older than 3.5 do not support 'recursive'; # TODO: below is good enough for most cases, but not recursive; #; # Parse the map file.; #; #; # Iterate over all the items at the top level of each file, and add them; # to the pkg.; #",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/pkg_templates/initializor.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/cling/python/cppyy_backend/pkg_templates/initializor.py
Testability,test,tests,"""""""; Pytest/nosetest tests.; """"""; """"""; Test cppyy_generator.; """"""; '''This method is run once before _each_ test method is executed'''; '''This method is run once after _each_ test method is executed'''; #; # Create mapping.; #; #; # Read mapping.; #",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy-backend/cling/tests/test_cppyy_backend.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/cling/tests/test_cppyy_backend.py
Deployability,install,installed,"#; # platform-dependent helpers; #; #; # customized commands; #; # /EHsc and sometimes /MT are hardwired in distutils, but the compiler/linker will; # let the last argument take precedence; # force the export results in the proper directory.; # export_symbols=[], # ie. all (hum, that puts the output in the wrong directory); # Custom clean. Clean everything except that which the base clean; # (see below) or create_src_directory.py is responsible for.; # remove build directories; # Base clean.; # depending on goal, copy over pre-installed tree; # base install; # custom install of backend; #outputs.append(os.path.join(self._get_install_path(), 'cppyy_backend')); # this is a universal, but platform-specific package; a combination; # that wheel does not recognize, thus simply fool it; #; # customized distribition to disable binaries; #; # pip does not resolve dependencies before building binaries, so unless; # packages are installed one-by-one, on old install is used or the build; # will simply fail hard. The following is not completely quiet, but at; # least a lot less conspicuous.; # Author details",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy-backend/clingwrapper/setup.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/clingwrapper/setup.py
Integrability,depend,dependent,"#; # platform-dependent helpers; #; #; # customized commands; #; # /EHsc and sometimes /MT are hardwired in distutils, but the compiler/linker will; # let the last argument take precedence; # force the export results in the proper directory.; # export_symbols=[], # ie. all (hum, that puts the output in the wrong directory); # Custom clean. Clean everything except that which the base clean; # (see below) or create_src_directory.py is responsible for.; # remove build directories; # Base clean.; # depending on goal, copy over pre-installed tree; # base install; # custom install of backend; #outputs.append(os.path.join(self._get_install_path(), 'cppyy_backend')); # this is a universal, but platform-specific package; a combination; # that wheel does not recognize, thus simply fool it; #; # customized distribition to disable binaries; #; # pip does not resolve dependencies before building binaries, so unless; # packages are installed one-by-one, on old install is used or the build; # will simply fail hard. The following is not completely quiet, but at; # least a lot less conspicuous.; # Author details",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy-backend/clingwrapper/setup.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/clingwrapper/setup.py
Usability,simpl,simply,"#; # platform-dependent helpers; #; #; # customized commands; #; # /EHsc and sometimes /MT are hardwired in distutils, but the compiler/linker will; # let the last argument take precedence; # force the export results in the proper directory.; # export_symbols=[], # ie. all (hum, that puts the output in the wrong directory); # Custom clean. Clean everything except that which the base clean; # (see below) or create_src_directory.py is responsible for.; # remove build directories; # Base clean.; # depending on goal, copy over pre-installed tree; # base install; # custom install of backend; #outputs.append(os.path.join(self._get_install_path(), 'cppyy_backend')); # this is a universal, but platform-specific package; a combination; # that wheel does not recognize, thus simply fool it; #; # customized distribition to disable binaries; #; # pip does not resolve dependencies before building binaries, so unless; # packages are installed one-by-one, on old install is used or the build; # will simply fail hard. The following is not completely quiet, but at; # least a lot less conspicuous.; # Author details",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/cppyy-backend/clingwrapper/setup.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/clingwrapper/setup.py
Deployability,install,install,"#; # platform-dependent helpers; #; # happens during egg_info and other non-build/install commands; #; # customized commands; #; # clang for same; # g++ >8.2, complaint of CPyFunction cast; # since clang/g++ don't have the same options; # C++17, Python headers; # not all Pythons provide this; # /EHsc and sometimes /MT are hardwired in distutils, but the compiler/linker will; # let the last argument take precedence; # Author details",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/CPyCppyy/setup.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/CPyCppyy/setup.py
Integrability,depend,dependent,"#; # platform-dependent helpers; #; # happens during egg_info and other non-build/install commands; #; # customized commands; #; # clang for same; # g++ >8.2, complaint of CPyFunction cast; # since clang/g++ don't have the same options; # C++17, Python headers; # not all Pythons provide this; # /EHsc and sometimes /MT are hardwired in distutils, but the compiler/linker will; # let the last argument take precedence; # Author details",MatchSource.CODE_COMMENT,bindings/pyroot/cppyy/CPyCppyy/setup.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/CPyCppyy/setup.py
Availability,down,down,"# Author: Enric Tejedor CERN 04/2019; ################################################################################; # Copyright (C) 1995-2019, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; """"""; Application class for PyROOT.; Configures the interactive usage of ROOT from Python.; """"""; # Construct a TApplication for PyROOT; # Integrate IPython >= 5 with ROOT's event loop; # Check for new GUI events until there is some user input to process; # PyOS_InputHook-based mechanism; # Point to a function which will be called when Python's interpreter prompt; # is about to become idle and wait for user input from the terminal; # Set the display hook; # sys.displayhook is called on the result of evaluating an expression entered; # in an interactive Python session.; # Therefore, this function will call EndOfLineAction after each interactive; # command (to update display etc.); """"""Configure ROOT graphics to be used interactively""""""; # Note that we only end up in this function if gROOT.IsBatch() is false; # ipython and notebooks, register our event processing with their hooks; # Python in interactive mode, use the PyOS_InputHook to call our event processing; # - sys.flags.interactive checks for the -i flags passed to python; # - __main__ does not have the attribute __file__ if the Python prompt is started directly; # - MacOS does not allow to run a second thread to process events, fall back to the input hook; # Python in script mode, start a separate thread for the event processing; # indicate that ProcessEvents called in different thread, let ignore thread id checks in RWebWindow; # Used to shut down the thread safely at teardown time; # The thread is joined at teardown time",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_application.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_application.py
Deployability,update,update,"# Author: Enric Tejedor CERN 04/2019; ################################################################################; # Copyright (C) 1995-2019, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; """"""; Application class for PyROOT.; Configures the interactive usage of ROOT from Python.; """"""; # Construct a TApplication for PyROOT; # Integrate IPython >= 5 with ROOT's event loop; # Check for new GUI events until there is some user input to process; # PyOS_InputHook-based mechanism; # Point to a function which will be called when Python's interpreter prompt; # is about to become idle and wait for user input from the terminal; # Set the display hook; # sys.displayhook is called on the result of evaluating an expression entered; # in an interactive Python session.; # Therefore, this function will call EndOfLineAction after each interactive; # command (to update display etc.); """"""Configure ROOT graphics to be used interactively""""""; # Note that we only end up in this function if gROOT.IsBatch() is false; # ipython and notebooks, register our event processing with their hooks; # Python in interactive mode, use the PyOS_InputHook to call our event processing; # - sys.flags.interactive checks for the -i flags passed to python; # - __main__ does not have the attribute __file__ if the Python prompt is started directly; # - MacOS does not allow to run a second thread to process events, fall back to the input hook; # Python in script mode, start a separate thread for the event processing; # indicate that ProcessEvents called in different thread, let ignore thread id checks in RWebWindow; # Used to shut down the thread safely at teardown time; # The thread is joined at teardown time",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_application.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_application.py
Safety,safe,safely,"# Author: Enric Tejedor CERN 04/2019; ################################################################################; # Copyright (C) 1995-2019, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; """"""; Application class for PyROOT.; Configures the interactive usage of ROOT from Python.; """"""; # Construct a TApplication for PyROOT; # Integrate IPython >= 5 with ROOT's event loop; # Check for new GUI events until there is some user input to process; # PyOS_InputHook-based mechanism; # Point to a function which will be called when Python's interpreter prompt; # is about to become idle and wait for user input from the terminal; # Set the display hook; # sys.displayhook is called on the result of evaluating an expression entered; # in an interactive Python session.; # Therefore, this function will call EndOfLineAction after each interactive; # command (to update display etc.); """"""Configure ROOT graphics to be used interactively""""""; # Note that we only end up in this function if gROOT.IsBatch() is false; # ipython and notebooks, register our event processing with their hooks; # Python in interactive mode, use the PyOS_InputHook to call our event processing; # - sys.flags.interactive checks for the -i flags passed to python; # - __main__ does not have the attribute __file__ if the Python prompt is started directly; # - MacOS does not allow to run a second thread to process events, fall back to the input hook; # Python in script mode, start a separate thread for the event processing; # indicate that ProcessEvents called in different thread, let ignore thread id checks in RWebWindow; # Used to shut down the thread safely at teardown time; # The thread is joined at teardown time",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_application.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_application.py
Availability,avail,available,"ave used execfile, but import is likely to give fewer surprises; # Run custom logon file (must be after creation of ROOT globals); # -n disables the reading of the logon file, just like with root; # If the .py version of rootlogon exists, the .C is ignored (the user can; # load the .C from the .py, if so desired).; # System logon, user logon, and local logon (skip Rint.Logon); # type: () -> types.ModuleType; """"""; Reduction function of the ROOT facade to customize the (pickle); serialization step. Defines the ingredients needed for a correct serialization of the; facade, that is a function that imports a Python module and the name of; that module, which corresponds to this facade's __name__ attribute. This; method helps serialization tools like `cloudpickle`, especially used in; distributed environments, that always need to include information about; the ROOT module in the serialization step. For example, the following; snippet would not work without this method::. import ROOT; import cloudpickle. def foo():; return ROOT.TH1F(). cloudpickle.loads(cloudpickle.dumps(foo)). In particular, it would raise::. TypeError: cannot pickle 'ROOTFacade' object; """"""; # Inject version as __version__ property in ROOT module; # Overload VecOps namespace; # The property gets the C++ namespace, adds the pythonizations and; # eventually deletes itself so that following calls go directly; # to the C++ namespace. This mechanic ensures that we pythonize the; # namespace lazily.; # Overload RDF namespace; # Make a copy of the arrays that have strides to make sure we read the correct values; # TODO a cleaner fix; # make a RDataFrame from a Pandas dataframe; # Inject Experimental.Distributed package into namespace RDF if available; # Overload RooFit namespace; # Overload TMVA namespace; # this line is needed to import the pythonizations in _tmva directory; # Create and overload Numba namespace; # Return something as it is a property function; # Get TPyDispatcher for programming GUI callbacks",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_facade.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_facade.py
Deployability,configurat,configuration,"Class for configuring PyROOT""""""; """"""Internal class to manage lookups of gROOT in the facade.; This wrapper calls _finalSetup on the facade when it; receives a lookup, unless the lookup is for SetBatch.; This allows to evaluate the command line parameters; before checking if batch mode is on in _finalSetup; """"""; """"""; Create the ROOT.RDF.Experimental.Distributed python module. This module will be injected into the ROOT.RDF namespace. Arguments:; parent: The ROOT.RDF namespace. Needed to define __package__. Returns:; types.ModuleType: The ROOT.RDF.Experimental.Distributed submodule.; """"""; # type: (str) -> types.ModuleType; """"""; Import and return the Python module with the input name. Helper function for the __reduce__ method of the ROOTFacade class.; """"""; """"""Facade class for ROOT module""""""; # Inject gROOT global; # Expose some functionality from CPyCppyy extension module; # For backwards compatibility; # Initialize configuration; # @pythonization decorator; # Redirect lookups to temporary helper methods; # This lets the user do some actions before all the machinery is in place:; # - Set batch mode in gROOT; # - Set options in PyConfig; # Return an indexable buffer of length 1, whose only element; # is the address of the object.; # The address of the buffer is the same as the address of the; # address of the object; # addr is the address of the address of the object; # Check for 64 bit as suggested here:; # https://docs.python.org/3/library/platform.html#cross-platform; # Create a buffer (LowLevelView) from address; # Try:; # - in the global namespace; # - in the ROOT namespace; # - in gROOT (ROOT lists such as list of files,; # memory mapped files, functions, geometries ecc.); # The first two attempts allow to lookup; # e.g. ROOT.ROOT.Math as ROOT.Math; # Note that hasattr caches the lookup for getattr; # Prevent this method from being re-entered through the gROOT wrapper; # Setup interactive usage from Python; # Set memory policy to kUseHeuristics.; # This restores th",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_facade.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_facade.py
Integrability,wrap,wrapper,"""""""Class for configuring PyROOT""""""; """"""Internal class to manage lookups of gROOT in the facade.; This wrapper calls _finalSetup on the facade when it; receives a lookup, unless the lookup is for SetBatch.; This allows to evaluate the command line parameters; before checking if batch mode is on in _finalSetup; """"""; """"""; Create the ROOT.RDF.Experimental.Distributed python module. This module will be injected into the ROOT.RDF namespace. Arguments:; parent: The ROOT.RDF namespace. Needed to define __package__. Returns:; types.ModuleType: The ROOT.RDF.Experimental.Distributed submodule.; """"""; # type: (str) -> types.ModuleType; """"""; Import and return the Python module with the input name. Helper function for the __reduce__ method of the ROOTFacade class.; """"""; """"""Facade class for ROOT module""""""; # Inject gROOT global; # Expose some functionality from CPyCppyy extension module; # For backwards compatibility; # Initialize configuration; # @pythonization decorator; # Redirect lookups to temporary helper methods; # This lets the user do some actions before all the machinery is in place:; # - Set batch mode in gROOT; # - Set options in PyConfig; # Return an indexable buffer of length 1, whose only element; # is the address of the object.; # The address of the buffer is the same as the address of the; # address of the object; # addr is the address of the address of the object; # Check for 64 bit as suggested here:; # https://docs.python.org/3/library/platform.html#cross-platform; # Create a buffer (LowLevelView) from address; # Try:; # - in the global namespace; # - in the ROOT namespace; # - in gROOT (ROOT lists such as list of files,; # memory mapped files, functions, geometries ecc.); # The first two attempts allow to lookup; # e.g. ROOT.ROOT.Math as ROOT.Math; # Note that hasattr caches the lookup for getattr; # Prevent this method from being re-entered through the gROOT wrapper; # Setup interactive usage from Python; # Set memory policy to kUseHeuristics.; # This restores ",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_facade.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_facade.py
Modifiability,config,configuring,"""""""Class for configuring PyROOT""""""; """"""Internal class to manage lookups of gROOT in the facade.; This wrapper calls _finalSetup on the facade when it; receives a lookup, unless the lookup is for SetBatch.; This allows to evaluate the command line parameters; before checking if batch mode is on in _finalSetup; """"""; """"""; Create the ROOT.RDF.Experimental.Distributed python module. This module will be injected into the ROOT.RDF namespace. Arguments:; parent: The ROOT.RDF namespace. Needed to define __package__. Returns:; types.ModuleType: The ROOT.RDF.Experimental.Distributed submodule.; """"""; # type: (str) -> types.ModuleType; """"""; Import and return the Python module with the input name. Helper function for the __reduce__ method of the ROOTFacade class.; """"""; """"""Facade class for ROOT module""""""; # Inject gROOT global; # Expose some functionality from CPyCppyy extension module; # For backwards compatibility; # Initialize configuration; # @pythonization decorator; # Redirect lookups to temporary helper methods; # This lets the user do some actions before all the machinery is in place:; # - Set batch mode in gROOT; # - Set options in PyConfig; # Return an indexable buffer of length 1, whose only element; # is the address of the object.; # The address of the buffer is the same as the address of the; # address of the object; # addr is the address of the address of the object; # Check for 64 bit as suggested here:; # https://docs.python.org/3/library/platform.html#cross-platform; # Create a buffer (LowLevelView) from address; # Try:; # - in the global namespace; # - in the ROOT namespace; # - in gROOT (ROOT lists such as list of files,; # memory mapped files, functions, geometries ecc.); # The first two attempts allow to lookup; # e.g. ROOT.ROOT.Math as ROOT.Math; # Note that hasattr caches the lookup for getattr; # Prevent this method from being re-entered through the gROOT wrapper; # Setup interactive usage from Python; # Set memory policy to kUseHeuristics.; # This restores ",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_facade.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_facade.py
Performance,cache,caches,"ule; # For backwards compatibility; # Initialize configuration; # @pythonization decorator; # Redirect lookups to temporary helper methods; # This lets the user do some actions before all the machinery is in place:; # - Set batch mode in gROOT; # - Set options in PyConfig; # Return an indexable buffer of length 1, whose only element; # is the address of the object.; # The address of the buffer is the same as the address of the; # address of the object; # addr is the address of the address of the object; # Check for 64 bit as suggested here:; # https://docs.python.org/3/library/platform.html#cross-platform; # Create a buffer (LowLevelView) from address; # Try:; # - in the global namespace; # - in the ROOT namespace; # - in gROOT (ROOT lists such as list of files,; # memory mapped files, functions, geometries ecc.); # The first two attempts allow to lookup; # e.g. ROOT.ROOT.Math as ROOT.Math; # Note that hasattr caches the lookup for getattr; # Prevent this method from being re-entered through the gROOT wrapper; # Setup interactive usage from Python; # Set memory policy to kUseHeuristics.; # This restores the default in PyROOT which was changed; # by new Cppyy; # Redirect lookups to cppyy's global namespace; # Register custom converters and executors; # Run rootlogon if exists; # Special case, to allow ""from ROOT import gROOT"" w/o starting the graphics; """"""Execute the 'rootlogon.py' module found at the given 'file_path'""""""; # Could also have used execfile, but import is likely to give fewer surprises; # Run custom logon file (must be after creation of ROOT globals); # -n disables the reading of the logon file, just like with root; # If the .py version of rootlogon exists, the .C is ignored (the user can; # load the .C from the .py, if so desired).; # System logon, user logon, and local logon (skip Rint.Logon); # type: () -> types.ModuleType; """"""; Reduction function of the ROOT facade to customize the (pickle); serialization step. Defines the ingredients needed for a co",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_facade.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_facade.py
Security,inject,injected,"""""""Class for configuring PyROOT""""""; """"""Internal class to manage lookups of gROOT in the facade.; This wrapper calls _finalSetup on the facade when it; receives a lookup, unless the lookup is for SetBatch.; This allows to evaluate the command line parameters; before checking if batch mode is on in _finalSetup; """"""; """"""; Create the ROOT.RDF.Experimental.Distributed python module. This module will be injected into the ROOT.RDF namespace. Arguments:; parent: The ROOT.RDF namespace. Needed to define __package__. Returns:; types.ModuleType: The ROOT.RDF.Experimental.Distributed submodule.; """"""; # type: (str) -> types.ModuleType; """"""; Import and return the Python module with the input name. Helper function for the __reduce__ method of the ROOTFacade class.; """"""; """"""Facade class for ROOT module""""""; # Inject gROOT global; # Expose some functionality from CPyCppyy extension module; # For backwards compatibility; # Initialize configuration; # @pythonization decorator; # Redirect lookups to temporary helper methods; # This lets the user do some actions before all the machinery is in place:; # - Set batch mode in gROOT; # - Set options in PyConfig; # Return an indexable buffer of length 1, whose only element; # is the address of the object.; # The address of the buffer is the same as the address of the; # address of the object; # addr is the address of the address of the object; # Check for 64 bit as suggested here:; # https://docs.python.org/3/library/platform.html#cross-platform; # Create a buffer (LowLevelView) from address; # Try:; # - in the global namespace; # - in the ROOT namespace; # - in gROOT (ROOT lists such as list of files,; # memory mapped files, functions, geometries ecc.); # The first two attempts allow to lookup; # e.g. ROOT.ROOT.Math as ROOT.Math; # Note that hasattr caches the lookup for getattr; # Prevent this method from being re-entered through the gROOT wrapper; # Setup interactive usage from Python; # Set memory policy to kUseHeuristics.; # This restores ",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_facade.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_facade.py
Testability,log,logon,"ggested here:; # https://docs.python.org/3/library/platform.html#cross-platform; # Create a buffer (LowLevelView) from address; # Try:; # - in the global namespace; # - in the ROOT namespace; # - in gROOT (ROOT lists such as list of files,; # memory mapped files, functions, geometries ecc.); # The first two attempts allow to lookup; # e.g. ROOT.ROOT.Math as ROOT.Math; # Note that hasattr caches the lookup for getattr; # Prevent this method from being re-entered through the gROOT wrapper; # Setup interactive usage from Python; # Set memory policy to kUseHeuristics.; # This restores the default in PyROOT which was changed; # by new Cppyy; # Redirect lookups to cppyy's global namespace; # Register custom converters and executors; # Run rootlogon if exists; # Special case, to allow ""from ROOT import gROOT"" w/o starting the graphics; """"""Execute the 'rootlogon.py' module found at the given 'file_path'""""""; # Could also have used execfile, but import is likely to give fewer surprises; # Run custom logon file (must be after creation of ROOT globals); # -n disables the reading of the logon file, just like with root; # If the .py version of rootlogon exists, the .C is ignored (the user can; # load the .C from the .py, if so desired).; # System logon, user logon, and local logon (skip Rint.Logon); # type: () -> types.ModuleType; """"""; Reduction function of the ROOT facade to customize the (pickle); serialization step. Defines the ingredients needed for a correct serialization of the; facade, that is a function that imports a Python module and the name of; that module, which corresponds to this facade's __name__ attribute. This; method helps serialization tools like `cloudpickle`, especially used in; distributed environments, that always need to include information about; the ROOT module in the serialization step. For example, the following; snippet would not work without this method::. import ROOT; import cloudpickle. def foo():; return ROOT.TH1F(). cloudpickle.loads(cloudpickle.",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_facade.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_facade.py
Energy Efficiency,allocate,allocated,"unction pointer in C++.; '''; # We return an RVec through pointers as part of the input arguments. Note that the; # pointer type in numba is always an int64 and is later on cast in C++ to the correct type.; # In addition, we provide the size of the data type of the array for preallocating memory of; # the returned array.; # See the Python wrapper for further information why we are using these types.; # Pointer to the data (the first element of the array); # Size of the array in elements; '''; Get numba signature as numba type objects from C++ typenames; '''; '''; Construct the type of an RVec input parameter for its use in the C++; wrapper function signature.; '''; '''; Construct the type of an RVec input parameter for its use in the cast; of the function pointer of the jitted Python wrapper.; '''; # Special treatment for bool: In numpy, bools have 1 byte; '''; Inner decorator without arguments, see outer decorator for documentation; '''; # Jit the given Python callable with numba; # return_type = ""int""; # Create Python wrapper with C++ friendly signature; # Define signature; # If we return an RVec, we return via pointer the pointer of the allocated data,; # the size in elements. In addition, we provide the size of the datatype in bytes.; # Define arguments for jit function; # Define return operation; # Build wrapper code; '''\; def pywrapper({SIGNATURE}):; """"""; Wrapper function for the jitted Python callable with special treatment of arrays; """"""; # If an RVec is given, define numba carray wrapper for the input types; {ARGS_DEF}; # Call the jitted Python function; r = nbjit({ARGS}); # Return the result; {RETURN}; '''; # Make a shallow copy of the dictionary so we don't pollute the global scope; # Execute the pywrapper code and generate the wrapper function; # which calls the jitted C function; # Jit the Python wrapper code; # Get address of jitted wrapper function; # Infer name of the C++ wrapper function; # Build C++ wrapper for jitting with cling; # Define:; # - In",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_numbadeclare.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_numbadeclare.py
Integrability,wrap,wrapper,"'''; Decorator for making Python callables accessible in C++ by just-in-time compilation; with numba and cling. The decorator takes the given Python callable and just-in-time compiles (jits); wrapper functions with the given C++ types for input and return types. Eventually,; the Python callable is accessible in the Numba namespace in C++. The implementation first jits with numba the Python callable. We support fundamental types and; ROOT::VecOps::RVecs thereof. Note that you can get the jitted Python callable by the attribute; numba_func. The C++ types are converted to the respective numba types and RVecs are accessible; in Python by numpy arrays. After jitting the actual Python callable, we jit another Python wrapper,; which converts the Python signature to a C-friendly signature. The wrapper code is accessible by; the attribute __py_wrapper__. Next, the Python wrapper is given to cling to jit a C++ wrapper function,; making the original Python callable accessible in C++. The wrapper code in C++ is accessible by; the attribute __cpp_wrapper__. Note that the callable is fully compiled without side-effects. The numba jitting uses the nopython; option which does not allow interaction with the Python interpreter. This means that you can use; the resulting function also safely in multi-threaded environments.; '''; # Make required imports; # Normalize input types by stripping ROOT and VecOps namespaces from input types; '''; Remove ROOT:: and VecOps:: namespaces; '''; # Helper functions to determine types; '''; Get inner typename of a templated C++ typename; '''; # therefore, type must use a shorthand alias; # discard a possible const modifier before ""RVec""; # alias type characters come after ""RVec""; '''; Get numba type object from a C++ fundamental typename. These are the types we use to jit the Python callable.; '''; '''; Get C friendly signature as numba type objects from C++ typenames. We need the types to jit a Python wrapper, which can be accessed as a function poin",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_numbadeclare.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_numbadeclare.py
Performance,multi-thread,multi-threaded,"he Python callable is accessible in the Numba namespace in C++. The implementation first jits with numba the Python callable. We support fundamental types and; ROOT::VecOps::RVecs thereof. Note that you can get the jitted Python callable by the attribute; numba_func. The C++ types are converted to the respective numba types and RVecs are accessible; in Python by numpy arrays. After jitting the actual Python callable, we jit another Python wrapper,; which converts the Python signature to a C-friendly signature. The wrapper code is accessible by; the attribute __py_wrapper__. Next, the Python wrapper is given to cling to jit a C++ wrapper function,; making the original Python callable accessible in C++. The wrapper code in C++ is accessible by; the attribute __cpp_wrapper__. Note that the callable is fully compiled without side-effects. The numba jitting uses the nopython; option which does not allow interaction with the Python interpreter. This means that you can use; the resulting function also safely in multi-threaded environments.; '''; # Make required imports; # Normalize input types by stripping ROOT and VecOps namespaces from input types; '''; Remove ROOT:: and VecOps:: namespaces; '''; # Helper functions to determine types; '''; Get inner typename of a templated C++ typename; '''; # therefore, type must use a shorthand alias; # discard a possible const modifier before ""RVec""; # alias type characters come after ""RVec""; '''; Get numba type object from a C++ fundamental typename. These are the types we use to jit the Python callable.; '''; '''; Get C friendly signature as numba type objects from C++ typenames. We need the types to jit a Python wrapper, which can be accessed as a function pointer in C++.; '''; # We return an RVec through pointers as part of the input arguments. Note that the; # pointer type in numba is always an int64 and is later on cast in C++ to the correct type.; # In addition, we provide the size of the data type of the array for preallocatin",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_numbadeclare.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_numbadeclare.py
Safety,safe,safely,"he Python callable is accessible in the Numba namespace in C++. The implementation first jits with numba the Python callable. We support fundamental types and; ROOT::VecOps::RVecs thereof. Note that you can get the jitted Python callable by the attribute; numba_func. The C++ types are converted to the respective numba types and RVecs are accessible; in Python by numpy arrays. After jitting the actual Python callable, we jit another Python wrapper,; which converts the Python signature to a C-friendly signature. The wrapper code is accessible by; the attribute __py_wrapper__. Next, the Python wrapper is given to cling to jit a C++ wrapper function,; making the original Python callable accessible in C++. The wrapper code in C++ is accessible by; the attribute __cpp_wrapper__. Note that the callable is fully compiled without side-effects. The numba jitting uses the nopython; option which does not allow interaction with the Python interpreter. This means that you can use; the resulting function also safely in multi-threaded environments.; '''; # Make required imports; # Normalize input types by stripping ROOT and VecOps namespaces from input types; '''; Remove ROOT:: and VecOps:: namespaces; '''; # Helper functions to determine types; '''; Get inner typename of a templated C++ typename; '''; # therefore, type must use a shorthand alias; # discard a possible const modifier before ""RVec""; # alias type characters come after ""RVec""; '''; Get numba type object from a C++ fundamental typename. These are the types we use to jit the Python callable.; '''; '''; Get C friendly signature as numba type objects from C++ typenames. We need the types to jit a Python wrapper, which can be accessed as a function pointer in C++.; '''; # We return an RVec through pointers as part of the input arguments. Note that the; # pointer type in numba is always an int64 and is later on cast in C++ to the correct type.; # In addition, we provide the size of the data type of the array for preallocatin",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_numbadeclare.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_numbadeclare.py
Security,access,accessible,"'''; Decorator for making Python callables accessible in C++ by just-in-time compilation; with numba and cling. The decorator takes the given Python callable and just-in-time compiles (jits); wrapper functions with the given C++ types for input and return types. Eventually,; the Python callable is accessible in the Numba namespace in C++. The implementation first jits with numba the Python callable. We support fundamental types and; ROOT::VecOps::RVecs thereof. Note that you can get the jitted Python callable by the attribute; numba_func. The C++ types are converted to the respective numba types and RVecs are accessible; in Python by numpy arrays. After jitting the actual Python callable, we jit another Python wrapper,; which converts the Python signature to a C-friendly signature. The wrapper code is accessible by; the attribute __py_wrapper__. Next, the Python wrapper is given to cling to jit a C++ wrapper function,; making the original Python callable accessible in C++. The wrapper code in C++ is accessible by; the attribute __cpp_wrapper__. Note that the callable is fully compiled without side-effects. The numba jitting uses the nopython; option which does not allow interaction with the Python interpreter. This means that you can use; the resulting function also safely in multi-threaded environments.; '''; # Make required imports; # Normalize input types by stripping ROOT and VecOps namespaces from input types; '''; Remove ROOT:: and VecOps:: namespaces; '''; # Helper functions to determine types; '''; Get inner typename of a templated C++ typename; '''; # therefore, type must use a shorthand alias; # discard a possible const modifier before ""RVec""; # alias type characters come after ""RVec""; '''; Get numba type object from a C++ fundamental typename. These are the types we use to jit the Python callable.; '''; '''; Get C friendly signature as numba type objects from C++ typenames. We need the types to jit a Python wrapper, which can be accessed as a function poin",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_numbadeclare.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_numbadeclare.py
Performance,cache,cache,"# Author: Enric Tejedor, Danilo Piparo CERN 06/2018; ################################################################################; # Copyright (C) 1995-2018, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Prevent cppyy's check for the PCH; # Prevent cppyy's check for extra header directory; # Prevent cppyy from filtering ROOT libraries; # Do setup specific to AddressSanitizer environments; # Build cache of commonly used python strings (the cache is python intern, so; # all strings are shared python-wide, not just in PyROOT).; # See: https://docs.python.org/3.2/library/sys.html?highlight=sys.intern#sys.intern; # Trigger the addition of the pythonizations; # Check if we are in the IPython shell; """"""; Dummy class used to trigger an ImportError on wildcard imports if the; `__all__` attribute of a module is an instance of this class.; """"""; """"""; Wildcard import e.g. `from module import *` is bad practice, so it is disallowed in ROOT. Please import explicitly.; """"""; # Prevent `from ROOT import *` by setting the __all__ attribute to something; # that will raise an ImportError on item retrieval.; # Configure ROOT facade module; # Configure meta-path finder for ROOT namespaces, following the Python; # documentation and an example:; #; # * https://docs.python.org/3/library/importlib.html#module-importlib.abc; #; # * https://python.plainenglish.io/metapathfinders-or-how-to-change-python-import-behavior-a1cf3b5a13ec; """"""; Determine if an object can be used as a Python module. This is the case for; objects that are actually of ModuleType, or C++ namespaces from cppyy.; """"""; # If the type is the module type, it can trivially be a module.; # Check if the object represents a C++ namespace. Since cppyy has no; # dedicated Python type for C++ namespaces, we check for th",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/__init__.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/__init__.py
Modifiability,variab,variable,"# -*- coding: utf-8 -*-; ## @package JsMVA.DataLoader; # @author Attila Bagoly <battila93@gmail.com>; # DataLoader module with the functions to be inserted to TMVA::DataLoader class and helper functions; ## Creates the input variable histogram and perform the transformations if necessary; # @param dl DataLoader object; # @param className string Signal/Background; # @param variableName string containing the variable name; # @param numBin for creating the histogram; # @param processTrfs string containing the list of transformations to be used on input variable; eg. ""I;N;D;P;U;G,D""; ## Get correlation matrix in JSON format; # This function is used by OutputTransformer; # @param varNames the bin labels; # @param className Signal/Background; # @param matrix the matrix; ## Draw correlation matrix; # This function uses the TMVA::DataLoader::GetCorrelationMatrix function added newly to root; # @param dl the object pointer; # @param className Signal/Background; ## Draw input variables; # This function uses the previously defined GetInputVariableHist function to create the histograms; # @param dl The object pointer; # @param variableName string containing the variable name; # @param numBin for creating the histogram; # @param processTrfs list of transformations to be used on input variable; eg. [""I"", ""N"", ""D"", ""P"", ""U"", ""G""]""; ## Rewrite TMVA::DataLoader::PrepareTrainingAndTestTree; # @param *args positional parameters; # @param **kwargs named parameters: this will be transformed to option string",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/JsMVA/DataLoader.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/JsMVA/DataLoader.py
Performance,perform,perform,"# -*- coding: utf-8 -*-; ## @package JsMVA.DataLoader; # @author Attila Bagoly <battila93@gmail.com>; # DataLoader module with the functions to be inserted to TMVA::DataLoader class and helper functions; ## Creates the input variable histogram and perform the transformations if necessary; # @param dl DataLoader object; # @param className string Signal/Background; # @param variableName string containing the variable name; # @param numBin for creating the histogram; # @param processTrfs string containing the list of transformations to be used on input variable; eg. ""I;N;D;P;U;G,D""; ## Get correlation matrix in JSON format; # This function is used by OutputTransformer; # @param varNames the bin labels; # @param className Signal/Background; # @param matrix the matrix; ## Draw correlation matrix; # This function uses the TMVA::DataLoader::GetCorrelationMatrix function added newly to root; # @param dl the object pointer; # @param className Signal/Background; ## Draw input variables; # This function uses the previously defined GetInputVariableHist function to create the histograms; # @param dl The object pointer; # @param variableName string containing the variable name; # @param numBin for creating the histogram; # @param processTrfs list of transformations to be used on input variable; eg. [""I"", ""N"", ""D"", ""P"", ""U"", ""G""]""; ## Rewrite TMVA::DataLoader::PrepareTrainingAndTestTree; # @param *args positional parameters; # @param **kwargs named parameters: this will be transformed to option string",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/JsMVA/DataLoader.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/JsMVA/DataLoader.py
Deployability,update,updates,"aram fac the object pointer; # @param datasetName the dataset name; # @param methodName we want to see the cut efficiencies of this method; # reading histograms; # formatting the style of histograms; # chop off useless stuff; # Adding labels and other information to plots.; ## Draw neural network; # @param fac the object pointer; # @param datasetName the dataset name; # @param methodName we want to see the network created by this method; ## Draw deep neural network; # @param fac the object pointer; # @param datasetName the dataset name; # @param methodName we want to see the deep network created by this method; ## This function puts the main thread to sleep until data points for tracking plots appear.; # @param m Method object; # @param sleep_time default sleeping time; ## Rewrite function for TMVA::Factory::TrainAllMethods. This function provides interactive training.; # The training will be started on separated thread. The main thread will periodically check for updates and will create; # the JS output which will update the plots and progress bars. The main thread must contain `while True`, because, if not; # it will cause crash (output will be flushed by tornado IOLoop (runs on main thread), but the output streams are; # C++ atomic types); # @param fac the factory object pointer; # stop button; # progress bar; ## Rewrite the constructor of TMVA::Factory; # @param *args positional parameters; # @param **kwargs named parameters: this will be transformed to option string; ## Rewrite TMVA::Factory::BookMethod; # @param *args positional parameters; # @param **kwargs named parameters: this will be transformed to option string; ## Rewrite the constructor of TMVA::Factory::EvaluateImportance; # @param *args positional parameters; # @param **kwargs named parameters: this will be transformed to option string; ## Background booking method for BookDNN; ## Graphical interface for booking DNN; # @param self object pointer; # @param loader the DataLoader object; # @param title ",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/JsMVA/Factory.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/JsMVA/Factory.py
Integrability,interface,interface,"eural network; # @param fac the object pointer; # @param datasetName the dataset name; # @param methodName we want to see the deep network created by this method; ## This function puts the main thread to sleep until data points for tracking plots appear.; # @param m Method object; # @param sleep_time default sleeping time; ## Rewrite function for TMVA::Factory::TrainAllMethods. This function provides interactive training.; # The training will be started on separated thread. The main thread will periodically check for updates and will create; # the JS output which will update the plots and progress bars. The main thread must contain `while True`, because, if not; # it will cause crash (output will be flushed by tornado IOLoop (runs on main thread), but the output streams are; # C++ atomic types); # @param fac the factory object pointer; # stop button; # progress bar; ## Rewrite the constructor of TMVA::Factory; # @param *args positional parameters; # @param **kwargs named parameters: this will be transformed to option string; ## Rewrite TMVA::Factory::BookMethod; # @param *args positional parameters; # @param **kwargs named parameters: this will be transformed to option string; ## Rewrite the constructor of TMVA::Factory::EvaluateImportance; # @param *args positional parameters; # @param **kwargs named parameters: this will be transformed to option string; ## Background booking method for BookDNN; ## Graphical interface for booking DNN; # @param self object pointer; # @param loader the DataLoader object; # @param title classifier title; ## This function gets the classifier information and weights in JSON formats, and the selected layers and it will create; # the weight heat map.; # @param net DNN in JSON format; # @param selectedLayers the selected layers; ## Show DNN weights in a heat map. It will produce an ipywidget element, where the layers can be selected.; # @param fac object pointer; # @param datasetName name of current dataset; # @param methodName DNN's name",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/JsMVA/Factory.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/JsMVA/Factory.py
Modifiability,variab,variables,"# -*- coding: utf-8 -*-; ## @package JsMVA.Factory; # @author Attila Bagoly <battila93@gmail.com>; # Factory module with the functions to be inserted to TMVA::Factory class and helper functions and classes; # This class contains the necessary HTML, JavaScript, CSS codes (templates); # for the new Factory methods. Some parts of these variables will be replaced and the new string will be the cell output.; # stop button; """"""; <script type=""text/javascript"">; require([""jquery""], function(jQ){; jQ(""input.stopTrainingButton"").on(""click"", function(){; IPython.notebook.kernel.interrupt();; jQ(this).css({; ""background-color"": ""rgba(200, 0, 0, 0.8)"",; ""color"": ""#fff"",; ""box-shadow"": ""0 3px 5px rgba(0, 0, 0, 0.3)"",; });; });; });; </script>; <style type=""text/css"">; input.stopTrainingButton {; background-color: #fff;; border: 1px solid #ccc;; width: 100%;; font-size: 16px;; font-weight: bold;; padding: 6px 12px;; cursor: pointer;; border-radius: 6px;; color: #333;; }; input.stopTrainingButton:hover {; background-color: rgba(204, 204, 204, 0.4);; }; </style>; <input type=""button"" value=""Stop"" class=""stopTrainingButton"" />; """"""; # progress bar; """"""; <script type=""text/javascript"" id=""progressBarScriptInc"">; require([""jquery""], function(jQ){; jQ(""#jsmva_bar_$id"").css(""width"", $progress + ""%"");; jQ(""#jsmva_label_$id"").text($progress + '%');; jQ(""#progressBarScriptInc"").parent().parent().remove();; });; </script>; """"""; """"""; <style>; #jsmva_progress_$id {; position: relative;; float: left;; height: 30px;; width: 100%;; background-color: #f5f5f5;; border-radius: 3px;; box-shadow: inset 0 3px 6px rgba(0, 0, 0, 0.1);; }; #jsmva_bar_$id {; position: absolute;; width: 1%;; height: 100%;; background-color: #337ab7;; }; #jsmva_label_$id {; text-align: center;; line-height: 30px;; color: white;; }; </style>; <div id=""jsmva_progress_$id"">; <div id=""jsmva_bar_$id"">; <div id=""jsmva_label_$id"">0%</div>; </div>; </div>; """"""; ## Getting method object from factory; # @param fac the TMVA::Factory ob",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/JsMVA/Factory.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/JsMVA/Factory.py
Performance,load,loader,"eural network; # @param fac the object pointer; # @param datasetName the dataset name; # @param methodName we want to see the deep network created by this method; ## This function puts the main thread to sleep until data points for tracking plots appear.; # @param m Method object; # @param sleep_time default sleeping time; ## Rewrite function for TMVA::Factory::TrainAllMethods. This function provides interactive training.; # The training will be started on separated thread. The main thread will periodically check for updates and will create; # the JS output which will update the plots and progress bars. The main thread must contain `while True`, because, if not; # it will cause crash (output will be flushed by tornado IOLoop (runs on main thread), but the output streams are; # C++ atomic types); # @param fac the factory object pointer; # stop button; # progress bar; ## Rewrite the constructor of TMVA::Factory; # @param *args positional parameters; # @param **kwargs named parameters: this will be transformed to option string; ## Rewrite TMVA::Factory::BookMethod; # @param *args positional parameters; # @param **kwargs named parameters: this will be transformed to option string; ## Rewrite the constructor of TMVA::Factory::EvaluateImportance; # @param *args positional parameters; # @param **kwargs named parameters: this will be transformed to option string; ## Background booking method for BookDNN; ## Graphical interface for booking DNN; # @param self object pointer; # @param loader the DataLoader object; # @param title classifier title; ## This function gets the classifier information and weights in JSON formats, and the selected layers and it will create; # the weight heat map.; # @param net DNN in JSON format; # @param selectedLayers the selected layers; ## Show DNN weights in a heat map. It will produce an ipywidget element, where the layers can be selected.; # @param fac object pointer; # @param datasetName name of current dataset; # @param methodName DNN's name",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/JsMVA/Factory.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/JsMVA/Factory.py
Usability,progress bar,progress bar,":Factory class and helper functions and classes; # This class contains the necessary HTML, JavaScript, CSS codes (templates); # for the new Factory methods. Some parts of these variables will be replaced and the new string will be the cell output.; # stop button; """"""; <script type=""text/javascript"">; require([""jquery""], function(jQ){; jQ(""input.stopTrainingButton"").on(""click"", function(){; IPython.notebook.kernel.interrupt();; jQ(this).css({; ""background-color"": ""rgba(200, 0, 0, 0.8)"",; ""color"": ""#fff"",; ""box-shadow"": ""0 3px 5px rgba(0, 0, 0, 0.3)"",; });; });; });; </script>; <style type=""text/css"">; input.stopTrainingButton {; background-color: #fff;; border: 1px solid #ccc;; width: 100%;; font-size: 16px;; font-weight: bold;; padding: 6px 12px;; cursor: pointer;; border-radius: 6px;; color: #333;; }; input.stopTrainingButton:hover {; background-color: rgba(204, 204, 204, 0.4);; }; </style>; <input type=""button"" value=""Stop"" class=""stopTrainingButton"" />; """"""; # progress bar; """"""; <script type=""text/javascript"" id=""progressBarScriptInc"">; require([""jquery""], function(jQ){; jQ(""#jsmva_bar_$id"").css(""width"", $progress + ""%"");; jQ(""#jsmva_label_$id"").text($progress + '%');; jQ(""#progressBarScriptInc"").parent().parent().remove();; });; </script>; """"""; """"""; <style>; #jsmva_progress_$id {; position: relative;; float: left;; height: 30px;; width: 100%;; background-color: #f5f5f5;; border-radius: 3px;; box-shadow: inset 0 3px 6px rgba(0, 0, 0, 0.1);; }; #jsmva_bar_$id {; position: absolute;; width: 1%;; height: 100%;; background-color: #337ab7;; }; #jsmva_label_$id {; text-align: center;; line-height: 30px;; color: white;; }; </style>; <div id=""jsmva_progress_$id"">; <div id=""jsmva_bar_$id"">; <div id=""jsmva_label_$id"">0%</div>; </div>; </div>; """"""; ## Getting method object from factory; # @param fac the TMVA::Factory object; # @param datasetName selecting the dataset; # @param methodName which method we want to get; ## Reads deep neural network weights from file and returns",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/JsMVA/Factory.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/JsMVA/Factory.py
Integrability,inject,inject,"# -*- coding: utf-8 -*-; ## @package JsMVA.JPyInterface; # @author Attila Bagoly <battila93@gmail.com>; # JPyInterface is responsible for adding the drawing methods to TMVA; # and for creating the JavaScript outputs from objects.; ## Function inserter class; # This class contains the methods which are invoked by using jsmva magic, and will inject the new methods; # to TMVA::Factory, TMVA::DataLoader; ## Threaded functions; ## The method inserter function; # @param target which class to insert; # @param source module which contains the methods to insert; # @param args list of methods to insert; ## This method change TMVA methods with new methods; # @param target which class to insert; # @param source module which contains the methods to insert; # @param args list of methods to insert; ## Get's special parameters from kwargs and converts to positional parameter; # args[0] = self; ## Converts object to TMVA style option string; ## The method removes inserted functions from class; # @param target from which class to remove functions; # @param args list of methods to remove; ## Reads all methods containing a selector from specified module; # @param module finding methods in this module; # @param selector if method in module contains this string will be selected; ## This function will register all functions which name contains ""Draw"" to TMVA.DataLoader and TMVA.Factory; # from DataLoader and Factory modules; ## This function will remove all functions which name contains ""Draw"" from TMVA.DataLoader and TMVA.Factory; # if the function was inserted from DataLoader and Factory modules; ## This function captures objects which are declared in noteboko cell. It's used to capture factory and data loader objects.; # @param args classes; ## Class for creating the output scripts and inserting them to cell output; ## Base repository; ## String containing the link to JavaScript files; ## String containing the link to CSS files; ## Drawing are sizes; ## id for drawing area; ## Template ",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/JsMVA/JPyInterface.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/JsMVA/JPyInterface.py
Modifiability,config,config,"ataLoader and Factory modules; ## This function will remove all functions which name contains ""Draw"" from TMVA.DataLoader and TMVA.Factory; # if the function was inserted from DataLoader and Factory modules; ## This function captures objects which are declared in noteboko cell. It's used to capture factory and data loader objects.; # @param args classes; ## Class for creating the output scripts and inserting them to cell output; ## Base repository; ## String containing the link to JavaScript files; ## String containing the link to CSS files; ## Drawing are sizes; ## id for drawing area; ## Template containing HTML code with draw area and drawing JavaScript; """"""; <div id=""$divid"" style=""width: ${width}px; height:${height}px""></div>; <script>; require(['JsMVA'],function(jsmva){; jsmva.$funcName('$divid','$dat');; });; </script>; """"""; ## Template containing JsMVA initialization code (adding JsMVA script location, and CSS); """"""; <script type=""text/javascript"">; require.config({; paths: {; 'JsMVA':'$PATH/JsMVA.min'; }; });; </script>; <link rel=""stylesheet"" href=""$CSSFile""></link>; """"""; ## Template containing data insertion JavaScript code; """"""<script id=""dataInserterScript"">; require(['JsMVA'],function(jsmva){; jsmva.$funcName('$divid', '$dat');; var script = document.getElementById(""dataInserterScript"");; script.parentElement.parentElement.remove();; });; </script>""""""; """"""<script>; require(['JsMVA'],function(jsmva){; jsmva.$funcName('$divid', '$dat');; });; </script>""""""; ## Inserts initialization codes to notebook; ## Inserts the draw area and drawing JavaScript to output; # @param obj ROOT object (will be converted to JSON) or JSON string containing the data to be drawn; # @param jsDrawMethod the JsMVA JavaScrip object method name to be used for drawing; # @param objIsJSON obj is ROOT object or JSON; ## Inserts CSS file; # @param cssName The CSS file name. File must be in jsMVACSSDir!; ## Inserts the data inserter JavaScript code to output; # @param obj ROOT object (wi",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/JsMVA/JPyInterface.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/JsMVA/JPyInterface.py
Performance,load,loader,"m source module which contains the methods to insert; # @param args list of methods to insert; ## Get's special parameters from kwargs and converts to positional parameter; # args[0] = self; ## Converts object to TMVA style option string; ## The method removes inserted functions from class; # @param target from which class to remove functions; # @param args list of methods to remove; ## Reads all methods containing a selector from specified module; # @param module finding methods in this module; # @param selector if method in module contains this string will be selected; ## This function will register all functions which name contains ""Draw"" to TMVA.DataLoader and TMVA.Factory; # from DataLoader and Factory modules; ## This function will remove all functions which name contains ""Draw"" from TMVA.DataLoader and TMVA.Factory; # if the function was inserted from DataLoader and Factory modules; ## This function captures objects which are declared in noteboko cell. It's used to capture factory and data loader objects.; # @param args classes; ## Class for creating the output scripts and inserting them to cell output; ## Base repository; ## String containing the link to JavaScript files; ## String containing the link to CSS files; ## Drawing are sizes; ## id for drawing area; ## Template containing HTML code with draw area and drawing JavaScript; """"""; <div id=""$divid"" style=""width: ${width}px; height:${height}px""></div>; <script>; require(['JsMVA'],function(jsmva){; jsmva.$funcName('$divid','$dat');; });; </script>; """"""; ## Template containing JsMVA initialization code (adding JsMVA script location, and CSS); """"""; <script type=""text/javascript"">; require.config({; paths: {; 'JsMVA':'$PATH/JsMVA.min'; }; });; </script>; <link rel=""stylesheet"" href=""$CSSFile""></link>; """"""; ## Template containing data insertion JavaScript code; """"""<script id=""dataInserterScript"">; require(['JsMVA'],function(jsmva){; jsmva.$funcName('$divid', '$dat');; var script = document.getElementById(""data",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/JsMVA/JPyInterface.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/JsMVA/JPyInterface.py
Security,inject,inject,"# -*- coding: utf-8 -*-; ## @package JsMVA.JPyInterface; # @author Attila Bagoly <battila93@gmail.com>; # JPyInterface is responsible for adding the drawing methods to TMVA; # and for creating the JavaScript outputs from objects.; ## Function inserter class; # This class contains the methods which are invoked by using jsmva magic, and will inject the new methods; # to TMVA::Factory, TMVA::DataLoader; ## Threaded functions; ## The method inserter function; # @param target which class to insert; # @param source module which contains the methods to insert; # @param args list of methods to insert; ## This method change TMVA methods with new methods; # @param target which class to insert; # @param source module which contains the methods to insert; # @param args list of methods to insert; ## Get's special parameters from kwargs and converts to positional parameter; # args[0] = self; ## Converts object to TMVA style option string; ## The method removes inserted functions from class; # @param target from which class to remove functions; # @param args list of methods to remove; ## Reads all methods containing a selector from specified module; # @param module finding methods in this module; # @param selector if method in module contains this string will be selected; ## This function will register all functions which name contains ""Draw"" to TMVA.DataLoader and TMVA.Factory; # from DataLoader and Factory modules; ## This function will remove all functions which name contains ""Draw"" from TMVA.DataLoader and TMVA.Factory; # if the function was inserted from DataLoader and Factory modules; ## This function captures objects which are declared in noteboko cell. It's used to capture factory and data loader objects.; # @param args classes; ## Class for creating the output scripts and inserting them to cell output; ## Base repository; ## String containing the link to JavaScript files; ## String containing the link to CSS files; ## Drawing are sizes; ## id for drawing area; ## Template ",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/JsMVA/JPyInterface.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/JsMVA/JPyInterface.py
Availability,error,error,"form one line of C++ output string to HTML.; # @param self object pointer; # @param line line to transform; ## Checks if a line is empty.; # @param self object pointer; # @param line line to check; ## Transforms all data set specific content.; # @param self object pointer; # @param firstLine first line which mach with header; # @param startIndex it defines where we are in self.lines array; # @param maxlen defines how many iterations we can do from startIndex; ## It transforms number of events related contents.; # @param self object pointer; # @param firstLine first line which mach with header; # @param startIndex it defines where we are in self.lines array; # @param maxlen defines how many iterations we can do from startIndex; ## Transform Variable related information to table.; # @param self object pointer; # @param headerMatch re.match object for the first line; # @param startIndex it defines where we are in self.lines array; # @param maxlen defines how many iterations we can do from startIndex; ## This function creates a correlation matrix from python matrix.; # @param self object pointer; # @param title first part of the title of histogram; # @param className Signal/Background: it will be on title; # @param varNames array with variable names; # @param matrix the correlation matrix; ## This function add different flag based on the line formation. It add's a specific class for each output type.; # @param self object pointer; # @param line current line; ## This function can transform one group of output. The group has only one header.; # @param self object pointer; # @param firstLine the header line, it defines the start; ## This is the main function, it will be registered as transformer function to JupyROOT, it will run every time; # when some ROOT function produces output. It get's the C++ style output and it returns it in HTML version.; # @param self object pointer; # @param output the content of C++ output stream; # @param error the content of C++ error stream",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/JsMVA/OutputTransformer.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/JsMVA/OutputTransformer.py
Modifiability,variab,variable,"form one line of C++ output string to HTML.; # @param self object pointer; # @param line line to transform; ## Checks if a line is empty.; # @param self object pointer; # @param line line to check; ## Transforms all data set specific content.; # @param self object pointer; # @param firstLine first line which mach with header; # @param startIndex it defines where we are in self.lines array; # @param maxlen defines how many iterations we can do from startIndex; ## It transforms number of events related contents.; # @param self object pointer; # @param firstLine first line which mach with header; # @param startIndex it defines where we are in self.lines array; # @param maxlen defines how many iterations we can do from startIndex; ## Transform Variable related information to table.; # @param self object pointer; # @param headerMatch re.match object for the first line; # @param startIndex it defines where we are in self.lines array; # @param maxlen defines how many iterations we can do from startIndex; ## This function creates a correlation matrix from python matrix.; # @param self object pointer; # @param title first part of the title of histogram; # @param className Signal/Background: it will be on title; # @param varNames array with variable names; # @param matrix the correlation matrix; ## This function add different flag based on the line formation. It add's a specific class for each output type.; # @param self object pointer; # @param line current line; ## This function can transform one group of output. The group has only one header.; # @param self object pointer; # @param firstLine the header line, it defines the start; ## This is the main function, it will be registered as transformer function to JupyROOT, it will run every time; # when some ROOT function produces output. It get's the C++ style output and it returns it in HTML version.; # @param self object pointer; # @param output the content of C++ output stream; # @param error the content of C++ error stream",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/JsMVA/OutputTransformer.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/JsMVA/OutputTransformer.py
Modifiability,inherit,inherited,"# Author: Enric Tejedor CERN 04/2019; ################################################################################; # Copyright (C) 1995-2019, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Parameters:; # self: Object being drawn; # args: arguments for Draw; # When drawing a TPad, it gets added to the list of primititves of its; # mother TPad (fMother) with kCanDelete == 1. This means that, when; # fMother is destructed, it will attempt to destroy its child TPad too.; # To prevent a double delete, here we instruct the Python proxy of the; # child C++ TPad being drawn not to destroy the latter (ROOT-10060).; # ; # A similar principle is applied to TButton, TColorWheel, TPolyLine3D,; # TPolyMarker and TPolyMarker3D, whose kCanDelete bit is set in one of; # their constructors. Later, when being drawn, they are appended to; # the list of primitives of gPad.; # Parameters:; # self: Object being initialized; # args: arguments for __init__; # TSlider is a special case, since it is appended to gPad already; # in one of its constructors, after setting kCanDelete.; # Therefore, we need to set the ownership here and not in Draw; # (TSlider does not need to be drawn). This is ROOT-10095.; # We have already set the ownership while initializing,; # so we do not need the custom Draw inherited from TPad to; # do it again in case it is executed.; # Parameters:; # klass: class to be pythonized; # Parameters:; # klass: class to be pythonized",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_drawables.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_drawables.py
Integrability,inject,injected,"# Author: Enric Tejedor CERN 4/2022; ################################################################################; # Copyright (C) 1995-2022, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; '''; Instances of this class can be injected in class proxies to replace method; templates that we want to pythonize. Similarly to what `partialmethod`; does, this class implements `__get__` to return a wrapper object of a; method that is bound to an instance of the pythonized class. Such object is; both callable and subscriptable. Attributes:; _original_method (cppyy TemplateProxy): original cppyy method template; being pythonized.; _wrapper_class (subclass of MethodTemplateWrapper): class that wraps a; pythonized method template.; _extra_args (tuple): extra arguments to be forwarded to; `_wrapper_class`'s __init__ method, to be used by the wrapper object; when receiving a call.; '''; '''; Initializes the getter object of a method template.; Saves the original implementation of the method template to be replaced; (i.e. the one provided by cppyy) so that it can be later used in the; implementation of the pythonization by the wrapper object. Args:; original_method (cppyy TemplateProxy): original cppyy method; template being pythonized.; wrapper_class (subclass of MethodTemplateWrapper): class that wraps; a pythonized method template.; extra_args (tuple, optional): extra arguments to be forwarded to; `wrapper_class`'s __init__ method, to be used by the wrapper; object when receiving a call.; '''; '''; Creates and returns a wrapper object for a method template. The type of; the wrapper is a subclass of MethodTemplateWrapper.; By implementing `__get__`, we obtain a handle to the instance of the; pythonized class on which the application accessed the method template.; That",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_pyz_utils.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_pyz_utils.py
Security,inject,injected,"# Author: Enric Tejedor CERN 4/2022; ################################################################################; # Copyright (C) 1995-2022, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; '''; Instances of this class can be injected in class proxies to replace method; templates that we want to pythonize. Similarly to what `partialmethod`; does, this class implements `__get__` to return a wrapper object of a; method that is bound to an instance of the pythonized class. Such object is; both callable and subscriptable. Attributes:; _original_method (cppyy TemplateProxy): original cppyy method template; being pythonized.; _wrapper_class (subclass of MethodTemplateWrapper): class that wraps a; pythonized method template.; _extra_args (tuple): extra arguments to be forwarded to; `_wrapper_class`'s __init__ method, to be used by the wrapper object; when receiving a call.; '''; '''; Initializes the getter object of a method template.; Saves the original implementation of the method template to be replaced; (i.e. the one provided by cppyy) so that it can be later used in the; implementation of the pythonization by the wrapper object. Args:; original_method (cppyy TemplateProxy): original cppyy method; template being pythonized.; wrapper_class (subclass of MethodTemplateWrapper): class that wraps; a pythonized method template.; extra_args (tuple, optional): extra arguments to be forwarded to; `wrapper_class`'s __init__ method, to be used by the wrapper; object when receiving a call.; '''; '''; Creates and returns a wrapper object for a method template. The type of; the wrapper is a subclass of MethodTemplateWrapper.; By implementing `__get__`, we obtain a handle to the instance of the; pythonized class on which the application accessed the method template.; That",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_pyz_utils.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_pyz_utils.py
Testability,log,logic,"wrapper_class`'s __init__ method, to be used by the wrapper; object when receiving a call.; '''; '''; Creates and returns a wrapper object for a method template. The type of; the wrapper is a subclass of MethodTemplateWrapper.; By implementing `__get__`, we obtain a handle to the instance of the; pythonized class on which the application accessed the method template.; That allows us to get an original implementation of the method template; that is bound to that instance, and pass such implementation along to; the wrapper object for later use. Args:; instance (class instance): instance of the pythonized class on; which the application accessed the method template.; instance_type (class type): type of the instance. Returns:; instance of MethodTemplateWrapper subclass: contains a handle to; the original implementation of the method template that is; bound to `instance` and, possibly, some extra arguments to be; used when receiving a call.; '''; '''; Abstract base class that defines some common logic to properly pythonize; method templates. More precisely, it provides an implementation of; `__getitem__` that makes wrappers subscriptable and allows them to capture; template arguments.; Subclasses of this class must redefine `__call__` with the actual; pythonization of the method template. Attributes:; _original_method (cppyy TemplateProxy): original implementation of the; method template that is bound to the instance on which the template; was accessed.; _extra_args (tuple): extra arguments to be used when receiving a call.; '''; '''; Constructor of a wrapper object for a method template. Args:; original_method (cppyy TemplateProxy): original implementation of; the method template that is bound to the instance on which the; template was accessed.; extra_args (tuple): extra arguments to be used when receiving a; call.; '''; '''; Captures the template arguments used to instantiate the method template. Args:; template_args (tuple): template arguments. Returns:; instance of M",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_pyz_utils.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_pyz_utils.py
Availability,alive,alive,"and `_extra_args` attributes of the; superclass, to invoke the original implementation of the method template; and get the model class, respectively.; """"""; """"""; Pythonization of HistoXD and ProfileXD method templates.; Checks whether the user made a call with a tuple as first argument; in; that case, extracts the tuple items to construct a model object and; calls the original implementation of the method with that object. Args:; args: arguments of a HistoXD or ProfileXD call. Returns:; return value of the original HistoXD or ProfileXD implementations.; """"""; # Construct the model with the elements of the tuple; # as arguments; # Call the original implementation of the method; # with the model as first argument; # Covers the case of the overloads with only model passed; # as argument; # If the first argument is not a tuple, nothing to do, just call; # the original implementation; # Parameters:; # klass: class to be pythonized; # Add asNumpy feature; # Replace the implementation of the following RDF methods; # to convert a tuple argument into a model object; # Replace the original implementation of the method; # with an object that can handle template arguments; # and stores a reference to such implementation; # Get name of key; # Convert value to RVec and attach to dictionary; # Add pairs of column name and associated RVec to signature; # For references to keep alive the NumPy arrays that are read by; # MakeNumpyDataFrame.; # How we keep the NumPy arrays around as long as the RDataSource is alive:; #; # 1. Cache a container with references to the NumPy arrays in a global; # dictionary. Note that we use a copy of the original dict as the; # container, because otherwise the caller of _MakeNumpyDataFrame can; # invalidate our cache by mutating the np_dict after the call.; #; # 2. Together with the array data, store a deleter function to delete the; # cache element in the cache itself.; #; # 3. The C++ side gets a reference to the deleter function via; # std::function. Not",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_rdataframe.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_rdataframe.py
Integrability,wrap,wrapping,"ziness when it comes to triggering the event loop. Attributes:; _columns (list): list of the names of the columns returned by; AsNumpy.; _py_arrays (dict): results of the AsNumpy action. The key is the; column name, the value is the NumPy array for that column.; _result_ptrs (dict): results of the AsNumpy action. The key is the; column name, the value is the result pointer for that column.; """"""; """"""Constructs an AsNumpyResult object. Parameters:; result_ptrs (dict): results of the AsNumpy action. The key is the; column name, the value is the result pointer for that column.; columns (list): list of the names of the columns returned by; AsNumpy.; """"""; """"""Triggers, if necessary, the event loop to run the Take actions for; the requested columns and produce the NumPy arrays as result. Returns:; dict: key is the column name, value is the NumPy array for that; column.; """"""; # Convert the C++ vectors to numpy arrays; # This adopts the memory of the C++ object.; # This creates only the wrapping of the objects and does not copy.; """"""; Merges the numpy arrays in the dictionary of this object with the numpy; arrays in the dictionary of the other object, modifying the attribute of; this object inplace. Raises:; - RuntimeError: if either of the method arguments doesn't already; have filled the internal dictionary of numpy arrays.; - ImportError: if the numpy module couldn't be imported.; - ValueError: If the dictionaries of numpy arrays of the two; arguments don't have exactly the same keys.; """"""; """"""; This function is called during the pickle serialization step. Return the; dictionary of numpy arrays (i.e. the actual result of this `AsNumpy`; call). Other attributes are not needed and the RResultPtr objects are; not serializable at all.; """"""; """"""; This function is called during unserialization step. Sets the dictionary; of numpy array of the unserialized object.; """"""; """"""; Clones the internal actions held by the input result and returns a new; result.; """"""; """"""; Subclass of Meth",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_rdataframe.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_rdataframe.py
Performance,perform,performant,"# Author: Stefan Wunsch, Massimiliano Galli, Enric Tejedor (02/2019), Pawan Johnson CERN 07/2022; ################################################################################; # Copyright (C) 1995-2022, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; """"""Read-out the RDataFrame as a collection of numpy arrays. The values of the dataframe are read out as numpy array of the respective type; if the type is a fundamental type such as float or int. If the type of the column; is a complex type, such as your custom class or a std::array, the returned numpy; array contains Python objects of this type interpreted via PyROOT. Be aware that reading out custom types is much less performant than reading out; fundamental types, such as int or float, which are supported directly by numpy. The reading is performed in multiple threads if the implicit multi-threading of; ROOT is enabled. Note that this is an instant action of the RDataFrame graph and will trigger the; event-loop. Parameters:; columns: If None return all branches as columns, otherwise specify names in iterable.; exclude: Exclude branches from selection.; lazy: Determines whether this action is instant (False, default) or lazy (True). Returns:; dict or AsNumpyResult: if instant (default), dict with column names as keys and; 1D numpy arrays with content as values; if lazy, AsNumpyResult containing; the result pointers obtained from the Take actions.; """"""; # Sanitize input arguments; # Early check for numpy; # Find all column names in the dataframe if no column are specified; # Exclude the specified columns; # Register Take action for each column; # bool columns should be taken as unsigned chars, because NumPy stores; # bools in bytes - different from the std::vector<bool> returned by the; # action, which migh",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_rdataframe.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_rdataframe.py
Integrability,depend,dependency,"# Author: Pawan Johnson, Vincenzo Eduardo Padulano, Enric Tejedor CERN 07/2022; ################################################################################; # Copyright (C) 1995-2022, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; """"""; Contains constants needed for _rdf_pyz to convert datatypes for numba declarable types.; It is in a separate module so as to avoid a numpy dependency for ROOT.; """"""",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_rdf_conversion_maps.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_rdf_conversion_maps.py
Safety,avoid,avoid,"# Author: Pawan Johnson, Vincenzo Eduardo Padulano, Enric Tejedor CERN 07/2022; ################################################################################; # Copyright (C) 1995-2022, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; """"""; Contains constants needed for _rdf_pyz to convert datatypes for numba declarable types.; It is in a separate module so as to avoid a numpy dependency for ROOT.; """"""",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_rdf_conversion_maps.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_rdf_conversion_maps.py
Availability,error,error,"t is to be jitted.; return_type: Return type of above callable; params: List of parameters of the above callable; func_args: Dict to store the value of each param; args_info: Information about the type and value of each param; func_sign: Contains the function signature; func_call: Contains the function call as a string. Example:; rdf = ROOT.RDataFrame(5).Define(""x"", ""(double) rdfentry_""); def x_greater_than_2(x):; return x>2; fj = FunctionJitter(rdf); func_call = fj.jit(x_greater_than_2); fil1 = rdf.Filter( ""Numba::"" + func_call, ""x is greater than 2""). """"""; # Variable to store previous functions so as to not rejit.; # Counter to name the lambda functions; """"""; Function to determine the type of a variable x.; If x is a string. It maps it to the corresponding column's type in RDF. (String constants are not supported); Else it determines the fundamental type of x. (int, float, bool); Else it is an a numpy array and maps it to corresponding RVec.; Otherwise flags a type error; Args:; 	 ` x: Variable whose type is to be determined. Returns:; type of the variable x; """"""; # Can be string constant or can be column name; # If x is a column; # The column is a fundamental type from tree; # The column type is a RVec<type>; # It is a RVec<RVec<T>>; # There are data type that leak into here. Not sure from where. But need to implement something here such that this condition is never met.; #! Numba Declare does not support ""string"" type. Check _numbadeclare.Thus, Cannot pass string constants to the filter/Defines..; #! Need to work out how to map things like tuples, dicts, lists...; """"""; Function to create a list of parameters func needs.; Updates the class to have a new attribute params which is a list of the parameters of func. Arguments:; func: A python callable. """"""; # Find the Return type; # List of input parameters for function; # ALl the input parameters the function needs.; """"""; Function to create a dictionary mapping the parameters of the function to the corresponding col",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_rdf_pyz.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_rdf_pyz.py
Integrability,depend,depends,"unction signature.; Updates the class with a new attribute args_info which contains the mapping of the parameter of the function to its type and value.; args_info: dict{parameter_name(str): (type, value)}; """"""; # the parameter value has been given in func_args; # Bool(s) in python are represented as True/False but in C++ are true/false. The following if statements are to account for that; # the parameter was not in func_args. Thus this parameter has to be mapped to a column of rdf; """"""; Generates a function call and signature. Gives a unique name to the function if it is a lambda function.; Updates the class with new attributes func_call and func_sign to hold them,; """"""; """"""; Function to generate the function params, args, signature and call.; """"""; """"""; Jits the provided function using ROOT's NumbaDeclare.; Also checks if the function was jitted earlier in which case it won't jit again but if signature does not match. It raises an error. Arguments:; func: A python callable; cols_list: A list of columns of RDF on which func depends on.; extra_args: A dict of extra arguments that func requires.; """"""; """"""; Converts a Python list of strings into an std::vector before passing such; argument to a Filter operation.; The purpose of this function is to workaround issue #10092 until there is; a proper fix in cppyy.; Arguments:; args: arguments passed to Filter, possibly including the list of column; names; Returns:; Tuple with arguments, possibly replacing a list of column names by a; vector; """"""; """"""; Checks whether the callable `func` is a cppyy proxy of one of these:; 1. C++ functor; 2. std::function. The cases above are supported by cppyy, so we can just invoke the original; cppyy TemplateProxy (Filter or Define) with the callable as argument. Prior to the invocation of the original cppyy TemplateProxy, though, we; need to explicitly instantiate the template using the type of the `func`; callable. Implicit instantiation won't work since the original; implementation of Filt",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_rdf_pyz.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_rdf_pyz.py
Modifiability,variab,variable,"tionary containing any other arguments for the callable.; It determines the function signature and Declares the function and returns the corresponding function call. Attributes; ----------; rdf: RDataFrame object on which the function's column arguments are in; col_names: The list of columns in rdf; func: The python callable that is to be jitted.; return_type: Return type of above callable; params: List of parameters of the above callable; func_args: Dict to store the value of each param; args_info: Information about the type and value of each param; func_sign: Contains the function signature; func_call: Contains the function call as a string. Example:; rdf = ROOT.RDataFrame(5).Define(""x"", ""(double) rdfentry_""); def x_greater_than_2(x):; return x>2; fj = FunctionJitter(rdf); func_call = fj.jit(x_greater_than_2); fil1 = rdf.Filter( ""Numba::"" + func_call, ""x is greater than 2""). """"""; # Variable to store previous functions so as to not rejit.; # Counter to name the lambda functions; """"""; Function to determine the type of a variable x.; If x is a string. It maps it to the corresponding column's type in RDF. (String constants are not supported); Else it determines the fundamental type of x. (int, float, bool); Else it is an a numpy array and maps it to corresponding RVec.; Otherwise flags a type error; Args:; 	 ` x: Variable whose type is to be determined. Returns:; type of the variable x; """"""; # Can be string constant or can be column name; # If x is a column; # The column is a fundamental type from tree; # The column type is a RVec<type>; # It is a RVec<RVec<T>>; # There are data type that leak into here. Not sure from where. But need to implement something here such that this condition is never met.; #! Numba Declare does not support ""string"" type. Check _numbadeclare.Thus, Cannot pass string constants to the filter/Defines..; #! Need to work out how to map things like tuples, dicts, lists...; """"""; Function to create a list of parameters func needs.; Updates the class",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_rdf_pyz.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_rdf_pyz.py
Integrability,wrap,wrapper,"# Author: Stefan Wunsch CERN, Vincenzo Eduardo Padulano (UniMiB, CERN) 07/2019; ################################################################################; # Copyright (C) 1995-2018, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; """"""; A wrapper class that inherits from `numpy.ndarray` and allows to attach the; result pointer of the `Take` action in an `RDataFrame` event loop to the; collection of values returned by that action. See; https://docs.scipy.org/doc/numpy/user/basics.subclassing.html for more; information on subclassing numpy arrays.; """"""; """"""; Dunder method invoked at the creation of an instance of this class. It; creates a numpy array with an `RResultPtr` as an additional; attribute.; """"""; """"""; Dunder method that fills in the instance default `result_ptr` value.; """"""",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_rdf_utils.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_rdf_utils.py
Modifiability,inherit,inherits,"# Author: Stefan Wunsch CERN, Vincenzo Eduardo Padulano (UniMiB, CERN) 07/2019; ################################################################################; # Copyright (C) 1995-2018, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; """"""; A wrapper class that inherits from `numpy.ndarray` and allows to attach the; result pointer of the `Take` action in an `RDataFrame` event loop to the; collection of values returned by that action. See; https://docs.scipy.org/doc/numpy/user/basics.subclassing.html for more; information on subclassing numpy arrays.; """"""; """"""; Dunder method invoked at the creation of an instance of this class. It; creates a numpy array with an `RResultPtr` as an additional; attribute.; """"""; """"""; Dunder method that fills in the instance default `result_ptr` value.; """"""",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_rdf_utils.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_rdf_utils.py
Availability,error,error,"# Author: Vincenzo Eduardo Padulano CERN 10/2023; ################################################################################; # Copyright (C) 1995-2023, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # TODO: vpadulan; # This module enables pickling/unpickling of the std::runtime_error Python proxy; # defined in cppyy (via CPPExcInstance). The same logic should be implemented; # in the CPython extension to be more generic.; """"""; Creates a new cppyy std::runtime_error proxy with the original message.; """"""; """"""; Establish the strategy to recreate a std::runtime_error when unpickling. Creates a new error from the original message. Need to use a separate free; function instead of an inline lambda to help pickle.; """"""; """"""Add serialization capabilities to std::runtime_error.""""""",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_runtime_error.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_runtime_error.py
Integrability,message,message,"# Author: Vincenzo Eduardo Padulano CERN 10/2023; ################################################################################; # Copyright (C) 1995-2023, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # TODO: vpadulan; # This module enables pickling/unpickling of the std::runtime_error Python proxy; # defined in cppyy (via CPPExcInstance). The same logic should be implemented; # in the CPython extension to be more generic.; """"""; Creates a new cppyy std::runtime_error proxy with the original message.; """"""; """"""; Establish the strategy to recreate a std::runtime_error when unpickling. Creates a new error from the original message. Need to use a separate free; function instead of an inline lambda to help pickle.; """"""; """"""Add serialization capabilities to std::runtime_error.""""""",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_runtime_error.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_runtime_error.py
Testability,log,logic,"# Author: Vincenzo Eduardo Padulano CERN 10/2023; ################################################################################; # Copyright (C) 1995-2023, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # TODO: vpadulan; # This module enables pickling/unpickling of the std::runtime_error Python proxy; # defined in cppyy (via CPPExcInstance). The same logic should be implemented; # in the CPython extension to be more generic.; """"""; Creates a new cppyy std::runtime_error proxy with the original message.; """"""; """"""; Establish the strategy to recreate a std::runtime_error when unpickling. Creates a new error from the original message. Need to use a separate free; function instead of an inline lambda to help pickle.; """"""; """"""Add serialization capabilities to std::runtime_error.""""""",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_runtime_error.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_runtime_error.py
Integrability,interface,interface,"# Author: Stefan Wunsch CERN 08/2018; ################################################################################; # Copyright (C) 1995-2018, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Get array interface of object; # Get the data-pointer; # Get the size of the contiguous memory; # Get the typestring and properties thereof; # Construct an RVec of the correct data-type; # Bind pyobject holding adopted memory to the RVec; # Numpy breaks for data pointer of 0 even though the array is empty.; # We set the pointer to 1 but the value itself is arbitrary and never accessed.; # Parameters:; # klass: class to be pythonized; # name: string containing the name of the class; # Add numpy array interface",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_rvec.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_rvec.py
Security,access,accessed,"# Author: Stefan Wunsch CERN 08/2018; ################################################################################; # Copyright (C) 1995-2018, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Get array interface of object; # Get the data-pointer; # Get the size of the contiguous memory; # Get the typestring and properties thereof; # Construct an RVec of the correct data-type; # Bind pyobject holding adopted memory to the RVec; # Numpy breaks for data pointer of 0 even though the array is empty.; # We set the pointer to 1 but the value itself is arbitrary and never accessed.; # Parameters:; # klass: class to be pythonized; # name: string containing the name of the class; # Add numpy array interface",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_rvec.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_rvec.py
Integrability,interface,interface,"# Author: Stefan Wunsch, Enric Tejedor CERN 08/2018; ################################################################################; # Copyright (C) 1995-2018, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # vector<char>::data() returns char*.; # Cppyy attempts to convert char* into Python string, but if the; # character sequence is not null-terminated the conversion fails.; # This is likely to happen with the result of vector<char>::data().; # For the conversion char* -> str to succeed when calling data(),; # temporarily append a null character to the vector<char>.; # Parameters:; # klass: class to be pythonized; # name: string containing the name of the class; # Add numpy array interface; # NOTE: The pythonization is reused from ROOT::VecOps::RVec; # Inject custom vector<char>::data(); # Pretty printing at the Python prompt",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_stl_vector.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_stl_vector.py
Modifiability,extend,extend,"# Author: Enric Tejedor CERN 01/2019; ################################################################################; # Copyright (C) 1995-2019, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Python-list-like methods; # Parameters:; # - self: collection; # - o: object to remove from the collection; # Parameters:; # - self: collection; # - c: collection to extend self with; # Parameters:; # - self: collection; # - o: object to be counted in the collection; # Returns:; # - Number of occurrences of the object in the collection; # Python operators; # Parameters:; # - self: first collection to be added; # - c: second collection to be added; # Returns:; # - self + c; # Parameters:; # - self: collection to be multiplied; # - n: factor to multiply the collection by; # Returns:; # - self * n; # Parameters:; # - self: collection to be multiplied (in place); # - n: factor to multiply the collection by; # Returns:; # - self *= n; # Python iteration; # Generator function to iterate on TCollections; # Parameters:; # - self: collection to be iterated; # TIter instances are iterable; # Parameters:; # klass: class to be pythonized; # Support `len(c)` as `c.GetEntries()`; # Add Python lists methods; # Define Python operators; # Make TCollections iterable",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tcollection.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tcollection.py
Availability,failure,failure,"# Author: Danilo Piparo, Stefan Wunsch CERN 08/2018; ################################################################################; # Copyright (C) 1995-2018, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; """"""Injection of TDirectory.__getitem__ that raises AttributeError on failure. Method that is assigned to TDirectory.__getitem__. It relies on Get to; obtain the object from the TDirectory and adds on top:; - Raising an AttributeError if the object does not exist; - Caching the result of a successful get for future re-attempts.; Once cached, the same object is retrieved every time.; This pythonisation is inherited by TDirectoryFile and TFile. Example:; ```; myfile.mydir.mysubdir.myHist.Draw(); ```; """"""; # Caching behavior seems to be more clear to the user; can always override said; # behavior (i.e. re-read from file) with an explicit Get() call; """"""For temporary backwards compatibility.""""""; # Caching behavior seems to be more clear to the user; can always override said; # behavior (i.e. re-read from file) with an explicit Get() call; """"""; Implements the WriteObject method of TDirectory; This method allows to write objects into TDirectory instances with this; syntax:; ```; myDir.WriteObject(myObj, ""myKeyName""); ```; """"""; # Implement a check on whether the object is derived from TObject or not.; # Similarly to what is done in TDirectory::WriteObject with SFINAE.; # Instant pythonization (executed at `import ROOT` time), no need of a; # decorator. This is a core class that is instantiated before cppyy's; # pythonization machinery is in place.",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tdirectory.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tdirectory.py
Modifiability,inherit,inherited,"# Author: Danilo Piparo, Stefan Wunsch CERN 08/2018; ################################################################################; # Copyright (C) 1995-2018, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; """"""Injection of TDirectory.__getitem__ that raises AttributeError on failure. Method that is assigned to TDirectory.__getitem__. It relies on Get to; obtain the object from the TDirectory and adds on top:; - Raising an AttributeError if the object does not exist; - Caching the result of a successful get for future re-attempts.; Once cached, the same object is retrieved every time.; This pythonisation is inherited by TDirectoryFile and TFile. Example:; ```; myfile.mydir.mysubdir.myHist.Draw(); ```; """"""; # Caching behavior seems to be more clear to the user; can always override said; # behavior (i.e. re-read from file) with an explicit Get() call; """"""For temporary backwards compatibility.""""""; # Caching behavior seems to be more clear to the user; can always override said; # behavior (i.e. re-read from file) with an explicit Get() call; """"""; Implements the WriteObject method of TDirectory; This method allows to write objects into TDirectory instances with this; syntax:; ```; myDir.WriteObject(myObj, ""myKeyName""); ```; """"""; # Implement a check on whether the object is derived from TObject or not.; # Similarly to what is done in TDirectory::WriteObject with SFINAE.; # Instant pythonization (executed at `import ROOT` time), no need of a; # decorator. This is a core class that is instantiated before cppyy's; # pythonization machinery is in place.",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tdirectory.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tdirectory.py
Performance,cache,cached,"# Author: Danilo Piparo, Stefan Wunsch CERN 08/2018; ################################################################################; # Copyright (C) 1995-2018, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; """"""Injection of TDirectory.__getitem__ that raises AttributeError on failure. Method that is assigned to TDirectory.__getitem__. It relies on Get to; obtain the object from the TDirectory and adds on top:; - Raising an AttributeError if the object does not exist; - Caching the result of a successful get for future re-attempts.; Once cached, the same object is retrieved every time.; This pythonisation is inherited by TDirectoryFile and TFile. Example:; ```; myfile.mydir.mysubdir.myHist.Draw(); ```; """"""; # Caching behavior seems to be more clear to the user; can always override said; # behavior (i.e. re-read from file) with an explicit Get() call; """"""For temporary backwards compatibility.""""""; # Caching behavior seems to be more clear to the user; can always override said; # behavior (i.e. re-read from file) with an explicit Get() call; """"""; Implements the WriteObject method of TDirectory; This method allows to write objects into TDirectory instances with this; syntax:; ```; myDir.WriteObject(myObj, ""myKeyName""); ```; """"""; # Implement a check on whether the object is derived from TObject or not.; # Similarly to what is done in TDirectory::WriteObject with SFINAE.; # Instant pythonization (executed at `import ROOT` time), no need of a; # decorator. This is a core class that is instantiated before cppyy's; # pythonization machinery is in place.",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tdirectory.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tdirectory.py
Usability,clear,clear,"# Author: Danilo Piparo, Stefan Wunsch CERN 08/2018; ################################################################################; # Copyright (C) 1995-2018, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; """"""Injection of TDirectory.__getitem__ that raises AttributeError on failure. Method that is assigned to TDirectory.__getitem__. It relies on Get to; obtain the object from the TDirectory and adds on top:; - Raising an AttributeError if the object does not exist; - Caching the result of a successful get for future re-attempts.; Once cached, the same object is retrieved every time.; This pythonisation is inherited by TDirectoryFile and TFile. Example:; ```; myfile.mydir.mysubdir.myHist.Draw(); ```; """"""; # Caching behavior seems to be more clear to the user; can always override said; # behavior (i.e. re-read from file) with an explicit Get() call; """"""For temporary backwards compatibility.""""""; # Caching behavior seems to be more clear to the user; can always override said; # behavior (i.e. re-read from file) with an explicit Get() call; """"""; Implements the WriteObject method of TDirectory; This method allows to write objects into TDirectory instances with this; syntax:; ```; myDir.WriteObject(myObj, ""myKeyName""); ```; """"""; # Implement a check on whether the object is derived from TObject or not.; # Similarly to what is done in TDirectory::WriteObject with SFINAE.; # Instant pythonization (executed at `import ROOT` time), no need of a; # decorator. This is a core class that is instantiated before cppyy's; # pythonization machinery is in place.",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tdirectory.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tdirectory.py
Integrability,inject,inject,"# Author: Danilo Piparo, Stefan Wunsch, Massimiliano Galli CERN 08/2018; ################################################################################; # Copyright (C) 1995-2018, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; """"""; Allow access to objects through the method Get(). This concerns both TDirectoryFile and TFile, since the latter; inherits the Get method from the former.; We decided not to inject this behavior directly in TDirectory; because this one already has a templated method Get which, when; invoked from Python, returns an object of the derived class (e.g. TH1F); and not a generic TObject.; In case the object is not found, a null pointer is returned.; """"""; # no key? for better or worse, call normal Get(); # Pythonizor function; """"""; TDirectoryFile inherits from TDirectory the pythonized attr syntax (__getattr__); and WriteObject method.; On the other side, the Get() method is pythonised only in TDirectoryFile.; Thus, the situation is now the following:. 1) __getattr__ : TDirectory --> TDirectoryFile --> TFile; 1.1) caches the returned object for future attempts; 1.2) raises AttributeError if object not found. 2) Get() : TDirectoryFile --> TFile; 2.1) does not cache the returned object; 2.2 returns nullptr if object not found; """"""",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tdirectoryfile.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tdirectoryfile.py
Modifiability,inherit,inherits,"# Author: Danilo Piparo, Stefan Wunsch, Massimiliano Galli CERN 08/2018; ################################################################################; # Copyright (C) 1995-2018, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; """"""; Allow access to objects through the method Get(). This concerns both TDirectoryFile and TFile, since the latter; inherits the Get method from the former.; We decided not to inject this behavior directly in TDirectory; because this one already has a templated method Get which, when; invoked from Python, returns an object of the derived class (e.g. TH1F); and not a generic TObject.; In case the object is not found, a null pointer is returned.; """"""; # no key? for better or worse, call normal Get(); # Pythonizor function; """"""; TDirectoryFile inherits from TDirectory the pythonized attr syntax (__getattr__); and WriteObject method.; On the other side, the Get() method is pythonised only in TDirectoryFile.; Thus, the situation is now the following:. 1) __getattr__ : TDirectory --> TDirectoryFile --> TFile; 1.1) caches the returned object for future attempts; 1.2) raises AttributeError if object not found. 2) Get() : TDirectoryFile --> TFile; 2.1) does not cache the returned object; 2.2 returns nullptr if object not found; """"""",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tdirectoryfile.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tdirectoryfile.py
Performance,cache,caches,"# Author: Danilo Piparo, Stefan Wunsch, Massimiliano Galli CERN 08/2018; ################################################################################; # Copyright (C) 1995-2018, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; """"""; Allow access to objects through the method Get(). This concerns both TDirectoryFile and TFile, since the latter; inherits the Get method from the former.; We decided not to inject this behavior directly in TDirectory; because this one already has a templated method Get which, when; invoked from Python, returns an object of the derived class (e.g. TH1F); and not a generic TObject.; In case the object is not found, a null pointer is returned.; """"""; # no key? for better or worse, call normal Get(); # Pythonizor function; """"""; TDirectoryFile inherits from TDirectory the pythonized attr syntax (__getattr__); and WriteObject method.; On the other side, the Get() method is pythonised only in TDirectoryFile.; Thus, the situation is now the following:. 1) __getattr__ : TDirectory --> TDirectoryFile --> TFile; 1.1) caches the returned object for future attempts; 1.2) raises AttributeError if object not found. 2) Get() : TDirectoryFile --> TFile; 2.1) does not cache the returned object; 2.2 returns nullptr if object not found; """"""",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tdirectoryfile.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tdirectoryfile.py
Security,access,access,"# Author: Danilo Piparo, Stefan Wunsch, Massimiliano Galli CERN 08/2018; ################################################################################; # Copyright (C) 1995-2018, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; """"""; Allow access to objects through the method Get(). This concerns both TDirectoryFile and TFile, since the latter; inherits the Get method from the former.; We decided not to inject this behavior directly in TDirectory; because this one already has a templated method Get which, when; invoked from Python, returns an object of the derived class (e.g. TH1F); and not a generic TObject.; In case the object is not found, a null pointer is returned.; """"""; # no key? for better or worse, call normal Get(); # Pythonizor function; """"""; TDirectoryFile inherits from TDirectory the pythonized attr syntax (__getattr__); and WriteObject method.; On the other side, the Get() method is pythonised only in TDirectoryFile.; Thus, the situation is now the following:. 1) __getattr__ : TDirectory --> TDirectoryFile --> TFile; 1.1) caches the returned object for future attempts; 1.2) raises AttributeError if object not found. 2) Get() : TDirectoryFile --> TFile; 2.1) does not cache the returned object; 2.2 returns nullptr if object not found; """"""",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tdirectoryfile.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tdirectoryfile.py
Modifiability,inherit,inherits,"# Author: Danilo Piparo, Massimiliano Galli CERN 08/2018; # Author: Vincenzo Eduardo Padulano CERN/UPV 03/2022; ################################################################################; # Copyright (C) 1995-2022, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Redefinition of ROOT.TFile(str, ...):; # check if the instance of TFile has IsZombie() = True; # and raise OSError if so.; # Parameters:; # self: instance of TFile class; # *args: arguments passed to the constructor; # Redefinition of ROOT.TFile.Open(str, ...):; # check if the instance of TFile is a C++ nullptr and raise a; # OSError if this is the case.; # Parameters:; # klass: TFile class; # *args: arguments passed to the constructor; # args[0] can be either a string or a TFileOpenHandle; """"""; Close the TFile object.; Signature and return value are imposed by Python, see; https://docs.python.org/3/library/stdtypes.html#typecontextmanager.; """"""; # A TFile might be storing references to objects retrieved by the user in; # a cache. Make sure the cache is cleaned at exit time rather than having; # to wait for the garbage collector.; """"""; TFile inherits from; - TDirectory the pythonized attr syntax (__getattr__) and WriteObject method.; - TDirectoryFile the pythonized Get method (pythonized only in Python); and defines the __enter__ and __exit__ methods to work as a context manager.; """"""; # Pythonizations for TFile::Open; # Pythonization for TFile constructor; # Pythonization for __enter__ and __exit__ methods; # These make TFile usable in a `with` statement as a context manager",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tfile.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tfile.py
Performance,cache,cache,"# Author: Danilo Piparo, Massimiliano Galli CERN 08/2018; # Author: Vincenzo Eduardo Padulano CERN/UPV 03/2022; ################################################################################; # Copyright (C) 1995-2022, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Redefinition of ROOT.TFile(str, ...):; # check if the instance of TFile has IsZombie() = True; # and raise OSError if so.; # Parameters:; # self: instance of TFile class; # *args: arguments passed to the constructor; # Redefinition of ROOT.TFile.Open(str, ...):; # check if the instance of TFile is a C++ nullptr and raise a; # OSError if this is the case.; # Parameters:; # klass: TFile class; # *args: arguments passed to the constructor; # args[0] can be either a string or a TFileOpenHandle; """"""; Close the TFile object.; Signature and return value are imposed by Python, see; https://docs.python.org/3/library/stdtypes.html#typecontextmanager.; """"""; # A TFile might be storing references to objects retrieved by the user in; # a cache. Make sure the cache is cleaned at exit time rather than having; # to wait for the garbage collector.; """"""; TFile inherits from; - TDirectory the pythonized attr syntax (__getattr__) and WriteObject method.; - TDirectoryFile the pythonized Get method (pythonized only in Python); and defines the __enter__ and __exit__ methods to work as a context manager.; """"""; # Pythonizations for TFile::Open; # Pythonization for TFile constructor; # Pythonization for __enter__ and __exit__ methods; # These make TFile usable in a `with` statement as a context manager",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tfile.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tfile.py
Usability,usab,usable,"# Author: Danilo Piparo, Massimiliano Galli CERN 08/2018; # Author: Vincenzo Eduardo Padulano CERN/UPV 03/2022; ################################################################################; # Copyright (C) 1995-2022, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Redefinition of ROOT.TFile(str, ...):; # check if the instance of TFile has IsZombie() = True; # and raise OSError if so.; # Parameters:; # self: instance of TFile class; # *args: arguments passed to the constructor; # Redefinition of ROOT.TFile.Open(str, ...):; # check if the instance of TFile is a C++ nullptr and raise a; # OSError if this is the case.; # Parameters:; # klass: TFile class; # *args: arguments passed to the constructor; # args[0] can be either a string or a TFileOpenHandle; """"""; Close the TFile object.; Signature and return value are imposed by Python, see; https://docs.python.org/3/library/stdtypes.html#typecontextmanager.; """"""; # A TFile might be storing references to objects retrieved by the user in; # a cache. Make sure the cache is cleaned at exit time rather than having; # to wait for the garbage collector.; """"""; TFile inherits from; - TDirectory the pythonized attr syntax (__getattr__) and WriteObject method.; - TDirectoryFile the pythonized Get method (pythonized only in Python); and defines the __enter__ and __exit__ methods to work as a context manager.; """"""; # Pythonizations for TFile::Open; # Pythonization for TFile constructor; # Pythonization for __enter__ and __exit__ methods; # These make TFile usable in a `with` statement as a context manager",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tfile.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tfile.py
Availability,error,error,"# Author: Enric Tejedor CERN 03/2019; ################################################################################; # Copyright (C) 1995-2019, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Parameters:; # - self: graph object; # - buf: buffer of doubles; # Returns:; # - buffer whose size has been set; # Create a composite pythonizor.; #; # A composite is a type of pythonizor, i.e. it is a callable that expects two; # parameters: a class proxy and a string with the name of that class.; # A composite is created with the following parameters:; # - A string to match the class/es to be pythonized; # - A string to match the method/s to be pythonized in the class/es; # - A callable that will post-process the return value of the matched method/s; #; # Here we create a composite that will match TGraph, TGraph2D and their error; # subclasses, and will pythonize their getter methods of the X,Y,Z coordinate; # and error arrays, which in C++ return a pointer to a double.; # The pythonization consists in setting the size of the array that the getter; # method returns, so that it is known in Python and the array is fully usable; # (its length can be obtained, it is iterable).; # class to match; # method to match; # post-process function; # Add the composite to the list of pythonizors",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tgraph.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tgraph.py
Usability,usab,usable,"# Author: Enric Tejedor CERN 03/2019; ################################################################################; # Copyright (C) 1995-2019, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Parameters:; # - self: graph object; # - buf: buffer of doubles; # Returns:; # - buffer whose size has been set; # Create a composite pythonizor.; #; # A composite is a type of pythonizor, i.e. it is a callable that expects two; # parameters: a class proxy and a string with the name of that class.; # A composite is created with the following parameters:; # - A string to match the class/es to be pythonized; # - A string to match the method/s to be pythonized in the class/es; # - A callable that will post-process the return value of the matched method/s; #; # Here we create a composite that will match TGraph, TGraph2D and their error; # subclasses, and will pythonize their getter methods of the X,Y,Z coordinate; # and error arrays, which in C++ return a pointer to a double.; # The pythonization consists in setting the size of the array that the getter; # method returns, so that it is known in Python and the array is fully usable; # (its length can be obtained, it is iterable).; # class to match; # method to match; # post-process function; # Add the composite to the list of pythonizors",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tgraph.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tgraph.py
Availability,error,error,"d. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Item access; # Parameters:; # - idx: index whose type needs to be checked; # - msg: message to show in case of type issue; # Parameters:; # - self: collection; # - idx: index to be checked; # Returns:; # - An index >= 0, equivalent to the input idx, which is verified; # to be an integer and within the boundaries of the collection; # Parameters:; # - self: collection to get the item/s from; # - idx: index/slice of the item/s; # Returns:; # - self[idx]; # Slice; # Number; # Parameters:; # - self: collection to remove the item from; # - idx: index of the item, always positive; # Parameters:; # - self: collection where to set item/s; # - idx: index/slice of the item/s; # - val: value to assign; # Slice; # The value we assign has to be iterable; # Prevent this new Python proxy from owning the C++ object; # Otherwise we get an 'already deleted' error in; # TList::Clear when the application ends; # No more indices in range, just append; # If range is longer than the number of elements in val,; # we need to remove the remaining elements of the range; # No more indices in range, we are done; # Number; # Parameters:; # - self: collection to delete item from; # - idx: index of the item; # Slice; # Need to remove starting from the end; # Number; # Python-list-like methods; # Parameters:; # - self: collection where to insert a new item; # - idx: index where to insert; # - val: value to insert; # Check index; # Parameters:; # - self: collection where to pop an item from; # - args: either empty or index to pop; # Returns:; # - If args is empty, it returns the last element of; # the collection, else it returns the element for; # which the index was specified.; # Check arguments; # Check index; # Parameters:; # - self: collection to be reversed; # Parameters:; # - self: collecti",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tseqcollection.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tseqcollection.py
Integrability,message,message,"d. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Item access; # Parameters:; # - idx: index whose type needs to be checked; # - msg: message to show in case of type issue; # Parameters:; # - self: collection; # - idx: index to be checked; # Returns:; # - An index >= 0, equivalent to the input idx, which is verified; # to be an integer and within the boundaries of the collection; # Parameters:; # - self: collection to get the item/s from; # - idx: index/slice of the item/s; # Returns:; # - self[idx]; # Slice; # Number; # Parameters:; # - self: collection to remove the item from; # - idx: index of the item, always positive; # Parameters:; # - self: collection where to set item/s; # - idx: index/slice of the item/s; # - val: value to assign; # Slice; # The value we assign has to be iterable; # Prevent this new Python proxy from owning the C++ object; # Otherwise we get an 'already deleted' error in; # TList::Clear when the application ends; # No more indices in range, just append; # If range is longer than the number of elements in val,; # we need to remove the remaining elements of the range; # No more indices in range, we are done; # Number; # Parameters:; # - self: collection to delete item from; # - idx: index of the item; # Slice; # Need to remove starting from the end; # Number; # Python-list-like methods; # Parameters:; # - self: collection where to insert a new item; # - idx: index where to insert; # - val: value to insert; # Check index; # Parameters:; # - self: collection where to pop an item from; # - args: either empty or index to pop; # Returns:; # - If args is empty, it returns the last element of; # the collection, else it returns the element for; # which the index was specified.; # Check arguments; # Check index; # Parameters:; # - self: collection to be reversed; # Parameters:; # - self: collecti",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tseqcollection.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tseqcollection.py
Security,access,access,"d. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Item access; # Parameters:; # - idx: index whose type needs to be checked; # - msg: message to show in case of type issue; # Parameters:; # - self: collection; # - idx: index to be checked; # Returns:; # - An index >= 0, equivalent to the input idx, which is verified; # to be an integer and within the boundaries of the collection; # Parameters:; # - self: collection to get the item/s from; # - idx: index/slice of the item/s; # Returns:; # - self[idx]; # Slice; # Number; # Parameters:; # - self: collection to remove the item from; # - idx: index of the item, always positive; # Parameters:; # - self: collection where to set item/s; # - idx: index/slice of the item/s; # - val: value to assign; # Slice; # The value we assign has to be iterable; # Prevent this new Python proxy from owning the C++ object; # Otherwise we get an 'already deleted' error in; # TList::Clear when the application ends; # No more indices in range, just append; # If range is longer than the number of elements in val,; # we need to remove the remaining elements of the range; # No more indices in range, we are done; # Number; # Parameters:; # - self: collection to delete item from; # - idx: index of the item; # Slice; # Need to remove starting from the end; # Number; # Python-list-like methods; # Parameters:; # - self: collection where to insert a new item; # - idx: index where to insert; # - val: value to insert; # Check index; # Parameters:; # - self: collection where to pop an item from; # - args: either empty or index to pop; # Returns:; # - If args is empty, it returns the last element of; # the collection, else it returns the element for; # which the index was specified.; # Check arguments; # Check index; # Parameters:; # - self: collection to be reversed; # Parameters:; # - self: collecti",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tseqcollection.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tseqcollection.py
Availability,alive,alive,"# Author: Enric Tejedor CERN 06/2018; ################################################################################; # Copyright (C) 1995-2018, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # TTree iterator; """"""Helper for the SetBranchAddress pythonization, extracting the relevant; address from a Python object if possible.; """"""; # If the branch is a leaf list, SetBranchAddress expects the; # address of the object that has the corresponding data members.; # Otherwise, SetBranchAddress is expecting a pointer to the address of; # the object, and the pointer needs to stay alive. Therefore, we create; # a container for the pointer and cache it in the original cppyy proxy.; # Finally, we have to return the address of the container; # Complete list from https://docs.python.org/3/library/array.html; """""" Figure out data_type in case addr is a numpy.ndarray or array.array.; """"""; # For NumPy arrays; # For the builtin array library; """"""; Pythonization for TTree::SetBranchAddress. Modify the behaviour of SetBranchAddress so that proxy references can be passed; as arguments from the Python side, more precisely in cases where the C++; implementation of the method expects the address of a pointer. For example:; ```; v = ROOT.std.vector('int')(); t.SetBranchAddress(""my_vector_branch"", v); ```; """"""; # Pythonization for cppyy proxies (of type CPPInstance); # Figure out data_type in case addr is a numpy.ndarray or array.array; # We call the template specialization if we know the data type; # Modify the behaviour if args is one of:; # ( const char*, void*, const char*, Int_t = 32000 ); # ( const char*, const char*, T**, Int_t = 32000, Int_t = 99 ); # ( const char*, T**, Int_t = 32000, Int_t = 99 ); # Fall back to the original implementation for the rest of overloads; """"""; A",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_ttree.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_ttree.py
Integrability,inject,inject,"tion of the method expects the address of a pointer. For example:; ```; v = ROOT.std.vector('int')(); t.SetBranchAddress(""my_vector_branch"", v); ```; """"""; # Pythonization for cppyy proxies (of type CPPInstance); # Figure out data_type in case addr is a numpy.ndarray or array.array; # We call the template specialization if we know the data type; # Modify the behaviour if args is one of:; # ( const char*, void*, const char*, Int_t = 32000 ); # ( const char*, const char*, T**, Int_t = 32000, Int_t = 99 ); # ( const char*, T**, Int_t = 32000, Int_t = 99 ); # Fall back to the original implementation for the rest of overloads; """"""; Allow branches to be accessed as attributes of a tree. Allow access to branches/leaves as if they were Python data attributes of; the tree (e.g. mytree.branch). To avoid using the CPyCppyy API, any necessary cast is done here on the; Python side. The GetBranchAttr() function encodes a necessary cast in the; second element of the output tuple, which is a string with the required; type name. Parameters:; self (TTree): The instance of the TTree object from which the attribute is being retrieved.; key (str): The name of the branch to retrieve from the TTree object.; """"""; # Parameters:; # klass: class to be pythonized; # name: string containing the name of the class; # Pythonizations that are common to TTree and its subclasses.; # To avoid duplicating the same logic in the pythonizors of; # the subclasses, inject the pythonizations for all the target; # classes here.; # Pythonic iterator; # tree.branch syntax; # SetBranchAddress; # Branch; # Parameters:; # klass: class to be pythonized; # TChain needs to be explicitly pythonized because it redefines; # SetBranchAddress in C++. As a consequence, TChain does not; # inherit TTree's pythonization for SetBranchAddress, which; # needs to be injected to TChain too. This is not the case for; # other classes like TNtuple, which will inherit all the; # pythonizations added here for TTree.; # SetBranchAddress",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_ttree.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_ttree.py
Modifiability,inherit,inherit,"tion of the method expects the address of a pointer. For example:; ```; v = ROOT.std.vector('int')(); t.SetBranchAddress(""my_vector_branch"", v); ```; """"""; # Pythonization for cppyy proxies (of type CPPInstance); # Figure out data_type in case addr is a numpy.ndarray or array.array; # We call the template specialization if we know the data type; # Modify the behaviour if args is one of:; # ( const char*, void*, const char*, Int_t = 32000 ); # ( const char*, const char*, T**, Int_t = 32000, Int_t = 99 ); # ( const char*, T**, Int_t = 32000, Int_t = 99 ); # Fall back to the original implementation for the rest of overloads; """"""; Allow branches to be accessed as attributes of a tree. Allow access to branches/leaves as if they were Python data attributes of; the tree (e.g. mytree.branch). To avoid using the CPyCppyy API, any necessary cast is done here on the; Python side. The GetBranchAttr() function encodes a necessary cast in the; second element of the output tuple, which is a string with the required; type name. Parameters:; self (TTree): The instance of the TTree object from which the attribute is being retrieved.; key (str): The name of the branch to retrieve from the TTree object.; """"""; # Parameters:; # klass: class to be pythonized; # name: string containing the name of the class; # Pythonizations that are common to TTree and its subclasses.; # To avoid duplicating the same logic in the pythonizors of; # the subclasses, inject the pythonizations for all the target; # classes here.; # Pythonic iterator; # tree.branch syntax; # SetBranchAddress; # Branch; # Parameters:; # klass: class to be pythonized; # TChain needs to be explicitly pythonized because it redefines; # SetBranchAddress in C++. As a consequence, TChain does not; # inherit TTree's pythonization for SetBranchAddress, which; # needs to be injected to TChain too. This is not the case for; # other classes like TNtuple, which will inherit all the; # pythonizations added here for TTree.; # SetBranchAddress",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_ttree.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_ttree.py
Performance,cache,cache,"# Author: Enric Tejedor CERN 06/2018; ################################################################################; # Copyright (C) 1995-2018, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # TTree iterator; """"""Helper for the SetBranchAddress pythonization, extracting the relevant; address from a Python object if possible.; """"""; # If the branch is a leaf list, SetBranchAddress expects the; # address of the object that has the corresponding data members.; # Otherwise, SetBranchAddress is expecting a pointer to the address of; # the object, and the pointer needs to stay alive. Therefore, we create; # a container for the pointer and cache it in the original cppyy proxy.; # Finally, we have to return the address of the container; # Complete list from https://docs.python.org/3/library/array.html; """""" Figure out data_type in case addr is a numpy.ndarray or array.array.; """"""; # For NumPy arrays; # For the builtin array library; """"""; Pythonization for TTree::SetBranchAddress. Modify the behaviour of SetBranchAddress so that proxy references can be passed; as arguments from the Python side, more precisely in cases where the C++; implementation of the method expects the address of a pointer. For example:; ```; v = ROOT.std.vector('int')(); t.SetBranchAddress(""my_vector_branch"", v); ```; """"""; # Pythonization for cppyy proxies (of type CPPInstance); # Figure out data_type in case addr is a numpy.ndarray or array.array; # We call the template specialization if we know the data type; # Modify the behaviour if args is one of:; # ( const char*, void*, const char*, Int_t = 32000 ); # ( const char*, const char*, T**, Int_t = 32000, Int_t = 99 ); # ( const char*, T**, Int_t = 32000, Int_t = 99 ); # Fall back to the original implementation for the rest of overloads; """"""; A",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_ttree.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_ttree.py
Safety,avoid,avoid,"dify the behaviour of SetBranchAddress so that proxy references can be passed; as arguments from the Python side, more precisely in cases where the C++; implementation of the method expects the address of a pointer. For example:; ```; v = ROOT.std.vector('int')(); t.SetBranchAddress(""my_vector_branch"", v); ```; """"""; # Pythonization for cppyy proxies (of type CPPInstance); # Figure out data_type in case addr is a numpy.ndarray or array.array; # We call the template specialization if we know the data type; # Modify the behaviour if args is one of:; # ( const char*, void*, const char*, Int_t = 32000 ); # ( const char*, const char*, T**, Int_t = 32000, Int_t = 99 ); # ( const char*, T**, Int_t = 32000, Int_t = 99 ); # Fall back to the original implementation for the rest of overloads; """"""; Allow branches to be accessed as attributes of a tree. Allow access to branches/leaves as if they were Python data attributes of; the tree (e.g. mytree.branch). To avoid using the CPyCppyy API, any necessary cast is done here on the; Python side. The GetBranchAttr() function encodes a necessary cast in the; second element of the output tuple, which is a string with the required; type name. Parameters:; self (TTree): The instance of the TTree object from which the attribute is being retrieved.; key (str): The name of the branch to retrieve from the TTree object.; """"""; # Parameters:; # klass: class to be pythonized; # name: string containing the name of the class; # Pythonizations that are common to TTree and its subclasses.; # To avoid duplicating the same logic in the pythonizors of; # the subclasses, inject the pythonizations for all the target; # classes here.; # Pythonic iterator; # tree.branch syntax; # SetBranchAddress; # Branch; # Parameters:; # klass: class to be pythonized; # TChain needs to be explicitly pythonized because it redefines; # SetBranchAddress in C++. As a consequence, TChain does not; # inherit TTree's pythonization for SetBranchAddress, which; # needs to be inje",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_ttree.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_ttree.py
Security,access,accessed,"ter and cache it in the original cppyy proxy.; # Finally, we have to return the address of the container; # Complete list from https://docs.python.org/3/library/array.html; """""" Figure out data_type in case addr is a numpy.ndarray or array.array.; """"""; # For NumPy arrays; # For the builtin array library; """"""; Pythonization for TTree::SetBranchAddress. Modify the behaviour of SetBranchAddress so that proxy references can be passed; as arguments from the Python side, more precisely in cases where the C++; implementation of the method expects the address of a pointer. For example:; ```; v = ROOT.std.vector('int')(); t.SetBranchAddress(""my_vector_branch"", v); ```; """"""; # Pythonization for cppyy proxies (of type CPPInstance); # Figure out data_type in case addr is a numpy.ndarray or array.array; # We call the template specialization if we know the data type; # Modify the behaviour if args is one of:; # ( const char*, void*, const char*, Int_t = 32000 ); # ( const char*, const char*, T**, Int_t = 32000, Int_t = 99 ); # ( const char*, T**, Int_t = 32000, Int_t = 99 ); # Fall back to the original implementation for the rest of overloads; """"""; Allow branches to be accessed as attributes of a tree. Allow access to branches/leaves as if they were Python data attributes of; the tree (e.g. mytree.branch). To avoid using the CPyCppyy API, any necessary cast is done here on the; Python side. The GetBranchAttr() function encodes a necessary cast in the; second element of the output tuple, which is a string with the required; type name. Parameters:; self (TTree): The instance of the TTree object from which the attribute is being retrieved.; key (str): The name of the branch to retrieve from the TTree object.; """"""; # Parameters:; # klass: class to be pythonized; # name: string containing the name of the class; # Pythonizations that are common to TTree and its subclasses.; # To avoid duplicating the same logic in the pythonizors of; # the subclasses, inject the pythonizations for all t",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_ttree.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_ttree.py
Testability,log,logic,"tion of the method expects the address of a pointer. For example:; ```; v = ROOT.std.vector('int')(); t.SetBranchAddress(""my_vector_branch"", v); ```; """"""; # Pythonization for cppyy proxies (of type CPPInstance); # Figure out data_type in case addr is a numpy.ndarray or array.array; # We call the template specialization if we know the data type; # Modify the behaviour if args is one of:; # ( const char*, void*, const char*, Int_t = 32000 ); # ( const char*, const char*, T**, Int_t = 32000, Int_t = 99 ); # ( const char*, T**, Int_t = 32000, Int_t = 99 ); # Fall back to the original implementation for the rest of overloads; """"""; Allow branches to be accessed as attributes of a tree. Allow access to branches/leaves as if they were Python data attributes of; the tree (e.g. mytree.branch). To avoid using the CPyCppyy API, any necessary cast is done here on the; Python side. The GetBranchAttr() function encodes a necessary cast in the; second element of the output tuple, which is a string with the required; type name. Parameters:; self (TTree): The instance of the TTree object from which the attribute is being retrieved.; key (str): The name of the branch to retrieve from the TTree object.; """"""; # Parameters:; # klass: class to be pythonized; # name: string containing the name of the class; # Pythonizations that are common to TTree and its subclasses.; # To avoid duplicating the same logic in the pythonizors of; # the subclasses, inject the pythonizations for all the target; # classes here.; # Pythonic iterator; # tree.branch syntax; # SetBranchAddress; # Branch; # Parameters:; # klass: class to be pythonized; # TChain needs to be explicitly pythonized because it redefines; # SetBranchAddress in C++. As a consequence, TChain does not; # inherit TTree's pythonization for SetBranchAddress, which; # needs to be injected to TChain too. This is not the case for; # other classes like TNtuple, which will inherit all the; # pythonizations added here for TTree.; # SetBranchAddress",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_ttree.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_ttree.py
Availability,error,error,"corator. Args:; target (string/iterable[string]): class name(s)/prefix(es). Returns:; list[string]: class name(s)/prefix(es) in `target`, with no repetitions.; '''; # Remove possible duplicates; '''; Checks that a given target of a pythonizor does not specify a namespace; (only the class name / prefix of a class name should be present). Args:; target (string): class name/prefix.; '''; '''; Checks the number of parameters of the `f` function. Args:; f (function): user pythonizor function. Returns:; int: number of positional parameters of `f`.; '''; '''; Invokes the given user pythonizor function with the right arguments. Args:; user_pythonizor (function): user pythonizor function.; npars (int): number of parameters of the user pythonizor function.; klass (class type): cppyy proxy of the class to be pythonized.; fqn (string): fully-qualified name of the class to be pythonized.; '''; # Propagate the error so that the class lookup that triggered this; # pythonization fails too and the application stops; '''; Finds already instantiated classes in namespace `ns` that pass the filter; of `passes_filter`. Every matching class is pythonized with the; `user_pythonizor` function.; This makes sure a pythonizor is also applied to classes that have already; been used at the time the pythonizor is registered. Args:; ns (string): namespace of the class names of prefixes in `targets`.; passes_filter (function): function that determines if a given class; is the target of `user_pythonizor`.; user_pythonizor (function): user pythonizor function.; npars (int): number of parameters of the user pythonizor function.; '''; # Namespace has not been used yet, no need to inspect more; # Check if name matches, excluding the namespace; # Pythonize right away!; # It's a class proxy; # If this is a template, pythonize the instances. Note that in; # older cppyy, template instantiations are cached by; # fully-qualified name directly in the namespace, so they are; # covered by the code branch above.; ",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/__init__.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/__init__.py
Integrability,inject,injection,"# Author: Enric Tejedor, Danilo Piparo CERN 06/2018; ################################################################################; # Copyright (C) 1995-2018, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; '''; Decorator that allows to pythonize C++ classes. To pythonize means to add; some extra behaviour to a C++ class that is used from Python via PyROOT,; so that such a class can be used in an easier / more ""pythonic"" way.; When a pythonization is registered with this decorator, the injection of; the new behaviour in the C++ class is done immediately, if the class has; already been used from the application, or lazily, i.e. only when the class; is first accessed from the application. Args:; class_name (string/iterable[string]): specifies either a single string or; multiple strings, where each string can be either (i) the name of a; C++ class to be pythonized, or (ii) a prefix to match all classes; whose name starts with that prefix.; ns (string): namespace of the classes to be pythonized. Default is the; global namespace (`::`).; is_prefix (boolean): if True, `class_name` contains one or multiple; prefixes, each prefix potentially matching multiple classes.; Default is False.; These are examples of prefixes and namespace and what they match:; - class_name="""", ns=""::"" : all classes in the global namespace.; - class_name=""C"", ns=""::"" : all classes in the global namespace; whose name starts with ""C""; - class_name="""", ns=""NS1::NS2"" : all classes in namespace ""NS1::NS2""; - class_name=""C"", ns=""NS1::NS2"" : all classes in namespace; ""NS1::NS2"" whose name starts with ""C"". Returns:; function: function that receives the user-defined function and; decorates it.; '''; # Type check and parsing of target argument.; # Retrieve the scope(s) of the class(es)/prefix(es) ",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/__init__.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/__init__.py
Performance,cache,cached,"nt: number of positional parameters of `f`.; '''; '''; Invokes the given user pythonizor function with the right arguments. Args:; user_pythonizor (function): user pythonizor function.; npars (int): number of parameters of the user pythonizor function.; klass (class type): cppyy proxy of the class to be pythonized.; fqn (string): fully-qualified name of the class to be pythonized.; '''; # Propagate the error so that the class lookup that triggered this; # pythonization fails too and the application stops; '''; Finds already instantiated classes in namespace `ns` that pass the filter; of `passes_filter`. Every matching class is pythonized with the; `user_pythonizor` function.; This makes sure a pythonizor is also applied to classes that have already; been used at the time the pythonizor is registered. Args:; ns (string): namespace of the class names of prefixes in `targets`.; passes_filter (function): function that determines if a given class; is the target of `user_pythonizor`.; user_pythonizor (function): user pythonizor function.; npars (int): number of parameters of the user pythonizor function.; '''; # Namespace has not been used yet, no need to inspect more; # Check if name matches, excluding the namespace; # Pythonize right away!; # It's a class proxy; # If this is a template, pythonize the instances. Note that in; # older cppyy, template instantiations are cached by; # fully-qualified name directly in the namespace, so they are; # covered by the code branch above.; # Make sure we don't do any redundant pythonization, e.g. if we; # use a version of cppyy that caches both in the namespace and; # in the _instantiations attribute.; '''; Finds and returns the proxy object of the `ns` namespace, if it has already; been accessed. Args:; ns (string): a namespace. Returns:; namespace proxy object, if the namespace has already been accessed,; otherwise None.; '''; # Get all namespaces in a list; '''; Registers the ROOT pythonizations with cppyy for lazy injection.; '''",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/__init__.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/__init__.py
Safety,redund,redundant,"nt: number of positional parameters of `f`.; '''; '''; Invokes the given user pythonizor function with the right arguments. Args:; user_pythonizor (function): user pythonizor function.; npars (int): number of parameters of the user pythonizor function.; klass (class type): cppyy proxy of the class to be pythonized.; fqn (string): fully-qualified name of the class to be pythonized.; '''; # Propagate the error so that the class lookup that triggered this; # pythonization fails too and the application stops; '''; Finds already instantiated classes in namespace `ns` that pass the filter; of `passes_filter`. Every matching class is pythonized with the; `user_pythonizor` function.; This makes sure a pythonizor is also applied to classes that have already; been used at the time the pythonizor is registered. Args:; ns (string): namespace of the class names of prefixes in `targets`.; passes_filter (function): function that determines if a given class; is the target of `user_pythonizor`.; user_pythonizor (function): user pythonizor function.; npars (int): number of parameters of the user pythonizor function.; '''; # Namespace has not been used yet, no need to inspect more; # Check if name matches, excluding the namespace; # Pythonize right away!; # It's a class proxy; # If this is a template, pythonize the instances. Note that in; # older cppyy, template instantiations are cached by; # fully-qualified name directly in the namespace, so they are; # covered by the code branch above.; # Make sure we don't do any redundant pythonization, e.g. if we; # use a version of cppyy that caches both in the namespace and; # in the _instantiations attribute.; '''; Finds and returns the proxy object of the `ns` namespace, if it has already; been accessed. Args:; ns (string): a namespace. Returns:; namespace proxy object, if the namespace has already been accessed,; otherwise None.; '''; # Get all namespaces in a list; '''; Registers the ROOT pythonizations with cppyy for lazy injection.; '''",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/__init__.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/__init__.py
Security,inject,injection,"# Author: Enric Tejedor, Danilo Piparo CERN 06/2018; ################################################################################; # Copyright (C) 1995-2018, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; '''; Decorator that allows to pythonize C++ classes. To pythonize means to add; some extra behaviour to a C++ class that is used from Python via PyROOT,; so that such a class can be used in an easier / more ""pythonic"" way.; When a pythonization is registered with this decorator, the injection of; the new behaviour in the C++ class is done immediately, if the class has; already been used from the application, or lazily, i.e. only when the class; is first accessed from the application. Args:; class_name (string/iterable[string]): specifies either a single string or; multiple strings, where each string can be either (i) the name of a; C++ class to be pythonized, or (ii) a prefix to match all classes; whose name starts with that prefix.; ns (string): namespace of the classes to be pythonized. Default is the; global namespace (`::`).; is_prefix (boolean): if True, `class_name` contains one or multiple; prefixes, each prefix potentially matching multiple classes.; Default is False.; These are examples of prefixes and namespace and what they match:; - class_name="""", ns=""::"" : all classes in the global namespace.; - class_name=""C"", ns=""::"" : all classes in the global namespace; whose name starts with ""C""; - class_name="""", ns=""NS1::NS2"" : all classes in namespace ""NS1::NS2""; - class_name=""C"", ns=""NS1::NS2"" : all classes in namespace; ""NS1::NS2"" whose name starts with ""C"". Returns:; function: function that receives the user-defined function and; decorates it.; '''; # Type check and parsing of target argument.; # Retrieve the scope(s) of the class(es)/prefix(es) ",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/__init__.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/__init__.py
Energy Efficiency,reduce,reduce,"# Authors:; # * Hinnerk C. Schmidt 02/2021; # * Jonas Rembser 03/2021; # * Harshal Shende 06/2021; ################################################################################; # Copyright (C) 1995-2020, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Redefinition of `RooAbsData.plotOn` for keyword arguments.; # Redefinition of `RooAbsData.createHistogram` for keyword arguments.; # Redefinition of `RooAbsData.reduce` for keyword arguments.; # Redefinition of `RooAbsData.statOn` for keyword arguments.",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_rooabsdata.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_rooabsdata.py
Availability,alive,alive,"# Authors:; # * Hinnerk C. Schmidt 02/2021; # * Jonas Rembser 03/2021; # * Harshal Shende 06/2021; ################################################################################; # Copyright (C) 1995-2020, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Redefinition of `RooAbsReal.plotOn` for keyword arguments.; # Redefinition of `RooAbsReal.createHistogram` for keyword arguments.; # Redefinition of `RooAbsReal.createIntegral` for keyword arguments.; # Redefinition of `RooAbsReal.createRunningIntegral` for keyword arguments.; # Redefinition of `RooAbsReal.createChi2` for keyword arguments.; # Redefinition of `RooAbsReal.chi2FitTo` for keyword arguments.; # We do the conversion to RooArgSet now, such that we can keep alive; # the normalization set by setting it as an attribute of this; # RooAbsReal.; # With the pythonizations, we have the opportunity to use the Python; # reference counting to make sure the last normalization set doesn't; # get deleted under our feet (RooFit tries to use it by pointer when; # you call getVal() without any normalization set the next time).; # Hardcode enum integer values here, because enum lookups cause; # some memory fiasco at the end.; # TODO: fix this in cppyy / PyROOT!",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_rooabsreal.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_rooabsreal.py
Modifiability,variab,variable,"# Authors:; # * Jonas Rembser 06/2021; # * Harshal Shende 06/2021; ################################################################################; # Copyright (C) 1995-2020, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Redefinition of `RooAbsRealLValue.createHistogram` for keyword arguments.; # Redefinition of `RooAbsRealLValue.frame` for keyword arguments.; # Add a Python reference to the plot variable to the RooPlot because; # the plot variable needs to survive at least as long as the plot.",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_rooabsreallvalue.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_rooabsreallvalue.py
Usability,simpl,simple,"# Authors:; # * Jonas Rembser 05/2021; ################################################################################; # Copyright (C) 1995-2020, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; """"""Pythonization of RooArgSet constructor to support implicit; conversion from Python sets.; """"""; # Note: This simple Pythonization caused me days of headache.; # Initially, I was also checking of `len(kwargs) == 0`, but it just; # didn't work. Eventually, I understood that when cppy attempts; # implicit conversion, a magic `__cppyy_no_implicit=True` keyword; # argument is added, hence the `len(kwargs) == 0` check breaks the; # implicit conversion!; # other than the RooArgList, the RooArgSet also supports string keys; # support for negative indexing",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_rooargset.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_rooargset.py
Availability,error,errors,"# Authors:; # * Jonas Rembser 06/2021; # * Harshal Shende 06/2021; ################################################################################; # Copyright (C) 1995-2020, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Redefinition of `RooDataHist` constructor for keyword arguments and converting python dict to std::map.; # Redefinition of `RooDataHist.plotOn` for keyword arguments.; # The keywords must correspond to the CmdArg of the `plotOn` function.; # Cross check that the product of the shape values is equal to the full; # length.; # Helper to create a numpy array from a raw array pointer.; #; # Note: The data is copied.; #; # Args:; # buffer (cppyy.LowLevelView):; # The pointer to the beginning of the array data, usually; # obtained from a C++ function that returns a `double *`.; #; # Returns:; # numpy.ndarray; # Check if buffer is a nullptr by using implicit conversion from; # `nullptr` to Bool (for some reason comparing with ROOT.nullptr; # doesn't work).; # Returns a list with a Bool for each dimension of the histogram that; # flags whether the variable in this dimension is a RooAbsCategory.; # Returns the low weight errors as numpy arrays.; # Returns the high weight errors as numpy arrays.; # Returns the low and high weight errors as numpy arrays.; # Returns the sum of squared weights that were used to fill each bin.; # name for internal binning that is created for the RooDataHist to adapt; # some reverse-computation that can't be avoided with the current C++ RooDataHist interface",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_roodatahist.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_roodatahist.py
Energy Efficiency,adapt,adapt,"# Authors:; # * Jonas Rembser 06/2021; # * Harshal Shende 06/2021; ################################################################################; # Copyright (C) 1995-2020, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Redefinition of `RooDataHist` constructor for keyword arguments and converting python dict to std::map.; # Redefinition of `RooDataHist.plotOn` for keyword arguments.; # The keywords must correspond to the CmdArg of the `plotOn` function.; # Cross check that the product of the shape values is equal to the full; # length.; # Helper to create a numpy array from a raw array pointer.; #; # Note: The data is copied.; #; # Args:; # buffer (cppyy.LowLevelView):; # The pointer to the beginning of the array data, usually; # obtained from a C++ function that returns a `double *`.; #; # Returns:; # numpy.ndarray; # Check if buffer is a nullptr by using implicit conversion from; # `nullptr` to Bool (for some reason comparing with ROOT.nullptr; # doesn't work).; # Returns a list with a Bool for each dimension of the histogram that; # flags whether the variable in this dimension is a RooAbsCategory.; # Returns the low weight errors as numpy arrays.; # Returns the high weight errors as numpy arrays.; # Returns the low and high weight errors as numpy arrays.; # Returns the sum of squared weights that were used to fill each bin.; # name for internal binning that is created for the RooDataHist to adapt; # some reverse-computation that can't be avoided with the current C++ RooDataHist interface",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_roodatahist.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_roodatahist.py
Integrability,interface,interface,"# Authors:; # * Jonas Rembser 06/2021; # * Harshal Shende 06/2021; ################################################################################; # Copyright (C) 1995-2020, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Redefinition of `RooDataHist` constructor for keyword arguments and converting python dict to std::map.; # Redefinition of `RooDataHist.plotOn` for keyword arguments.; # The keywords must correspond to the CmdArg of the `plotOn` function.; # Cross check that the product of the shape values is equal to the full; # length.; # Helper to create a numpy array from a raw array pointer.; #; # Note: The data is copied.; #; # Args:; # buffer (cppyy.LowLevelView):; # The pointer to the beginning of the array data, usually; # obtained from a C++ function that returns a `double *`.; #; # Returns:; # numpy.ndarray; # Check if buffer is a nullptr by using implicit conversion from; # `nullptr` to Bool (for some reason comparing with ROOT.nullptr; # doesn't work).; # Returns a list with a Bool for each dimension of the histogram that; # flags whether the variable in this dimension is a RooAbsCategory.; # Returns the low weight errors as numpy arrays.; # Returns the high weight errors as numpy arrays.; # Returns the low and high weight errors as numpy arrays.; # Returns the sum of squared weights that were used to fill each bin.; # name for internal binning that is created for the RooDataHist to adapt; # some reverse-computation that can't be avoided with the current C++ RooDataHist interface",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_roodatahist.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_roodatahist.py
Modifiability,variab,variable,"# Authors:; # * Jonas Rembser 06/2021; # * Harshal Shende 06/2021; ################################################################################; # Copyright (C) 1995-2020, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Redefinition of `RooDataHist` constructor for keyword arguments and converting python dict to std::map.; # Redefinition of `RooDataHist.plotOn` for keyword arguments.; # The keywords must correspond to the CmdArg of the `plotOn` function.; # Cross check that the product of the shape values is equal to the full; # length.; # Helper to create a numpy array from a raw array pointer.; #; # Note: The data is copied.; #; # Args:; # buffer (cppyy.LowLevelView):; # The pointer to the beginning of the array data, usually; # obtained from a C++ function that returns a `double *`.; #; # Returns:; # numpy.ndarray; # Check if buffer is a nullptr by using implicit conversion from; # `nullptr` to Bool (for some reason comparing with ROOT.nullptr; # doesn't work).; # Returns a list with a Bool for each dimension of the histogram that; # flags whether the variable in this dimension is a RooAbsCategory.; # Returns the low weight errors as numpy arrays.; # Returns the high weight errors as numpy arrays.; # Returns the low and high weight errors as numpy arrays.; # Returns the sum of squared weights that were used to fill each bin.; # name for internal binning that is created for the RooDataHist to adapt; # some reverse-computation that can't be avoided with the current C++ RooDataHist interface",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_roodatahist.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_roodatahist.py
Safety,avoid,avoided,"# Authors:; # * Jonas Rembser 06/2021; # * Harshal Shende 06/2021; ################################################################################; # Copyright (C) 1995-2020, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Redefinition of `RooDataHist` constructor for keyword arguments and converting python dict to std::map.; # Redefinition of `RooDataHist.plotOn` for keyword arguments.; # The keywords must correspond to the CmdArg of the `plotOn` function.; # Cross check that the product of the shape values is equal to the full; # length.; # Helper to create a numpy array from a raw array pointer.; #; # Note: The data is copied.; #; # Args:; # buffer (cppyy.LowLevelView):; # The pointer to the beginning of the array data, usually; # obtained from a C++ function that returns a `double *`.; #; # Returns:; # numpy.ndarray; # Check if buffer is a nullptr by using implicit conversion from; # `nullptr` to Bool (for some reason comparing with ROOT.nullptr; # doesn't work).; # Returns a list with a Bool for each dimension of the histogram that; # flags whether the variable in this dimension is a RooAbsCategory.; # Returns the low weight errors as numpy arrays.; # Returns the high weight errors as numpy arrays.; # Returns the low and high weight errors as numpy arrays.; # Returns the sum of squared weights that were used to fill each bin.; # name for internal binning that is created for the RooDataHist to adapt; # some reverse-computation that can't be avoided with the current C++ RooDataHist interface",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_roodatahist.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_roodatahist.py
Availability,mask,mask,"r keyword arguments.; # Redefinition of `RooDataSet.plotOnXY` for keyword arguments.; """"""Create a RooDataSet from a dictionary of numpy arrays.; Args:; data (dict): Dictionary with strings as keys and numpy arrays as; values, to be imported into the RooDataSet.; variables (RooArgSet, or list/tuple of RooAbsArgs):; Specification of the variables in the RooDataSet, will be; forwarded to the RooDataSet constructor. Both real values and; categories are supported.; name (str): Name of the RooDataSet, `None` is equivalent to an; empty string.; title (str): Title of the RooDataSet, `None` is equivalent to an; empty string.; weight_name (str): Key of the array in `data` that will be used for; the dataset weights. Returns:; RooDataSet; """"""; """"""Log a string to the RooFit message log for the WARNING level on; the DataHandling topic.; """"""; # For categories, we need to check whether the elements of the; # array are in the set of category state indices; # Get a mask that filters out all entries that are outside the variable definition range; # If all entries are in the range, we don't need a mask; # Make sure that the array is contiguous so we can std::copy() it.; # In the implementation of ascontiguousarray(), no copy is done if; # the array is already contiguous, which is exactly what we want.; # The next part works because arr is guaranteed to be C-contiguous; """"""Export a RooDataSet to a dictionary of numpy arrays. Args:; copy (bool): If False, the data will not be copied. Use with; caution, as the numpy arrays and the RooAbsData now; own the same memory. If the dataset uses a; RooTreeDataStore, there will always be a copy and the; copy argument is ignored. Returns:; dict: A dictionary with the variable or weight names as keys and; the numpy arrays as values.; """"""; # first create a VectorDataStore so we can read arrays; """"""Create a RooDataSet from a pandas DataFrame.; Args:; df (pandas.DataFrame): Pandas DataFrame to import.; variables (RooArgSet, or list/tuple of RooAbsArgs):",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_roodataset.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_roodataset.py
Integrability,message,message,"nsing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Redefinition of `RooDataSet` constructor for keyword arguments.; # Redefinition of `RooDataSet.plotOnXY` for keyword arguments.; """"""Create a RooDataSet from a dictionary of numpy arrays.; Args:; data (dict): Dictionary with strings as keys and numpy arrays as; values, to be imported into the RooDataSet.; variables (RooArgSet, or list/tuple of RooAbsArgs):; Specification of the variables in the RooDataSet, will be; forwarded to the RooDataSet constructor. Both real values and; categories are supported.; name (str): Name of the RooDataSet, `None` is equivalent to an; empty string.; title (str): Title of the RooDataSet, `None` is equivalent to an; empty string.; weight_name (str): Key of the array in `data` that will be used for; the dataset weights. Returns:; RooDataSet; """"""; """"""Log a string to the RooFit message log for the WARNING level on; the DataHandling topic.; """"""; # For categories, we need to check whether the elements of the; # array are in the set of category state indices; # Get a mask that filters out all entries that are outside the variable definition range; # If all entries are in the range, we don't need a mask; # Make sure that the array is contiguous so we can std::copy() it.; # In the implementation of ascontiguousarray(), no copy is done if; # the array is already contiguous, which is exactly what we want.; # The next part works because arr is guaranteed to be C-contiguous; """"""Export a RooDataSet to a dictionary of numpy arrays. Args:; copy (bool): If False, the data will not be copied. Use with; caution, as the numpy arrays and the RooAbsData now; own the same memory. If the dataset uses a; RooTreeDataStore, there will always be a copy and the; copy argument is ignored. Returns:; dict: A dictionary with the variable or weight names as keys and; the numpy arrays as val",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_roodataset.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_roodataset.py
Modifiability,variab,variables,"# Authors:; # * Jonas Rembser 06/2021; # * Harshal Shende 06/2021; ################################################################################; # Copyright (C) 1995-2020, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Redefinition of `RooDataSet` constructor for keyword arguments.; # Redefinition of `RooDataSet.plotOnXY` for keyword arguments.; """"""Create a RooDataSet from a dictionary of numpy arrays.; Args:; data (dict): Dictionary with strings as keys and numpy arrays as; values, to be imported into the RooDataSet.; variables (RooArgSet, or list/tuple of RooAbsArgs):; Specification of the variables in the RooDataSet, will be; forwarded to the RooDataSet constructor. Both real values and; categories are supported.; name (str): Name of the RooDataSet, `None` is equivalent to an; empty string.; title (str): Title of the RooDataSet, `None` is equivalent to an; empty string.; weight_name (str): Key of the array in `data` that will be used for; the dataset weights. Returns:; RooDataSet; """"""; """"""Log a string to the RooFit message log for the WARNING level on; the DataHandling topic.; """"""; # For categories, we need to check whether the elements of the; # array are in the set of category state indices; # Get a mask that filters out all entries that are outside the variable definition range; # If all entries are in the range, we don't need a mask; # Make sure that the array is contiguous so we can std::copy() it.; # In the implementation of ascontiguousarray(), no copy is done if; # the array is already contiguous, which is exactly what we want.; # The next part works because arr is guaranteed to be C-contiguous; """"""Export a RooDataSet to a dictionary of numpy arrays. Args:; copy (bool): If False, the data will not be copied. Use with; caution, as the numpy ",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_roodataset.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_roodataset.py
Testability,log,log,"nsing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Redefinition of `RooDataSet` constructor for keyword arguments.; # Redefinition of `RooDataSet.plotOnXY` for keyword arguments.; """"""Create a RooDataSet from a dictionary of numpy arrays.; Args:; data (dict): Dictionary with strings as keys and numpy arrays as; values, to be imported into the RooDataSet.; variables (RooArgSet, or list/tuple of RooAbsArgs):; Specification of the variables in the RooDataSet, will be; forwarded to the RooDataSet constructor. Both real values and; categories are supported.; name (str): Name of the RooDataSet, `None` is equivalent to an; empty string.; title (str): Title of the RooDataSet, `None` is equivalent to an; empty string.; weight_name (str): Key of the array in `data` that will be used for; the dataset weights. Returns:; RooDataSet; """"""; """"""Log a string to the RooFit message log for the WARNING level on; the DataHandling topic.; """"""; # For categories, we need to check whether the elements of the; # array are in the set of category state indices; # Get a mask that filters out all entries that are outside the variable definition range; # If all entries are in the range, we don't need a mask; # Make sure that the array is contiguous so we can std::copy() it.; # In the implementation of ascontiguousarray(), no copy is done if; # the array is already contiguous, which is exactly what we want.; # The next part works because arr is guaranteed to be C-contiguous; """"""Export a RooDataSet to a dictionary of numpy arrays. Args:; copy (bool): If False, the data will not be copied. Use with; caution, as the numpy arrays and the RooAbsData now; own the same memory. If the dataset uses a; RooTreeDataStore, there will always be a copy and the; copy argument is ignored. Returns:; dict: A dictionary with the variable or weight names as keys and; the numpy arrays as val",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_roodataset.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_roodataset.py
Integrability,wrap,wrapping,"# Authors:; # * Jonas Rembser 06/2021; # * Harshal Shende 06/2021; ################################################################################; # Copyright (C) 1995-2020, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Redefinition of `FitOptions` for keyword arguments.; # Redefinition of `Format` for keyword arguments.; # Redefinition of `Frame` for keyword arguments.; # Redefinition of `MultiArg` for keyword arguments.; # Redefinition of `YVar` for keyword arguments.; # Redefinition of `ZVar` for keyword arguments.; # Redefinition of `Slice` for keyword arguments and converting python dict to std::map.; # Redefinition of `Import` for keyword arguments and converting python dict to std::map.; # Redefinition of `Link` for keyword arguments and converting python dict to std::map.; # Redefinition of `DataError` to also accept `str` or `NoneType` to get the; # corresponding enum values from RooAbsData.DataError.; # One of the possible enum values is ""None"", and we want the user to be; # able to pass None also as a NoneType for convenience.; """"""; Wrap an arbitrary function defined in Python or C++. If you're wrapping a Python function, it must take numpy arrays of type; float64 as input and output types. Parameters:; - name (str): Name of the function.; - func (callable): Function that defines the function.; - variables (list): List of variables to be used in the function. Returns:; - RooAbsReal wrapping the given function; """"""; # use the C++ version if dealing with C++ function",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_rooglobalfunc.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_rooglobalfunc.py
Modifiability,variab,variables,"# Authors:; # * Jonas Rembser 06/2021; # * Harshal Shende 06/2021; ################################################################################; # Copyright (C) 1995-2020, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Redefinition of `FitOptions` for keyword arguments.; # Redefinition of `Format` for keyword arguments.; # Redefinition of `Frame` for keyword arguments.; # Redefinition of `MultiArg` for keyword arguments.; # Redefinition of `YVar` for keyword arguments.; # Redefinition of `ZVar` for keyword arguments.; # Redefinition of `Slice` for keyword arguments and converting python dict to std::map.; # Redefinition of `Import` for keyword arguments and converting python dict to std::map.; # Redefinition of `Link` for keyword arguments and converting python dict to std::map.; # Redefinition of `DataError` to also accept `str` or `NoneType` to get the; # corresponding enum values from RooAbsData.DataError.; # One of the possible enum values is ""None"", and we want the user to be; # able to pass None also as a NoneType for convenience.; """"""; Wrap an arbitrary function defined in Python or C++. If you're wrapping a Python function, it must take numpy arrays of type; float64 as input and output types. Parameters:; - name (str): Name of the function.; - func (callable): Function that defines the function.; - variables (list): List of variables to be used in the function. Returns:; - RooAbsReal wrapping the given function; """"""; # use the C++ version if dealing with C++ function",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_rooglobalfunc.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_rooglobalfunc.py
Availability,error,error,"# Author: Stephan Hageboeck, CERN 04/2020; ################################################################################; # Copyright (C) 1995-2020, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Redefinition of `RooWorkspace` constructor for keyword arguments.; # To enable accessing objects in the RooWorkspace with dictionary-like syntax.; # The key is passed to the general `RooWorkspace::obj()` function.; # Check if initialization is done with string; # Initializes variables; # Initializes functions and p.d.f.s; # Else raises a Syntax error; # Many people pythonized the RooWorkspace themselves, by adding a new; # attribute `_import` that calls getattr(self, ""import"") under the; # hood. However, `_import` is now the reference to the original cppyy; # overload, and resetting it with a wrapper around `import` would cause; # infinite recursions! We prevent resetting any import-related function; # here, which results in a clearer error to the user than an infinite; # call stack involving the internal pythonization code.; # Redefinition of `RooWorkspace.import()` for keyword arguments.",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_rooworkspace.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_rooworkspace.py
Integrability,wrap,wrapper,"# Author: Stephan Hageboeck, CERN 04/2020; ################################################################################; # Copyright (C) 1995-2020, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Redefinition of `RooWorkspace` constructor for keyword arguments.; # To enable accessing objects in the RooWorkspace with dictionary-like syntax.; # The key is passed to the general `RooWorkspace::obj()` function.; # Check if initialization is done with string; # Initializes variables; # Initializes functions and p.d.f.s; # Else raises a Syntax error; # Many people pythonized the RooWorkspace themselves, by adding a new; # attribute `_import` that calls getattr(self, ""import"") under the; # hood. However, `_import` is now the reference to the original cppyy; # overload, and resetting it with a wrapper around `import` would cause; # infinite recursions! We prevent resetting any import-related function; # here, which results in a clearer error to the user than an infinite; # call stack involving the internal pythonization code.; # Redefinition of `RooWorkspace.import()` for keyword arguments.",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_rooworkspace.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_rooworkspace.py
Modifiability,variab,variables,"# Author: Stephan Hageboeck, CERN 04/2020; ################################################################################; # Copyright (C) 1995-2020, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Redefinition of `RooWorkspace` constructor for keyword arguments.; # To enable accessing objects in the RooWorkspace with dictionary-like syntax.; # The key is passed to the general `RooWorkspace::obj()` function.; # Check if initialization is done with string; # Initializes variables; # Initializes functions and p.d.f.s; # Else raises a Syntax error; # Many people pythonized the RooWorkspace themselves, by adding a new; # attribute `_import` that calls getattr(self, ""import"") under the; # hood. However, `_import` is now the reference to the original cppyy; # overload, and resetting it with a wrapper around `import` would cause; # infinite recursions! We prevent resetting any import-related function; # here, which results in a clearer error to the user than an infinite; # call stack involving the internal pythonization code.; # Redefinition of `RooWorkspace.import()` for keyword arguments.",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_rooworkspace.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_rooworkspace.py
Security,access,accessing,"# Author: Stephan Hageboeck, CERN 04/2020; ################################################################################; # Copyright (C) 1995-2020, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Redefinition of `RooWorkspace` constructor for keyword arguments.; # To enable accessing objects in the RooWorkspace with dictionary-like syntax.; # The key is passed to the general `RooWorkspace::obj()` function.; # Check if initialization is done with string; # Initializes variables; # Initializes functions and p.d.f.s; # Else raises a Syntax error; # Many people pythonized the RooWorkspace themselves, by adding a new; # attribute `_import` that calls getattr(self, ""import"") under the; # hood. However, `_import` is now the reference to the original cppyy; # overload, and resetting it with a wrapper around `import` would cause; # infinite recursions! We prevent resetting any import-related function; # here, which results in a clearer error to the user than an infinite; # call stack involving the internal pythonization code.; # Redefinition of `RooWorkspace.import()` for keyword arguments.",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_rooworkspace.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_rooworkspace.py
Usability,clear,clearer,"# Author: Stephan Hageboeck, CERN 04/2020; ################################################################################; # Copyright (C) 1995-2020, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Redefinition of `RooWorkspace` constructor for keyword arguments.; # To enable accessing objects in the RooWorkspace with dictionary-like syntax.; # The key is passed to the general `RooWorkspace::obj()` function.; # Check if initialization is done with string; # Initializes variables; # Initializes functions and p.d.f.s; # Else raises a Syntax error; # Many people pythonized the RooWorkspace themselves, by adding a new; # attribute `_import` that calls getattr(self, ""import"") under the; # hood. However, `_import` is now the reference to the original cppyy; # overload, and resetting it with a wrapper around `import` would cause; # infinite recursions! We prevent resetting any import-related function; # here, which results in a clearer error to the user than an infinite; # call stack involving the internal pythonization code.; # Redefinition of `RooWorkspace.import()` for keyword arguments.",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_rooworkspace.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/_rooworkspace.py
Security,access,access,"# Authors:; # * Harshal Shende 03/2021; # * Hinnerk C. Schmidt 02/2021; # * Jonas Rembser 03/2021; ################################################################################; # Copyright (C) 1995-2020, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # list of python classes that are used to pythonize RooFit classes; # list of python functions that are used to pythonize RooGlobalFunc function in RooFit; # create a dictionary for convenient access to python classes; """"""; Get all class attributes that are defined in a given class or optionally in; any of its base classes (except for `object`).; """"""; # get a list of this class and all its base classes, excluding `object`; """"""; Bind the instance method `from_class.func_name` also to class `to_class`.; """"""; # the @classmethod case; # any other case in Python 3 is trivial; """"""Return the name that we will give to the original cppyy function.""""""; # special treatment of magic functions, e.g.: __getitem__ > _getitem; # Parameters:; # klass: class to pythonize; # name: string containing the name of the class; # list of functions to pythonize, which are assumed to be all functions in; # that are manually defined in the Python classes or their superclasses; # if the RooFit class already has a function with the same name as our; # pythonization, we rename it and prefix it with an underscore; # new name for original function",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/__init__.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_roofit/__init__.py
Availability,avail,available,"efine the shape and size of the; output. Returns:; np.ndarray: data sample; """"""; # Split the target and weight; """"""Convert a RTensor into a NumPy array. Args:; batch (RTensor): Batch returned from the RBatchGenerator. Returns:; np.ndarray: converted batch; """"""; # Splice target column from the data if weight is given; # Splice weights column from the data if weight is given; """"""Convert a RTensor into a PyTorch tensor. Args:; batch (RTensor): Batch returned from the RBatchGenerator. Returns:; torch.Tensor: converted batch; """"""; # Splice target column from the data if weight is given; # Splice weights column from the data if weight is given; """"""; PLACEHOLDER: at this moment this function only calls the; ConvertBatchToNumpy function. In the Future this function can be; used to convert to TF tensors directly. Args:; batch (RTensor): Batch returned from the RBatchGenerator. Returns:; np.ndarray: converted batch; """"""; # import tensorflow as tf; # TODO: improve this by returning tensorflow tensors; # Return a batch when available; """"""Return the next training batch of data from the given RDataFrame. Returns:; (np.ndarray): Batch of data of size.; """"""; """"""Return the next training batch of data from the given RDataFrame. Returns:; (np.ndarray): Batch of data of size.; """"""; # Context that activates and deactivates the loading thread of the Cpp class; # This ensures that the thread will always be deleted properly; """"""; A generator that returns the training batches of the given; base generator. Args:; base_generator (BaseGenerator):; The base connection to the Cpp code; conversion_function (Callable[RTensor, np.NDArray|torch.Tensor]):; Function that converts a given RTensor into a python batch; """"""; """"""Start the loading of training batches""""""; """"""Stop the loading of batches""""""; """"""Start the loading of batches and Yield the results. Yields:; Union[np.NDArray, torch.Tensor]: A batch of data; """"""; """"""; A generator that returns the validation batches of the given base; generator. NOTE",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tmva/_batchgenerator.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tmva/_batchgenerator.py
Performance,load,loaded,"""""""; Generate a template for the RBatchGenerator based on the given; RDataFrame and columns. Args:; file_name (str): name of the root file.; tree_name (str): name of the tree in the root file.; columns (list[str]): Columns that should be loaded.; Defaults to loading all columns; in the given RDataFrame; max_vec_sizes (list[int]): The length of each vector based column. Returns:; template (str): Template for the RBatchGenerator; """"""; # from cppyy.gbl.ROOT import RDataFrame; # Get the types of the different columns; # Add column for each element if column is a vector; """"""Wrapper around the Cpp RBatchGenerator. Args:; tree_name (str): Name of the tree in the ROOT file; file_name (str): Path to the ROOT file; batch_size (int): Size of the returned chunks.; chunk_size (int):; The size of the chunks loaded from the ROOT file. Higher chunk; size results in better randomization, but higher memory usage.; columns (list[str], optional):; Columns to be returned. If not given, all columns are used.; filters (list[str], optional):; Filters to apply during loading. If not given, no filters; are applied.; max_vec_sizes (dict[std, int], optional):; Size of each column that consists of vectors.; Required when using vector based columns.; vec_padding (int):; Value to pad vectors with if the vector is smaller; than the given max vector length. Defaults is 0; target (str, optional): Column that is used as target.; weights (str, optional):; Column used to weight events.; Can only be used when a target is given.; validation_split (float, optional):; The ratio of batches being kept for validation.; Value has to be between 0 and 1. Defaults to 0.0.; max_chunks (int, optional):; The number of chunks that should be loaded for an epoch.; If not given, the whole file is used.; shuffle (bool):; Batches consist of random events and are shuffled every epoch.; Defaults to True.; """"""; # TODO: better linking when importing into ROOT; # ROOT.gInterpreter.ProcessLine(; # f'#include ""{main_folder}Cpp_fi",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tmva/_batchgenerator.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tmva/_batchgenerator.py
Safety,safe,safety," vector based columns.; vec_padding (int):; Value to pad vectors with if the vector is smaller; than the given max vector length. Defaults is 0; target (str, optional): Column that is used as target.; weights (str, optional):; Column used to weight events.; Can only be used when a target is given.; validation_split (float, optional):; The ratio of batches being kept for validation.; Value has to be between 0 and 1. Defaults to 0.0.; max_chunks (int, optional):; The number of chunks that should be loaded for an epoch.; If not given, the whole file is used.; shuffle (bool):; Batches consist of random events and are shuffled every epoch.; Defaults to True.; """"""; # TODO: better linking when importing into ROOT; # ROOT.gInterpreter.ProcessLine(; # f'#include ""{main_folder}Cpp_files/RBatchGenerator.cpp""'); # Handle target; # Handle weights; # The RBatchGenerator will create a separate C++ thread for I/O.; # Enable thread safety in ROOT from here, to make sure there is no; # interference between the main Python thread (which might call into; # cling via cppyy) and the I/O thread.; """"""Initialize the generator to be used for a loop""""""; """"""Initialize the generator to be used for a loop""""""; """"""; Return a sample of data that has the same size and types as the actual; result. This sample can be used to define the shape and size of the; output. Returns:; np.ndarray: data sample; """"""; # Split the target and weight; """"""Convert a RTensor into a NumPy array. Args:; batch (RTensor): Batch returned from the RBatchGenerator. Returns:; np.ndarray: converted batch; """"""; # Splice target column from the data if weight is given; # Splice weights column from the data if weight is given; """"""Convert a RTensor into a PyTorch tensor. Args:; batch (RTensor): Batch returned from the RBatchGenerator. Returns:; torch.Tensor: converted batch; """"""; # Splice target column from the data if weight is given; # Splice weights column from the data if weight is given; """"""; PLACEHOLDER: at this moment this fun",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tmva/_batchgenerator.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tmva/_batchgenerator.py
Security,validat,validation,"n is a vector; """"""Wrapper around the Cpp RBatchGenerator. Args:; tree_name (str): Name of the tree in the ROOT file; file_name (str): Path to the ROOT file; batch_size (int): Size of the returned chunks.; chunk_size (int):; The size of the chunks loaded from the ROOT file. Higher chunk; size results in better randomization, but higher memory usage.; columns (list[str], optional):; Columns to be returned. If not given, all columns are used.; filters (list[str], optional):; Filters to apply during loading. If not given, no filters; are applied.; max_vec_sizes (dict[std, int], optional):; Size of each column that consists of vectors.; Required when using vector based columns.; vec_padding (int):; Value to pad vectors with if the vector is smaller; than the given max vector length. Defaults is 0; target (str, optional): Column that is used as target.; weights (str, optional):; Column used to weight events.; Can only be used when a target is given.; validation_split (float, optional):; The ratio of batches being kept for validation.; Value has to be between 0 and 1. Defaults to 0.0.; max_chunks (int, optional):; The number of chunks that should be loaded for an epoch.; If not given, the whole file is used.; shuffle (bool):; Batches consist of random events and are shuffled every epoch.; Defaults to True.; """"""; # TODO: better linking when importing into ROOT; # ROOT.gInterpreter.ProcessLine(; # f'#include ""{main_folder}Cpp_files/RBatchGenerator.cpp""'); # Handle target; # Handle weights; # The RBatchGenerator will create a separate C++ thread for I/O.; # Enable thread safety in ROOT from here, to make sure there is no; # interference between the main Python thread (which might call into; # cling via cppyy) and the I/O thread.; """"""Initialize the generator to be used for a loop""""""; """"""Initialize the generator to be used for a loop""""""; """"""; Return a sample of data that has the same size and types as the actual; result. This sample can be used to define the shape and size of t",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tmva/_batchgenerator.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tmva/_batchgenerator.py
Deployability,update,update,"# Authors:; # * Sanjiban Sengupta 01/2023; # * Lorenzo Moneta 01/2023; ################################################################################; # Copyright (C) 1995-2020, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; """"""; Get the activation function for the model. Parameters:; model: The graph_nets' component model to extract the activation function from.; The component model can be either of the update functions for; nodes, edges or globals. Returns:; The activation function enum value.; """"""; """"""; Create an MLP model and add it to the GNN Initializer. Parameters:; gin: The GNN Initializer to which the model will be added.; model: The model extracted from graph_nets's GNN component; function_target: Target for the function to update either of nodes, edges or globals; graph_type: The type of the graph, i.e. GNN or GraphIndependent. """"""; """"""; Create an Linear model and add it to the GNN Initializer. Parameters:; gin: The GNN Initializer to which the model will be added.; model: The model extracted from graph_nets's GNN component; function_target: Target for the function to update either of nodes, edges or globals; graph_type: The type of the graph, i.e. GNN or GraphIndependent. """"""; """"""; Add a LayerNormalization operator to the particular function target; in the Graph Initializer. Parameters:; gin: The GNN Initializer to which the LayerNorm operator will be added; module_layer: Extracted LayerNorm from graph_nets' model; function_target: Target for the function to update either of nodes, edges or globals. """"""; """"""; Add weights to respective function targets, either of nodes, edges or globals. Parameters:; gin: The GNN Initializer to which the weights will be added; weights: Weight information, containing the names, shapes and values of initialized t",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tmva/_gnn.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tmva/_gnn.py
Energy Efficiency,reduce,reducer," model extracted from graph_nets's GNN component; function_target: Target for the function to update either of nodes, edges or globals; graph_type: The type of the graph, i.e. GNN or GraphIndependent. """"""; """"""; Add a LayerNormalization operator to the particular function target; in the Graph Initializer. Parameters:; gin: The GNN Initializer to which the LayerNorm operator will be added; module_layer: Extracted LayerNorm from graph_nets' model; function_target: Target for the function to update either of nodes, edges or globals. """"""; """"""; Add weights to respective function targets, either of nodes, edges or globals. Parameters:; gin: The GNN Initializer to which the weights will be added; weights: Weight information, containing the names, shapes and values of initialized tensors; function_target: Target for the function to update either of nodes, edges or globals. """"""; """"""; Add aggregate function to the Graph Initializer. Parameters:; gin: The GNN Initializer to which the Aggregate function will be added; reducer: Specifies the means of aggregate, i.e. sum or mean of supplied values; relation: Specifies the relation of aggregate, i.e. Node-Edge, Global-Edge or Global-Node. """"""; """"""; Add update function for respective function target, either of nodes, edges or globals; based on the supplied component_model. Parameters:; gin: The GNN Initializer to which the update function will be added; component_model: The update function to add, either of MLP or Sequential; graph_type: The type of the graph, i.e. GNN or GraphIndependent; function_target: Target for the function to update either of nodes, edges or globals. """"""; """"""; Wrapper class for graph_nets' GNN model;s parsing and inference generation. graph_nets' GNN model comprises of three components, the nodes, edges and globals.; The entire model and its inference is based on the respective update functions,; and aggregate function with other components.; """"""; """"""; Parse graph_nets' GraphNetwork model and create RModel_GNN",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tmva/_gnn.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tmva/_gnn.py
Usability,simpl,simple,"dges or globals. """"""; """"""; Wrapper class for graph_nets' GNN model;s parsing and inference generation. graph_nets' GNN model comprises of three components, the nodes, edges and globals.; The entire model and its inference is based on the respective update functions,; and aggregate function with other components.; """"""; """"""; Parse graph_nets' GraphNetwork model and create RModel_GNN. Parameters:; graph_module: The graph module built from graph_nets; graph_data: Sample graph input data required for parsing of graph_nets' model; containing dict with keys: {""globals"", ""nodes"", ""edges"", ""senders"", ""receivers""}; filename: The filename to be used for output of inference code. Returns:; An instance of RModel_GNN.; """"""; # extracting the edges; # adding the node update function; # adding the edge update function; # adding the global update function; # adding edge-node aggregate function; # adding node-global aggregate function; # adding edge-global aggregate function; """"""; Wrapper class for graph_nets' GraphIndependent model's parsing and inference generation. graph_nets' GraphIndependent model is similar to the GNN implementation, with the; difference being that it has no aggregate function. GraphIndependent is useful; for independent transformation on the graph data. """"""; """"""; Parse graph_nets' GraphIndependent model and create RModel_GraphIndependent. Parameters:; graph_module: The graph module built from graph_nets; graph_data: Sample graph input data required for parsing of graph_nets' model; containing dict with keys: {""globals"", ""nodes"", ""edges"", ""senders"", ""receivers""}; filename: The filename to be used for output of inference code. Returns:; An instance of RModel_GraphIndependent.; """"""; # extracting the edges; # adding the node update function; # when an update is present in graph_nets, the update function has the _model attribute; # otherwise it is just a simple function defining output = input.; # adding the edge update function; # adding the global update function",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tmva/_gnn.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tmva/_gnn.py
Availability,error,error-handling,"# Author: Stefan Wunsch CERN 09/2019; ################################################################################; # Copyright (C) 1995-2019, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Import numpy lazily; # numpy.array is a factory and the actual type of a numpy array is numpy.ndarray; # As fall-through we go to the original compute function and use the error-handling from cppyy; # Parameters:; # klass: class to be pythonized",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tmva/_rbdt.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tmva/_rbdt.py
Integrability,interface,interface,"# Author: Stefan Wunsch CERN 07/2019; ################################################################################; # Copyright (C) 1995-2019, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Get array interface of object; # Get the data-pointer; # Get the size of the contiguous memory; # Get the typestring and properties thereof; # Get strides; # Infer memory layout from strides; # Construct an RTensor of the correct data-type; # Bind pyobject holding adopted memory to the RTensor; """"""; Return the array interface dictionary. Parameters:; self: RTensor object; Returns:; Dictionary following the Numpy array interface specifications; """"""; # Numpy breaks for data pointer of 0 even though the array is empty.; # We set the pointer to 1 but the value itself is arbitrary and never accessed.; """"""; Attach the array interface as property if the data-type of the RTensor; elements is one of the supported basic types. Parameters:; klass: class to be pythonized; name: string containing the name of the class; """"""; """"""; Implementation of the __getitem__ special function for RTensor. Parameters:; self: RTensor object; idx: Indices passed to RTensor[indices] operator; Returns:; New RTensor object if indices represent a slice or the requested element; """"""; # Make single index iterable and convert to list; # Check shape; # Convert negative indices and Nones; # If a slice is requested, return a new RTensor; # Otherwise, access element by array of indices; # conversion from numpy to buffer float/double * works only if C order; # Parameters:; # klass: class to be pythonized; # name: string containing the name of the class; # Add numpy array interface; # Get elements, including slices; # add initialization of RTensor (pythonization of constructor)",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tmva/_rtensor.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tmva/_rtensor.py
Security,access,accessed,"# Author: Stefan Wunsch CERN 07/2019; ################################################################################; # Copyright (C) 1995-2019, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; # Get array interface of object; # Get the data-pointer; # Get the size of the contiguous memory; # Get the typestring and properties thereof; # Get strides; # Infer memory layout from strides; # Construct an RTensor of the correct data-type; # Bind pyobject holding adopted memory to the RTensor; """"""; Return the array interface dictionary. Parameters:; self: RTensor object; Returns:; Dictionary following the Numpy array interface specifications; """"""; # Numpy breaks for data pointer of 0 even though the array is empty.; # We set the pointer to 1 but the value itself is arbitrary and never accessed.; """"""; Attach the array interface as property if the data-type of the RTensor; elements is one of the supported basic types. Parameters:; klass: class to be pythonized; name: string containing the name of the class; """"""; """"""; Implementation of the __getitem__ special function for RTensor. Parameters:; self: RTensor object; idx: Indices passed to RTensor[indices] operator; Returns:; New RTensor object if indices represent a slice or the requested element; """"""; # Make single index iterable and convert to list; # Check shape; # Convert negative indices and Nones; # If a slice is requested, return a new RTensor; # Otherwise, access element by array of indices; # conversion from numpy to buffer float/double * works only if C order; # Parameters:; # klass: class to be pythonized; # name: string containing the name of the class; # Add numpy array interface; # Get elements, including slices; # add initialization of RTensor (pythonization of constructor)",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tmva/_rtensor.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tmva/_rtensor.py
Deployability,update,updater,"# Author: Stefan Wunsch CERN 09/2019; ################################################################################; # Copyright (C) 1995-2019, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; """"""Get base score from an XGBoost sklearn estimator. Copy-pasted from XGBoost unit test code. See also:; * https://github.com/dmlc/xgboost/blob/a99bb38bd2762e35e6a1673a0c11e09eddd8e723/python-package/xgboost/testing/updater.py#L13; * https://github.com/dmlc/xgboost/issues/9347; * https://discuss.xgboost.ai/t/how-to-get-base-score-from-trained-booster/3192; """"""; """"""; Saves the XGBoost model to a ROOT file as a TMVA::Experimental::RBDT object. Args:; xgb_model: The trained XGBoost model.; key_name (str): The name to use for storing the RBDT in the output file.; output_path (str): The path to save the output file.; num_inputs (int): The number of input features used in the model. Raises:; Exception: If the XGBoost model has an unsupported objective.; """"""; # Extract objective; # Naming the objective softmax is more common today; # Determine number of outputs; # Dump XGB model as json file; # Dump XGB model as txt file",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tmva/_tree_inference.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tmva/_tree_inference.py
Testability,test,test,"# Author: Stefan Wunsch CERN 09/2019; ################################################################################; # Copyright (C) 1995-2019, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; """"""Get base score from an XGBoost sklearn estimator. Copy-pasted from XGBoost unit test code. See also:; * https://github.com/dmlc/xgboost/blob/a99bb38bd2762e35e6a1673a0c11e09eddd8e723/python-package/xgboost/testing/updater.py#L13; * https://github.com/dmlc/xgboost/issues/9347; * https://discuss.xgboost.ai/t/how-to-get-base-score-from-trained-booster/3192; """"""; """"""; Saves the XGBoost model to a ROOT file as a TMVA::Experimental::RBDT object. Args:; xgb_model: The trained XGBoost model.; key_name (str): The name to use for storing the RBDT in the output file.; output_path (str): The path to save the output file.; num_inputs (int): The number of input features used in the model. Raises:; Exception: If the XGBoost model has an unsupported objective.; """"""; # Extract objective; # Naming the objective softmax is more common today; # Determine number of outputs; # Dump XGB model as json file; # Dump XGB model as txt file",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tmva/_tree_inference.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tmva/_tree_inference.py
Availability,avail,available,"# Authors:; # * Harshal Shende 04/2022; # * Lorenzo Moneta 04/2022; ################################################################################; # Copyright (C) 1995-2020, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; #this should be available only when xgboost is there ?; # We probably don't need a protection here since the code is run only when there is xgboost; # list of python classes that are used to pythonize TMVA classes; # create a dictionary for convenient access to python classes; """"""; Get all class attributes that are defined in a given class or optionally in; any of its base classes (except for `object`).; """"""; # get a list of this class and all its base classes, excluding `object`; """"""; Bind the instance method `from_class.func_name` also to class `to_class`.; """"""; # the @classmethod case; # any other case in Python 3 is trivial; """"""Return the name that we will give to the original cppyy function.""""""; # special treatment of magic functions, e.g.: __getitem__ > _getitem; # Parameters:; # klass: class to pythonize; # name: string containing the name of the class; # need to strip the TMVA namespace; # list of functions to pythonize, which are assumed to be all functions in; # that are manually defined in the Python classes or their superclasses; # if the TMVA class already has a function with the same name as our; # pythonization, we rename it and prefix it with an underscore; # new name for original function",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tmva/__init__.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tmva/__init__.py
Security,access,access,"# Authors:; # * Harshal Shende 04/2022; # * Lorenzo Moneta 04/2022; ################################################################################; # Copyright (C) 1995-2020, Rene Brun and Fons Rademakers. #; # All rights reserved. #; # #; # For the licensing terms see $ROOTSYS/LICENSE. #; # For the list of contributors see $ROOTSYS/README/CREDITS. #; ################################################################################; #this should be available only when xgboost is there ?; # We probably don't need a protection here since the code is run only when there is xgboost; # list of python classes that are used to pythonize TMVA classes; # create a dictionary for convenient access to python classes; """"""; Get all class attributes that are defined in a given class or optionally in; any of its base classes (except for `object`).; """"""; # get a list of this class and all its base classes, excluding `object`; """"""; Bind the instance method `from_class.func_name` also to class `to_class`.; """"""; # the @classmethod case; # any other case in Python 3 is trivial; """"""Return the name that we will give to the original cppyy function.""""""; # special treatment of magic functions, e.g.: __getitem__ > _getitem; # Parameters:; # klass: class to pythonize; # name: string containing the name of the class; # need to strip the TMVA namespace; # list of functions to pythonize, which are assumed to be all functions in; # that are manually defined in the Python classes or their superclasses; # if the TMVA class already has a function with the same name as our; # pythonization, we rename it and prefix it with an underscore; # new name for original function",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tmva/__init__.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/python/ROOT/_pythonization/_tmva/__init__.py
Integrability,interface,interface,"""""""; Test memory adoption of std::vector and ROOT::RVec with the numpy; array interface.; """"""; # Helpers; # Tests; """"""; Test correct adoption of different datatypes for std::vector; """"""; """"""; Test correct adoption of different datatypes for ROOT::RVec; """"""; """"""; Test adoption of empty std::vector; """"""; """"""; Test adoption of empty ROOT::RVec; """"""",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/array_interface.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/array_interface.py
Deployability,configurat,configuration,"""""""; Test which libraries are loaded during importing ROOT; """"""; # The whitelist is a list of regex expressions that mark wanted libraries; # Note that the regex has to result in an exact match with the library name.; # libCore and dependencies; # libCling and dependencies; # by libncurses (on some older platforms); # libTree and dependencies; # by libssl; # by libRIO if uring option is enabled; # On centos7 libssl links against kerberos pulling in all dependencies below, removed with libssl1.1.0; # cppyy and Python libraries; # System libraries and others; # AddressSanitizer runtime and ROOT configuration; # Verbose mode of the test; """"""; Test libraries loaded after importing ROOT; """"""; # Split paths; # Get library name without full path and .so* suffix; # Check that the loaded libraries are white listed",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/import_load_libs.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/import_load_libs.py
Integrability,depend,dependencies,"""""""; Test which libraries are loaded during importing ROOT; """"""; # The whitelist is a list of regex expressions that mark wanted libraries; # Note that the regex has to result in an exact match with the library name.; # libCore and dependencies; # libCling and dependencies; # by libncurses (on some older platforms); # libTree and dependencies; # by libssl; # by libRIO if uring option is enabled; # On centos7 libssl links against kerberos pulling in all dependencies below, removed with libssl1.1.0; # cppyy and Python libraries; # System libraries and others; # AddressSanitizer runtime and ROOT configuration; # Verbose mode of the test; """"""; Test libraries loaded after importing ROOT; """"""; # Split paths; # Get library name without full path and .so* suffix; # Check that the loaded libraries are white listed",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/import_load_libs.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/import_load_libs.py
Modifiability,config,configuration,"""""""; Test which libraries are loaded during importing ROOT; """"""; # The whitelist is a list of regex expressions that mark wanted libraries; # Note that the regex has to result in an exact match with the library name.; # libCore and dependencies; # libCling and dependencies; # by libncurses (on some older platforms); # libTree and dependencies; # by libssl; # by libRIO if uring option is enabled; # On centos7 libssl links against kerberos pulling in all dependencies below, removed with libssl1.1.0; # cppyy and Python libraries; # System libraries and others; # AddressSanitizer runtime and ROOT configuration; # Verbose mode of the test; """"""; Test libraries loaded after importing ROOT; """"""; # Split paths; # Get library name without full path and .so* suffix; # Check that the loaded libraries are white listed",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/import_load_libs.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/import_load_libs.py
Performance,load,loaded,"""""""; Test which libraries are loaded during importing ROOT; """"""; # The whitelist is a list of regex expressions that mark wanted libraries; # Note that the regex has to result in an exact match with the library name.; # libCore and dependencies; # libCling and dependencies; # by libncurses (on some older platforms); # libTree and dependencies; # by libssl; # by libRIO if uring option is enabled; # On centos7 libssl links against kerberos pulling in all dependencies below, removed with libssl1.1.0; # cppyy and Python libraries; # System libraries and others; # AddressSanitizer runtime and ROOT configuration; # Verbose mode of the test; """"""; Test libraries loaded after importing ROOT; """"""; # Split paths; # Get library name without full path and .so* suffix; # Check that the loaded libraries are white listed",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/import_load_libs.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/import_load_libs.py
Testability,test,test,"""""""; Test which libraries are loaded during importing ROOT; """"""; # The whitelist is a list of regex expressions that mark wanted libraries; # Note that the regex has to result in an exact match with the library name.; # libCore and dependencies; # libCling and dependencies; # by libncurses (on some older platforms); # libTree and dependencies; # by libssl; # by libRIO if uring option is enabled; # On centos7 libssl links against kerberos pulling in all dependencies below, removed with libssl1.1.0; # cppyy and Python libraries; # System libraries and others; # AddressSanitizer runtime and ROOT configuration; # Verbose mode of the test; """"""; Test libraries loaded after importing ROOT; """"""; # Split paths; # Get library name without full path and .so* suffix; # Check that the loaded libraries are white listed",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/import_load_libs.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/import_load_libs.py
Testability,test,test,"""""""Regression test for https://github.com/root-project/root/issues/15703""""""; """"""; #ifndef TEST_15703; #define TEST_15703; #include <string>; class foo {; public:; const std::string leak (std::size_t size) const {; std::string result;; result.reserve(size);; return result;; }; };. auto get_rss_KB() {; ProcInfo_t info;; gSystem->GetProcInfo(&info);; return info.fMemResident;; }; #endif // TEST_15703; """"""",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/memory.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/memory.py
Deployability,integrat,integration,"# Check whether these tests should be skipped; """"""; Test decorator to create C++ wrapper for Python callables using numba with fundamental types; """"""; # Test refcounts; """"""; Test refcount of decorator; """"""; """"""; Test refcount of decorated callable; """"""; # Test optional name; """"""; Test optional name of wrapper function; """"""; # Test attributes; """"""; Test additional attributes; """"""; # Test cling integration; """"""; Test function call in cling; """"""; # Test RDataFrame integration; """"""; Test function call as part of RDataFrame; """"""; """"""; Test passing a temporary from an RDataFrame operation; """"""; # Test wrappings; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; # NOTE: There is no double in Python because everything is a double.; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test decorator to create C++ wrapper for Python callables using numba with RVecs; """"""; # The global module index does not have RVec entities preloaded and; # gInterpreter.Declare is not allowed to load libROOTVecOps for RVec.; # Preload the library now.; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output config",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/numbadeclare.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/numbadeclare.py
Integrability,wrap,wrapper,"# Check whether these tests should be skipped; """"""; Test decorator to create C++ wrapper for Python callables using numba with fundamental types; """"""; # Test refcounts; """"""; Test refcount of decorator; """"""; """"""; Test refcount of decorated callable; """"""; # Test optional name; """"""; Test optional name of wrapper function; """"""; # Test attributes; """"""; Test additional attributes; """"""; # Test cling integration; """"""; Test function call in cling; """"""; # Test RDataFrame integration; """"""; Test function call as part of RDataFrame; """"""; """"""; Test passing a temporary from an RDataFrame operation; """"""; # Test wrappings; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; # NOTE: There is no double in Python because everything is a double.; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test decorator to create C++ wrapper for Python callables using numba with RVecs; """"""; # The global module index does not have RVec entities preloaded and; # gInterpreter.Declare is not allowed to load libROOTVecOps for RVec.; # Preload the library now.; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output config",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/numbadeclare.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/numbadeclare.py
Modifiability,config,configurations,"# Check whether these tests should be skipped; """"""; Test decorator to create C++ wrapper for Python callables using numba with fundamental types; """"""; # Test refcounts; """"""; Test refcount of decorator; """"""; """"""; Test refcount of decorated callable; """"""; # Test optional name; """"""; Test optional name of wrapper function; """"""; # Test attributes; """"""; Test additional attributes; """"""; # Test cling integration; """"""; Test function call in cling; """"""; # Test RDataFrame integration; """"""; Test function call as part of RDataFrame; """"""; """"""; Test passing a temporary from an RDataFrame operation; """"""; # Test wrappings; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; # NOTE: There is no double in Python because everything is a double.; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test decorator to create C++ wrapper for Python callables using numba with RVecs; """"""; # The global module index does not have RVec entities preloaded and; # gInterpreter.Declare is not allowed to load libROOTVecOps for RVec.; # Preload the library now.; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output config",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/numbadeclare.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/numbadeclare.py
Performance,load,load,"wrapper with different input/output configurations; """"""; # NOTE: There is no double in Python because everything is a double.; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test decorator to create C++ wrapper for Python callables using numba with RVecs; """"""; # The global module index does not have RVec entities preloaded and; # gInterpreter.Declare is not allowed to load libROOTVecOps for RVec.; # Preload the library now.; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; ",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/numbadeclare.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/numbadeclare.py
Testability,test,tests,"# Check whether these tests should be skipped; """"""; Test decorator to create C++ wrapper for Python callables using numba with fundamental types; """"""; # Test refcounts; """"""; Test refcount of decorator; """"""; """"""; Test refcount of decorated callable; """"""; # Test optional name; """"""; Test optional name of wrapper function; """"""; # Test attributes; """"""; Test additional attributes; """"""; # Test cling integration; """"""; Test function call in cling; """"""; # Test RDataFrame integration; """"""; Test function call as part of RDataFrame; """"""; """"""; Test passing a temporary from an RDataFrame operation; """"""; # Test wrappings; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; # NOTE: There is no double in Python because everything is a double.; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test decorator to create C++ wrapper for Python callables using numba with RVecs; """"""; # The global module index does not have RVec entities preloaded and; # gInterpreter.Declare is not allowed to load libROOTVecOps for RVec.; # Preload the library now.; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output configurations; """"""; """"""; Test wrapper with different input/output config",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/numbadeclare.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/numbadeclare.py
Modifiability,inherit,inherited,"# Helpers; # Tests; # https://github.com/root-project/root/issues/12817; # The threshold for the memory used is generously chosen to avoid tests; # spuriously failing b/c of fluctuations and is also well below the; # memory needed before the fix to the issue mentioned above, i.e. about; # 4 GB and several minutes to complete (!); '''; float GetRSSMB() {; ProcInfo_t info;; gSystem->GetProcInfo(&info);; return info.fMemResident/1024.;}; '''; # std::string is not pythonized with the pretty printing, because:; # 1. gInterpreter->ToString(""s"") returns """"s""""; # 2. cppyy already does the right thing; # Test fall-back to __repr__; # ROOT-9935: test null proxied cpp object; # ROOT-10967: Respect existing __str__ method defined in C++; # Test inherited class; # TNamed and TObject are not pythonized because these object are touched; # by PyROOT before any pythonizations are added. Following, the classes; # are not piped through the pythonizor functions again.; """"""; def test_TNamed(self):; x = ROOT.TNamed(""name"", ""title""); self._print(x); self.assertEqual(""Name: name Title: title"", x.__str__()). def test_TObject(self):; x = ROOT.TObject(); self._print(x); self.assertEqual(""Name: TObject Title: Basic ROOT object"", x.__str__()); """"""",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/pretty_printing.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/pretty_printing.py
Safety,avoid,avoid,"# Helpers; # Tests; # https://github.com/root-project/root/issues/12817; # The threshold for the memory used is generously chosen to avoid tests; # spuriously failing b/c of fluctuations and is also well below the; # memory needed before the fix to the issue mentioned above, i.e. about; # 4 GB and several minutes to complete (!); '''; float GetRSSMB() {; ProcInfo_t info;; gSystem->GetProcInfo(&info);; return info.fMemResident/1024.;}; '''; # std::string is not pythonized with the pretty printing, because:; # 1. gInterpreter->ToString(""s"") returns """"s""""; # 2. cppyy already does the right thing; # Test fall-back to __repr__; # ROOT-9935: test null proxied cpp object; # ROOT-10967: Respect existing __str__ method defined in C++; # Test inherited class; # TNamed and TObject are not pythonized because these object are touched; # by PyROOT before any pythonizations are added. Following, the classes; # are not piped through the pythonizor functions again.; """"""; def test_TNamed(self):; x = ROOT.TNamed(""name"", ""title""); self._print(x); self.assertEqual(""Name: name Title: title"", x.__str__()). def test_TObject(self):; x = ROOT.TObject(); self._print(x); self.assertEqual(""Name: TObject Title: Basic ROOT object"", x.__str__()); """"""",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/pretty_printing.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/pretty_printing.py
Testability,test,tests,"# Helpers; # Tests; # https://github.com/root-project/root/issues/12817; # The threshold for the memory used is generously chosen to avoid tests; # spuriously failing b/c of fluctuations and is also well below the; # memory needed before the fix to the issue mentioned above, i.e. about; # 4 GB and several minutes to complete (!); '''; float GetRSSMB() {; ProcInfo_t info;; gSystem->GetProcInfo(&info);; return info.fMemResident/1024.;}; '''; # std::string is not pythonized with the pretty printing, because:; # 1. gInterpreter->ToString(""s"") returns """"s""""; # 2. cppyy already does the right thing; # Test fall-back to __repr__; # ROOT-9935: test null proxied cpp object; # ROOT-10967: Respect existing __str__ method defined in C++; # Test inherited class; # TNamed and TObject are not pythonized because these object are touched; # by PyROOT before any pythonizations are added. Following, the classes; # are not piped through the pythonizor functions again.; """"""; def test_TNamed(self):; x = ROOT.TNamed(""name"", ""title""); self._print(x); self.assertEqual(""Name: name Title: title"", x.__str__()). def test_TObject(self):; x = ROOT.TObject(); self._print(x); self.assertEqual(""Name: TObject Title: Basic ROOT object"", x.__str__()); """"""",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/pretty_printing.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/pretty_printing.py
Performance,cache,cached,"Define test classes; # Define pythonizor function for prefix.; # Match all classes in global namespace; # Trigger execution of pythonizor; # Test @pythonization('', ns='NS', is_prefix=True); # Define test classes; # Define pythonizor function for prefix.; # Match all classes in NS namespace; # Trigger execution of pythonizor; # Test @pythonization(['MyClass', 'MyClass']); # Define test class; # Define pythonizor function for classes; # Trigger execution of pythonizor; # The following should be true since @pythonization removes repetitions; # Test that @pythonization filters out non-matching classes; # Trigger execution of pythonizors.; # Non-matching classes should be filtered out; # The list should be empty if non-matching classes have been discarded; # Test passing as target an iterable that is not a list; # Define test classes; # Define pythonizor function for classes; # Trigger execution of pythonizor; # Test pythonizor with a single parameter (the class proxy); # Define test class; # Define pythonizor function for class; # Trigger execution of pythonizor; # Test pythonizor with wrong number of parameters; # Define test class; # Define pythonizor function for class.; # Registration of pythonizor should fail; # Test @pythonization where class_name (wrongly) includes namespace; # Define test class; # Define pythonizor function for class.; # Registration of pythonizor should fail; # Test stacking of @pythonization decorators; # Define test classes; # Stack two @pythonization; # Trigger execution of pythonizor; # The pythonizor should have been triggered twice; # Test pythonization of already instantiated classes; # Define test classes; # Instantiate classes; # Immediate pythonization should happen.; # Accesses classes are cached by cppyy using their class name as key in; # their namespace; # Immediate pythonization should happen.; # Instantiated templates are also tested because they are cached by; # cppyy using their fully-qualified name as key in their namespace",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/pythonization_decorator.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/pythonization_decorator.py
Testability,test,tests,"""""""; Test the @pythonization decorator for user-defined classes.; """"""; # Some already instantiated ROOT classes may match targets of @pythonization; # in some tests, and because of immediate pythonization they will be; # processed by the pythonizors. Just ignore them; # Helpers; '''; namespace {ns} {{; class {cn} {{}};; }}'''; # Test @pythonization('MyClass'); # Define test class; # Define pythonizor function for class; # Trigger execution of pythonizor; # Test @pythonization('NS::MyClass'); # Define test class; # Define pythonizor function for class; # Trigger execution of pythonizor; # Test @pythonization('Prefix', is_prefix=True); # Define test classes; # Define pythonizor function for prefix; # Trigger execution of pythonizor; # Test @pythonization('NS::Prefix', is_prefix=True); # Define test classes; # Define pythonizor function for prefix; # Trigger execution of pythonizor; # Test @pythonization(['OneClass', 'AnotherClass']); # Define test classes; # Define pythonizor function for classes; # Trigger execution of pythonizor; # Test @pythonization(['OneClass', 'AnotherClass'], ns='NS'); # Define test classes; # Define pythonizor function for classes; # Trigger execution of pythonizor; # Test @pythonization(['OnePrefix', 'AnotherPrefix'], is_prefix=True); # Define test classes; # Define pythonizor function for prefixes; # Trigger execution of pythonizor; # Test @pythonization(['OnePrefix', 'AnotherPrefix'], ns='NS', is_prefix=True); # Define test classes; # Define pythonizor function for prefixes; # Trigger execution of pythonizor; # Test @pythonization('', is_prefix=True); # Define test classes; # Define pythonizor function for prefix.; # Match all classes in global namespace; # Trigger execution of pythonizor; # Test @pythonization('', ns='NS', is_prefix=True); # Define test classes; # Define pythonizor function for prefix.; # Match all classes in NS namespace; # Trigger execution of pythonizor; # Test @pythonization(['MyClass', 'MyClass']); # Define test class;",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/pythonization_decorator.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/pythonization_decorator.py
Integrability,inject,injected,"eate_array(unsigned int n) {; return std::array<unsigned int, 3>({n, n, n});; }; """"""; """"""; Testing reading a TH1F; """"""; """"""; TH1F create_histo(unsigned int n) {; const auto str = TString::Format(""h%i"", n);; return TH1F(str, str, 4, 0, 1);; }; """"""; """"""; Testing reading a std::vector with constant size; """"""; """"""; std::vector<unsigned int> create_vector_constantsize(unsigned int n) {; return std::vector<unsigned int>({n, n, n});; }; """"""; """"""; Testing reading a std::vector with variable size; """"""; """"""; std::vector<unsigned int> create_vector_variablesize(unsigned int n) {; return std::vector<unsigned int>(n);; }; """"""; """"""; Testing reading a TLorentzVector; """"""; # The global module index does not have it preloaded and; # gInterpreter.Declare is not allowed to load libPhysics for; # TLorentzVector. Preload the library now.; """"""; TLorentzVector create_tlorentzvector() {; auto v = TLorentzVector();; v.SetPtEtaPhiM(1, 2, 3, 4);; return v;; }; """"""; """"""; Testing reading a custom class injected in the interpreter; """"""; """"""; struct CustomClass {; unsigned int fMember = 42;; };; CustomClass create_custom_class() {; return CustomClass();; }; """"""; """"""; Testing reading defined columns; """"""; """"""; Testing excluding columns from read-out; """"""; """"""; Testing result pointer being attribute of returned numpy array; """"""; """"""; Testing ownership of numpy array as owner of the data; """"""; """"""; Testing ownership of numpy array as view on data; """"""; """"""; Testing readout of empty std::vectors; """"""; """"""; Testing readout of empty selection; """"""; """"""; Testing pickling of returned numpy array; """"""; """"""; Testing the adoption of the memory from the C++ side for fundamental types; """"""; """"""; Testing the adoption of the memory from the C++ side for complex types; """"""; """"""; Testing cloning of AsNumpy results; """"""; # Get the result for the first range; # To return an AsNumpyResult; # Clone the result for following ranges; """"""; Testing converting bool columns to NumPy arrays.; """"""; # test values; # test type",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/rdataframe_asnumpy.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/rdataframe_asnumpy.py
Modifiability,variab,variable,"""""""; Make a tree with branches of different data-types; """"""; """"""; Read-out as a numpy array and return a slice which is a view on the data; """"""; """"""; Testing of RDataFrame.AsNumpy pythonization; """"""; """"""; Test supported data-types for read-out; """"""; """"""; Test bool data-type as a special case since we cannot adopt; the std::vector<bool> with numpy arrays; """"""; """"""; Testing reading a std::array; """"""; """"""; std::array<unsigned int, 3> create_array(unsigned int n) {; return std::array<unsigned int, 3>({n, n, n});; }; """"""; """"""; Testing reading a TH1F; """"""; """"""; TH1F create_histo(unsigned int n) {; const auto str = TString::Format(""h%i"", n);; return TH1F(str, str, 4, 0, 1);; }; """"""; """"""; Testing reading a std::vector with constant size; """"""; """"""; std::vector<unsigned int> create_vector_constantsize(unsigned int n) {; return std::vector<unsigned int>({n, n, n});; }; """"""; """"""; Testing reading a std::vector with variable size; """"""; """"""; std::vector<unsigned int> create_vector_variablesize(unsigned int n) {; return std::vector<unsigned int>(n);; }; """"""; """"""; Testing reading a TLorentzVector; """"""; # The global module index does not have it preloaded and; # gInterpreter.Declare is not allowed to load libPhysics for; # TLorentzVector. Preload the library now.; """"""; TLorentzVector create_tlorentzvector() {; auto v = TLorentzVector();; v.SetPtEtaPhiM(1, 2, 3, 4);; return v;; }; """"""; """"""; Testing reading a custom class injected in the interpreter; """"""; """"""; struct CustomClass {; unsigned int fMember = 42;; };; CustomClass create_custom_class() {; return CustomClass();; }; """"""; """"""; Testing reading defined columns; """"""; """"""; Testing excluding columns from read-out; """"""; """"""; Testing result pointer being attribute of returned numpy array; """"""; """"""; Testing ownership of numpy array as owner of the data; """"""; """"""; Testing ownership of numpy array as view on data; """"""; """"""; Testing readout of empty std::vectors; """"""; """"""; Testing readout of empty selection; """"""; """"""; Testing pickling of r",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/rdataframe_asnumpy.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/rdataframe_asnumpy.py
Performance,load,load,"t supported data-types for read-out; """"""; """"""; Test bool data-type as a special case since we cannot adopt; the std::vector<bool> with numpy arrays; """"""; """"""; Testing reading a std::array; """"""; """"""; std::array<unsigned int, 3> create_array(unsigned int n) {; return std::array<unsigned int, 3>({n, n, n});; }; """"""; """"""; Testing reading a TH1F; """"""; """"""; TH1F create_histo(unsigned int n) {; const auto str = TString::Format(""h%i"", n);; return TH1F(str, str, 4, 0, 1);; }; """"""; """"""; Testing reading a std::vector with constant size; """"""; """"""; std::vector<unsigned int> create_vector_constantsize(unsigned int n) {; return std::vector<unsigned int>({n, n, n});; }; """"""; """"""; Testing reading a std::vector with variable size; """"""; """"""; std::vector<unsigned int> create_vector_variablesize(unsigned int n) {; return std::vector<unsigned int>(n);; }; """"""; """"""; Testing reading a TLorentzVector; """"""; # The global module index does not have it preloaded and; # gInterpreter.Declare is not allowed to load libPhysics for; # TLorentzVector. Preload the library now.; """"""; TLorentzVector create_tlorentzvector() {; auto v = TLorentzVector();; v.SetPtEtaPhiM(1, 2, 3, 4);; return v;; }; """"""; """"""; Testing reading a custom class injected in the interpreter; """"""; """"""; struct CustomClass {; unsigned int fMember = 42;; };; CustomClass create_custom_class() {; return CustomClass();; }; """"""; """"""; Testing reading defined columns; """"""; """"""; Testing excluding columns from read-out; """"""; """"""; Testing result pointer being attribute of returned numpy array; """"""; """"""; Testing ownership of numpy array as owner of the data; """"""; """"""; Testing ownership of numpy array as view on data; """"""; """"""; Testing readout of empty std::vectors; """"""; """"""; Testing readout of empty selection; """"""; """"""; Testing pickling of returned numpy array; """"""; """"""; Testing the adoption of the memory from the C++ side for fundamental types; """"""; """"""; Testing the adoption of the memory from the C++ side for complex types; """"""; """"""; Testing ",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/rdataframe_asnumpy.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/rdataframe_asnumpy.py
Security,inject,injected,"eate_array(unsigned int n) {; return std::array<unsigned int, 3>({n, n, n});; }; """"""; """"""; Testing reading a TH1F; """"""; """"""; TH1F create_histo(unsigned int n) {; const auto str = TString::Format(""h%i"", n);; return TH1F(str, str, 4, 0, 1);; }; """"""; """"""; Testing reading a std::vector with constant size; """"""; """"""; std::vector<unsigned int> create_vector_constantsize(unsigned int n) {; return std::vector<unsigned int>({n, n, n});; }; """"""; """"""; Testing reading a std::vector with variable size; """"""; """"""; std::vector<unsigned int> create_vector_variablesize(unsigned int n) {; return std::vector<unsigned int>(n);; }; """"""; """"""; Testing reading a TLorentzVector; """"""; # The global module index does not have it preloaded and; # gInterpreter.Declare is not allowed to load libPhysics for; # TLorentzVector. Preload the library now.; """"""; TLorentzVector create_tlorentzvector() {; auto v = TLorentzVector();; v.SetPtEtaPhiM(1, 2, 3, 4);; return v;; }; """"""; """"""; Testing reading a custom class injected in the interpreter; """"""; """"""; struct CustomClass {; unsigned int fMember = 42;; };; CustomClass create_custom_class() {; return CustomClass();; }; """"""; """"""; Testing reading defined columns; """"""; """"""; Testing excluding columns from read-out; """"""; """"""; Testing result pointer being attribute of returned numpy array; """"""; """"""; Testing ownership of numpy array as owner of the data; """"""; """"""; Testing ownership of numpy array as view on data; """"""; """"""; Testing readout of empty std::vectors; """"""; """"""; Testing readout of empty selection; """"""; """"""; Testing pickling of returned numpy array; """"""; """"""; Testing the adoption of the memory from the C++ side for fundamental types; """"""; """"""; Testing the adoption of the memory from the C++ side for complex types; """"""; """"""; Testing cloning of AsNumpy results; """"""; # Get the result for the first range; # To return an AsNumpyResult; # Clone the result for following ranges; """"""; Testing converting bool columns to NumPy arrays.; """"""; # test values; # test type",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/rdataframe_asnumpy.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/rdataframe_asnumpy.py
Testability,test,test,"eate_array(unsigned int n) {; return std::array<unsigned int, 3>({n, n, n});; }; """"""; """"""; Testing reading a TH1F; """"""; """"""; TH1F create_histo(unsigned int n) {; const auto str = TString::Format(""h%i"", n);; return TH1F(str, str, 4, 0, 1);; }; """"""; """"""; Testing reading a std::vector with constant size; """"""; """"""; std::vector<unsigned int> create_vector_constantsize(unsigned int n) {; return std::vector<unsigned int>({n, n, n});; }; """"""; """"""; Testing reading a std::vector with variable size; """"""; """"""; std::vector<unsigned int> create_vector_variablesize(unsigned int n) {; return std::vector<unsigned int>(n);; }; """"""; """"""; Testing reading a TLorentzVector; """"""; # The global module index does not have it preloaded and; # gInterpreter.Declare is not allowed to load libPhysics for; # TLorentzVector. Preload the library now.; """"""; TLorentzVector create_tlorentzvector() {; auto v = TLorentzVector();; v.SetPtEtaPhiM(1, 2, 3, 4);; return v;; }; """"""; """"""; Testing reading a custom class injected in the interpreter; """"""; """"""; struct CustomClass {; unsigned int fMember = 42;; };; CustomClass create_custom_class() {; return CustomClass();; }; """"""; """"""; Testing reading defined columns; """"""; """"""; Testing excluding columns from read-out; """"""; """"""; Testing result pointer being attribute of returned numpy array; """"""; """"""; Testing ownership of numpy array as owner of the data; """"""; """"""; Testing ownership of numpy array as view on data; """"""; """"""; Testing readout of empty std::vectors; """"""; """"""; Testing readout of empty selection; """"""; """"""; Testing pickling of returned numpy array; """"""; """"""; Testing the adoption of the memory from the C++ side for fundamental types; """"""; """"""; Testing the adoption of the memory from the C++ side for complex types; """"""; """"""; Testing cloning of AsNumpy results; """"""; # Get the result for the first range; # To return an AsNumpyResult; # Clone the result for following ranges; """"""; Testing converting bool columns to NumPy arrays.; """"""; # test values; # test type",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/rdataframe_asnumpy.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/rdataframe_asnumpy.py
Deployability,release,releases,"""""""; Tests for the FromNumpy feature enabling to read numpy arrays; with RDataFrame.; """"""; """"""; Test reading different datatypes; """"""; """"""; Test reading multiple columns; """"""; # Test column names; # Test mean; """"""; Check refcounts of associated PyObjects; """"""; """"""; Test the use of transformations; """"""; """"""; Test behaviour with data dictionary going out of scope; """"""; """"""; Test behaviour with numpy array going out of scope; """"""; """"""; Test behaviour with inplace dictionary; """"""; """"""; Test lifetime of numpy array; """"""; """"""; Test lifetime of datasource. Datasource survives until last node of the graph goes out of scope; """"""; # Data source has dictionary with RVecs attached, which take a reference; # to the numpy array; # Deleting the root node does not change anything since the datasource; # owns the RVecs; # Deleting the last node releases the RVecs and releases the reference; # to the numpy array; """"""; Test correct reading of a sliced numpy array (#13690); """"""",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/rdataframe_makenumpy.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/rdataframe_makenumpy.py
Testability,test,test,"""""""; This function generates the root files of various datatypes with random values to test them.; Datatypes could be generated are Strings, Char_t, UChar_t; """"""; # function to create random numbers.. gRandom did not give me signed integers; # df with 100 entries; # 16 Data Types",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/rdf_filter_pyz_helper.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/rdf_filter_pyz_helper.py
Testability,test,tests,"# Helper functions for the tests; """"""; Check that a Python object has a nested attribute that can be retrieved by; recursively using a given list of keys.; """"""; """"""; Check if the ROOT module has a certain attribute, that can also be nested.; For example:. root_module_has(""RooFit.Experimental""); """"""; """"""; Testing features of the ROOT module implemented in the ROOT module facade; """"""; """"""; Test import; """"""; """"""; Test relative import; """"""; """"""; Test __version__ property; """"""; # Fix for #14068: we take into account the different way of expressing the version; # number before and starting with 6.32.00; """"""; Test module flag to ignore command line options; """"""; """"""; Test importing implicitly from the ROOT namespace; """"""; """"""; Test that we can correctly import C++ namespaces and other things that; should behave like Python modules, including nested cases.; """"""; #",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/root_module.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/root_module.py
Integrability,interface,interface,"""""""; Test AsRTensor adoption mechanism; """"""; # Helpers; # Tests; """"""; Test adoption of numpy arrays with different data types; """"""; # test also direct conversion; # Tests; """"""; Test adoption of the memory layout; """"""; """"""; Test adoption of the strides. Note that numpy multiplies the strides with the size of the element; in bytes.; """"""; """"""; Test memory adoption of RTensor array interface.; """"""; # Helpers; # Tests; """"""; Test correct adoption of different datatypes; """"""; """"""; Test adoption of the memory layout; """"""; """"""; Test ownership of adopted numpy array; """"""; """"""; Test compliance of the RTensor methods with the numpy interface; """"""; """"""; Test np.transpose vs RTensor::Transpose; """"""; """"""; Test np.expand_dims vs RTensor::ExpandDims; """"""; """"""; Test np.squeeze vs RTensor::Squeeze; """"""; """"""; Test np.reshape vs RTensor::Reshape; """"""; """"""; Test slicing operations; """"""; # We know that we differ in this case since numpy; # does only squeeze the dimensions of a slice if; # the dimension is requested by a single index.",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/rtensor.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/rtensor.py
Testability,test,test,"""""""; Test AsRTensor adoption mechanism; """"""; # Helpers; # Tests; """"""; Test adoption of numpy arrays with different data types; """"""; # test also direct conversion; # Tests; """"""; Test adoption of the memory layout; """"""; """"""; Test adoption of the strides. Note that numpy multiplies the strides with the size of the element; in bytes.; """"""; """"""; Test memory adoption of RTensor array interface.; """"""; # Helpers; # Tests; """"""; Test correct adoption of different datatypes; """"""; """"""; Test adoption of the memory layout; """"""; """"""; Test ownership of adopted numpy array; """"""; """"""; Test compliance of the RTensor methods with the numpy interface; """"""; """"""; Test np.transpose vs RTensor::Transpose; """"""; """"""; Test np.expand_dims vs RTensor::ExpandDims; """"""; """"""; Test np.squeeze vs RTensor::Squeeze; """"""; """"""; Test np.reshape vs RTensor::Reshape; """"""; """"""; Test slicing operations; """"""; # We know that we differ in this case since numpy; # does only squeeze the dimensions of a slice if; # the dimension is requested by a single index.",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/rtensor.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/rtensor.py
Performance,optimiz,optimization,"'''; Test the iteration over the iterator of an iterator. This breaks if __iter__ or tp_iter is not defined for the iterator and causes; issues, e.g., in comparison to numpy arrays.; '''; # Prevent potential optimization of the loop; '''; Test that RVec is accessible from the ROOT and ROOT::VecOps namespace; '''",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/rvec.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/rvec.py
Security,access,accessible,"'''; Test the iteration over the iterator of an iterator. This breaks if __iter__ or tp_iter is not defined for the iterator and causes; issues, e.g., in comparison to numpy arrays.; '''; # Prevent potential optimization of the loop; '''; Test that RVec is accessible from the ROOT and ROOT::VecOps namespace; '''",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/rvec.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/rvec.py
Integrability,interface,interface,"""""""; Tests for the AsRVec feature enabling to adopt memory of Python objects; with an array interface member using RVec as C++ container.; """"""; # Helpers; # Tests; """"""; Test adoption of numpy arrays with different data types; """"""; """"""; Test adoption of multi-dimensional numpy arrays; """"""; """"""; Test adoption of numpy array with size 0; """"""; """"""; Test adoption of RVecs; """"""; """"""; Test ownership of returned RVec (to be owned by Python); """"""; """"""; Test __adopted__ attribute of returned RVecs; """"""; """"""; Test reference count of returned RVec. We expect a refcount of 2 for the RVec because the call to sys.getrefcount; creates a second reference by itself.; We attach the adopted pyobject to the RVec and increase the refcount of the; numpy array. After deletion of the rvec, the refcount of the numpy array; is decreased.; """"""",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/rvec_asrvec.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/rvec_asrvec.py
Modifiability,variab,variables,"""""""; Tests for the pythonizations of std::set.; """"""; """"""; Test that a std::set of char behaves as a Python set.; """"""; """"""; Instantiate std::set with different types.; """"""; """"""; Test that the boolean conversion of a std::set works as expected.; https://github.com/root-project/root/issues/14573; """"""; """"""; Test that a TTree with a std::set branch behaves as expected.; """"""; # Create variables to store std::vector elements; # Create branches in the TTree",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/stl_set.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/stl_set.py
Modifiability,variab,variables,"""""""; Tests for the pythonizations of std::vector. ; """"""; """"""; Test that calling std::vector<char>::data() returns a Python string; that contains the characters of the vector and no exception is raised.; Check also that the iteration over the vector runs normally (#9632).; """"""; """"""; Test that creating a std::vector<const char*> does not raise any; exception (#11581).; """"""; """"""; Test that the boolean conversion of a std::vector works as expected.; https://github.com/root-project/root/issues/14573; """"""; """"""; Test that the boolean conversion of a std::vector works as expected inside a TTree.; Also checks that the contents are correctly filled and read back.; https://github.com/root-project/root/issues/14573; """"""; # Create a TTree; # list of random arrays with lengths between 0 and 5 (0 is always included); # Create variables to store std::vector elements; # Create branches in the TTree; # Fill the TTree with 100 entries; # numpy arrays cannot be converted to bool",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/stl_vector.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/stl_vector.py
Availability,avail,availability,"""""""; Test the availability of std::string_view; """"""; # Create file.root to avoid errors in the RDF constructor",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/string_view.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/string_view.py
Safety,avoid,avoid,"""""""; Test the availability of std::string_view; """"""; # Create file.root to avoid errors in the RDF constructor",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/string_view.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/string_view.py
Security,access,access,"""""""; Test for the pythonization that allows to access the number of elements of a; TArray (or subclass) by calling `len` on it.; """"""; # Helpers; # Tests",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/tarray_len.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/tarray_len.py
Security,access,access,"""""""; Test for the pythonization that allows to access the number of elements of a; TCollection (or subclass) by calling `len` on it.; """"""; # Helpers; # Tests",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/tcollection_len.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/tcollection_len.py
Modifiability,extend,extend,"""""""; Test for the Python-list-like methods added to TCollection (and subclasses):; append, remove, extend, count; """"""; # Helpers; # Prevent immediate deletion of C++ TObjects; # Tests; # Skip elements that were already there; # Check that `o` is indeed the last element; # Skip elements that were already there; # Compare with elements of second collection",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/tcollection_listmethods.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/tcollection_listmethods.py
Performance,concurren,concurrently,"""""""; Test of TContext used as context manager; """"""; """"""; Check status of gDirectory with default constructor.; """"""; # Create a file to change gDirectory; """"""; Check status of gDirectory with constructor taking a new directory.; """"""; """"""; Check status of gDirectory with constructor taking the previous directory and a new one.; """"""; """"""; Run all tests of this class sequentially.; The tests of this class rely on the current directory, which can be changed; unpredictably if they are run concurrently.; """"""",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/tcontext_contextmanager.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/tcontext_contextmanager.py
Testability,test,tests,"""""""; Test of TContext used as context manager; """"""; """"""; Check status of gDirectory with default constructor.; """"""; # Create a file to change gDirectory; """"""; Check status of gDirectory with constructor taking a new directory.; """"""; """"""; Check status of gDirectory with constructor taking the previous directory and a new one.; """"""; """"""; Run all tests of this class sequentially.; The tests of this class rely on the current directory, which can be changed; unpredictably if they are run concurrently.; """"""",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/tcontext_contextmanager.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/tcontext_contextmanager.py
Performance,cache,cached,"""""""; Test for the attr syntax and Get method of TDirectoryFile.; """"""; # Setup; # this must be there otherwise the histogram is not attached to dir0; # Tests; # check that object is not cached initially; # check that the value in __dict__ is actually the object; # inside the directory",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/tdirectoryfile_attrsyntax_get.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/tdirectoryfile_attrsyntax_get.py
Performance,cache,cached,"""""""; Test for the getitem syntax of TDirectory.; """"""; # Setup; # this must be there otherwise the histogram is not attached to dir0; # Tests; # check that object is not cached initially; # check that the cached value in is actually the object; # inside the directory",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/tdirectory_attrsyntax.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/tdirectory_attrsyntax.py
Performance,cache,cached,"""""""; Test for the TFile.Open factory like creation of TFile; """"""; # Setup; # Tests; # check that object is not cached initially; # check that the value in __dict__ is actually the object; # inside the directory; # check that an OSError is raised when an inexistent file is opened; # both with a string and an instance of TFileOpenHandle as arguments; """"""; Test that the TKey related to a histogram in the file contains the; histogram title as described in #9989.; """"""",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/tfile_attrsyntax_get_writeobject_open.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/tfile_attrsyntax_get_writeobject_open.py
Security,access,access,"""""""; Test of TFile used as context manager; """"""; """"""; Check status of the TFile after the context manager and correctness of; the data it contains.; """"""; # The TFile object is still there; # And it is correctly closed; """"""; Write a histogram in a file within a context manager, using TDirectory::WriteObject.; """"""; """"""; Write a histogram in a file within a context manager, using TH1::Write.; """"""; """"""; Write a histogram in a file within a context manager, using TFile::Write.; """"""; """"""; Detach histogram from file and access it outside of the context, both when writing and reading.; """"""",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/tfile_context_manager.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/tfile_context_manager.py
Energy Efficiency,charge,charges,"""""""; Tests for passing Python callables when constructing TFX classes. This feature is not implemented by a PyROOT pythonization, but by a converter of; Cppyy that creates a C++ wrapper to invoke the Python callable.; """"""; """"""; Test passing Python callables to ROOT::TF1; """"""; """"""; Test simple function without parameters; """"""; """"""; Test function with parameters; """"""; """"""; Test function provided as callable; """"""; """"""; Test fitting a histogram to a Python function; """"""; # Gaus function; # scale; # mean; # standard deviation; # Sample gauss in histogram; # Normalize as density; # Fit to histogram and get parameters; """"""; Test the 2D Numpy array pythonisations for TF1::EvalPar; """"""; # x dataset: 5 pairs of particle charges; # Distance between charges r; # Coulomb constant k (in N·m²/C²); # Additional factor for modulation; # Slice to avoid the dummy column of 10's; """"""; Test the 2D NumPy pythonisations with dynamic TF1 data dimensions; """"""; # Here we do not set the ndims, defaults to 1; # x dataset with ndims 3; """"""; Test passing Python callables to ROOT::TF2; """"""; """"""; Test function with parameters; """"""; """"""; Test passing Python callables to ROOT::TF3; """"""; """"""; Test function with parameters; """"""",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/tf_pycallables.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/tf_pycallables.py
Integrability,wrap,wrapper,"""""""; Tests for passing Python callables when constructing TFX classes. This feature is not implemented by a PyROOT pythonization, but by a converter of; Cppyy that creates a C++ wrapper to invoke the Python callable.; """"""; """"""; Test passing Python callables to ROOT::TF1; """"""; """"""; Test simple function without parameters; """"""; """"""; Test function with parameters; """"""; """"""; Test function provided as callable; """"""; """"""; Test fitting a histogram to a Python function; """"""; # Gaus function; # scale; # mean; # standard deviation; # Sample gauss in histogram; # Normalize as density; # Fit to histogram and get parameters; """"""; Test the 2D Numpy array pythonisations for TF1::EvalPar; """"""; # x dataset: 5 pairs of particle charges; # Distance between charges r; # Coulomb constant k (in N·m²/C²); # Additional factor for modulation; # Slice to avoid the dummy column of 10's; """"""; Test the 2D NumPy pythonisations with dynamic TF1 data dimensions; """"""; # Here we do not set the ndims, defaults to 1; # x dataset with ndims 3; """"""; Test passing Python callables to ROOT::TF2; """"""; """"""; Test function with parameters; """"""; """"""; Test passing Python callables to ROOT::TF3; """"""; """"""; Test function with parameters; """"""",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/tf_pycallables.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/tf_pycallables.py
Safety,avoid,avoid,"""""""; Tests for passing Python callables when constructing TFX classes. This feature is not implemented by a PyROOT pythonization, but by a converter of; Cppyy that creates a C++ wrapper to invoke the Python callable.; """"""; """"""; Test passing Python callables to ROOT::TF1; """"""; """"""; Test simple function without parameters; """"""; """"""; Test function with parameters; """"""; """"""; Test function provided as callable; """"""; """"""; Test fitting a histogram to a Python function; """"""; # Gaus function; # scale; # mean; # standard deviation; # Sample gauss in histogram; # Normalize as density; # Fit to histogram and get parameters; """"""; Test the 2D Numpy array pythonisations for TF1::EvalPar; """"""; # x dataset: 5 pairs of particle charges; # Distance between charges r; # Coulomb constant k (in N·m²/C²); # Additional factor for modulation; # Slice to avoid the dummy column of 10's; """"""; Test the 2D NumPy pythonisations with dynamic TF1 data dimensions; """"""; # Here we do not set the ndims, defaults to 1; # x dataset with ndims 3; """"""; Test passing Python callables to ROOT::TF2; """"""; """"""; Test function with parameters; """"""; """"""; Test passing Python callables to ROOT::TF3; """"""; """"""; Test function with parameters; """"""",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/tf_pycallables.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/tf_pycallables.py
Usability,simpl,simple,"""""""; Tests for passing Python callables when constructing TFX classes. This feature is not implemented by a PyROOT pythonization, but by a converter of; Cppyy that creates a C++ wrapper to invoke the Python callable.; """"""; """"""; Test passing Python callables to ROOT::TF1; """"""; """"""; Test simple function without parameters; """"""; """"""; Test function with parameters; """"""; """"""; Test function provided as callable; """"""; """"""; Test fitting a histogram to a Python function; """"""; # Gaus function; # scale; # mean; # standard deviation; # Sample gauss in histogram; # Normalize as density; # Fit to histogram and get parameters; """"""; Test the 2D Numpy array pythonisations for TF1::EvalPar; """"""; # x dataset: 5 pairs of particle charges; # Distance between charges r; # Coulomb constant k (in N·m²/C²); # Additional factor for modulation; # Slice to avoid the dummy column of 10's; """"""; Test the 2D NumPy pythonisations with dynamic TF1 data dimensions; """"""; # Here we do not set the ndims, defaults to 1; # x dataset with ndims 3; """"""; Test passing Python callables to ROOT::TF2; """"""; """"""; Test function with parameters; """"""; """"""; Test passing Python callables to ROOT::TF3; """"""; """"""; Test function with parameters; """"""",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/tf_pycallables.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/tf_pycallables.py
Availability,error,error,"""""""; Test for the pythonization of TGraph, TGraph2D and their error; subclasses, in particular of their X,Y,Z coordinates and errors; getters, which sets the size of the returned buffers.; """"""; # Tests; # x and y are buffers of doubles; # We can get the size of the buffers; # The buffers are iterable; # x, y, z, ex, ey and ez are buffers of doubles; # We can get the size of the buffers; # The buffers are iterable; # All of the next calls return C-style arrays of doubles; # In cppyy they are converted to 'LowLevelView' objects; # The Pythonizations of the methods make sure to call 'reshape'; # So that cppyy can understand the shape of the arrays.; # All of the next calls return C-style arrays of doubles; # In cppyy they are converted to 'LowLevelView' objects; # The Pythonizations of the methods make sure to call 'reshape'; # So that cppyy can understand the shape of the arrays.; # All of the next calls return C-style arrays of doubles; # In cppyy they are converted to 'LowLevelView' objects; # The Pythonizations of the methods make sure to call 'reshape'; # So that cppyy can understand the shape of the arrays.",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/tgraph_getters.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/tgraph_getters.py
Security,access,accessible,"""""""; Test that the method pulled in via using decls from TH1 are accessible; """"""; # Tests",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/th2.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/th2.py
Availability,redundant,redundant,"""""""; Test for the pythonization that allows instances of TIter to; behave as Python iterators.; """"""; # Helpers; # Prevent immediate deletion of C++ TObjects; # Tests; # Check that TIter instances are iterable; # An iterator of an iterator is itself; # Check that TIter instances are iterators; # Somehow redundant, but good to test with real syntax",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/titer_iterator.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/titer_iterator.py
Safety,redund,redundant,"""""""; Test for the pythonization that allows instances of TIter to; behave as Python iterators.; """"""; # Helpers; # Prevent immediate deletion of C++ TObjects; # Tests; # Check that TIter instances are iterable; # An iterator of an iterator is itself; # Check that TIter instances are iterators; # Somehow redundant, but good to test with real syntax",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/titer_iterator.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/titer_iterator.py
Testability,test,test,"""""""; Test for the pythonization that allows instances of TIter to; behave as Python iterators.; """"""; # Helpers; # Prevent immediate deletion of C++ TObjects; # Tests; # Check that TIter instances are iterable; # An iterator of an iterator is itself; # Check that TIter instances are iterators; # Somehow redundant, but good to test with real syntax",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/titer_iterator.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/titer_iterator.py
Security,access,access,"""""""; Test for the item access methods added to TSeqCollection (and subclasses):; __getitem__, __setitem__, __delitem__.; Both the index (l[i]) and slice (l[i:j:k]) syntaxes are tested.; """"""; # Helpers; # Prevent immediate deletion of C++ TObjects; # Tests; # Get items; # Get items, negative indices; # Check invalid index cases; # All items; # First two items; # Last two items; # First and third items; # All items, reverse order; # First and third items, reverse order; # Step cannot be zero; # Set items; # Check previously set items; # Set items, negative indices; # Check previously set items; # Check invalid index cases; # Replace all items; # Append items; # first half; # second half; # Assign second item.; # This time use a Python list as assigned value; # Assign second and third items to just one item.; # This tests that the third item is removed; # Assign with step; # Assign with step (start from end); # Step cannot be zero; # Delete all elements; # Delete o2; # Only o1s should be there; # Check invalid index cases; # Delete all items; # Do not delete anything (slice out of range); # Delete first two items; # Delete first and third items; # Delete first and third items (start from end); # Step cannot be zero",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/tseqcollection_itemaccess.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/tseqcollection_itemaccess.py
Testability,test,tested,"""""""; Test for the item access methods added to TSeqCollection (and subclasses):; __getitem__, __setitem__, __delitem__.; Both the index (l[i]) and slice (l[i:j:k]) syntaxes are tested.; """"""; # Helpers; # Prevent immediate deletion of C++ TObjects; # Tests; # Get items; # Get items, negative indices; # Check invalid index cases; # All items; # First two items; # Last two items; # First and third items; # All items, reverse order; # First and third items, reverse order; # Step cannot be zero; # Set items; # Check previously set items; # Set items, negative indices; # Check previously set items; # Check invalid index cases; # Replace all items; # Append items; # first half; # second half; # Assign second item.; # This time use a Python list as assigned value; # Assign second and third items to just one item.; # This tests that the third item is removed; # Assign with step; # Assign with step (start from end); # Step cannot be zero; # Delete all elements; # Delete o2; # Only o1s should be there; # Check invalid index cases; # Delete all items; # Do not delete anything (slice out of range); # Delete first two items; # Delete first and third items; # Delete first and third items (start from end); # Step cannot be zero",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/tseqcollection_itemaccess.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/tseqcollection_itemaccess.py
Security,hash,hash,"""""""; Test for the Python-list-like methods added to TSeqCollection; (and subclasses): insert, pop, reverse, sort, index; """"""; # Helpers; # Tests; # Insert with positive index; # Insert with negative index (starts from end); # Insert with index beyond lower boundary.; # Inserts at the beginning; # Insert with index beyond upper boundary.; # Inserts at the end; # No arguments, pop last item; # Pop first item, positive index; # Pop last item, negative index; # Pop from empty collection; # Index out of range, positive; # Index out of range, negative; # Pop with non-integer argument; # Pop a repeated element; # Empty collection; # Regular sort, rely on TList::Sort; # We need to set `key` until the pythonization to; # make TObjString comparable is there; # Python sort, key and reverse arguments.; # Sort by hash in reverse order; # Empty collection; # Check all elements of collection; # Check element not in collection",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/tseqcollection_listmethods.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/tseqcollection_listmethods.py
Testability,test,testing,"""""""; Test for the pythonization of TTree::Branch, which allows to pass proxy; references as arguments from the Python side. Example:; `v = ROOT.std.vector('int')()`; `t.Branch('my_vector_branch', v)`; """"""; # Setup; """"""; struct MyStruct {; int myint1;; int myint2;; };; """"""; # Helpers; # Prevent double deletion of the tree (Python and C++ TFile); # Tests; # Basic type and array do not actually need the pythonization,; # but testing anyway for the sake of completeness; # dtype='float64'; # Struct and vector do benefit from the pythonization; # Test overloads; # Use `addressof` to get the address of the struct members; # Test aliases; # Test overloads; # Test an overload that uses the original Branch proxy",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/ttree_branch.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/ttree_branch.py
Security,access,access,"""""""; Test for the pythonization that allows to access top-level tree branches/leaves as attributes; (i.e. `mytree.mybranch`). Since this pythonization is common to TTree and its subclasses, TChain, TNtuple; and TNtupleD are also tested here.; """"""; # Setup; # Helpers; # Prevent double deletion of the tree (Python and C++ TFile); # Read first entry; # Read first entry; # Tests; # Synchronized with arraysizeInner in TreeHelper.h",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/ttree_branch_attr.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/ttree_branch_attr.py
Testability,test,tested,"""""""; Test for the pythonization that allows to access top-level tree branches/leaves as attributes; (i.e. `mytree.mybranch`). Since this pythonization is common to TTree and its subclasses, TChain, TNtuple; and TNtupleD are also tested here.; """"""; # Setup; # Helpers; # Prevent double deletion of the tree (Python and C++ TFile); # Read first entry; # Read first entry; # Tests; # Synchronized with arraysizeInner in TreeHelper.h",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/ttree_branch_attr.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/ttree_branch_attr.py
Testability,test,tested,"""""""; Test for the pythonization that makes TTree instances iterable in Python. ; For example, this allows to do:; `for event in mytree:`; `...`. Since this pythonization is common to TTree and its subclasses, TChain, TNtuple; and TNtupleD are also tested here.; """"""; # Setup; # Helpers; # Prevent double deletion of the tree (Python and C++ TFile); # Tests",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/ttree_iterable.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/ttree_iterable.py
Availability,error,error,"""""""; Test for the pythonization of TTree::SetBranchAddress, which allows to pass proxy; references as arguments from the Python side. Example:; `v = ROOT.std.vector('int')()`; `t.SetBranchAddress(""my_vector_branch"", v)`. Since this pythonization is common to TTree and its subclasses, TChain, TNtuple; and TNtupleD are also tested here.; """"""; # Setup; # Helpers; # Prevent double deletion of the tree (Python and C++ TFile); # Tests; # Basic type, array and struct leaf list do not actually need the pythonization,; # but testing anyway for the sake of completeness; # should return status 0 for success; # check if type mismatch is correctly detected; # should return error code because of type mismatch; # dtype='float64'; # should return status 0 for success; # check if type mismatch is correctly detected; # should return error code because of type mismatch; # Vector and struct do benefit from the pythonization; # Test an overload that uses the original SetBranchAddress proxy; # 6468",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/ttree_setbranchaddress.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/ttree_setbranchaddress.py
Safety,detect,detected,"""""""; Test for the pythonization of TTree::SetBranchAddress, which allows to pass proxy; references as arguments from the Python side. Example:; `v = ROOT.std.vector('int')()`; `t.SetBranchAddress(""my_vector_branch"", v)`. Since this pythonization is common to TTree and its subclasses, TChain, TNtuple; and TNtupleD are also tested here.; """"""; # Setup; # Helpers; # Prevent double deletion of the tree (Python and C++ TFile); # Tests; # Basic type, array and struct leaf list do not actually need the pythonization,; # but testing anyway for the sake of completeness; # should return status 0 for success; # check if type mismatch is correctly detected; # should return error code because of type mismatch; # dtype='float64'; # should return status 0 for success; # check if type mismatch is correctly detected; # should return error code because of type mismatch; # Vector and struct do benefit from the pythonization; # Test an overload that uses the original SetBranchAddress proxy; # 6468",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/ttree_setbranchaddress.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/ttree_setbranchaddress.py
Testability,test,tested,"""""""; Test for the pythonization of TTree::SetBranchAddress, which allows to pass proxy; references as arguments from the Python side. Example:; `v = ROOT.std.vector('int')()`; `t.SetBranchAddress(""my_vector_branch"", v)`. Since this pythonization is common to TTree and its subclasses, TChain, TNtuple; and TNtupleD are also tested here.; """"""; # Setup; # Helpers; # Prevent double deletion of the tree (Python and C++ TFile); # Tests; # Basic type, array and struct leaf list do not actually need the pythonization,; # but testing anyway for the sake of completeness; # should return status 0 for success; # check if type mismatch is correctly detected; # should return error code because of type mismatch; # dtype='float64'; # should return status 0 for success; # check if type mismatch is correctly detected; # should return error code because of type mismatch; # Vector and struct do benefit from the pythonization; # Test an overload that uses the original SetBranchAddress proxy; # 6468",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/ttree_setbranchaddress.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/ttree_setbranchaddress.py
Integrability,inject,injected,"""""""; Test for RooAbsCollection pythonizations.; """"""; # Setup; # Clear TList before Python list deletes the objects; # Parametrized tests for RooAbsCollection child classes; # The next variable has a duplicate name on purpose the check if it's; # really the name that is used as the key in RooAbsCollections.; # expected behaviour; # ensure consistency with RooAbsCollection::find; # check if negative indexing works; # Tests; # The RooArgList doesn't support string keys; # The RooArgSet supports string keys; # Check explicitly that both overloads of addClone work.; # addClone(const RooAbsCollection& list); # addClone(const RooAbsArg& var); """"""STL sequence iterator injected in RooAbsCollection, inherited by RooArgSet""""""; # ROOT-10606",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/roofit/rooabscollection.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/roofit/rooabscollection.py
Modifiability,variab,variable,"""""""; Test for RooAbsCollection pythonizations.; """"""; # Setup; # Clear TList before Python list deletes the objects; # Parametrized tests for RooAbsCollection child classes; # The next variable has a duplicate name on purpose the check if it's; # really the name that is used as the key in RooAbsCollections.; # expected behaviour; # ensure consistency with RooAbsCollection::find; # check if negative indexing works; # Tests; # The RooArgList doesn't support string keys; # The RooArgSet supports string keys; # Check explicitly that both overloads of addClone work.; # addClone(const RooAbsCollection& list); # addClone(const RooAbsArg& var); """"""STL sequence iterator injected in RooAbsCollection, inherited by RooArgSet""""""; # ROOT-10606",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/roofit/rooabscollection.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/roofit/rooabscollection.py
Security,inject,injected,"""""""; Test for RooAbsCollection pythonizations.; """"""; # Setup; # Clear TList before Python list deletes the objects; # Parametrized tests for RooAbsCollection child classes; # The next variable has a duplicate name on purpose the check if it's; # really the name that is used as the key in RooAbsCollections.; # expected behaviour; # ensure consistency with RooAbsCollection::find; # check if negative indexing works; # Tests; # The RooArgList doesn't support string keys; # The RooArgSet supports string keys; # Check explicitly that both overloads of addClone work.; # addClone(const RooAbsCollection& list); # addClone(const RooAbsArg& var); """"""STL sequence iterator injected in RooAbsCollection, inherited by RooArgSet""""""; # ROOT-10606",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/roofit/rooabscollection.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/roofit/rooabscollection.py
Testability,test,tests,"""""""; Test for RooAbsCollection pythonizations.; """"""; # Setup; # Clear TList before Python list deletes the objects; # Parametrized tests for RooAbsCollection child classes; # The next variable has a duplicate name on purpose the check if it's; # really the name that is used as the key in RooAbsCollections.; # expected behaviour; # ensure consistency with RooAbsCollection::find; # check if negative indexing works; # Tests; # The RooArgList doesn't support string keys; # The RooArgSet supports string keys; # Check explicitly that both overloads of addClone work.; # addClone(const RooAbsCollection& list); # addClone(const RooAbsArg& var); """"""STL sequence iterator injected in RooAbsCollection, inherited by RooArgSet""""""; # ROOT-10606",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/roofit/rooabscollection.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/roofit/rooabscollection.py
Availability,error,error,"""""""; Test for the FitTo callable.; """"""; # after every fit, we have to reset the initial values because; # otherwise the fit result is not considered the same; # test that kwargs can be passed; # and lead to correct result; # test that AttributeError is raised; # if keyword does not correspong to CmdArg; # test that fitting with keyword arguments leads to the same result; # as doing the same fit with passed ROOT objects; # test that no error is causes if python style and cpp style; # args are provided to fitto and that results are identical",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/roofit/rooabspdf_fitto.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/roofit/rooabspdf_fitto.py
Testability,test,test,"""""""; Test for the FitTo callable.; """"""; # after every fit, we have to reset the initial values because; # otherwise the fit result is not considered the same; # test that kwargs can be passed; # and lead to correct result; # test that AttributeError is raised; # if keyword does not correspong to CmdArg; # test that fitting with keyword arguments leads to the same result; # as doing the same fit with passed ROOT objects; # test that no error is causes if python style and cpp style; # args are provided to fitto and that results are identical",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/roofit/rooabspdf_fitto.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/roofit/rooabspdf_fitto.py
Availability,error,error,"""""""; Test for the PlotOn callable.; """"""; # test that kwargs can be passed; # and lead to correct result; # test that AttributeError is raised; # if keyword does not correspong to CmdArg; # test that fitting with keyword arguments leads to the same result; # as doing the same plot with passed ROOT objects; # test that no error is causes if python style and cpp style; # args are provided to plotOn and that results are identical",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/roofit/rooabsreal_ploton.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/roofit/rooabsreal_ploton.py
Testability,test,test,"""""""; Test for the PlotOn callable.; """"""; # test that kwargs can be passed; # and lead to correct result; # test that AttributeError is raised; # if keyword does not correspong to CmdArg; # test that fitting with keyword arguments leads to the same result; # as doing the same plot with passed ROOT objects; # test that no error is causes if python style and cpp style; # args are provided to plotOn and that results are identical",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/roofit/rooabsreal_ploton.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/roofit/rooabsreal_ploton.py
Testability,test,test,"""""""; Test for RooArgList pythonizations.; """"""; # General check that the conversion from string or tuple works, using; # constants to get compact test code.; # Let's make sure that we can add two arguments with the same name to; # the RooArgList. Here, we try to add the same RooConst two times. The; # motivation for this test if the RooArgList is created via an; # intermediate RooArgSet, which should not happen.",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/roofit/rooarglist.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/roofit/rooarglist.py
Testability,test,test,"""""""; Initially, this was a test for the pythonization that allowed; RooDataHist to use the overloads of plotOn defined in RooAbsData.; Currently, such functionality is automatically provided by Cppyy; and ROOT meta: the overloads obtained with 'using' declarations; are taken into account when calling a method.; We keep this test to check that the aforementioned functionality; works properly in a case that is important for RooFit.; """"""; # Helpers; # Inspired by the code of rf402_datahandling.py; # Tests; # Overload in RooDataHist; # RooPlot* RooDataHist::plotOn(RooPlot* frame, RooAbsData::PlotOpt o); # Overload taken from RooAbsData; # RooPlot* RooAbsData::plotOn(RooPlot* frame, const RooCmdArg& arg1 = {},; # const RooCmdArg& arg2 = {}, const RooCmdArg& arg3 = {},; # const RooCmdArg& arg4 = {}, const RooCmdArg& arg5 = {},; # const RooCmdArg& arg6 = {}, const RooCmdArg& arg7 = {},; # const RooCmdArg& arg8 = {}); # Overload taken from RooAbsData; # RooPlot* RooAbsData::plotOn(RooPlot* frame, const RooLinkedList& cmdList)",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/roofit/roodatahist_ploton.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/roofit/roodatahist_ploton.py
Modifiability,variab,variable,"""""""Basic test with a real value and a category.""""""; """"""Test with a weighted dataset.""""""; """"""Test if the optional computation of derived weights works.""""""; # Create a data set with a derived weight; """"""Test exporting to numpy and then importing back a non-weighted dataset.""""""; """"""Test exporting to numpy and then importing back a weighted dataset.""""""; # Construct formula to calculate (fake) weight for events; # Add column with variable w to previously generated dataset; # Instruct dataset wdata to use w as event weight and not observable; """"""Test that rows with out-of-range values are skipped, both for; real-valued columns and categories.; """"""; # Dataset with ""x"" randomly distributed between -3 and 3, and ""cat""; # being either -1, 0, or +1.; # The RooFit variable ""x"" is only defined from -1 to 2, and the; # category doesn't have the 0-state.; # Use manual loop because we had some problems with numpys boolean; # comparisons in the past (see GitHub issue #12162).; """"""Test whether the import also works with non-contiguous arrays.; Covers GitHub issue #13605.; """"""; # so that all combination of values are in the dataset; # To make sure the array is really not C-contiguous",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/roofit/roodataset_numpy.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/roofit/roodataset_numpy.py
Testability,test,test,"""""""Basic test with a real value and a category.""""""; """"""Test with a weighted dataset.""""""; """"""Test if the optional computation of derived weights works.""""""; # Create a data set with a derived weight; """"""Test exporting to numpy and then importing back a non-weighted dataset.""""""; """"""Test exporting to numpy and then importing back a weighted dataset.""""""; # Construct formula to calculate (fake) weight for events; # Add column with variable w to previously generated dataset; # Instruct dataset wdata to use w as event weight and not observable; """"""Test that rows with out-of-range values are skipped, both for; real-valued columns and categories.; """"""; # Dataset with ""x"" randomly distributed between -3 and 3, and ""cat""; # being either -1, 0, or +1.; # The RooFit variable ""x"" is only defined from -1 to 2, and the; # category doesn't have the 0-state.; # Use manual loop because we had some problems with numpys boolean; # comparisons in the past (see GitHub issue #12162).; """"""Test whether the import also works with non-contiguous arrays.; Covers GitHub issue #13605.; """"""; # so that all combination of values are in the dataset; # To make sure the array is really not C-contiguous",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/roofit/roodataset_numpy.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/roofit/roodataset_numpy.py
Testability,test,test,"""""""; Tests for the RooLinkedList.; """"""; # test if we can correctly iterate over a RooLinkedList, also in; # reverse.",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/roofit/roolinkedlist.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/roofit/roolinkedlist.py
Modifiability,variab,variables,"""""""; Test for the pythonizations of RooWorkspace.; """"""; # Setup; # Tests; # Prepare workspace with variables and a PDF; # Test that rename argument has worked; # Test to check if new variables are created; # Test to check if new p.d.f.s are created; # Test to check if new variables are created; # Test to check if new functions are created; # Test to check if new p.d.f.s are created",MatchSource.CODE_COMMENT,bindings/pyroot/pythonizations/test/roofit/rooworkspace.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/pythonizations/test/roofit/rooworkspace.py
Testability,test,testTPythonExec,"""""""; Testing features of TPython from Python, to see if they still work when the; Python interpreter was not initialized by TPython on the C++ side.; """"""; """"""; Test TPython::Exec.; """"""; """""". // Test TPython::Exec from multiple threads.; int testTPythonExec(int nIn); {; std::any out;; std::stringstream cmd;; cmd << ""_anyresult = ROOT.std.make_any['int']("" << nIn << "")"";; TPython::Exec(cmd.str().c_str(), &out);; return std::any_cast<int>(out);; }; """"""",MatchSource.CODE_COMMENT,bindings/tpython/test/test_tpython.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/tpython/test/test_tpython.py
Testability,test,test,"------------; """"""; Search for the include paths after the line; 'static const char* includePaths[]'; Return them as list; """"""; # remove the "","" and the two '""'; #-------------------------------------------------------------------------------; """"""; Add undefines and defines if the directory needs them; """"""; #-------------------------------------------------------------------------------; #-------------------------------------------------------------------------------; """"""; Extract the linkdef files; """"""; #-------------------------------------------------------------------------------; # now get the ones in the inc directory; #-------------------------------------------------------------------------------; #-------------------------------------------------------------------------------; """"""; Sort, clean, no duplicates; cat $cppflags.tmp | sort | uniq | grep -v $srcdir | grep -v `pwd` > $cppflags; We must resolve softlinks.; returns a string; """"""; #-------------------------------------------------------------------------------; #-------------------------------------------------------------------------------; #-------------------------------------------------------------------------------; #-------------------------------------------------------------------------------; """""" Get extra headers which do not fall in other special categories; """"""; #-------------------------------------------------------------------------------; """""" remove unwanted headers, e.g. the ones used for dictionaries but not desirable in the pch; """"""; #-------------------------------------------------------------------------------; """"""; Create the input for the pch file, i.e. 3 files:; * etc/dictpch/allLinkDefs.h; * etc/dictpch/allHeaders.h; * etc/dictpch/allCppflags.txt; """"""; # Make sure we don't get warnings from the old RooFit test statistics; # headers that are deprecated. This line can be removed once the deprecaded; # headers are gone (ROOT 6.32.00):; # Loop over the dictionaries, ROOT modules",MatchSource.CODE_COMMENT,cmake/unix/makepchinput.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/cmake/unix/makepchinput.py
Energy Efficiency,reduce,reduce,"#! /usr/bin/env python; '''; An utility to smartly ""cat"" rootmap files.; '''; #-------------------------------------------------------------------------------; #-------------------------------------------------------------------------------; # Now we reduce the fwd declarations; #-------------------------------------------------------------------------------; #-------------------------------------------------------------------------------",MatchSource.CODE_COMMENT,cmake/unix/rootmapcat.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/cmake/unix/rootmapcat.py
Availability,avail,available,"""""""ROOTs Object-Oriented Technologies.\n; root is an interactive interpreter of C++ code. It uses the ROOT framework. For more information on ROOT, please refer to\n; An extensive Users Guide is available from that site (see below).; """"""",MatchSource.CODE_COMMENT,core/base/src/root-argparse.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/base/src/root-argparse.py
Modifiability,inherit,inheriting,"e names. The (optional) file LinkDef.h looks like:. #ifdef __CLING__. #pragma link off all globals;; #pragma link off all classes;; #pragma link off all functions;. #pragma link C++ class TAxis;; #pragma link C++ class TAttAxis-;; #pragma link C++ class TArrayC-!;; #pragma link C++ class AliEvent+;. #pragma link C++ function StrDup;; #pragma link C++ function operator+(const TString&,const TString&);. #pragma link C++ global gROOT;; #pragma link C++ global gEnv;. #pragma link C++ enum EMessageTypes;. #endif. This file tells rootcling which classes will be persisted on disk and what; entities will trigger automatic load of the shared library which contains; it. A trailing - in the class name tells rootcling to not generate the; Streamer() method. This is necessary for those classes that need a; customized Streamer() method. A trailing ! in the class name tells rootcling; to not generate the operator>>(TBuffer &b, MyClass *&obj) function. This is; necessary to be able to write pointers to objects of classes not inheriting; from TObject. See for an example the source of the TArrayF class.; If the class contains a ClassDef macro, a trailing + in the class; name tells rootcling to generate an automatic Streamer(), i.e. a; streamer that let ROOT do automatic schema evolution. Otherwise, a; trailing + in the class name tells rootcling to generate a ShowMember; function and a Shadow Class. The + option is mutually exclusive with; the - option. For legacy reasons it is not yet the default.; When the linkdef file is not specified a default version exporting; the classes with the names equal to the include files minus the .h; is generated. The default constructor used by the ROOT I/O can be customized by; using the rootcling pragma:; #pragma link C++ ioctortype UserClass;; For example, with this pragma and a class named MyClass,; this method will called the first of the following 3; constructors which exists and is public:; MyClass(UserClass*);; MyClass(TRootIOCtor*);; MyClass",MatchSource.CODE_COMMENT,core/dictgen/src/rootcling-argparse.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/dictgen/src/rootcling-argparse.py
Performance,load,load,"st argument on the rootcling command line.; 2) Note that the LinkDef file name must contain the string:; LinkDef.h, Linkdef.h or linkdef.h, i.e. NA49_LinkDef.h. Before specifying the first header file one can also add include; file directories to be searched and preprocessor defines, like:; -I$MYPROJECT/include -DDebug=1. NOTA BENE: the dictionaries that will be used within the same project must; have unique names. The (optional) file LinkDef.h looks like:. #ifdef __CLING__. #pragma link off all globals;; #pragma link off all classes;; #pragma link off all functions;. #pragma link C++ class TAxis;; #pragma link C++ class TAttAxis-;; #pragma link C++ class TArrayC-!;; #pragma link C++ class AliEvent+;. #pragma link C++ function StrDup;; #pragma link C++ function operator+(const TString&,const TString&);. #pragma link C++ global gROOT;; #pragma link C++ global gEnv;. #pragma link C++ enum EMessageTypes;. #endif. This file tells rootcling which classes will be persisted on disk and what; entities will trigger automatic load of the shared library which contains; it. A trailing - in the class name tells rootcling to not generate the; Streamer() method. This is necessary for those classes that need a; customized Streamer() method. A trailing ! in the class name tells rootcling; to not generate the operator>>(TBuffer &b, MyClass *&obj) function. This is; necessary to be able to write pointers to objects of classes not inheriting; from TObject. See for an example the source of the TArrayF class.; If the class contains a ClassDef macro, a trailing + in the class; name tells rootcling to generate an automatic Streamer(), i.e. a; streamer that let ROOT do automatic schema evolution. Otherwise, a; trailing + in the class name tells rootcling to generate a ShowMember; function and a Shadow Class. The + option is mutually exclusive with; the - option. For legacy reasons it is not yet the default.; When the linkdef file is not specified a default version exporting; the classes with",MatchSource.CODE_COMMENT,core/dictgen/src/rootcling-argparse.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/dictgen/src/rootcling-argparse.py
Deployability,install,installed,"#!/usr/bin/env python; # Author: Pau Miquel i Mir <pau.miquel.mir@cern.ch> <pmm1g15@soton.ac.uk>>; # Date: July, 2016; #; # DISCLAIMER: This script is a prototype and a work in progress. Indeed, it is possible that; # it may not work for certain tutorials, and that it, or the tutorial, might need to be; # tweaked slightly to ensure full functionality. Please do not hesitate to email the author; # with any questions or with examples that do not work.; #; # HELP IT DOESN'T WORK: Two possible solutions:; # 1. If the tutorial takes a long time to execute (more than 90 seconds), add the name of the; # tutorial to the list of long tutorials listLongTutorials, in the function findTimeout.; # 2. Check that helper functions are recognised correctly in split(text).; #; # REQUIREMENTS: This script needs jupyter to be properly installed, as it uses the python; # package nbformat and calls the shell commands `jupyter nbconvert` and `jupyter trust`. The; # rest of the packages used should be included in a standard installation of python. The script; # is intended to be run on a UNIX based system.; #; #; # FUNCTIONING:; # -----------; # The converttonotebook script creates Jupyter notebooks from raw C++ or python files.; # Particularly, it is indicated to convert the ROOT tutorials found in the ROOT; # repository.; #; # The script should be called from bash with the following format:; # python /path/to/script/converttonotebook.py /path/to/<macro>.C /path/to/outdir; #; # Indeed the script takes two arguments, the path to the macro and the path to the directory; # where the notebooks will be created; #; # The script's general functioning is as follows. The macro to be converted is imported as a string.; # A series of modifications are made to this string, for instance delimiting where markdown and; # code cells begin and end. Then, this string is converted into ipynb format using a function; # in the nbconvert package. Finally, the notebook is executed and output.; #; # For convertin",MatchSource.CODE_COMMENT,documentation/doxygen/converttonotebook.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/doxygen/converttonotebook.py
Integrability,depend,depending,"uToNuE_Oscillation.cxx"" // so that it can be executed directly\n#else\n#include ""../tutorials/roostats/NuMuToNuE_Oscillation.cxx+"" // so that it can be executed directly\n""""""; """"""TString tutDir = gROOT->GetTutorialDir();\nTString headerDir = TString::Format(""#include \\\""%s/roostats/NuMuToNuE_Oscillation.h\\\"""", tutDir.Data());\nTString impDir = TString::Format(""#include \\\""%s/roostats/NuMuToNuE_Oscillation.cxx\\\"""", tutDir.Data());\ngROOT->ProcessLine(headerDir);\ngROOT->ProcessLine(impDir);""""""; """"""#include \""../test/Event.h\""""""""; """"""# <codecell>\nTString dir = ""$ROOTSYS/test/Event.h"";\ngSystem->ExpandPathName(dir);\nTString includeCommand = TString::Format(""#include \\\""%s\\\"""" , dir.Data());\ngROOT->ProcessLine(includeCommand);""""""; #declareNamespace,; """"""; Return True if extension is a C++ file; """"""; # -------------------------------------; # ------------ Main Program------------; # -------------------------------------; """"""; Main function. Calls all other functions, depending on whether the macro input is in python or c++.; It adds the header information. Also, it adds a cell that draws all canvases. The working text is; then converted to a version 3 jupyter notebook, subsequently updated to a version 4. Then, metadata; associated with the language the macro is written in is attatched to he notebook. Finally the; notebook is executed and output as a Jupyter notebook.; """"""; # Modify text from macros to suit a notebook; # Remove function, Unindent, and convert comments to Markdown cells; # Convert top level code comments to Markdown cells; # Construct text by starting with top level code, then the helper functions, and finally the main function.; # Also add cells for headerfile, or keepfunction; # Convert comments into Markdown cells; # Perform last minute fixes to the notebook, used for specific fixes needed by some tutorials; # Change to standard Markdown; # Add the title and header of the notebook; # Add cell at the end of the notebook that draws all the canvas",MatchSource.CODE_COMMENT,documentation/doxygen/converttonotebook.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/doxygen/converttonotebook.py
Modifiability,variab,variable,"ption of the tutorial; ... ##; ... ## \\macro_image; ... ## \\macro_code; ... ##; ... ## \\\\author John Brown; ... def tutorialfuncion()'''); ('def tutorialfuncion()\\n', 'This is the description of the tutorial\\n\\n\\n', 'John Brown', True, True, False, False); >>> readHeaderPython('''## \\file; ... ## \\ingroup tutorials; ... ## \\\\notebook -nodraw; ... ## This is the description of the tutorial; ... ##; ... ## \\macro_image; ... ## \\macro_code; ... ##; ... ## \\\\author John Brown; ... def tutorialfuncion()'''); ('def tutorialfuncion()\\n', 'This is the description of the tutorial\\n\\n\\n', 'John Brown', True, False, True, False); """"""; """"""; Converts comments delimited by # or ## and on a new line into a markdown cell.; For python files only; >>> pythonComments('''## This is a; ... ## multiline comment; ... def function()'''); '# <markdowncell>\\n## This is a\\n## multiline comment\\n# <codecell>\\ndef function()\\n'; >>> pythonComments('''def function():; ... variable = 5 # Comment not in cell; ... # Comment also not in cell'''); 'def function():\\n variable = 5 # Comment not in cell\\n # Comment also not in cell\\n'; """"""; # True if first line of comment; # True if first line after comment; """"""if __name__ == ""__main__"":""""""; """"""if __name__ == '__main__':""""""; """"""Replace formula tags used by doxygen to the ones used in notebooks.""""""; """"""; Extract author and description from header, eliminate header from text. Also returns; notebook boolean, which is True if the string \notebook is present in the header; Also determine options (-js, -nodraw, -header) passed in \notebook command, and; return their booleans; >>> readHeaderCpp('''/// \\file; ... /// \\ingroup tutorials; ... /// \\\\notebook; ... /// This is the description of the tutorial; ... ///; ... /// \\macro_image; ... /// \\macro_code; ... ///; ... /// \\\\author John Brown; ... void tutorialfuncion(){}'''); ('void tutorialfuncion(){}\\n', '# This is the description of the tutorial\\n# \\n# \\n', 'John Brown",MatchSource.CODE_COMMENT,documentation/doxygen/converttonotebook.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/doxygen/converttonotebook.py
Security,access,access,"notebook. Finally the; notebook is executed and output as a Jupyter notebook.; """"""; # Modify text from macros to suit a notebook; # Remove function, Unindent, and convert comments to Markdown cells; # Convert top level code comments to Markdown cells; # Construct text by starting with top level code, then the helper functions, and finally the main function.; # Also add cells for headerfile, or keepfunction; # Convert comments into Markdown cells; # Perform last minute fixes to the notebook, used for specific fixes needed by some tutorials; # Change to standard Markdown; # Add the title and header of the notebook; # Add cell at the end of the notebook that draws all the canvasses. Add a Markdown cell before explaining it.; # Create a notebook from the working text; # Upgrade v3 to v4; # Load notebook string into json format, essentially creating a dictionary; # add the corresponding metadata; # write the json file with the metadata; # Call commmand that executes the notebook and creates a new notebook with the output; # Work around glitches in NB conversion; # If notebook conversion did not work, try again without the option --execute; # Only remove notebook without output if nbconvert succeeds; # -------------------------------------; # ----- Preliminary definitions--------; # -------------------------------------; # Extract and define the name of the file as well as its derived names; #tutTitle = re.sub( r""([A-Z\d])"", r"" \1"", tutName).title(); # Extract output directory; # Find and define the time and date this script is run; # -------------------------------------; # -------------------------------------; # -------------------------------------; # Set DYLD_LIBRARY_PATH. When run without root access or as a different user, especially from Mac systems,; # it is possible for security reasons that the environment does not include this definition, so it is manually defined.; # Open the file to be converted; # Extract information from header and remove header from text",MatchSource.CODE_COMMENT,documentation/doxygen/converttonotebook.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/doxygen/converttonotebook.py
Testability,test,test,"nt2 = value2;\\nargument3 = value3;\\nargument4 = value4;\\n# <codecell>\\n'); >>> processmain('''TCanvas function(){; ... content of function; ... spanning several; ... lines; ... return c1; ... }'''); ('TCanvas function(){\\n content of function\\n spanning several \\n lines\\n return c1\\n}', ''); """"""; #, flags = re.DOTALL) #| re.MULTILINE); # now define text transformers; """"""#include ""../tutorials/roostats/NuMuToNuE_Oscillation.h""\n#include ""../tutorials/roostats/NuMuToNuE_Oscillation.cxx"" // so that it can be executed directly\n#else\n#include ""../tutorials/roostats/NuMuToNuE_Oscillation.cxx+"" // so that it can be executed directly\n""""""; """"""TString tutDir = gROOT->GetTutorialDir();\nTString headerDir = TString::Format(""#include \\\""%s/roostats/NuMuToNuE_Oscillation.h\\\"""", tutDir.Data());\nTString impDir = TString::Format(""#include \\\""%s/roostats/NuMuToNuE_Oscillation.cxx\\\"""", tutDir.Data());\ngROOT->ProcessLine(headerDir);\ngROOT->ProcessLine(impDir);""""""; """"""#include \""../test/Event.h\""""""""; """"""# <codecell>\nTString dir = ""$ROOTSYS/test/Event.h"";\ngSystem->ExpandPathName(dir);\nTString includeCommand = TString::Format(""#include \\\""%s\\\"""" , dir.Data());\ngROOT->ProcessLine(includeCommand);""""""; #declareNamespace,; """"""; Return True if extension is a C++ file; """"""; # -------------------------------------; # ------------ Main Program------------; # -------------------------------------; """"""; Main function. Calls all other functions, depending on whether the macro input is in python or c++.; It adds the header information. Also, it adds a cell that draws all canvases. The working text is; then converted to a version 3 jupyter notebook, subsequently updated to a version 4. Then, metadata; associated with the language the macro is written in is attatched to he notebook. Finally the; notebook is executed and output as a Jupyter notebook.; """"""; # Modify text from macros to suit a notebook; # Remove function, Unindent, and convert comments to Markdown cells; # Convert",MatchSource.CODE_COMMENT,documentation/doxygen/converttonotebook.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/doxygen/converttonotebook.py
Availability,error,error,"#; # Draw a graph with error bars and fit a function to it; #; # superimpose fit results; #make nice; #define some data points . . .; # . . . and hand over to TGraphErros object; # request user action before ending (and deleting graphics window)",MatchSource.CODE_COMMENT,documentation/primer/macros/TGraphFit.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/macros/TGraphFit.py
Deployability,update,updated,"#!/usr/bin/python; # Character positions for extracting data from ""mass_width_2023.txt""; #------------------------; # Gets the text document; #------------------------; #-------------------------------------; # Gets the correct Data for particles; #-------------------------------------; # If it's an Antiparticle; # If it's not an Antiparticle; # Formatting the output line with the updated mass and width; #---------------------------------; # Save the data of the mass_width; #---------------------------------; # Array to store the data; # Skip comments; # Extracting relevant information based on the character column positions; # No value is given; # No value is given; # Extract only the name, excluding the charge; # Storing the data in the dictionary; #------------------------------; # Create updated pdg_table.txt; #------------------------------; #---------------------------------------; # Object for the data of the mass_width; #---------------------------------------; #------------------; # Setup everything; #------------------; # Fetch mass_width_2023.txt content from URLs",MatchSource.CODE_COMMENT,etc/pdg_table_update.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/etc/pdg_table_update.py
Energy Efficiency,charge,charge,"#!/usr/bin/python; # Character positions for extracting data from ""mass_width_2023.txt""; #------------------------; # Gets the text document; #------------------------; #-------------------------------------; # Gets the correct Data for particles; #-------------------------------------; # If it's an Antiparticle; # If it's not an Antiparticle; # Formatting the output line with the updated mass and width; #---------------------------------; # Save the data of the mass_width; #---------------------------------; # Array to store the data; # Skip comments; # Extracting relevant information based on the character column positions; # No value is given; # No value is given; # Extract only the name, excluding the charge; # Storing the data in the dictionary; #------------------------------; # Create updated pdg_table.txt; #------------------------------; #---------------------------------------; # Object for the data of the mass_width; #---------------------------------------; #------------------; # Setup everything; #------------------; # Fetch mass_width_2023.txt content from URLs",MatchSource.CODE_COMMENT,etc/pdg_table_update.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/etc/pdg_table_update.py
Availability,error,errors,"#! /usr/bin/env python; #; # Build a pch for the headers and linkdefs in root-build-dir/etc/dictpch/.; # root-build-dir is first tried as ./ - if that doesn't exist, $ROOTSYS; # is taken as root-build-dir.; #; # $1: PCH output file name; # $2: cxxflags (optional; required if extra headers are supplied); # $3: extra headers to be included in the PCH (optional); #; # exit code 1 for invocation errors; else exit code of rootcling invocation.; #; # Copyright (c) 2014 Rene Brun and Fons Rademakers; # Author: Axel Naumann <axel@cern.ch>, 2014-10-16; # Translated to python by Danilo Piparo, 2015-4-23; #-------------------------------------------------------------------------------; #-------------------------------------------------------------------------------; #-------------------------------------------------------------------------------; """"""; Create a pch starting from the following arguments; 1) pch file name; 2) Compiler flags - optional, required if extra headers are supplied; 3) Extra headers - optional; """"""",MatchSource.CODE_COMMENT,etc/dictpch/makepch.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/etc/dictpch/makepch.py
Deployability,configurat,configuration,"# JupyROOT sample configuration file for nbconvert; # Custom C++ highlighter; # Custom Jinja template",MatchSource.CODE_COMMENT,etc/notebook/html/sample_config.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/etc/notebook/html/sample_config.py
Modifiability,config,configuration,"# JupyROOT sample configuration file for nbconvert; # Custom C++ highlighter; # Custom Jinja template",MatchSource.CODE_COMMENT,etc/notebook/html/sample_config.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/etc/notebook/html/sample_config.py
Availability,avail,available,"ectories to ignore when looking for source files.; # The reST default role (used for this markup: `text`) to use for all documents.; #default_role = None; # If true, '()' will be appended to :func: etc. cross-reference text.; #add_function_parentheses = True; # If true, the current module name will be prepended to all description; # unit titles (such as .. function::).; #add_module_names = True; # If true, sectionauthor and moduleauthor directives will be shown in the; # output. They are ignored by default.; #show_authors = False; # The name of the Pygments (syntax highlighting) style to use.; # A list of ignored prefixes for module index sorting.; #modindex_common_prefix = []; # -- Options for HTML output ---------------------------------------------------; # The theme to use for HTML and HTML Help pages. See the documentation for; # a list of builtin themes.; # Theme options are theme-specific and customize the look and feel of a theme; # further. For a list of options available for each theme, see the; # documentation.; #html_theme_options = {}; # Add any paths that contain custom themes here, relative to this directory.; #html_theme_path = []; # The name for this set of Sphinx documents. If None, it defaults to; # ""<project> v<release> documentation"".; #html_title = None; # A shorter title for the navigation bar. Default is the same as html_title.; #html_short_title = None; # The name of an image file (relative to this directory) to place at the top; # of the sidebar.; #html_logo = None; # The name of an image file (within the static path) to use as favicon of the; # docs. This file should be a Windows icon file (.ico) being 16x16 or 32x32; # pixels large.; #html_favicon = None; # Add any paths that contain custom static files (such as style sheets) here,; # relative to this directory. They are copied after the builtin static files,; # so a file named ""default.css"" will overwrite the builtin ""default.css"".; # If not '', a 'Last updated on:' timestamp is inserted",MatchSource.CODE_COMMENT,interpreter/cling/docs/conf.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/docs/conf.py
Deployability,configurat,configuration,"# -*- coding: utf-8 -*-; #; # Cling documentation build configuration file, created by; # sphinx-quickstart on Sun Dec 9 20:01:55 2012.; #; # This file is execfile()d with the current directory set to its containing dir.; #; # Note that not all possible configuration values are present in this; # autogenerated file.; #; # All configuration values have a default; values that are commented out; # serve to show the default.; # If extensions (or modules to document with autodoc) are in another directory,; # add these directories to sys.path here. If the directory is relative to the; # documentation root, use os.path.abspath to make it absolute, like shown here.; #sys.path.insert(0, os.path.abspath('.')); # -- General configuration -----------------------------------------------------; # If your documentation needs a minimal Sphinx version, state it here.; #needs_sphinx = '1.0'; # Add any Sphinx extension module names here, as strings. They can be extensions; # coming with Sphinx (named 'sphinx.ext.*') or your custom ones.; # Add any paths that contain templates here, relative to this directory.; # The suffix of source filenames.; # The encoding of source files.; #source_encoding = 'utf-8-sig'; # The master toctree document.; # General information about the project.; # The version info for the project you're documenting, acts as replacement for; # |version| and |release|, also used in various other places throughout the; # built documents.; #; # The short version.; # The full version, including alpha/beta/rc tags.; # The language for content autogenerated by Sphinx. Refer to documentation; # for a list of supported languages.; #language = None; # There are two options for replacing |today|: either, you set today to some; # non-false value, then it is used:; #today = ''; # Else, today_fmt is used as the format for a strftime call.; #today_fmt = '%B %d, %Y'; # List of patterns, relative to source directory, that match files and; # directories to ignore when looking for sour",MatchSource.CODE_COMMENT,interpreter/cling/docs/conf.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/docs/conf.py
Modifiability,config,configuration,"# -*- coding: utf-8 -*-; #; # Cling documentation build configuration file, created by; # sphinx-quickstart on Sun Dec 9 20:01:55 2012.; #; # This file is execfile()d with the current directory set to its containing dir.; #; # Note that not all possible configuration values are present in this; # autogenerated file.; #; # All configuration values have a default; values that are commented out; # serve to show the default.; # If extensions (or modules to document with autodoc) are in another directory,; # add these directories to sys.path here. If the directory is relative to the; # documentation root, use os.path.abspath to make it absolute, like shown here.; #sys.path.insert(0, os.path.abspath('.')); # -- General configuration -----------------------------------------------------; # If your documentation needs a minimal Sphinx version, state it here.; #needs_sphinx = '1.0'; # Add any Sphinx extension module names here, as strings. They can be extensions; # coming with Sphinx (named 'sphinx.ext.*') or your custom ones.; # Add any paths that contain templates here, relative to this directory.; # The suffix of source filenames.; # The encoding of source files.; #source_encoding = 'utf-8-sig'; # The master toctree document.; # General information about the project.; # The version info for the project you're documenting, acts as replacement for; # |version| and |release|, also used in various other places throughout the; # built documents.; #; # The short version.; # The full version, including alpha/beta/rc tags.; # The language for content autogenerated by Sphinx. Refer to documentation; # for a list of supported languages.; #language = None; # There are two options for replacing |today|: either, you set today to some; # non-false value, then it is used:; #today = ''; # Else, today_fmt is used as the format for a strftime call.; #today_fmt = '%B %d, %Y'; # List of patterns, relative to source directory, that match files and; # directories to ignore when looking for sour",MatchSource.CODE_COMMENT,interpreter/cling/docs/conf.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/docs/conf.py
Usability,guid,guide,"ional stuff for the LaTeX preamble.; #'preamble': '',; # Grouping the document tree into LaTeX files. List of tuples; # (source start file, target name, title, author, documentclass [howto/manual]).; # The name of an image file (relative to this directory) to place at the top of; # the title page.; #latex_logo = None; # For ""manual"" documents, if this is true, then toplevel headings are parts,; # not chapters.; #latex_use_parts = False; # If true, show page references after internal links.; #latex_show_pagerefs = False; # If true, show URL addresses after external links.; #latex_show_urls = False; # Documents to append as an appendix to all manuals.; #latex_appendices = []; # If false, no module index is generated.; #latex_domain_indices = True; # -- Options for manual page output --------------------------------------------; # One entry per manual page. List of tuples; # (source start file, name, description, authors, manual section).; # Automatically derive the list of man pages from the contents of the command; # guide subdirectory. This was copied from llvm/docs/conf.py.; #basedir = os.path.dirname(__file__); #man_page_authors = u'Maintained by the Cling Team (<http://compiler-research.org>)'; #command_guide_subpath = 'CommandGuide'; #command_guide_path = os.path.join(basedir, command_guide_subpath); #for name in os.listdir(command_guide_path):; # # Ignore non-ReST files and the index page.; # if not name.endswith('.rst') or name in ('index.rst',):; # continue; #; # # Otherwise, automatically extract the description.; # file_subpath = os.path.join(command_guide_subpath, name); # with open(os.path.join(command_guide_path, name)) as f:; # title = f.readline().rstrip('\n'); # header = f.readline().rstrip('\n'); #; # if len(header) != len(title):; # print((; # ""error: invalid header in %r (does not match title)"" % (; # file_subpath,)), file=sys.stderr); # if ' - ' not in title:; # print((; # (""error: invalid title in %r ""; # ""(expected '<name> - <description>')"") % ",MatchSource.CODE_COMMENT,interpreter/cling/docs/conf.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/docs/conf.py
Availability,error,error,"#!/usr/bin/env python3; #------------------------------------------------------------------------------; # CLING - the C++ LLVM-based InterpreterG :); # author: Min RK; # Copyright (c) Min RK; #; # This file is dual-licensed: you can choose to license it under the University; # of Illinois Open Source License or the GNU Lesser General Public License. See; # LICENSE.TXT for details.; #------------------------------------------------------------------------------; """"""; Cling Kernel for Jupyter. Talks to Cling via ctypes; """"""; # libc.stdout is has a funny name on OS X; """"""Stream replacement by pipes.""""""; # make pipe_out non-blocking; # and restore original stdout/stderr; """"""Cling Kernel for Jupyter""""""; # Used in handle_input(); #If cling is not a symlink try a regular file; #readlink returns POSIX error EINVAL (22) if the; #argument is not a symlink; #build -std=c++11 or -std=c++14 option; #from IPython.utils import io; #io.rprint(""DBG: Using {}"".format(stdopt.decode('utf-8'))); # Environment variable CLING_OPTS used to pass arguments to cling; # The sideband_pipe is used by cling::Jupyter::pushOutput() to publish MIME data to Jupyter.; #c_char_p; """"""Read from the pipe, send it to IOPub as name stream.""""""; # send output; """"""Receive a serialized dict on a pipe. Returns the dictionary.; """"""; # Wire format:; # // Pipe sees (all numbers are longs, except for the first):; # // - num bytes in a long (sent as a single unsigned char!); # // - num elements of the MIME dictionary; Jupyter selects one to display.; # // For each MIME dictionary element:; # // - length of MIME type key; # // - MIME type key; # // - size of MIME data buffer (including the terminating 0 for; # // 0-terminated strings); # // - MIME data buffer; """"""publish display-data messages on IOPub.; """"""; """"""Put the forwarding pipes in place for stdout, stderr.""""""; """"""Capture stdout, stderr and sideband. Forward them as stream messages.""""""; # create pipe for stdout, stderr; # nothing to read, flush libc's stdout an",MatchSource.CODE_COMMENT,interpreter/cling/tools/Jupyter/kernel/clingkernel.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/tools/Jupyter/kernel/clingkernel.py
Integrability,message,messages,"acement by pipes.""""""; # make pipe_out non-blocking; # and restore original stdout/stderr; """"""Cling Kernel for Jupyter""""""; # Used in handle_input(); #If cling is not a symlink try a regular file; #readlink returns POSIX error EINVAL (22) if the; #argument is not a symlink; #build -std=c++11 or -std=c++14 option; #from IPython.utils import io; #io.rprint(""DBG: Using {}"".format(stdopt.decode('utf-8'))); # Environment variable CLING_OPTS used to pass arguments to cling; # The sideband_pipe is used by cling::Jupyter::pushOutput() to publish MIME data to Jupyter.; #c_char_p; """"""Read from the pipe, send it to IOPub as name stream.""""""; # send output; """"""Receive a serialized dict on a pipe. Returns the dictionary.; """"""; # Wire format:; # // Pipe sees (all numbers are longs, except for the first):; # // - num bytes in a long (sent as a single unsigned char!); # // - num elements of the MIME dictionary; Jupyter selects one to display.; # // For each MIME dictionary element:; # // - length of MIME type key; # // - MIME type key; # // - size of MIME data buffer (including the terminating 0 for; # // 0-terminated strings); # // - MIME data buffer; """"""publish display-data messages on IOPub.; """"""; """"""Put the forwarding pipes in place for stdout, stderr.""""""; """"""Capture stdout, stderr and sideband. Forward them as stream messages.""""""; # create pipe for stdout, stderr; # nothing to read, flush libc's stdout and check again; """"""Close the forwarding pipes.""""""; """"""Run code in cling, storing the expression result or an empty string if there is none.""""""; """"""Runs code in cling and handles input; returns the evaluation result.""""""; # Redirect stdout, stderr so handle_input() can pick it up.; # Run code in cling in a thread.; # self.run_cell() has returned.; # Any leftovers?; # Execution has finished; we have a result.; """"""Provide completions here""""""; # if cursor_pos = cursor_start = cursor_end,; # matches should be a list of strings to be appended after the cursor; """"""launch a cling kernel""""""",MatchSource.CODE_COMMENT,interpreter/cling/tools/Jupyter/kernel/clingkernel.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/tools/Jupyter/kernel/clingkernel.py
Modifiability,variab,variable,"------------------------------------------; # CLING - the C++ LLVM-based InterpreterG :); # author: Min RK; # Copyright (c) Min RK; #; # This file is dual-licensed: you can choose to license it under the University; # of Illinois Open Source License or the GNU Lesser General Public License. See; # LICENSE.TXT for details.; #------------------------------------------------------------------------------; """"""; Cling Kernel for Jupyter. Talks to Cling via ctypes; """"""; # libc.stdout is has a funny name on OS X; """"""Stream replacement by pipes.""""""; # make pipe_out non-blocking; # and restore original stdout/stderr; """"""Cling Kernel for Jupyter""""""; # Used in handle_input(); #If cling is not a symlink try a regular file; #readlink returns POSIX error EINVAL (22) if the; #argument is not a symlink; #build -std=c++11 or -std=c++14 option; #from IPython.utils import io; #io.rprint(""DBG: Using {}"".format(stdopt.decode('utf-8'))); # Environment variable CLING_OPTS used to pass arguments to cling; # The sideband_pipe is used by cling::Jupyter::pushOutput() to publish MIME data to Jupyter.; #c_char_p; """"""Read from the pipe, send it to IOPub as name stream.""""""; # send output; """"""Receive a serialized dict on a pipe. Returns the dictionary.; """"""; # Wire format:; # // Pipe sees (all numbers are longs, except for the first):; # // - num bytes in a long (sent as a single unsigned char!); # // - num elements of the MIME dictionary; Jupyter selects one to display.; # // For each MIME dictionary element:; # // - length of MIME type key; # // - MIME type key; # // - size of MIME data buffer (including the terminating 0 for; # // 0-terminated strings); # // - MIME data buffer; """"""publish display-data messages on IOPub.; """"""; """"""Put the forwarding pipes in place for stdout, stderr.""""""; """"""Capture stdout, stderr and sideband. Forward them as stream messages.""""""; # create pipe for stdout, stderr; # nothing to read, flush libc's stdout and check again; """"""Close the forwarding pipes.""""""; """"""Run cod",MatchSource.CODE_COMMENT,interpreter/cling/tools/Jupyter/kernel/clingkernel.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/tools/Jupyter/kernel/clingkernel.py
Safety,sanity check,sanity check,"#!/usr/bin/env python3; # coding: utf-8; #------------------------------------------------------------------------------; # CLING - the C++ LLVM-based InterpreterG :); # author: Min RK; # Copyright (c) Min RK; #; # This file is dual-licensed: you can choose to license it under the University; # of Illinois Open Source License or the GNU Lesser General Public License. See; # LICENSE.TXT for details.; #------------------------------------------------------------------------------; # the name of the project; #-----------------------------------------------------------------------------; # Minimal Python version sanity check; #-----------------------------------------------------------------------------; #-----------------------------------------------------------------------------; # get on with it; #-----------------------------------------------------------------------------",MatchSource.CODE_COMMENT,interpreter/cling/tools/Jupyter/kernel/setup.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/tools/Jupyter/kernel/setup.py
Availability,down,download,"ra and SUSE support; # TODO Refactor all fetch_ functions to use this class will remove a lot of dup; # exec_subprocess_call('git clean -f -x -d', clangdir); # if arg == 'last-stable':; # checkout_branch = exec_subprocess_check_output('git describe --match v* --abbrev=0 --tags | head -n 1',; # CLING_SRC_DIR); # exec_subprocess_call('git stash', CLING_SRC_DIR); # exec_subprocess_call('git clean -f -x -d', CLING_SRC_DIR); # if arg == 'last-stable':; # checkout_branch = exec_subprocess_check_output('git describe --match v* --abbrev=0 --tags | head -n 1',; # CLING_SRC_DIR); # If development release, then add revision to the version; # Travis CI, GCC crashes if more than 4 cores used.; # Cleanup previous installation directory if any; # Cleanup previous build directory if exists; # FIX: Target isn't being set properly on Travis OS X; # Either because ccache(when enabled) or maybe the virtualization environment; # configure cling; # Run cling once, dumping the include paths, helps debug issues; # Cleanup previous installation directory if any; # Cleanup previous build directory if exists; # Run cling once, dumping the include paths, helps debug issues; # We get zip instead of git clone to not download git history; # Run single tests on CI with this; # runSingleTest('Prompt/ValuePrinter/Regression.C'); # runSingleTest('Prompt/ValuePrinter'); ###############################################################################; # Debian specific functions (ported from debianize.sh) #; ###############################################################################; # This section is no longer valid. I have kept it as a reference if we plan to; # distribute libcling.so or any other library with the package.; '''; #! /bin/sh -e; # postinst script for cling; #; # see: dh_installdeb(1). set -e. # Call ldconfig on libclang.so; ldconfig -l /usr/lib/libclang.so. # dh_installdeb will replace this with shell code automatically; # generated by other debhelper scripts. #DEBHELPER#. exit 0; '",MatchSource.CODE_COMMENT,interpreter/cling/tools/packaging/cpt.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/tools/packaging/cpt.py
Deployability,install,install,"Python script to launch Cling Packaging Tool (CPT); #; # Documentation: tools/packaging/README.md; #; # Author: Anirudha Bose <ani07nov@gmail.com>; #; # This file is dual-licensed: you can choose to license it under the University; # of Illinois Open Source License or the GNU Lesser General Public License. See; # LICENSE.TXT for details.; #; ###############################################################################; ###############################################################################; # Platform independent functions (formerly indep.py) #; ###############################################################################; # Communicate return code to the calling program if any; '''; ╔══════════════════════════════════════════════════════════════════════════════╗; ║ %s ║; ╚══════════════════════════════════════════════════════════════════════════════╝'''; '''; +=============================================================================+; | %s|; +=============================================================================+'''; '''; ┌──────────────────────────────────────────────────────────────────────────────┐; │ %s%s │; └──────────────────────────────────────────────────────────────────────────────┘'''; '''; +-----------------------------------------------------------------------------+; | %s%s|; +-----------------------------------------------------------------------------+'''; # Needs brew install python. We should only install if we need the; # functionality; # exec_subprocess_call('git clean -f -x -d', srcdir); # FIXME: Add Fedora and SUSE support; # TODO Refactor all fetch_ functions to use this class will remove a lot of dup; # exec_subprocess_call('git clean -f -x -d', clangdir); # if arg == 'last-stable':; # checkout_branch = exec_subprocess_check_output('git describe --match v* --abbrev=0 --tags | head -n 1',; # CLING_SRC_DIR); # exec_subprocess_call('git stash', CLING_SRC_DIR); # exec_subprocess_call('git clean -f -x -d', CLING_SRC_DIR); # i",MatchSource.CODE_COMMENT,interpreter/cling/tools/packaging/cpt.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/tools/packaging/cpt.py
Integrability,integrat,integrated,"ithout a pager.; ###############################################################################; # Platform initialization #; ###############################################################################; '''; CPT will now attempt to install the distro package automatically.; Do you want to continue? [yes/no]: '''; # Error out; # Extensions will be detected anyway by set_ext(); # TODO: Need to test this in other platforms; # logic is too confusing supporting both at the same time; # This is needed in Windows; '''; CPT will now attempt to update/install the requisite packages automatically.; Do you want to continue? [yes/no]: '''; # Need to communicate values to the shell. Do not use exec_subprocess_call(); '''; Install/update the required packages by:; sudo apt-get update; sudo apt-get install {0} {1}; '''; # Check Windows registry for keys that prove an MS Visual Studio 14.0 installation; '''; Refer to the documentation of CPT for information on setting up your Windows environment.; [tools/packaging/README.md]; '''; '''; CPT will now attempt to update/install the requisite packages automatically.; Do you want to continue? [yes/no]: '''; # Need to communicate values to the shell. Do not use exec_subprocess_call(); '''; Install/update the required packages by:; sudo yum install git cmake gcc gcc-c++ rpm-build; '''; '''; CPT will now attempt to update/install the requisite packages automatically. Make sure you have MacPorts installed.; Do you want to continue? [yes/no]: '''; # Need to communicate values to the shell. Do not use exec_subprocess_call(); '''; Install/update the required packages by:; sudo port -v selfupdate; sudo port install {0} {1}; '''; # Travis has already cloned the repo out, so don;t do it again; # Particularly important for building a pull-request; # Cannot move the directory: it is being used by another process; # Check validity and show some info; # FIXME; # This is an internal option in CPT, meant to be integrated into Cling's build system.",MatchSource.CODE_COMMENT,interpreter/cling/tools/packaging/cpt.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/tools/packaging/cpt.py
Modifiability,config,configure,"ra and SUSE support; # TODO Refactor all fetch_ functions to use this class will remove a lot of dup; # exec_subprocess_call('git clean -f -x -d', clangdir); # if arg == 'last-stable':; # checkout_branch = exec_subprocess_check_output('git describe --match v* --abbrev=0 --tags | head -n 1',; # CLING_SRC_DIR); # exec_subprocess_call('git stash', CLING_SRC_DIR); # exec_subprocess_call('git clean -f -x -d', CLING_SRC_DIR); # if arg == 'last-stable':; # checkout_branch = exec_subprocess_check_output('git describe --match v* --abbrev=0 --tags | head -n 1',; # CLING_SRC_DIR); # If development release, then add revision to the version; # Travis CI, GCC crashes if more than 4 cores used.; # Cleanup previous installation directory if any; # Cleanup previous build directory if exists; # FIX: Target isn't being set properly on Travis OS X; # Either because ccache(when enabled) or maybe the virtualization environment; # configure cling; # Run cling once, dumping the include paths, helps debug issues; # Cleanup previous installation directory if any; # Cleanup previous build directory if exists; # Run cling once, dumping the include paths, helps debug issues; # We get zip instead of git clone to not download git history; # Run single tests on CI with this; # runSingleTest('Prompt/ValuePrinter/Regression.C'); # runSingleTest('Prompt/ValuePrinter'); ###############################################################################; # Debian specific functions (ported from debianize.sh) #; ###############################################################################; # This section is no longer valid. I have kept it as a reference if we plan to; # distribute libcling.so or any other library with the package.; '''; #! /bin/sh -e; # postinst script for cling; #; # see: dh_installdeb(1). set -e. # Call ldconfig on libclang.so; ldconfig -l /usr/lib/libclang.so. # dh_installdeb will replace this with shell code automatically; # generated by other debhelper scripts. #DEBHELPER#. exit 0; '",MatchSource.CODE_COMMENT,interpreter/cling/tools/packaging/cpt.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/tools/packaging/cpt.py
Performance,perform,performance,"hare; '''; # Optimize binary compression; '''; Source: cling; Section: devel; Priority: optional; Maintainer: Cling Developer Team <cling-dev@cern.ch>; Uploaders: %s; Build-Depends: debhelper (>= 9.0.0); Standards-Version: 3.9.5; Homepage: http://cling.web.cern.ch/; Vcs-Git: http://root.cern.ch/git/cling.git; Vcs-Browser: http://root.cern.ch/gitweb?p=cling.git;a=summary. Package: cling; Priority: optional; Architecture: any; Depends: ${shlibs:Depends}, ${misc:Depends}; Description: interactive C++ interpreter; Cling is a new and interactive C++11 standard compliant interpreter built; on the top of Clang and LLVM compiler infrastructure. Its advantages over; the standard interpreters are that it has command line prompt and uses; Just In Time (JIT) compiler for compilation. Many of the developers; (e.g. Mono in their project called CSharpRepl) of such kind of software; applications name them interactive compilers.; .; One of Cling's main goals is to provide contemporary, high-performance; alternative of the current C++ interpreter in the ROOT project - CINT. Cling; serves as a core component of the ROOT system for storing and analyzing the; data of the Large Hadron Collider (LHC) experiments. The; backward-compatibility with CINT is major priority during the development.; '''; '''; Format: http://www.debian.org/doc/packaging-manuals/copyright-format/1.0/; Upstream-Name: cling; Source: http://root.cern.ch/gitweb?p=cling.git;a=summary. Files: *; Copyright: 2007-2014 by the Authors; License: LGPL-2.0+; Comment: Developed by The ROOT Team; CERN and Fermilab. Files: debian/*; Copyright: 2014 Anirudha Bose <ani07nov@gmail.com>; License: LGPL-2.0+. License: LGPL-2.0+; This package is free software; you can redistribute it and/or; modify it under the terms of the GNU Lesser General Public; License as published by the Free Software Foundation; either; version 2 of the License, or (at your option) any later version.; .; This package is distributed in the hope that it will be us",MatchSource.CODE_COMMENT,interpreter/cling/tools/packaging/cpt.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/tools/packaging/cpt.py
Safety,detect,detect,"SMPROGRAMS\\Cling\\Cling.lnk"" ""$INSTDIR\\bin\\cling.exe"" """" ""${MUI_ICON}"" 0; CreateDirectory ""$SMPROGRAMS\\Cling\\Documentation""; CreateShortCut ""$SMPROGRAMS\\Cling\\Documentation\\Cling (PS).lnk"" ""$INSTDIR\\docs\\llvm\\ps\\cling.ps"" """" """" 0; CreateShortCut ""$SMPROGRAMS\\Cling\\Documentation\\Cling (HTML).lnk"" ""$INSTDIR\\docs\\llvm\\html\\cling\\cling.html"" """" """" 0. SectionEnd. Section ""Uninstall"". DeleteRegKey HKLM ""Software\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\Cling""; DeleteRegKey HKLM ""Software\\Cling"". ; Remove shortcuts; Delete ""$SMPROGRAMS\\Cling\\*.*""; Delete ""$SMPROGRAMS\\Cling\\Documentation\\*.*""; Delete ""$SMPROGRAMS\\Cling\\Documentation""; RMDir ""$SMPROGRAMS\\Cling"". '''; # insert dir list (depth-first order) for uninstall files; # last bit of the uninstaller; '''; SectionEnd. ; Function to detect Windows version and abort if Cling is unsupported in the current platform; Function DetectWinVer; Push $0; Push $1; ReadRegStr $0 HKLM ""SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion"" CurrentVersion; IfErrors is_error is_winnt; is_winnt:; StrCpy $1 $0 1; StrCmp $1 4 is_error ; Aborting installation for Windows versions older than Windows 2000; StrCmp $0 ""5.0"" is_error ; Removing Windows 2000 as supported Windows version; StrCmp $0 ""5.1"" is_winnt_XP; StrCmp $0 ""5.2"" is_winnt_2003; StrCmp $0 ""6.0"" is_winnt_vista; StrCmp $0 ""6.1"" is_winnt_7; StrCmp $0 ""6.2"" is_winnt_8; StrCmp $1 6 is_winnt_8 ; Checking for future versions of Windows 8; Goto is_error. is_winnt_XP:; is_winnt_2003:; is_winnt_vista:; is_winnt_7:; is_winnt_8:; Goto done; is_error:; StrCpy $1 $0; ReadRegStr $0 HKLM ""SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion"" ProductName; IfErrors 0 +4; ReadRegStr $0 HKLM ""SOFTWARE\\Microsoft\\Windows\\CurrentVersion"" Version; IfErrors 0 +2; StrCpy $0 ""Unknown""; MessageBox MB_ICONSTOP|MB_OK ""This version of Cling cannot be installed on this system. Cling is supported only on Windows NT systems. Current system: $0 (version: $1)""; Abort; done:; Pop $1; Pop",MatchSource.CODE_COMMENT,interpreter/cling/tools/packaging/cpt.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/tools/packaging/cpt.py
Testability,test,tests,"ra and SUSE support; # TODO Refactor all fetch_ functions to use this class will remove a lot of dup; # exec_subprocess_call('git clean -f -x -d', clangdir); # if arg == 'last-stable':; # checkout_branch = exec_subprocess_check_output('git describe --match v* --abbrev=0 --tags | head -n 1',; # CLING_SRC_DIR); # exec_subprocess_call('git stash', CLING_SRC_DIR); # exec_subprocess_call('git clean -f -x -d', CLING_SRC_DIR); # if arg == 'last-stable':; # checkout_branch = exec_subprocess_check_output('git describe --match v* --abbrev=0 --tags | head -n 1',; # CLING_SRC_DIR); # If development release, then add revision to the version; # Travis CI, GCC crashes if more than 4 cores used.; # Cleanup previous installation directory if any; # Cleanup previous build directory if exists; # FIX: Target isn't being set properly on Travis OS X; # Either because ccache(when enabled) or maybe the virtualization environment; # configure cling; # Run cling once, dumping the include paths, helps debug issues; # Cleanup previous installation directory if any; # Cleanup previous build directory if exists; # Run cling once, dumping the include paths, helps debug issues; # We get zip instead of git clone to not download git history; # Run single tests on CI with this; # runSingleTest('Prompt/ValuePrinter/Regression.C'); # runSingleTest('Prompt/ValuePrinter'); ###############################################################################; # Debian specific functions (ported from debianize.sh) #; ###############################################################################; # This section is no longer valid. I have kept it as a reference if we plan to; # distribute libcling.so or any other library with the package.; '''; #! /bin/sh -e; # postinst script for cling; #; # see: dh_installdeb(1). set -e. # Call ldconfig on libclang.so; ldconfig -l /usr/lib/libclang.so. # dh_installdeb will replace this with shell code automatically; # generated by other debhelper scripts. #DEBHELPER#. exit 0; '",MatchSource.CODE_COMMENT,interpreter/cling/tools/packaging/cpt.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/tools/packaging/cpt.py
Deployability,configurat,configuration,"# -*- coding: utf-8 -*-; # Window position in ((x, y), (w, h)) format; # General view configuration; # Icon view configuration; # List view configuration",MatchSource.CODE_COMMENT,interpreter/cling/tools/packaging/settings.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/tools/packaging/settings.py
Modifiability,config,configuration,"# -*- coding: utf-8 -*-; # Window position in ((x, y), (w, h)) format; # General view configuration; # Icon view configuration; # List view configuration",MatchSource.CODE_COMMENT,interpreter/cling/tools/packaging/settings.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/tools/packaging/settings.py
Availability,avail,available,"%Y'; # List of patterns, relative to source directory, that match files and; # directories to ignore when looking for source files.; # The reST default role (used for this markup: `text`) to use for all documents.; # default_role = None; # If true, '()' will be appended to :func: etc. cross-reference text.; # add_function_parentheses = True; # If true, the current module name will be prepended to all description; # unit titles (such as .. function::).; # add_module_names = True; # If true, sectionauthor and moduleauthor directives will be shown in the; # output. They are ignored by default.; # show_authors = False; # The name of the Pygments (syntax highlighting) style to use.; # -- Options for HTML output ---------------------------------------------------; # The theme to use for HTML and HTML Help pages. See the documentation for; # a list of builtin themes.; # Theme options are theme-specific and customize the look and feel of a theme; # further. For a list of options available for each theme, see the; # documentation.; # html_theme_options = {}; # Add any paths that contain custom themes here, relative to this directory.; # html_theme_path = []; # The name for this set of Sphinx documents. If None, it defaults to; # ""<project> v<release> documentation"".; # html_title = None; # A shorter title for the navigation bar. Default is the same as html_title.; # html_short_title = None; # The name of an image file (relative to this directory) to place at the top; # of the sidebar.; # html_logo = None; # The name of an image file (within the static path) to use as favicon of the; # docs. This file should be a Windows icon file (.ico) being 16x16 or 32x32; # pixels large.; # html_favicon = None; # Add any paths that contain custom static files (such as style sheets) here,; # relative to this directory. They are copied after the builtin static files,; # so a file named ""default.css"" will overwrite the builtin ""default.css"".; # If not '', a 'Last updated on:' timestamp is in",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/docs/conf.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/conf.py
Deployability,configurat,configuration,"# -*- coding: utf-8 -*-; #; # Clang documentation build configuration file, created by; # sphinx-quickstart on Sun Dec 9 20:01:55 2012.; #; # This file is execfile()d with the current directory set to its containing dir.; #; # Note that not all possible configuration values are present in this; # autogenerated file.; #; # All configuration values have a default; values that are commented out; # serve to show the default.; # If extensions (or modules to document with autodoc) are in another directory,; # add these directories to sys.path here. If the directory is relative to the; # documentation root, use os.path.abspath to make it absolute, like shown here.; # sys.path.insert(0, os.path.abspath('.')); # -- General configuration -----------------------------------------------------; # If your documentation needs a minimal Sphinx version, state it here.; # needs_sphinx = '1.0'; # Add any Sphinx extension module names here, as strings. They can be extensions; # coming with Sphinx (named 'sphinx.ext.*') or your custom ones.; # Add any paths that contain templates here, relative to this directory.; # The encoding of source files.; # source_encoding = 'utf-8-sig'; # The master toctree document.; # General information about the project.; # The language for content autogenerated by Sphinx. Refer to documentation; # for a list of supported languages.; # language = None; # There are two options for replacing |today|: either, you set today to some; # non-false value, then it is used:; # today = ''; # Else, today_fmt is used as the format for a strftime call.; # today_fmt = '%B %d, %Y'; # List of patterns, relative to source directory, that match files and; # directories to ignore when looking for source files.; # The reST default role (used for this markup: `text`) to use for all documents.; # default_role = None; # If true, '()' will be appended to :func: etc. cross-reference text.; # add_function_parentheses = True; # If true, the current module name will be prepended to all ",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/docs/conf.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/conf.py
Modifiability,config,configuration,"# -*- coding: utf-8 -*-; #; # Clang documentation build configuration file, created by; # sphinx-quickstart on Sun Dec 9 20:01:55 2012.; #; # This file is execfile()d with the current directory set to its containing dir.; #; # Note that not all possible configuration values are present in this; # autogenerated file.; #; # All configuration values have a default; values that are commented out; # serve to show the default.; # If extensions (or modules to document with autodoc) are in another directory,; # add these directories to sys.path here. If the directory is relative to the; # documentation root, use os.path.abspath to make it absolute, like shown here.; # sys.path.insert(0, os.path.abspath('.')); # -- General configuration -----------------------------------------------------; # If your documentation needs a minimal Sphinx version, state it here.; # needs_sphinx = '1.0'; # Add any Sphinx extension module names here, as strings. They can be extensions; # coming with Sphinx (named 'sphinx.ext.*') or your custom ones.; # Add any paths that contain templates here, relative to this directory.; # The encoding of source files.; # source_encoding = 'utf-8-sig'; # The master toctree document.; # General information about the project.; # The language for content autogenerated by Sphinx. Refer to documentation; # for a list of supported languages.; # language = None; # There are two options for replacing |today|: either, you set today to some; # non-false value, then it is used:; # today = ''; # Else, today_fmt is used as the format for a strftime call.; # today_fmt = '%B %d, %Y'; # List of patterns, relative to source directory, that match files and; # directories to ignore when looking for source files.; # The reST default role (used for this markup: `text`) to use for all documents.; # default_role = None; # If true, '()' will be appended to :func: etc. cross-reference text.; # add_function_parentheses = True; # If true, the current module name will be prepended to all ",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/docs/conf.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/conf.py
Usability,guid,guide,"-----------------------------------------; # The paper size ('letterpaper' or 'a4paper').; #'papersize': 'letterpaper',; # The font size ('10pt', '11pt' or '12pt').; #'pointsize': '10pt',; # Additional stuff for the LaTeX preamble.; #'preamble': '',; # Grouping the document tree into LaTeX files. List of tuples; # (source start file, target name, title, author, documentclass [howto/manual]).; # The name of an image file (relative to this directory) to place at the top of; # the title page.; # latex_logo = None; # For ""manual"" documents, if this is true, then toplevel headings are parts,; # not chapters.; # latex_use_parts = False; # If true, show page references after internal links.; # latex_show_pagerefs = False; # If true, show URL addresses after external links.; # latex_show_urls = False; # Documents to append as an appendix to all manuals.; # latex_appendices = []; # If false, no module index is generated.; # latex_domain_indices = True; # -- Options for manual page output --------------------------------------------; # One entry per manual page. List of tuples; # (source start file, name, description, authors, manual section).; # Automatically derive the list of man pages from the contents of the command; # guide subdirectory. This was copied from llvm/docs/conf.py.; # Ignore non-ReST files and the index page.; # Otherwise, automatically extract the description.; # Split the name out of the title.; # If true, show URL addresses after external links.; # man_show_urls = False; # -- Options for Texinfo output ------------------------------------------------; # Grouping the document tree into Texinfo files. List of tuples; # (source start file, target name, title, author,; # dir menu entry, description, category); # Documents to append as an appendix to all manuals.; # texinfo_appendices = []; # If false, no module index is generated.; # texinfo_domain_indices = True; # How to display URL addresses: 'footnote', 'no', or 'inline'.; # texinfo_show_urls = 'footnote'",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/docs/conf.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/conf.py
Availability,avail,available,"%Y'; # List of patterns, relative to source directory, that match files and; # directories to ignore when looking for source files.; # The reST default role (used for this markup: `text`) to use for all documents.; # default_role = None; # If true, '()' will be appended to :func: etc. cross-reference text.; # add_function_parentheses = True; # If true, the current module name will be prepended to all description; # unit titles (such as .. function::).; # add_module_names = True; # If true, sectionauthor and moduleauthor directives will be shown in the; # output. They are ignored by default.; # show_authors = False; # The name of the Pygments (syntax highlighting) style to use.; # -- Options for HTML output ---------------------------------------------------; # The theme to use for HTML and HTML Help pages. See the documentation for; # a list of builtin themes.; # Theme options are theme-specific and customize the look and feel of a theme; # further. For a list of options available for each theme, see the; # documentation.; # html_theme_options = {}; # Add any paths that contain custom themes here, relative to this directory.; # html_theme_path = []; # The name for this set of Sphinx documents. If None, it defaults to; # ""<project> v<release> documentation"".; # html_title = None; # A shorter title for the navigation bar. Default is the same as html_title.; # html_short_title = None; # The name of an image file (relative to this directory) to place at the top; # of the sidebar.; # html_logo = None; # The name of an image file (within the static path) to use as favicon of the; # docs. This file should be a Windows icon file (.ico) being 16x16 or 32x32; # pixels large.; # html_favicon = None; # Add any paths that contain custom static files (such as style sheets) here,; # relative to this directory. They are copied after the builtin static files,; # so a file named ""default.css"" will overwrite the builtin ""default.css"".; # If not '', a 'Last updated on:' timestamp is in",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/docs/analyzer/conf.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/conf.py
Deployability,configurat,configuration,"# -*- coding: utf-8 -*-; #; # Clang Static Analyzer documentation build configuration file, created by; # sphinx-quickstart on Wed Jan 2 15:54:28 2013.; #; # This file is execfile()d with the current directory set to its containing dir.; #; # Note that not all possible configuration values are present in this; # autogenerated file.; #; # All configuration values have a default; values that are commented out; # serve to show the default.; # If extensions (or modules to document with autodoc) are in another directory,; # add these directories to sys.path here. If the directory is relative to the; # documentation root, use os.path.abspath to make it absolute, like shown here.; # sys.path.insert(0, os.path.abspath('.')); # -- General configuration -----------------------------------------------------; # If your documentation needs a minimal Sphinx version, state it here.; # needs_sphinx = '1.0'; # Add any Sphinx extension module names here, as strings. They can be extensions; # coming with Sphinx (named 'sphinx.ext.*') or your custom ones.; # Add any paths that contain templates here, relative to this directory.; # The suffix of source filenames.; # The encoding of source files.; # source_encoding = 'utf-8-sig'; # The master toctree document.; # General information about the project.; # The version info for the project you're documenting, acts as replacement for; # |version| and |release|, also used in various other places throughout the; # built documents.; #; # The short version.; # The full version, including alpha/beta/rc tags.; # The language for content autogenerated by Sphinx. Refer to documentation; # for a list of supported languages.; # language = None; # There are two options for replacing |today|: either, you set today to some; # non-false value, then it is used:; # today = ''; # Else, today_fmt is used as the format for a strftime call.; # today_fmt = '%B %d, %Y'; # List of patterns, relative to source directory, that match files and; # directories to ignore",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/docs/analyzer/conf.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/conf.py
Modifiability,config,configuration,"# -*- coding: utf-8 -*-; #; # Clang Static Analyzer documentation build configuration file, created by; # sphinx-quickstart on Wed Jan 2 15:54:28 2013.; #; # This file is execfile()d with the current directory set to its containing dir.; #; # Note that not all possible configuration values are present in this; # autogenerated file.; #; # All configuration values have a default; values that are commented out; # serve to show the default.; # If extensions (or modules to document with autodoc) are in another directory,; # add these directories to sys.path here. If the directory is relative to the; # documentation root, use os.path.abspath to make it absolute, like shown here.; # sys.path.insert(0, os.path.abspath('.')); # -- General configuration -----------------------------------------------------; # If your documentation needs a minimal Sphinx version, state it here.; # needs_sphinx = '1.0'; # Add any Sphinx extension module names here, as strings. They can be extensions; # coming with Sphinx (named 'sphinx.ext.*') or your custom ones.; # Add any paths that contain templates here, relative to this directory.; # The suffix of source filenames.; # The encoding of source files.; # source_encoding = 'utf-8-sig'; # The master toctree document.; # General information about the project.; # The version info for the project you're documenting, acts as replacement for; # |version| and |release|, also used in various other places throughout the; # built documents.; #; # The short version.; # The full version, including alpha/beta/rc tags.; # The language for content autogenerated by Sphinx. Refer to documentation; # for a list of supported languages.; # language = None; # There are two options for replacing |today|: either, you set today to some; # non-false value, then it is used:; # today = ''; # Else, today_fmt is used as the format for a strftime call.; # today_fmt = '%B %d, %Y'; # List of patterns, relative to source directory, that match files and; # directories to ignore",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/docs/analyzer/conf.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/conf.py
Deployability,update,update,"#!/usr/bin/env python3; # A tool to parse ASTMatchers.h and update the documentation in; # ../LibASTMatchersReference.html automatically. Run from the; # directory in which this file is located to update the docs.; # Each matcher is documented in one row of the form:; # result | name | argA; # The subsequent row contains the documentation and is hidden by default,; # becoming visible via javascript when the user clicks the matcher name.; """"""; <tr><td>%(result)s</td><td class=""name"" onclick=""toggle('%(id)s')""><a name=""%(id)sAnchor"">%(name)s</a></td><td>%(args)s</td></tr>; <tr><td colspan=""4"" class=""doc"" id=""%(id)s""><pre>%(comment)s</pre></td></tr>; """"""; # We categorize the matchers into these three categories in the reference:; # We output multiple rows per matcher if the matcher can be used on multiple; # node types. Thus, we need a new id per row to control the documentation; # pop-up. ids[name] keeps track of those ids.; # Cache for doxygen urls we have already verified.; """"""Escape any html in the given text.""""""; """"""Wrap a likely AST node name in a link to its clang docs. We want to do this only if the page exists, in which case it will be; referenced from the class index page.; """"""; """"""Extracts a list of result types from the given comment. We allow annotations in the comment of the matcher to specify what; nodes a matcher can match on. Those comments have the form:; Usable as: Any Matcher | (Matcher<T1>[, Matcher<t2>[, ...]]). Returns ['*'] in case of 'Any Matcher', or ['T1', 'T2', ...].; Returns the empty list if no 'Usable as' specification could be; parsed.; """"""; """"""Returns the given comment without \-escaped words.""""""; # If there is only a doxygen keyword in the line, delete the whole line.; # If there is a doxygen \see command, change the \see prefix into ""See also:"".; # FIXME: it would be better to turn this into a link to the target instead.; # Delete the doxygen command and the following whitespace.; """"""Gets rid of anything the user doesn't care about in ",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/docs/tools/dump_ast_matchers.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/dump_ast_matchers.py
Testability,assert,asserts,"rds.""""""; # If there is only a doxygen keyword in the line, delete the whole line.; # If there is a doxygen \see command, change the \see prefix into ""See also:"".; # FIXME: it would be better to turn this into a link to the target instead.; # Delete the doxygen command and the following whitespace.; """"""Gets rid of anything the user doesn't care about in the argument list.""""""; """"""Gets rid of anything the user doesn't care about in the type name.""""""; """"""Adds a matcher to one of our categories.""""""; # FIXME: Figure out whether we want to support the 'id' matcher.; # Use a heuristic to figure out whether a matcher is a narrowing or; # traversal matcher. By default, matchers that take other matchers as; # arguments (and are not node matchers) do traversal. We specifically; # exclude known narrowing matchers that also take other matchers as; # arguments.; """"""Parse the matcher out of the given declaration and comment. If 'allowed_types' is set, it contains a list of node types the matcher; can match on, as extracted from the static type asserts in the matcher; definition.; """"""; # Node matchers are defined by writing:; # VariadicDynCastAllOfMatcher<ResultType, ArgumentType> name;; # Special case of type matchers:; # AstTypeMatcher<ArgumentType> name; # FIXME: re-enable once we have implemented casting on the TypeLoc; # hierarchy.; # add_matcher('TypeLoc', '%sLoc' % name, 'Matcher<%sLoc>...' % inner,; # comment, is_dyncast=True); # Parse the various matcher definition macros.; """""".*AST_TYPE(LOC)?_TRAVERSE_MATCHER(?:_DECL)?\(; \s*([^\s,]+\s*),; \s*(?:[^\s,]+\s*),; \s*AST_POLYMORPHIC_SUPPORTED_TYPES\(([^)]*)\); \)\s*;\s*$""""""; # if loc:; # add_matcher('%sLoc' % result_type, '%sLoc' % name, 'Matcher<TypeLoc>',; # comment); """"""; If the matcher is used in clang-query, RegexFlags parameter; should be passed as a quoted string. e.g: ""NoFlags"".; Flags can be combined with '|' example \""IgnoreCase | BasicRegex\""; """"""; """"""; If the matcher is used in clang-query, RegexFlags parameter; shou",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/docs/tools/dump_ast_matchers.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/dump_ast_matchers.py
Deployability,update,update,"#!/usr/bin/env python3; # A tool to parse the output of `clang-format --help` and update the; # documentation in ../ClangFormat.rst automatically.; """""".. code-block:: console. $ clang-format --help; """"""",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/docs/tools/dump_format_help.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/dump_format_help.py
Deployability,update,update,"#!/usr/bin/env python3; # A tool to parse the FormatStyle struct from Format.h and update the; # documentation in ../ClangFormatStyleOptions.rst automatically.; # Run from the directory in which this file is located to update the docs.; # check code block indentation; # if not version:; # self.__warning(f""missing version for {field_name}"", line); # Enum member without documentation. Must be documented where the enum; # is used.; # Enum member without documentation. Must be; # documented where the enum is used.",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/docs/tools/dump_format_style.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/dump_format_style.py
Security,hash,hash,"#!/usr/bin/env python; # A tool to parse creates a document outlining how clang formatted the; # LLVM project is.; """"""Get the get SHA in short hash form.""""""; """"""Determine if this directory is good based on the number of clean; files vs the number of files in total.""""""; """"""\; .. raw:: html. <style type=""text/css"">; .total {{ font-weight: bold; }}; .none {{ background-color: #FFFF99; height: 20px; display: inline-block; width: 120px; text-align: center; border-radius: 5px; color: #000000; font-family=""Verdana,Geneva,DejaVu Sans,sans-serif"" }}; .part {{ background-color: #FFCC99; height: 20px; display: inline-block; width: 120px; text-align: center; border-radius: 5px; color: #000000; font-family=""Verdana,Geneva,DejaVu Sans,sans-serif"" }}; .good {{ background-color: #2CCCFF; height: 20px; display: inline-block; width: 120px; text-align: center; border-radius: 5px; color: #000000; font-family=""Verdana,Geneva,DejaVu Sans,sans-serif"" }}; </style>. .. role:: none; .. role:: part; .. role:: good; .. role:: total. ======================; Clang Formatted Status; ======================. :doc:`ClangFormattedStatus` describes the state of LLVM source; tree in terms of conformance to :doc:`ClangFormat` as of: {today} (`{sha} <https://github.com/llvm/llvm-project/commit/{sha}>`_). .. list-table:: LLVM Clang-Format Status; :widths: 50 25 25 25 25; :header-rows: 1\n; * - Directory; - Total Files; - Formatted Files; - Unformatted Files; - % Complete; """"""; """"""\; * - {path}; - {style}`{count}`; - {style}`{passes}`; - {style}`{fails}`; - {style2}`{percent}%`; """"""; # Check the git index to see if the directory contains tracked; # files. Reditect the output to a null descriptor as we aren't; # interested in it, just the return code.",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/docs/tools/generate_formatted_state.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/generate_formatted_state.py
Deployability,patch,patch,"#!/usr/bin/env python3; #; # ===- clang-format-diff.py - ClangFormat Diff Reformatter ----*- python -*--===#; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ===------------------------------------------------------------------------===#; """"""; This script reads input from a unified diff and reformats all the changed; lines. This is useful to reformat all the lines touched by a specific patch.; Example usage for git/svn users:. git diff -U0 --no-color --relative HEAD^ | {clang_format_diff} -p1 -i; svn diff --diff-cmd=diff -x-U0 | {clang_format_diff} -i. It should be noted that the filename contained in the diff is used unmodified; to determine the source file to update. Users calling this script directly; should be careful to ensure that the path in the diff is correct relative to the; current working directory.; """"""; # Extract changed lines for each file.; # The input is something like; #; # @@ -1, +0,0 @@; #; # which means no lines were added.; # Also format lines range if line_count is 0 in case of deleting; # surrounding statements.; # Reformat files containing changes in place.; # Give the user more context when clang-format isn't; # found/isn't executable, etc.",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/clang-format/clang-format-diff.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-format/clang-format-diff.py
Deployability,integrat,integration,"# This file is a minimal clang-format sublime-integration. To install:; # - Change 'binary' if clang-format is not on the path (see below).; # - Put this file into your sublime Packages directory, e.g. on Linux:; # ~/.config/sublime-text-2/Packages/User/clang-format-sublime.py; # - Add a key binding:; # { ""keys"": [""ctrl+shift+c""], ""command"": ""clang_format"" },; #; # With this integration you can press the bound key and clang-format will; # format the current lines and selections for all cursor positions. The lines; # or regions are extended to the next bigger syntactic entities.; #; # It operates on the current, potentially unsaved buffer and does not create; # or save any files. To revert a formatting, just undo.; # Change this to the full path if clang-format is not on the path.; # Change this to format according to other formatting styles. See the output of; # 'clang-format --help' for a list of supported styles. The default looks for; # a '.clang-format' or '_clang-format' file to indicate the style that should be; # used.; # FIXME: Without the 10ms delay, the viewport sometimes jumps.",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/clang-format/clang-format-sublime.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-format/clang-format-sublime.py
Integrability,integrat,integration,"# This file is a minimal clang-format sublime-integration. To install:; # - Change 'binary' if clang-format is not on the path (see below).; # - Put this file into your sublime Packages directory, e.g. on Linux:; # ~/.config/sublime-text-2/Packages/User/clang-format-sublime.py; # - Add a key binding:; # { ""keys"": [""ctrl+shift+c""], ""command"": ""clang_format"" },; #; # With this integration you can press the bound key and clang-format will; # format the current lines and selections for all cursor positions. The lines; # or regions are extended to the next bigger syntactic entities.; #; # It operates on the current, potentially unsaved buffer and does not create; # or save any files. To revert a formatting, just undo.; # Change this to the full path if clang-format is not on the path.; # Change this to format according to other formatting styles. See the output of; # 'clang-format --help' for a list of supported styles. The default looks for; # a '.clang-format' or '_clang-format' file to indicate the style that should be; # used.; # FIXME: Without the 10ms delay, the viewport sometimes jumps.",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/clang-format/clang-format-sublime.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-format/clang-format-sublime.py
Modifiability,config,config,"# This file is a minimal clang-format sublime-integration. To install:; # - Change 'binary' if clang-format is not on the path (see below).; # - Put this file into your sublime Packages directory, e.g. on Linux:; # ~/.config/sublime-text-2/Packages/User/clang-format-sublime.py; # - Add a key binding:; # { ""keys"": [""ctrl+shift+c""], ""command"": ""clang_format"" },; #; # With this integration you can press the bound key and clang-format will; # format the current lines and selections for all cursor positions. The lines; # or regions are extended to the next bigger syntactic entities.; #; # It operates on the current, potentially unsaved buffer and does not create; # or save any files. To revert a formatting, just undo.; # Change this to the full path if clang-format is not on the path.; # Change this to format according to other formatting styles. See the output of; # 'clang-format --help' for a list of supported styles. The default looks for; # a '.clang-format' or '_clang-format' file to indicate the style that should be; # used.; # FIXME: Without the 10ms delay, the viewport sometimes jumps.",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/clang-format/clang-format-sublime.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-format/clang-format-sublime.py
Usability,undo,undo,"# This file is a minimal clang-format sublime-integration. To install:; # - Change 'binary' if clang-format is not on the path (see below).; # - Put this file into your sublime Packages directory, e.g. on Linux:; # ~/.config/sublime-text-2/Packages/User/clang-format-sublime.py; # - Add a key binding:; # { ""keys"": [""ctrl+shift+c""], ""command"": ""clang_format"" },; #; # With this integration you can press the bound key and clang-format will; # format the current lines and selections for all cursor positions. The lines; # or regions are extended to the next bigger syntactic entities.; #; # It operates on the current, potentially unsaved buffer and does not create; # or save any files. To revert a formatting, just undo.; # Change this to the full path if clang-format is not on the path.; # Change this to format according to other formatting styles. See the output of; # 'clang-format --help' for a list of supported styles. The default looks for; # a '.clang-format' or '_clang-format' file to indicate the style that should be; # used.; # FIXME: Without the 10ms delay, the viewport sometimes jumps.",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/clang-format/clang-format-sublime.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-format/clang-format-sublime.py
Deployability,integrat,integration,"# This file is a minimal clang-format vim-integration. To install:; # - Change 'binary' if clang-format is not on the path (see below).; # - Add to your .vimrc:; #; # if has('python'); # map <C-I> :pyf <path-to-this-file>/clang-format.py<cr>; # imap <C-I> <c-o>:pyf <path-to-this-file>/clang-format.py<cr>; # elseif has('python3'); # map <C-I> :py3f <path-to-this-file>/clang-format.py<cr>; # imap <C-I> <c-o>:py3f <path-to-this-file>/clang-format.py<cr>; # endif; #; # The if-elseif-endif conditional should pick either the python3 or python2; # integration depending on your vim setup.; #; # The first mapping enables clang-format for NORMAL and VISUAL mode, the second; # mapping adds support for INSERT mode. Change ""C-I"" to another binding if you; # need clang-format on a different key (C-I stands for Ctrl+i).; #; # With this integration you can press the bound key and clang-format will; # format the current line in NORMAL and INSERT mode or the selected region in; # VISUAL mode. The line or region is extended to the next bigger syntactic; # entity.; #; # You can also pass in the variable ""l:lines"" to choose the range for; # formatting. This variable can either contain ""<start line>:<end line>"" or; # ""all"" to format the full file. So, to format the full file, write a function; # like:; # :function FormatFile(); # : let l:lines=""all""; # : if has('python'); # : pyf <path-to-this-file>/clang-format.py; # : elseif has('python3'); # : py3f <path-to-this-file>/clang-format.py; # : endif; # :endfunction; #; # It operates on the current, potentially unsaved buffer and does not create; # or save any files. To revert a formatting, just undo.; # set g:clang_format_path to the path to clang-format if it is not on the path; # Change this to the full path if clang-format is not on the path.; # Change this to format according to other formatting styles. See the output of; # 'clang-format --help' for a list of supported styles. The default looks for; # a '.clang-format' or '_clang-format",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/clang-format/clang-format.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-format/clang-format.py
Integrability,integrat,integration,"# This file is a minimal clang-format vim-integration. To install:; # - Change 'binary' if clang-format is not on the path (see below).; # - Add to your .vimrc:; #; # if has('python'); # map <C-I> :pyf <path-to-this-file>/clang-format.py<cr>; # imap <C-I> <c-o>:pyf <path-to-this-file>/clang-format.py<cr>; # elseif has('python3'); # map <C-I> :py3f <path-to-this-file>/clang-format.py<cr>; # imap <C-I> <c-o>:py3f <path-to-this-file>/clang-format.py<cr>; # endif; #; # The if-elseif-endif conditional should pick either the python3 or python2; # integration depending on your vim setup.; #; # The first mapping enables clang-format for NORMAL and VISUAL mode, the second; # mapping adds support for INSERT mode. Change ""C-I"" to another binding if you; # need clang-format on a different key (C-I stands for Ctrl+i).; #; # With this integration you can press the bound key and clang-format will; # format the current line in NORMAL and INSERT mode or the selected region in; # VISUAL mode. The line or region is extended to the next bigger syntactic; # entity.; #; # You can also pass in the variable ""l:lines"" to choose the range for; # formatting. This variable can either contain ""<start line>:<end line>"" or; # ""all"" to format the full file. So, to format the full file, write a function; # like:; # :function FormatFile(); # : let l:lines=""all""; # : if has('python'); # : pyf <path-to-this-file>/clang-format.py; # : elseif has('python3'); # : py3f <path-to-this-file>/clang-format.py; # : endif; # :endfunction; #; # It operates on the current, potentially unsaved buffer and does not create; # or save any files. To revert a formatting, just undo.; # set g:clang_format_path to the path to clang-format if it is not on the path; # Change this to the full path if clang-format is not on the path.; # Change this to format according to other formatting styles. See the output of; # 'clang-format --help' for a list of supported styles. The default looks for; # a '.clang-format' or '_clang-format",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/clang-format/clang-format.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-format/clang-format.py
Modifiability,extend,extended," clang-format vim-integration. To install:; # - Change 'binary' if clang-format is not on the path (see below).; # - Add to your .vimrc:; #; # if has('python'); # map <C-I> :pyf <path-to-this-file>/clang-format.py<cr>; # imap <C-I> <c-o>:pyf <path-to-this-file>/clang-format.py<cr>; # elseif has('python3'); # map <C-I> :py3f <path-to-this-file>/clang-format.py<cr>; # imap <C-I> <c-o>:py3f <path-to-this-file>/clang-format.py<cr>; # endif; #; # The if-elseif-endif conditional should pick either the python3 or python2; # integration depending on your vim setup.; #; # The first mapping enables clang-format for NORMAL and VISUAL mode, the second; # mapping adds support for INSERT mode. Change ""C-I"" to another binding if you; # need clang-format on a different key (C-I stands for Ctrl+i).; #; # With this integration you can press the bound key and clang-format will; # format the current line in NORMAL and INSERT mode or the selected region in; # VISUAL mode. The line or region is extended to the next bigger syntactic; # entity.; #; # You can also pass in the variable ""l:lines"" to choose the range for; # formatting. This variable can either contain ""<start line>:<end line>"" or; # ""all"" to format the full file. So, to format the full file, write a function; # like:; # :function FormatFile(); # : let l:lines=""all""; # : if has('python'); # : pyf <path-to-this-file>/clang-format.py; # : elseif has('python3'); # : py3f <path-to-this-file>/clang-format.py; # : endif; # :endfunction; #; # It operates on the current, potentially unsaved buffer and does not create; # or save any files. To revert a formatting, just undo.; # set g:clang_format_path to the path to clang-format if it is not on the path; # Change this to the full path if clang-format is not on the path.; # Change this to format according to other formatting styles. See the output of; # 'clang-format --help' for a list of supported styles. The default looks for; # a '.clang-format' or '_clang-format' file to indicate the s",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/clang-format/clang-format.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-format/clang-format.py
Usability,undo,undo," NORMAL and VISUAL mode, the second; # mapping adds support for INSERT mode. Change ""C-I"" to another binding if you; # need clang-format on a different key (C-I stands for Ctrl+i).; #; # With this integration you can press the bound key and clang-format will; # format the current line in NORMAL and INSERT mode or the selected region in; # VISUAL mode. The line or region is extended to the next bigger syntactic; # entity.; #; # You can also pass in the variable ""l:lines"" to choose the range for; # formatting. This variable can either contain ""<start line>:<end line>"" or; # ""all"" to format the full file. So, to format the full file, write a function; # like:; # :function FormatFile(); # : let l:lines=""all""; # : if has('python'); # : pyf <path-to-this-file>/clang-format.py; # : elseif has('python3'); # : py3f <path-to-this-file>/clang-format.py; # : endif; # :endfunction; #; # It operates on the current, potentially unsaved buffer and does not create; # or save any files. To revert a formatting, just undo.; # set g:clang_format_path to the path to clang-format if it is not on the path; # Change this to the full path if clang-format is not on the path.; # Change this to format according to other formatting styles. See the output of; # 'clang-format --help' for a list of supported styles. The default looks for; # a '.clang-format' or '_clang-format' file to indicate the style that should be; # used.; # Get the current text.; # Join the buffer into a single string with a terminating newline; # Determine range to format.; # Convert cursor (line, col) to bytes.; # Don't use line2byte: https://github.com/vim/vim/issues/5930; # 1-based; # Avoid flashing an ugly, ugly cmd prompt on Windows when invoking clang-format.; # Call formatter.; # If successful, replace buffer contents.; # Strip off the trailing newline (added above).; # This maintains trailing empty lines present in the buffer if; # the -lines specification requests them to remain unchanged.; # Convert cursor bytes to ",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/clang-format/clang-format.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-format/clang-format.py
Deployability,integrat,integration,"""""""; Minimal clang-rename integration with Vim. Before installing make sure one of the following is satisfied:. * clang-rename is in your PATH; * `g:clang_rename_path` in ~/.vimrc points to valid clang-rename executable; * `binary` in clang-rename.py points to valid to clang-rename executable. To install, simply put this into your ~/.vimrc for python2 support. noremap <leader>cr :pyf <path-to>/clang-rename.py<cr>. For python3 use the following command (note the change from :pyf to :py3f). noremap <leader>cr :py3f <path-to>/clang-rename.py<cr>. IMPORTANT NOTE: Before running the tool, make sure you saved the file. All you have to do now is to place a cursor on a variable/function/class which; you would like to rename and press '<leader>cr'. You will be prompted for a new; name if the cursor points to a valid symbol.; """"""; # Get arguments for clang-rename binary.; # Call clang-rename.; # FIXME: make it possible to run the tool on unsaved file.; # Reload all buffers in Vim.",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/clang-rename/clang-rename.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-rename/clang-rename.py
Integrability,integrat,integration,"""""""; Minimal clang-rename integration with Vim. Before installing make sure one of the following is satisfied:. * clang-rename is in your PATH; * `g:clang_rename_path` in ~/.vimrc points to valid clang-rename executable; * `binary` in clang-rename.py points to valid to clang-rename executable. To install, simply put this into your ~/.vimrc for python2 support. noremap <leader>cr :pyf <path-to>/clang-rename.py<cr>. For python3 use the following command (note the change from :pyf to :py3f). noremap <leader>cr :py3f <path-to>/clang-rename.py<cr>. IMPORTANT NOTE: Before running the tool, make sure you saved the file. All you have to do now is to place a cursor on a variable/function/class which; you would like to rename and press '<leader>cr'. You will be prompted for a new; name if the cursor points to a valid symbol.; """"""; # Get arguments for clang-rename binary.; # Call clang-rename.; # FIXME: make it possible to run the tool on unsaved file.; # Reload all buffers in Vim.",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/clang-rename/clang-rename.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-rename/clang-rename.py
Modifiability,variab,variable,"""""""; Minimal clang-rename integration with Vim. Before installing make sure one of the following is satisfied:. * clang-rename is in your PATH; * `g:clang_rename_path` in ~/.vimrc points to valid clang-rename executable; * `binary` in clang-rename.py points to valid to clang-rename executable. To install, simply put this into your ~/.vimrc for python2 support. noremap <leader>cr :pyf <path-to>/clang-rename.py<cr>. For python3 use the following command (note the change from :pyf to :py3f). noremap <leader>cr :py3f <path-to>/clang-rename.py<cr>. IMPORTANT NOTE: Before running the tool, make sure you saved the file. All you have to do now is to place a cursor on a variable/function/class which; you would like to rename and press '<leader>cr'. You will be prompted for a new; name if the cursor points to a valid symbol.; """"""; # Get arguments for clang-rename binary.; # Call clang-rename.; # FIXME: make it possible to run the tool on unsaved file.; # Reload all buffers in Vim.",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/clang-rename/clang-rename.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-rename/clang-rename.py
Usability,simpl,simply,"""""""; Minimal clang-rename integration with Vim. Before installing make sure one of the following is satisfied:. * clang-rename is in your PATH; * `g:clang_rename_path` in ~/.vimrc points to valid clang-rename executable; * `binary` in clang-rename.py points to valid to clang-rename executable. To install, simply put this into your ~/.vimrc for python2 support. noremap <leader>cr :pyf <path-to>/clang-rename.py<cr>. For python3 use the following command (note the change from :pyf to :py3f). noremap <leader>cr :py3f <path-to>/clang-rename.py<cr>. IMPORTANT NOTE: Before running the tool, make sure you saved the file. All you have to do now is to place a cursor on a variable/function/class which; you would like to rename and press '<leader>cr'. You will be prompted for a new; name if the cursor points to a valid symbol.; """"""; # Get arguments for clang-rename binary.; # Call clang-rename.; # FIXME: make it possible to run the tool on unsaved file.; # Reload all buffers in Vim.",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/clang-rename/clang-rename.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-rename/clang-rename.py
Integrability,wrap,wrapped,"--------------------------------------------===#; # unqualifed symbol name, e.g. ""move""; # namespace of the symbol (with trailing ""::""), e.g. ""std::"", """" (global scope); # None for C symbols.; # a list of corresponding headers; """"""Parse symbol page and retrieve the include header defined in this page.; The symbol page provides header for the symbol, specifically in; ""Defined in header <header>"" section. An example:. <tr class=""t-dsc-header"">; <td colspan=""2""> <div>Defined in header <code>&lt;ratio&gt;</code> </div>; </td></tr>. Returns a list of headers.; """"""; # Rows in table are like:; # Defined in header <foo> .t-dsc-header; # Defined in header <bar> .t-dsc-header; # decl1 .t-dcl; # Defined in header <baz> .t-dsc-header; # decl2 .t-dcl; # Symbols are in the first cell.; # If we saw a decl since the last header, this is a new block of headers; # for a new block of decls.; # There are also .t-dsc-header for ""defined in namespace"".; # The interesting header content (e.g. <cstdlib>) is wrapped in <code>.; # If the symbol was never named, consider all named headers.; """"""Parse index page.; The index page lists all std symbols and hrefs to their detailed pages; (which contain the defined header). An example:. <a href=""abs.html"" title=""abs""><tt>abs()</tt></a> (int) <br>; <a href=""acos.html"" title=""acos""><tt>acos()</tt></a> <br>. Returns a list of tuple (symbol_name, relative_path_to_symbol_page, variant).; """"""; # Ignore annotated symbols like ""acos<>() (std::complex)"".; # These tend to be overloads, and we the primary is more useful.; # This accidentally accepts begin/end despite the (iterator) caption: the; # (since C++11) note is first. They are good symbols, so the bug is unfixed.; # strip any trailing <>(); """"""Get all symbols listed in the index page. All symbols should be in the; given namespace. Returns a list of Symbols.; """"""; # Workflow steps:; # 1. Parse index page which lists all symbols to get symbol; # name (unqualified name) and its href link to the symbol pag",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/include-mapping/cppreference_parser.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/include-mapping/cppreference_parser.py
Availability,avail,available,"// Used to build a lookup table (qualified names => include headers) for %s; // Standard Library symbols.; //; // This file was generated automatically by; // clang/tools/include-mapping/gen_std.py, DO NOT EDIT!; //; // Generated from cppreference offline HTML book (modified on %s).; //===----------------------------------------------------------------------===//; """"""; # IO-related symbols declared in the <iosfwd> header, per C++; # [iosfwd.syn 31.3.1]:; # <iostream> is preferred than <iosfwd>; # <iostream> is an alternative of <streambuf>, <istream>, <ostream>, <ios>.; # per C++ [iostream.syn 31.4.1]; # C++ form of the C standard headers.; # C++ [support.c.headers.other] 17.14.7; # ..., behaves as if each name placed in the standard library namespace by; # the corresponding <cname> header is placed within the global namespace; # scope, except for the functions described in [sf.cmath], the; # std​::​lerp function overloads ([c.math.lerp]), the declaration of; # std​::​byte ([cstddef.syn]), and the functions and function templates; # described in [support.types.byteops].; # Introduce two more entries, both in the global namespace, one using the; # C++-compat header and another using the C header.; # avoid printing duplicated entries, for C macros!; # <cstdio> => <stdio.h>; # std sub-namespace symbols have separated pages.; # We don't index std literal operators (e.g.; # std::literals::chrono_literals::operator""""d), these symbols can't be; # accessed by std::<symbol_name>.; #; # std::placeholders symbols are handled manually in StdSpecialSymbolMap.inc; # std::ranges::views can be accessed as std::views.; # Zombie symbols that were available from the Standard Library, but are; # removed in the following standards.; # We don't have version information from the unzipped offline HTML files.; # so we use the modified time of the symbol_index.html as the version.; # SYMBOL(unqualified_name, namespace, header); # FIXME: support symbols with multiple headers (e.g. std::move).",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/include-mapping/gen_std.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/include-mapping/gen_std.py
Deployability,install,installing-beautiful-soup,"usr/bin/env python3; # ===- gen_std.py - ------------------------------------------*- python -*--===#; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ===------------------------------------------------------------------------===#; """"""gen_std.py is a tool to generate a lookup table (from qualified names to; include headers) for C/C++ Standard Library symbols by parsing archived HTML; files from cppreference. The generated files are located in clang/include/Tooling/Inclusions. Caveats and FIXMEs:; - only symbols directly in ""std"" namespace are added, we should also add std's; subnamespace symbols (e.g. chrono).; - symbols with multiple variants or defined in multiple headers aren't added,; e.g. std::move, std::swap. Usage:; 1. Install BeautifulSoup dependency, see instruction:; https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-beautiful-soup; 2. Download cppreference offline HTML files (html_book_20220730.zip in Unofficial Release) at; https://en.cppreference.com/w/Cppreference:Archives; 3. Unzip the zip file from step 2 (e.g., to a ""cppreference"" directory). You should; get a ""cppreference/reference"" directory.; 4. Run the command:; // Generate C++ symbols; python3 gen_std.py -cppreference cppreference/reference -symbols=cpp > StdSymbolMap.inc; // Generate C symbols; python3 gen_std.py -cppreference cppreference/reference -symbols=c > CSymbolMap.inc; """"""; """"""\; //===-- gen_std.py generated file -------------------------------*- C++ -*-===//; //; // Used to build a lookup table (qualified names => include headers) for %s; // Standard Library symbols.; //; // This file was generated automatically by; // clang/tools/include-mapping/gen_std.py, DO NOT EDIT!; //; // Generated from cppreference offline HTML book (modified on %s).; //===----------------------------------------------------------------",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/include-mapping/gen_std.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/include-mapping/gen_std.py
Integrability,depend,dependency,"#!/usr/bin/env python3; # ===- gen_std.py - ------------------------------------------*- python -*--===#; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ===------------------------------------------------------------------------===#; """"""gen_std.py is a tool to generate a lookup table (from qualified names to; include headers) for C/C++ Standard Library symbols by parsing archived HTML; files from cppreference. The generated files are located in clang/include/Tooling/Inclusions. Caveats and FIXMEs:; - only symbols directly in ""std"" namespace are added, we should also add std's; subnamespace symbols (e.g. chrono).; - symbols with multiple variants or defined in multiple headers aren't added,; e.g. std::move, std::swap. Usage:; 1. Install BeautifulSoup dependency, see instruction:; https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-beautiful-soup; 2. Download cppreference offline HTML files (html_book_20220730.zip in Unofficial Release) at; https://en.cppreference.com/w/Cppreference:Archives; 3. Unzip the zip file from step 2 (e.g., to a ""cppreference"" directory). You should; get a ""cppreference/reference"" directory.; 4. Run the command:; // Generate C++ symbols; python3 gen_std.py -cppreference cppreference/reference -symbols=cpp > StdSymbolMap.inc; // Generate C symbols; python3 gen_std.py -cppreference cppreference/reference -symbols=c > CSymbolMap.inc; """"""; """"""\; //===-- gen_std.py generated file -------------------------------*- C++ -*-===//; //; // Used to build a lookup table (qualified names => include headers) for %s; // Standard Library symbols.; //; // This file was generated automatically by; // clang/tools/include-mapping/gen_std.py, DO NOT EDIT!; //; // Generated from cppreference offline HTML book (modified on %s).; //===-------------------------------------------------------------",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/include-mapping/gen_std.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/include-mapping/gen_std.py
Safety,avoid,avoid,"// Used to build a lookup table (qualified names => include headers) for %s; // Standard Library symbols.; //; // This file was generated automatically by; // clang/tools/include-mapping/gen_std.py, DO NOT EDIT!; //; // Generated from cppreference offline HTML book (modified on %s).; //===----------------------------------------------------------------------===//; """"""; # IO-related symbols declared in the <iosfwd> header, per C++; # [iosfwd.syn 31.3.1]:; # <iostream> is preferred than <iosfwd>; # <iostream> is an alternative of <streambuf>, <istream>, <ostream>, <ios>.; # per C++ [iostream.syn 31.4.1]; # C++ form of the C standard headers.; # C++ [support.c.headers.other] 17.14.7; # ..., behaves as if each name placed in the standard library namespace by; # the corresponding <cname> header is placed within the global namespace; # scope, except for the functions described in [sf.cmath], the; # std​::​lerp function overloads ([c.math.lerp]), the declaration of; # std​::​byte ([cstddef.syn]), and the functions and function templates; # described in [support.types.byteops].; # Introduce two more entries, both in the global namespace, one using the; # C++-compat header and another using the C header.; # avoid printing duplicated entries, for C macros!; # <cstdio> => <stdio.h>; # std sub-namespace symbols have separated pages.; # We don't index std literal operators (e.g.; # std::literals::chrono_literals::operator""""d), these symbols can't be; # accessed by std::<symbol_name>.; #; # std::placeholders symbols are handled manually in StdSpecialSymbolMap.inc; # std::ranges::views can be accessed as std::views.; # Zombie symbols that were available from the Standard Library, but are; # removed in the following standards.; # We don't have version information from the unzipped offline HTML files.; # so we use the modified time of the symbol_index.html as the version.; # SYMBOL(unqualified_name, namespace, header); # FIXME: support symbols with multiple headers (e.g. std::move).",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/include-mapping/gen_std.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/include-mapping/gen_std.py
Security,access,accessed,"// Used to build a lookup table (qualified names => include headers) for %s; // Standard Library symbols.; //; // This file was generated automatically by; // clang/tools/include-mapping/gen_std.py, DO NOT EDIT!; //; // Generated from cppreference offline HTML book (modified on %s).; //===----------------------------------------------------------------------===//; """"""; # IO-related symbols declared in the <iosfwd> header, per C++; # [iosfwd.syn 31.3.1]:; # <iostream> is preferred than <iosfwd>; # <iostream> is an alternative of <streambuf>, <istream>, <ostream>, <ios>.; # per C++ [iostream.syn 31.4.1]; # C++ form of the C standard headers.; # C++ [support.c.headers.other] 17.14.7; # ..., behaves as if each name placed in the standard library namespace by; # the corresponding <cname> header is placed within the global namespace; # scope, except for the functions described in [sf.cmath], the; # std​::​lerp function overloads ([c.math.lerp]), the declaration of; # std​::​byte ([cstddef.syn]), and the functions and function templates; # described in [support.types.byteops].; # Introduce two more entries, both in the global namespace, one using the; # C++-compat header and another using the C header.; # avoid printing duplicated entries, for C macros!; # <cstdio> => <stdio.h>; # std sub-namespace symbols have separated pages.; # We don't index std literal operators (e.g.; # std::literals::chrono_literals::operator""""d), these symbols can't be; # accessed by std::<symbol_name>.; #; # std::placeholders symbols are handled manually in StdSpecialSymbolMap.inc; # std::ranges::views can be accessed as std::views.; # Zombie symbols that were available from the Standard Library, but are; # removed in the following standards.; # We don't have version information from the unzipped offline HTML files.; # so we use the modified time of the symbol_index.html as the version.; # SYMBOL(unqualified_name, namespace, header); # FIXME: support symbols with multiple headers (e.g. std::move).",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/include-mapping/gen_std.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/include-mapping/gen_std.py
Testability,test,test,"#!/usr/bin/env python; # ===- test.py - ---------------------------------------------*- python -*--===#; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ===------------------------------------------------------------------------===#; """"""; <a href=""abs.html"" title=""abs""><tt>abs()</tt></a> (int) <br>; <a href=""complex/abs.html"" title=""abs""><tt>abs&lt;&gt;()</tt></a> (std::complex) <br>; <a href=""acos.html"" title=""acos""><tt>acos()</tt></a> <br>; <a href=""acosh.html"" title=""acosh""><tt>acosh()</tt></a> <span class=""t-mark-rev"">(since C++11)</span> <br>; <a href=""as_bytes.html"" title=""as bytes""><tt>as_bytes&lt;&gt;()</tt></a> <span class=""t-mark-rev t-since-cxx20"">(since C++20)</span> <br>; """"""; # Defined in header <cmath>; """"""; <table class=""t-dcl-begin""><tbody>; <tr class=""t-dsc-header"">; <td> <div>Defined in header <code><a href=""cmath.html"" title=""cmath"">&lt;cmath&gt;</a></code>; </div></td>; <td></td>; <td></td>; </tr>; <tr class=""t-dcl"">; <td>void foo()</td>; <td>this is matched</td>; </tr>; </tbody></table>; """"""; # Defined in header <cstddef>; # Defined in header <cstdio>; # Defined in header <cstdlib>; """"""; <table class=""t-dcl-begin""><tbody>; <tr class=""t-dsc-header"">; <td> <div>Defined in header <code><a href=""cstddef.html"" title=""cstddef"">&lt;cstddef&gt;</a></code>; </div></td>; <td></td>; <td></td>; </tr>; <tr class=""t-dcl"">; <td>void bar()</td>; <td>this mentions foo, but isn't matched</td>; </tr>; <tr class=""t-dsc-header"">; <td> <div>Defined in header <code><a href=""cstdio.html"" title=""cstdio"">&lt;cstdio&gt;</a></code>; </div></td>; <td></td>; <td></td>; </tr>; <tr class=""t-dsc-header"">; <td> <div>Defined in header <code><a href="".cstdlib.html"" title=""ccstdlib"">&lt;cstdlib&gt;</a></code>; </div></td>; <td></td>; <td></td>; </tr>; <tr class=""t-dcl"">; <td>; <span>void</span>; foo; <span>()</span>; </td>;",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/include-mapping/test.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/include-mapping/test.py
Integrability,interface,interface,"# -*- coding: utf-8 -*-; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; """""" This module compiles the intercept library. """"""; """"""Returns the full path to the 'libear' library.""""""; """"""Make subprocess execution silent.""""""; """"""Abstract class to represent different toolset.""""""; """"""part of public interface""""""; """"""part of public interface""""""; """"""part of public interface""""""; """"""#include <INCLUDE>; int main() { return ((int*)(&SYMBOL))[0]; }""""""",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-build-py/lib/libear/__init__.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/lib/libear/__init__.py
Availability,error,error," - AST file pairs.; :rtype: List of (str, str) tuples.; """"""; """"""Merge individual external definition maps into a global one. As the collect phase runs parallel on multiple threads, all compilation; units are separately mapped into a temporary file in CTU_TEMP_DEFMAP_FOLDER.; These definition maps contain the mangled names and the source; (AST generated from the source) which had their definition.; These files should be merged at the end into a global map file:; CTU_EXTDEF_MAP_FILENAME.""""""; """"""Iterate over all lines of input files in a determined order.""""""; """"""Write (mangled name, ast file) pairs into final file.""""""; # Remove all temporary files; """"""Runs the analyzer against the given compilation database.""""""; """"""Return true when any excluded directory prefix the filename.""""""; # filename is either absolute or relative to directory. Need to turn; # it to absolute since 'args.excludes' are absolute paths.; # when verbose output requested execute sequentially; # display error message from the static analyzer; """"""Governs multiple runs in CTU mode or runs once in normal mode.""""""; # If we do a CTU collect (1st phase) we remove all previous collection; # data first.; # If the user asked for a collect (1st) and analyze (2nd) phase, we do an; # all-in-one run where we deliberately remove collection data before and; # also after the run. If the user asks only for a single phase data is; # left so multiple analyze runs can use the same data gathered by a single; # collection run.; # CTU strings are coming from args.ctu_dir and extdef_map_cmd,; # so we can leave it empty; # Single runs (collect or analyze) are launched from here.; """"""Set up environment for build command to interpose compiler wrapper.""""""; """"""Entry point for `analyze-cc` and `analyze-c++` compiler wrappers.""""""; """"""Implements analyzer compiler wrapper functionality.""""""; # don't run analyzer when compilation fails. or when it's not requested.; # check is it a compilation?; # collect the needed parameters from environ",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/analyze.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/analyze.py
Deployability,configurat,configuration," the analyzer against the captured commands.; # Run build command and analyzer with compiler wrappers.; # Cover report generation and bug counting.; # Set exit status as it was requested.; """"""Entry point for analyze-build command.""""""; # will re-assign the report directory as new output; # Run the analyzer against a compilation db.; # Cover report generation and bug counting.; # Set exit status as it was requested.; """"""Check the intent of the build command. When static analyzer run against project configure step, it should be; silent and no need to run the analyzer or generate report. To run `scan-build` against the configure step might be necessary,; when compiler wrappers are used. That's the moment when build setup; check the compiler and capture the location for the build process.""""""; """"""From a sequence create another sequence where every second element; is from the original sequence and the odd elements are the prefix. eg.: prefix_with(0, [1,2,3]) creates [0, 1, 0, 2, 0, 3]""""""; """"""CTU configuration is created from the chosen phases and dir.""""""; """"""CTU configuration is created from the chosen phases and dir.""""""; # Recover namedtuple from json when coming from analyze-cc or analyze-c++; """"""Takes iterator of individual external definition maps and creates a; global map keeping only unique names. We leave conflicting names out of; CTU. :param extdef_map_lines: Contains the id of a definition (mangled name) and; the originating source (the corresponding AST file) name.; :type extdef_map_lines: Iterator of str.; :returns: Mangled name - AST file pairs.; :rtype: List of (str, str) tuples.; """"""; """"""Merge individual external definition maps into a global one. As the collect phase runs parallel on multiple threads, all compilation; units are separately mapped into a temporary file in CTU_TEMP_DEFMAP_FOLDER.; These definition maps contain the mangled names and the source; (AST generated from the source) which had their definition.; These files should be merged at the end i",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/analyze.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/analyze.py
Integrability,wrap,wrappers,"# -*- coding: utf-8 -*-; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; """""" This module implements the 'scan-build' command API. To run the static analyzer against a build is done in multiple steps:. -- Intercept: capture the compilation command during the build,; -- Analyze: run the analyzer against the captured commands,; -- Report: create a cover report from the analyzer outputs. """"""; """"""Entry point for scan-build command.""""""; # will re-assign the report directory as new output; # Run against a build command. there are cases, when analyzer run; # is not required. But we need to set up everything for the; # wrappers, because 'configure' needs to capture the CC/CXX values; # for the Makefile.; # Run build command with intercept module.; # Run the analyzer against the captured commands.; # Run build command and analyzer with compiler wrappers.; # Cover report generation and bug counting.; # Set exit status as it was requested.; """"""Entry point for analyze-build command.""""""; # will re-assign the report directory as new output; # Run the analyzer against a compilation db.; # Cover report generation and bug counting.; # Set exit status as it was requested.; """"""Check the intent of the build command. When static analyzer run against project configure step, it should be; silent and no need to run the analyzer or generate report. To run `scan-build` against the configure step might be necessary,; when compiler wrappers are used. That's the moment when build setup; check the compiler and capture the location for the build process.""""""; """"""From a sequence create another sequence where every second element; is from the original sequence and the odd elements are the prefix. eg.: prefix_with(0, [1,2,3]) creates [0, 1, 0, 2, 0, 3]""""""; """"""CTU configuration is created from the chosen phases and dir.""""""; """"""CTU configuration is creat",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/analyze.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/analyze.py
Modifiability,config,configure,"# -*- coding: utf-8 -*-; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; """""" This module implements the 'scan-build' command API. To run the static analyzer against a build is done in multiple steps:. -- Intercept: capture the compilation command during the build,; -- Analyze: run the analyzer against the captured commands,; -- Report: create a cover report from the analyzer outputs. """"""; """"""Entry point for scan-build command.""""""; # will re-assign the report directory as new output; # Run against a build command. there are cases, when analyzer run; # is not required. But we need to set up everything for the; # wrappers, because 'configure' needs to capture the CC/CXX values; # for the Makefile.; # Run build command with intercept module.; # Run the analyzer against the captured commands.; # Run build command and analyzer with compiler wrappers.; # Cover report generation and bug counting.; # Set exit status as it was requested.; """"""Entry point for analyze-build command.""""""; # will re-assign the report directory as new output; # Run the analyzer against a compilation db.; # Cover report generation and bug counting.; # Set exit status as it was requested.; """"""Check the intent of the build command. When static analyzer run against project configure step, it should be; silent and no need to run the analyzer or generate report. To run `scan-build` against the configure step might be necessary,; when compiler wrappers are used. That's the moment when build setup; check the compiler and capture the location for the build process.""""""; """"""From a sequence create another sequence where every second element; is from the original sequence and the odd elements are the prefix. eg.: prefix_with(0, [1,2,3]) creates [0, 1, 0, 2, 0, 3]""""""; """"""CTU configuration is created from the chosen phases and dir.""""""; """"""CTU configuration is creat",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/analyze.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/analyze.py
Testability,assert,assert,"""""""Decorator for checking the required values in state. It checks the required attributes in the passed state and stop when; any of those is missing.""""""; # entry from compilation database; # entry from compilation database; # entry from compilation database; # clang executable name (and path); # arguments from command line; # kill non debug macros; # where generated report files shall go; # it's 'plist', 'html', 'plist-html', 'plist-multi-file', 'sarif', or 'sarif-html'; # generate crash reports or not; # ctu control options; """"""Entry point to run (or not) static analyzer against a single entry; of the compilation database. This complex task is decomposed into smaller methods which are calling; each other in chain. If the analysis is not possible the given method; just return and break the chain. The passed parameter is a python dictionary. Each method first check; that the needed parameters received. (This is done by the 'require'; decorator. It's like an 'assert' to check the contract between the; caller and the called method.)""""""; """"""Create report when analyzer failed. The major report is the preprocessor output. The output filename generated; randomly. The compiler output also captured into '.stderr.txt' file.; And some more execution context also saved into '.info.txt' file.""""""; """"""Generate preprocessor file extension.""""""; """"""Creates failures directory if not exits yet.""""""; # Classify error type: when Clang terminated by a signal it's a 'Crash'.; # (python subprocess Popen.returncode is negative when child terminated; # by signal.) Everything else is 'Other Error'.; # Create preprocessor output file name. (This is blindly following the; # Perl implementation.); # Execute Clang again, but run the syntax check only.; # write general information about the crash; # write the captured output too; """"""It assembles the analysis command line and executes it. Capture the; output of the analysis and returns with it. If failure reports are; requested, it calls the continuat",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/analyze.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/analyze.py
Availability,failure,failures,"ry. (You can specify this option multiple times.); Could be useful when project contains 3rd party libraries.""""""; """"""Specifies the output directory for analyzer reports.; Subdirectory will be created if default directory is targeted.""""""; """"""Don't remove the build results directory even if no issues; were reported.""""""; """"""Specify the title used on generated HTML pages.; If not specified, a default title will be used.""""""; """"""Cause the results as a set of .plist files.""""""; """"""Cause the results as a set of .html and .plist files.""""""; """"""Cause the results as a set of .plist files with extra; information on related files.""""""; """"""Cause the results as a result.sarif file.""""""; """"""Cause the results as a result.sarif file and .html files.""""""; """"""'%(prog)s' uses the 'clang' executable relative to itself for; static analysis. One can override this behavior with this option by; using the 'clang' packaged with Xcode (on OS X) or from the PATH.""""""; """"""Do not create a 'failures' subdirectory that includes analyzer; crash reports and preprocessed source files.""""""; """"""Also analyze functions in #included files. By default, such; functions are skipped unless they are called by functions within the; main source file.""""""; """"""Generates visitation statistics for the project.""""""; """"""Generate internal analyzer statistics.""""""; """"""Specify the number of times a block can be visited before; giving up. Increase for more comprehensive coverage at a cost of; speed.""""""; """"""Specify the store model used by the analyzer. 'region'; specifies a field- sensitive store model. 'basic' which is far less; precise but can more quickly analyze code. 'basic' was the default; store model for checker-0.221 and earlier.""""""; """"""Specify the constraint engine used by the analyzer. Specifying; 'basic' uses a simpler, less powerful constraint model used by; checker-0.160 and earlier.""""""; """"""Provide options to pass through to the analyzer's; -analyzer-config flag. Several options are separated with comma:; 'key1=val1,key2",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/arguments.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/arguments.py
Deployability,continuous,continuously,"spath as it is needed inside clang; """"""Command line parsing is done by the argparse module, but semantic; validation still needs to be done. This method is doing it for; analyze-build and scan-build commands. :param parser: The command line parser object.; :param args: Parsed argument object.; :param from_build_command: Boolean value tells is the command suppose; to run the analyzer against a build command or a compilation db.; :return: No return value, but this call might throw when validation; fails.""""""; # If the user wants CTU mode; # If CTU analyze_only, the input directory should exist; # Check CTU capability via checking clang-extdef-mapping; """"""This version of clang does not support CTU; functionality or clang-extdef-mapping command not found.""""""; """"""Creates a parser for command-line arguments to 'intercept'.""""""; """"""Extend existing compilation database with new entries.; Duplicate entries are detected and not present in the final output.; The output is not continuously updated, it's done when the build; command finished. """"""; """"""Command to run.""""""; """"""Creates a parser for command-line arguments to 'analyze'.""""""; """"""Run the build commands first, intercept compiler; calls and then run the static analyzer afterwards.; Generally speaking it has better coverage on build commands.; With '--override-compiler' it use compiler wrapper, but does; not run the analyzer till the build is finished.""""""; """"""The exit status of '%(prog)s' is the same as the executed; build command. This option ignores the build exit status and sets to; be non zero if it found potential bugs or zero otherwise.""""""; """"""Do not run static analyzer against files found in this; directory. (You can specify this option multiple times.); Could be useful when project contains 3rd party libraries.""""""; """"""Specifies the output directory for analyzer reports.; Subdirectory will be created if default directory is targeted.""""""; """"""Don't remove the build results directory even if no issues; were reported.""""""; """"",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/arguments.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/arguments.py
Energy Efficiency,power,powerful,"lf for; static analysis. One can override this behavior with this option by; using the 'clang' packaged with Xcode (on OS X) or from the PATH.""""""; """"""Do not create a 'failures' subdirectory that includes analyzer; crash reports and preprocessed source files.""""""; """"""Also analyze functions in #included files. By default, such; functions are skipped unless they are called by functions within the; main source file.""""""; """"""Generates visitation statistics for the project.""""""; """"""Generate internal analyzer statistics.""""""; """"""Specify the number of times a block can be visited before; giving up. Increase for more comprehensive coverage at a cost of; speed.""""""; """"""Specify the store model used by the analyzer. 'region'; specifies a field- sensitive store model. 'basic' which is far less; precise but can more quickly analyze code. 'basic' was the default; store model for checker-0.221 and earlier.""""""; """"""Specify the constraint engine used by the analyzer. Specifying; 'basic' uses a simpler, less powerful constraint model used by; checker-0.160 and earlier.""""""; """"""Provide options to pass through to the analyzer's; -analyzer-config flag. Several options are separated with comma:; 'key1=val1,key2=val2'. Available options:; stable-report-filename=true or false (default). Switch the page naming to:; report-<filename>-<function/method name>-<id>.html; instead of report-XXXXXX.html""""""; """"""Tells analyzer to enable assertions in code even if they were; disabled during compilation, enabling more precise results.""""""; """"""Loading external checkers using the clang plugin interface.""""""; """"""Enable specific checker.""""""; """"""Disable specific checker.""""""; """"""A default group of checkers is run unless explicitly disabled.; Exactly which checkers constitute the default group is a function of; the operating system in use. These can be printed with this flag.""""""; """"""Print all available checkers and mark the enabled ones.""""""; """"""Command to run.""""""; """"""Perform cross translation unit (ctu) analysis (both c",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/arguments.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/arguments.py
Integrability,interface,interfaces,"# -*- coding: utf-8 -*-; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; """""" This module parses and validates arguments for command-line interfaces. It uses argparse module to create the command line parser. (This library is; in the standard python library since 3.2 and backported to 2.7, but not; earlier.). It also implements basic validation methods, related to the command.; Validations are mostly calling specific help methods, or mangling values.; """"""; """"""Parse and validate command-line arguments for intercept-build.""""""; # short validation logic; """"""Parse and validate command-line arguments for analyze-build.""""""; """"""Parse and validate command-line arguments for scan-build.""""""; """"""Normalize parsed arguments for analyze-build and scan-build. :param args: Parsed argument object. (Will be mutated.); :param from_build_command: Boolean value tells is the command suppose; to run the analyzer against a build command or a compilation db.""""""; # make plugins always a list. (it might be None when not specified.); # make exclude directory list unique and absolute.; # because shared codes for all tools, some common used methods are; # expecting some argument to be present. so, instead of query the args; # object about the presence of the flag, we fake it here. to make those; # methods more readable. (it's an arguable choice, took it only for those; # which have good default value.); # add cdb parameter invisibly to make report module working.; # Make ctu_dir an abspath as it is needed inside clang; """"""Command line parsing is done by the argparse module, but semantic; validation still needs to be done. This method is doing it for; analyze-build and scan-build commands. :param parser: The command line parser object.; :param args: Parsed argument object.; :param from_build_command: Boolean value tells is the command suppose; to ru",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/arguments.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/arguments.py
Modifiability,plugin,plugins,"ps://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; """""" This module parses and validates arguments for command-line interfaces. It uses argparse module to create the command line parser. (This library is; in the standard python library since 3.2 and backported to 2.7, but not; earlier.). It also implements basic validation methods, related to the command.; Validations are mostly calling specific help methods, or mangling values.; """"""; """"""Parse and validate command-line arguments for intercept-build.""""""; # short validation logic; """"""Parse and validate command-line arguments for analyze-build.""""""; """"""Parse and validate command-line arguments for scan-build.""""""; """"""Normalize parsed arguments for analyze-build and scan-build. :param args: Parsed argument object. (Will be mutated.); :param from_build_command: Boolean value tells is the command suppose; to run the analyzer against a build command or a compilation db.""""""; # make plugins always a list. (it might be None when not specified.); # make exclude directory list unique and absolute.; # because shared codes for all tools, some common used methods are; # expecting some argument to be present. so, instead of query the args; # object about the presence of the flag, we fake it here. to make those; # methods more readable. (it's an arguable choice, took it only for those; # which have good default value.); # add cdb parameter invisibly to make report module working.; # Make ctu_dir an abspath as it is needed inside clang; """"""Command line parsing is done by the argparse module, but semantic; validation still needs to be done. This method is doing it for; analyze-build and scan-build commands. :param parser: The command line parser object.; :param args: Parsed argument object.; :param from_build_command: Boolean value tells is the command suppose; to run the analyzer against a build command or a compilation db.; :return: No return value, but this call might throw w",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/arguments.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/arguments.py
Safety,detect,detected," cdb parameter invisibly to make report module working.; # Make ctu_dir an abspath as it is needed inside clang; """"""Command line parsing is done by the argparse module, but semantic; validation still needs to be done. This method is doing it for; analyze-build and scan-build commands. :param parser: The command line parser object.; :param args: Parsed argument object.; :param from_build_command: Boolean value tells is the command suppose; to run the analyzer against a build command or a compilation db.; :return: No return value, but this call might throw when validation; fails.""""""; # If the user wants CTU mode; # If CTU analyze_only, the input directory should exist; # Check CTU capability via checking clang-extdef-mapping; """"""This version of clang does not support CTU; functionality or clang-extdef-mapping command not found.""""""; """"""Creates a parser for command-line arguments to 'intercept'.""""""; """"""Extend existing compilation database with new entries.; Duplicate entries are detected and not present in the final output.; The output is not continuously updated, it's done when the build; command finished. """"""; """"""Command to run.""""""; """"""Creates a parser for command-line arguments to 'analyze'.""""""; """"""Run the build commands first, intercept compiler; calls and then run the static analyzer afterwards.; Generally speaking it has better coverage on build commands.; With '--override-compiler' it use compiler wrapper, but does; not run the analyzer till the build is finished.""""""; """"""The exit status of '%(prog)s' is the same as the executed; build command. This option ignores the build exit status and sets to; be non zero if it found potential bugs or zero otherwise.""""""; """"""Do not run static analyzer against files found in this; directory. (You can specify this option multiple times.); Could be useful when project contains 3rd party libraries.""""""; """"""Specifies the output directory for analyzer reports.; Subdirectory will be created if default directory is targeted.""""""; """"""Don'",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/arguments.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/arguments.py
Security,validat,validates,"# -*- coding: utf-8 -*-; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; """""" This module parses and validates arguments for command-line interfaces. It uses argparse module to create the command line parser. (This library is; in the standard python library since 3.2 and backported to 2.7, but not; earlier.). It also implements basic validation methods, related to the command.; Validations are mostly calling specific help methods, or mangling values.; """"""; """"""Parse and validate command-line arguments for intercept-build.""""""; # short validation logic; """"""Parse and validate command-line arguments for analyze-build.""""""; """"""Parse and validate command-line arguments for scan-build.""""""; """"""Normalize parsed arguments for analyze-build and scan-build. :param args: Parsed argument object. (Will be mutated.); :param from_build_command: Boolean value tells is the command suppose; to run the analyzer against a build command or a compilation db.""""""; # make plugins always a list. (it might be None when not specified.); # make exclude directory list unique and absolute.; # because shared codes for all tools, some common used methods are; # expecting some argument to be present. so, instead of query the args; # object about the presence of the flag, we fake it here. to make those; # methods more readable. (it's an arguable choice, took it only for those; # which have good default value.); # add cdb parameter invisibly to make report module working.; # Make ctu_dir an abspath as it is needed inside clang; """"""Command line parsing is done by the argparse module, but semantic; validation still needs to be done. This method is doing it for; analyze-build and scan-build commands. :param parser: The command line parser object.; :param args: Parsed argument object.; :param from_build_command: Boolean value tells is the command suppose; to ru",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/arguments.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/arguments.py
Testability,log,logic,"# -*- coding: utf-8 -*-; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; """""" This module parses and validates arguments for command-line interfaces. It uses argparse module to create the command line parser. (This library is; in the standard python library since 3.2 and backported to 2.7, but not; earlier.). It also implements basic validation methods, related to the command.; Validations are mostly calling specific help methods, or mangling values.; """"""; """"""Parse and validate command-line arguments for intercept-build.""""""; # short validation logic; """"""Parse and validate command-line arguments for analyze-build.""""""; """"""Parse and validate command-line arguments for scan-build.""""""; """"""Normalize parsed arguments for analyze-build and scan-build. :param args: Parsed argument object. (Will be mutated.); :param from_build_command: Boolean value tells is the command suppose; to run the analyzer against a build command or a compilation db.""""""; # make plugins always a list. (it might be None when not specified.); # make exclude directory list unique and absolute.; # because shared codes for all tools, some common used methods are; # expecting some argument to be present. so, instead of query the args; # object about the presence of the flag, we fake it here. to make those; # methods more readable. (it's an arguable choice, took it only for those; # which have good default value.); # add cdb parameter invisibly to make report module working.; # Make ctu_dir an abspath as it is needed inside clang; """"""Command line parsing is done by the argparse module, but semantic; validation still needs to be done. This method is doing it for; analyze-build and scan-build commands. :param parser: The command line parser object.; :param args: Parsed argument object.; :param from_build_command: Boolean value tells is the command suppose; to ru",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/arguments.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/arguments.py
Usability,simpl,simpler,"lf for; static analysis. One can override this behavior with this option by; using the 'clang' packaged with Xcode (on OS X) or from the PATH.""""""; """"""Do not create a 'failures' subdirectory that includes analyzer; crash reports and preprocessed source files.""""""; """"""Also analyze functions in #included files. By default, such; functions are skipped unless they are called by functions within the; main source file.""""""; """"""Generates visitation statistics for the project.""""""; """"""Generate internal analyzer statistics.""""""; """"""Specify the number of times a block can be visited before; giving up. Increase for more comprehensive coverage at a cost of; speed.""""""; """"""Specify the store model used by the analyzer. 'region'; specifies a field- sensitive store model. 'basic' which is far less; precise but can more quickly analyze code. 'basic' was the default; store model for checker-0.221 and earlier.""""""; """"""Specify the constraint engine used by the analyzer. Specifying; 'basic' uses a simpler, less powerful constraint model used by; checker-0.160 and earlier.""""""; """"""Provide options to pass through to the analyzer's; -analyzer-config flag. Several options are separated with comma:; 'key1=val1,key2=val2'. Available options:; stable-report-filename=true or false (default). Switch the page naming to:; report-<filename>-<function/method name>-<id>.html; instead of report-XXXXXX.html""""""; """"""Tells analyzer to enable assertions in code even if they were; disabled during compilation, enabling more precise results.""""""; """"""Loading external checkers using the clang plugin interface.""""""; """"""Enable specific checker.""""""; """"""Disable specific checker.""""""; """"""A default group of checkers is run unless explicitly disabled.; Exactly which checkers constitute the default group is a function of; the operating system in use. These can be printed with this flag.""""""; """"""Print all available checkers and mark the enabled ones.""""""; """"""Command to run.""""""; """"""Perform cross translation unit (ctu) analysis (both c",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/arguments.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/arguments.py
Availability,avail,available,"the current working directory; :return: the detailed front-end invocation command""""""; # The relevant information is in the last line of the output.; # Don't check if finding last line fails, would throw exception anyway.; """"""Get the active checker list. :param clang: the compiler we are using; :param plugins: list of plugins which was requested by the user; :return: list of checker names which are active. To get the default checkers we execute Clang to print how this; compilation would be called. And take out the enabled checker from the; arguments. For input file we specify stdin and pass only language; information.""""""; """"""Returns a list of active checkers for the given language.""""""; """"""Returns a method, which classifies the checker active or not,; based on the received checker name list.""""""; """"""Returns True if the given checker is active.""""""; """"""Parse clang -analyzer-checker-help output. Below the line 'CHECKERS:' are there the name description pairs.; Many of them are in one line, but some long named checker has the; name and the description in separate lines. The checker name is always prefixed with two space character. The; name contains no whitespaces. Then followed by newline (if it's; too long) or other space characters comes the description of the; checker. The description ends with a newline character. :param stream: list of lines to parse; :return: generator of tuples. (<checker name>, <checker description>)""""""; # find checkers header; # find entries; """"""Get all the available checkers from default and from the plugins. :param clang: the compiler we are using; :param plugins: list of plugins which was requested by the user; :return: a dictionary of all available checkers and its status. {<checker name>: (<checker description>, <is active by default>)}""""""; """"""Detects if the current (or given) clang and external definition mapping; executables are CTU compatible.""""""; """"""Returns the architecture part of the target triple for the given; compilation command.""""""",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/clang.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/clang.py
Integrability,interface,interface,"# -*- coding: utf-8 -*-; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; """""" This module is responsible for the Clang executable. Since Clang command line interface is so rich, but this project is using only; a subset of that, it makes sense to create a function specific wrapper. """"""; # regex for activated checker; """"""Returns the compiler version as string. :param clang: the compiler we are using; :return: the version string printed to stderr""""""; # the relevant version info is in the first line; """"""Capture Clang invocation. :param command: the compilation command; :param cwd: the current working directory; :return: the detailed front-end invocation command""""""; # The relevant information is in the last line of the output.; # Don't check if finding last line fails, would throw exception anyway.; """"""Get the active checker list. :param clang: the compiler we are using; :param plugins: list of plugins which was requested by the user; :return: list of checker names which are active. To get the default checkers we execute Clang to print how this; compilation would be called. And take out the enabled checker from the; arguments. For input file we specify stdin and pass only language; information.""""""; """"""Returns a list of active checkers for the given language.""""""; """"""Returns a method, which classifies the checker active or not,; based on the received checker name list.""""""; """"""Returns True if the given checker is active.""""""; """"""Parse clang -analyzer-checker-help output. Below the line 'CHECKERS:' are there the name description pairs.; Many of them are in one line, but some long named checker has the; name and the description in separate lines. The checker name is always prefixed with two space character. The; name contains no whitespaces. Then followed by newline (if it's; too long) or other space characters comes the descrip",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/clang.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/clang.py
Modifiability,plugin,plugins,"che License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; """""" This module is responsible for the Clang executable. Since Clang command line interface is so rich, but this project is using only; a subset of that, it makes sense to create a function specific wrapper. """"""; # regex for activated checker; """"""Returns the compiler version as string. :param clang: the compiler we are using; :return: the version string printed to stderr""""""; # the relevant version info is in the first line; """"""Capture Clang invocation. :param command: the compilation command; :param cwd: the current working directory; :return: the detailed front-end invocation command""""""; # The relevant information is in the last line of the output.; # Don't check if finding last line fails, would throw exception anyway.; """"""Get the active checker list. :param clang: the compiler we are using; :param plugins: list of plugins which was requested by the user; :return: list of checker names which are active. To get the default checkers we execute Clang to print how this; compilation would be called. And take out the enabled checker from the; arguments. For input file we specify stdin and pass only language; information.""""""; """"""Returns a list of active checkers for the given language.""""""; """"""Returns a method, which classifies the checker active or not,; based on the received checker name list.""""""; """"""Returns True if the given checker is active.""""""; """"""Parse clang -analyzer-checker-help output. Below the line 'CHECKERS:' are there the name description pairs.; Many of them are in one line, but some long named checker has the; name and the description in separate lines. The checker name is always prefixed with two space character. The; name contains no whitespaces. Then followed by newline (if it's; too long) or other space characters comes the description of the; checker. The description ends with a newline characte",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/clang.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/clang.py
Integrability,wrap,wrappers,"# -*- coding: utf-8 -*-; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; """""" This module is responsible to capture the compiler invocation of any; build process. The result of that should be a compilation database. This implementation is using the LD_PRELOAD or DYLD_INSERT_LIBRARIES; mechanisms provided by the dynamic linker. The related library is implemented; in C language and can be found under 'libear' directory. The 'libear' library is capturing all child process creation and logging the; relevant information about it into separate files in a specified directory.; The parameter of this process is the output directory name, where the report; files shall be placed. This parameter is passed as an environment variable. The module also implements compiler wrappers to intercept the compiler calls. The module implements the build command execution and the post-processing of; the output files, which will condensates into a compilation database. """"""; # same as in ear.c; """"""Entry point for 'intercept-build' command.""""""; """"""The entry point of build command interception.""""""; """"""To make a compilation database, it needs to filter out commands; which are not compiler calls. Needs to find the source file name; from the arguments. And do shell escaping on the command. To support incremental builds, it is desired to read elements from; an existing compilation database from a previous run. These elements; shall be merged with the new elements.""""""; # create entries from the current run; # creates a sequence of entry generators from an exec,; # read entries from previous run; # filter out duplicate entries from both; # run the build command; # read the intercepted exec calls; # do post processing; # dump the compilation database; """"""Sets up the environment for the build command. It sets the required environment variables and execute",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/intercept.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/intercept.py
Modifiability,variab,variable,"# -*- coding: utf-8 -*-; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; """""" This module is responsible to capture the compiler invocation of any; build process. The result of that should be a compilation database. This implementation is using the LD_PRELOAD or DYLD_INSERT_LIBRARIES; mechanisms provided by the dynamic linker. The related library is implemented; in C language and can be found under 'libear' directory. The 'libear' library is capturing all child process creation and logging the; relevant information about it into separate files in a specified directory.; The parameter of this process is the output directory name, where the report; files shall be placed. This parameter is passed as an environment variable. The module also implements compiler wrappers to intercept the compiler calls. The module implements the build command execution and the post-processing of; the output files, which will condensates into a compilation database. """"""; # same as in ear.c; """"""Entry point for 'intercept-build' command.""""""; """"""The entry point of build command interception.""""""; """"""To make a compilation database, it needs to filter out commands; which are not compiler calls. Needs to find the source file name; from the arguments. And do shell escaping on the command. To support incremental builds, it is desired to read elements from; an existing compilation database from a previous run. These elements; shall be merged with the new elements.""""""; # create entries from the current run; # creates a sequence of entry generators from an exec,; # read entries from previous run; # filter out duplicate entries from both; # run the build command; # read the intercepted exec calls; # do post processing; # dump the compilation database; """"""Sets up the environment for the build command. It sets the required environment variables and execute",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/intercept.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/intercept.py
Safety,detect,detected,"""""Sets up the environment for the build command. It sets the required environment variables and execute the given command.; The exec calls will be logged by the 'libear' preloaded library or by the; 'wrapper' programs.""""""; """"""Entry point for `intercept-cc` and `intercept-c++`.""""""; """"""Implement intercept compiler wrapper functionality. It does generate execution report into target directory.; The target directory name is from environment variables.""""""; # write current execution info to the pid file; """"""Write execution report file. This method shall be sync with the execution report writer in interception; library. The entry in the file is a JSON objects. :param filename: path to the output execution trace file,; :param entry: the Execution object to append to that file.""""""; """"""Parse the file generated by the 'libear' preloaded library. Given filename points to a file which contains the basic report; generated by the interception library or wrapper command. A single; report file _might_ contain multiple process creation info.""""""; """"""Generate the desired fields for compilation database entries.""""""; """"""Create normalized absolute path from input filename.""""""; """"""Library-based interposition will fail silently if SIP is enabled,; so this should be detected. You can detect whether SIP is enabled on; Darwin by checking whether (1) there is a binary called 'csrutil' in; the path and, if so, (2) whether the output of executing 'csrutil status'; contains 'System Integrity Protection status: enabled'. :param platform: name of the platform (returned by sys.platform),; :return: True if library preload will fail by the dynamic linker.""""""; """"""Implement unique hash method for compilation database entries.""""""; # For faster lookup in set filename is reverted; # For faster lookup in set directory is reverted; # On OS X the 'cc' and 'c++' compilers are wrappers for; # 'clang' therefore both call would be logged. To avoid; # this the hash does not contain the first word of the; # command.",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/intercept.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/intercept.py
Security,hash,hash,"""""Sets up the environment for the build command. It sets the required environment variables and execute the given command.; The exec calls will be logged by the 'libear' preloaded library or by the; 'wrapper' programs.""""""; """"""Entry point for `intercept-cc` and `intercept-c++`.""""""; """"""Implement intercept compiler wrapper functionality. It does generate execution report into target directory.; The target directory name is from environment variables.""""""; # write current execution info to the pid file; """"""Write execution report file. This method shall be sync with the execution report writer in interception; library. The entry in the file is a JSON objects. :param filename: path to the output execution trace file,; :param entry: the Execution object to append to that file.""""""; """"""Parse the file generated by the 'libear' preloaded library. Given filename points to a file which contains the basic report; generated by the interception library or wrapper command. A single; report file _might_ contain multiple process creation info.""""""; """"""Generate the desired fields for compilation database entries.""""""; """"""Create normalized absolute path from input filename.""""""; """"""Library-based interposition will fail silently if SIP is enabled,; so this should be detected. You can detect whether SIP is enabled on; Darwin by checking whether (1) there is a binary called 'csrutil' in; the path and, if so, (2) whether the output of executing 'csrutil status'; contains 'System Integrity Protection status: enabled'. :param platform: name of the platform (returned by sys.platform),; :return: True if library preload will fail by the dynamic linker.""""""; """"""Implement unique hash method for compilation database entries.""""""; # For faster lookup in set filename is reverted; # For faster lookup in set directory is reverted; # On OS X the 'cc' and 'c++' compilers are wrappers for; # 'clang' therefore both call would be logged. To avoid; # this the hash does not contain the first word of the; # command.",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/intercept.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/intercept.py
Testability,log,logging,"# -*- coding: utf-8 -*-; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; """""" This module is responsible to capture the compiler invocation of any; build process. The result of that should be a compilation database. This implementation is using the LD_PRELOAD or DYLD_INSERT_LIBRARIES; mechanisms provided by the dynamic linker. The related library is implemented; in C language and can be found under 'libear' directory. The 'libear' library is capturing all child process creation and logging the; relevant information about it into separate files in a specified directory.; The parameter of this process is the output directory name, where the report; files shall be placed. This parameter is passed as an environment variable. The module also implements compiler wrappers to intercept the compiler calls. The module implements the build command execution and the post-processing of; the output files, which will condensates into a compilation database. """"""; # same as in ear.c; """"""Entry point for 'intercept-build' command.""""""; """"""The entry point of build command interception.""""""; """"""To make a compilation database, it needs to filter out commands; which are not compiler calls. Needs to find the source file name; from the arguments. And do shell escaping on the command. To support incremental builds, it is desired to read elements from; an existing compilation database from a previous run. These elements; shall be merged with the new elements.""""""; # create entries from the current run; # creates a sequence of entry generators from an exec,; # read entries from previous run; # filter out duplicate entries from both; # run the build command; # read the intercepted exec calls; # do post processing; # dump the compilation database; """"""Sets up the environment for the build command. It sets the required environment variables and execute",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/intercept.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/intercept.py
Deployability,update,updates,"f=""{file}"">preprocessor output</a></td>; | <td><a href=""{stderr}"">analyzer std err</a></td>; | </tr>""""""; """"""; | </tbody>; |</table>""""""; """"""Generate a unique sequence of crashes from given output directory.""""""; # type: (str, bool) -> Generator[Dict[str, Any], None, None]; """"""Generate a unique sequence of bugs from given output directory. Duplicates can be in a project if the same module was compiled multiple; times with different compiler options. These would be better to show in; the final report (cover) only once.""""""; # get the right parser for the job.; # get the input files, which are not empty.; """"""Reads and merges all .sarif files in the given output directory. Each sarif file in the output directory is understood as a single run; and thus appear separate in the top level runs array. This requires; modifying the run index of any embedded links in messages.; """"""; """"""; Given a SARIF object, checks its dictionary entries for a 'message' property.; If it exists, updates the message index of embedded links in the run index. Recursively looks through entries in the dictionary.; """"""; # iterate through subobjects and update it.; # do nothing; """"""; Given a SARIF message object, checks if the text property contains an embedded link and; updates the run index if necessary.; """"""; # we only merge runs, so we only need to update the run index; # update matches from right to left to make increasing character length (9->10) smoother; # exposed for testing since the order of files returned by glob is not guaranteed to be sorted; # start with the first file; # extract the run and append it to the merged output; """"""Returns the generator of bugs from a single .plist file.""""""; """"""Parse out the bug information from HTML output.""""""; # compatibility with < clang-3.5; # do not read the file further; # search for the right lines; """"""Parse out the crash information from the report file.""""""; # this is a workaround to fix windows read '\r\n' as new lines.; """"""Create a new bug attribute from",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/report.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/report.py
Integrability,message,messages,"td>; | <td>Preprocessed File</td>; | <td>STDERR Output</td>; | </tr>; | </thead>; | <tbody>""""""; """"""; | <tr>; | <td>{problem}</td>; | <td>{source}</td>; | <td><a href=""{file}"">preprocessor output</a></td>; | <td><a href=""{stderr}"">analyzer std err</a></td>; | </tr>""""""; """"""; | </tbody>; |</table>""""""; """"""Generate a unique sequence of crashes from given output directory.""""""; # type: (str, bool) -> Generator[Dict[str, Any], None, None]; """"""Generate a unique sequence of bugs from given output directory. Duplicates can be in a project if the same module was compiled multiple; times with different compiler options. These would be better to show in; the final report (cover) only once.""""""; # get the right parser for the job.; # get the input files, which are not empty.; """"""Reads and merges all .sarif files in the given output directory. Each sarif file in the output directory is understood as a single run; and thus appear separate in the top level runs array. This requires; modifying the run index of any embedded links in messages.; """"""; """"""; Given a SARIF object, checks its dictionary entries for a 'message' property.; If it exists, updates the message index of embedded links in the run index. Recursively looks through entries in the dictionary.; """"""; # iterate through subobjects and update it.; # do nothing; """"""; Given a SARIF message object, checks if the text property contains an embedded link and; updates the run index if necessary.; """"""; # we only merge runs, so we only need to update the run index; # update matches from right to left to make increasing character length (9->10) smoother; # exposed for testing since the order of files returned by glob is not guaranteed to be sorted; # start with the first file; # extract the run and append it to the merged output; """"""Returns the generator of bugs from a single .plist file.""""""; """"""Parse out the bug information from HTML output.""""""; # compatibility with < clang-3.5; # do not read the file further; # search for the right li",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/report.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/report.py
Safety,safe,safe," Given a SARIF message object, checks if the text property contains an embedded link and; updates the run index if necessary.; """"""; # we only merge runs, so we only need to update the run index; # update matches from right to left to make increasing character length (9->10) smoother; # exposed for testing since the order of files returned by glob is not guaranteed to be sorted; # start with the first file; # extract the run and append it to the merged output; """"""Returns the generator of bugs from a single .plist file.""""""; """"""Parse out the bug information from HTML output.""""""; # compatibility with < clang-3.5; # do not read the file further; # search for the right lines; """"""Parse out the crash information from the report file.""""""; # this is a workaround to fix windows read '\r\n' as new lines.; """"""Create a new bug attribute from bug by category and type. The result will be used as CSS class selector in the final report.""""""; """"""Make value ready to be HTML attribute value.""""""; """"""Create counters for bug statistics. Two entries are maintained: 'total' is an integer, represents the; number of bugs. The 'categories' is a two level categorisation of bug; counters. The first level is 'bug category' the second is 'bug type'.; Each entry in this classification is a dictionary of 'count', 'type'; and 'label'.""""""; """"""Make safe this values to embed into HTML.""""""; """"""Make safe this values to embed into HTML.""""""; """"""Copy the javascript and css files to the report directory.""""""; """"""Run 'encode' on 'container[key]' value and update it.""""""; """"""Create 'filename' from '/prefix/filename'""""""; """"""Paranoid HTML escape method. (Python version independent)""""""; """"""Utility function to format html output and keep indentation.""""""; """"""Utility function to format meta information as comment.""""""; """"""Create file prefix from a compilation database entries.""""""; """"""Fixed version of os.path.commonprefix. :param files: list of file names.; :return: the longest path prefix that is a prefix of all files.""""""",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/report.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/report.py
Security,expose,exposed," in; the final report (cover) only once.""""""; # get the right parser for the job.; # get the input files, which are not empty.; """"""Reads and merges all .sarif files in the given output directory. Each sarif file in the output directory is understood as a single run; and thus appear separate in the top level runs array. This requires; modifying the run index of any embedded links in messages.; """"""; """"""; Given a SARIF object, checks its dictionary entries for a 'message' property.; If it exists, updates the message index of embedded links in the run index. Recursively looks through entries in the dictionary.; """"""; # iterate through subobjects and update it.; # do nothing; """"""; Given a SARIF message object, checks if the text property contains an embedded link and; updates the run index if necessary.; """"""; # we only merge runs, so we only need to update the run index; # update matches from right to left to make increasing character length (9->10) smoother; # exposed for testing since the order of files returned by glob is not guaranteed to be sorted; # start with the first file; # extract the run and append it to the merged output; """"""Returns the generator of bugs from a single .plist file.""""""; """"""Parse out the bug information from HTML output.""""""; # compatibility with < clang-3.5; # do not read the file further; # search for the right lines; """"""Parse out the crash information from the report file.""""""; # this is a workaround to fix windows read '\r\n' as new lines.; """"""Create a new bug attribute from bug by category and type. The result will be used as CSS class selector in the final report.""""""; """"""Make value ready to be HTML attribute value.""""""; """"""Create counters for bug statistics. Two entries are maintained: 'total' is an integer, represents the; number of bugs. The 'categories' is a two level categorisation of bug; counters. The first level is 'bug category' the second is 'bug type'.; Each entry in this classification is a dictionary of 'count', 'type'; and 'label'.",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/report.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/report.py
Testability,test,testing," in; the final report (cover) only once.""""""; # get the right parser for the job.; # get the input files, which are not empty.; """"""Reads and merges all .sarif files in the given output directory. Each sarif file in the output directory is understood as a single run; and thus appear separate in the top level runs array. This requires; modifying the run index of any embedded links in messages.; """"""; """"""; Given a SARIF object, checks its dictionary entries for a 'message' property.; If it exists, updates the message index of embedded links in the run index. Recursively looks through entries in the dictionary.; """"""; # iterate through subobjects and update it.; # do nothing; """"""; Given a SARIF message object, checks if the text property contains an embedded link and; updates the run index if necessary.; """"""; # we only merge runs, so we only need to update the run index; # update matches from right to left to make increasing character length (9->10) smoother; # exposed for testing since the order of files returned by glob is not guaranteed to be sorted; # start with the first file; # extract the run and append it to the merged output; """"""Returns the generator of bugs from a single .plist file.""""""; """"""Parse out the bug information from HTML output.""""""; # compatibility with < clang-3.5; # do not read the file further; # search for the right lines; """"""Parse out the crash information from the report file.""""""; # this is a workaround to fix windows read '\r\n' as new lines.; """"""Create a new bug attribute from bug by category and type. The result will be used as CSS class selector in the final report.""""""; """"""Make value ready to be HTML attribute value.""""""; """"""Create counters for bug statistics. Two entries are maintained: 'total' is an integer, represents the; number of bugs. The 'categories' is a two level categorisation of bug; counters. The first level is 'bug category' the second is 'bug type'.; Each entry in this classification is a dictionary of 'count', 'type'; and 'label'.",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/report.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/report.py
Availability,error,errors,"duplicated entries. Unique hash method can be use to detect duplicates. Entries are; represented as dictionaries, which has no default hash method.; This implementation uses a set datatype to store the unique hash values. This method returns a method which can detect the duplicate values.""""""; """"""Run and report build command execution. :param command: array of tokens; :return: exit code of the process; """"""; """"""Run a given command and report the execution. :param command: array of tokens; :param cwd: the working directory where the command will be executed; :return: output of the command; """"""; """"""check_output returns bytes or string depend on python version""""""; """"""Reconfigure logging level and format based on the verbose flag. :param verbose_level: number of `-v` flags received by the command; :return: no return value; """"""; # Exit when nothing to do.; # Tune logging level.; # Be verbose with messages.; """"""Decorator for command entry methods. The decorator initialize/shutdown logging and guard on programming; errors (catch exceptions). The decorated method can have arbitrary parameters, the return value will; be the exit code of the process.""""""; """"""Do housekeeping tasks and execute the wrapped method.""""""; # This hack to get the executable name as %(name).; # Signal received exit code for bash.; # Some non used exit code for internal errors.; """"""Implements compiler wrapper base functionality. A compiler wrapper executes the real compiler, then implement some; functionality, then returns with the real compiler exit code. :param function: the extra functionality what the wrapper want to; do on top of the compiler call. If it throws exception, it will be; caught and logged.; :return: the exit code of the real compiler. The :param function: will receive the following arguments:. :param result: the exit code of the compilation.; :param execution: the command executed by the wrapper.""""""; """"""Find out was it a C++ compiler call. Compiler wrapper names; contain the compiler type.",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/__init__.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/__init__.py
Integrability,depend,depend,"# -*- coding: utf-8 -*-; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; """""" This module is a collection of methods commonly used in this project. """"""; """"""Predicate to detect duplicated entries. Unique hash method can be use to detect duplicates. Entries are; represented as dictionaries, which has no default hash method.; This implementation uses a set datatype to store the unique hash values. This method returns a method which can detect the duplicate values.""""""; """"""Run and report build command execution. :param command: array of tokens; :return: exit code of the process; """"""; """"""Run a given command and report the execution. :param command: array of tokens; :param cwd: the working directory where the command will be executed; :return: output of the command; """"""; """"""check_output returns bytes or string depend on python version""""""; """"""Reconfigure logging level and format based on the verbose flag. :param verbose_level: number of `-v` flags received by the command; :return: no return value; """"""; # Exit when nothing to do.; # Tune logging level.; # Be verbose with messages.; """"""Decorator for command entry methods. The decorator initialize/shutdown logging and guard on programming; errors (catch exceptions). The decorated method can have arbitrary parameters, the return value will; be the exit code of the process.""""""; """"""Do housekeeping tasks and execute the wrapped method.""""""; # This hack to get the executable name as %(name).; # Signal received exit code for bash.; # Some non used exit code for internal errors.; """"""Implements compiler wrapper base functionality. A compiler wrapper executes the real compiler, then implement some; functionality, then returns with the real compiler exit code. :param function: the extra functionality what the wrapper want to; do on top of the compiler call. If it throws exception, it will ",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/__init__.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/__init__.py
Safety,detect,detect,"# -*- coding: utf-8 -*-; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; """""" This module is a collection of methods commonly used in this project. """"""; """"""Predicate to detect duplicated entries. Unique hash method can be use to detect duplicates. Entries are; represented as dictionaries, which has no default hash method.; This implementation uses a set datatype to store the unique hash values. This method returns a method which can detect the duplicate values.""""""; """"""Run and report build command execution. :param command: array of tokens; :return: exit code of the process; """"""; """"""Run a given command and report the execution. :param command: array of tokens; :param cwd: the working directory where the command will be executed; :return: output of the command; """"""; """"""check_output returns bytes or string depend on python version""""""; """"""Reconfigure logging level and format based on the verbose flag. :param verbose_level: number of `-v` flags received by the command; :return: no return value; """"""; # Exit when nothing to do.; # Tune logging level.; # Be verbose with messages.; """"""Decorator for command entry methods. The decorator initialize/shutdown logging and guard on programming; errors (catch exceptions). The decorated method can have arbitrary parameters, the return value will; be the exit code of the process.""""""; """"""Do housekeeping tasks and execute the wrapped method.""""""; # This hack to get the executable name as %(name).; # Signal received exit code for bash.; # Some non used exit code for internal errors.; """"""Implements compiler wrapper base functionality. A compiler wrapper executes the real compiler, then implement some; functionality, then returns with the real compiler exit code. :param function: the extra functionality what the wrapper want to; do on top of the compiler call. If it throws exception, it will ",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/__init__.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/__init__.py
Security,hash,hash,"# -*- coding: utf-8 -*-; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; """""" This module is a collection of methods commonly used in this project. """"""; """"""Predicate to detect duplicated entries. Unique hash method can be use to detect duplicates. Entries are; represented as dictionaries, which has no default hash method.; This implementation uses a set datatype to store the unique hash values. This method returns a method which can detect the duplicate values.""""""; """"""Run and report build command execution. :param command: array of tokens; :return: exit code of the process; """"""; """"""Run a given command and report the execution. :param command: array of tokens; :param cwd: the working directory where the command will be executed; :return: output of the command; """"""; """"""check_output returns bytes or string depend on python version""""""; """"""Reconfigure logging level and format based on the verbose flag. :param verbose_level: number of `-v` flags received by the command; :return: no return value; """"""; # Exit when nothing to do.; # Tune logging level.; # Be verbose with messages.; """"""Decorator for command entry methods. The decorator initialize/shutdown logging and guard on programming; errors (catch exceptions). The decorated method can have arbitrary parameters, the return value will; be the exit code of the process.""""""; """"""Do housekeeping tasks and execute the wrapped method.""""""; # This hack to get the executable name as %(name).; # Signal received exit code for bash.; # Some non used exit code for internal errors.; """"""Implements compiler wrapper base functionality. A compiler wrapper executes the real compiler, then implement some; functionality, then returns with the real compiler exit code. :param function: the extra functionality what the wrapper want to; do on top of the compiler call. If it throws exception, it will ",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/__init__.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/__init__.py
Testability,log,logging,"# -*- coding: utf-8 -*-; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; """""" This module is a collection of methods commonly used in this project. """"""; """"""Predicate to detect duplicated entries. Unique hash method can be use to detect duplicates. Entries are; represented as dictionaries, which has no default hash method.; This implementation uses a set datatype to store the unique hash values. This method returns a method which can detect the duplicate values.""""""; """"""Run and report build command execution. :param command: array of tokens; :return: exit code of the process; """"""; """"""Run a given command and report the execution. :param command: array of tokens; :param cwd: the working directory where the command will be executed; :return: output of the command; """"""; """"""check_output returns bytes or string depend on python version""""""; """"""Reconfigure logging level and format based on the verbose flag. :param verbose_level: number of `-v` flags received by the command; :return: no return value; """"""; # Exit when nothing to do.; # Tune logging level.; # Be verbose with messages.; """"""Decorator for command entry methods. The decorator initialize/shutdown logging and guard on programming; errors (catch exceptions). The decorated method can have arbitrary parameters, the return value will; be the exit code of the process.""""""; """"""Do housekeeping tasks and execute the wrapped method.""""""; # This hack to get the executable name as %(name).; # Signal received exit code for bash.; # Some non used exit code for internal errors.; """"""Implements compiler wrapper base functionality. A compiler wrapper executes the real compiler, then implement some; functionality, then returns with the real compiler exit code. :param function: the extra functionality what the wrapper want to; do on top of the compiler call. If it throws exception, it will ",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/__init__.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-build-py/lib/libscanbuild/__init__.py
Availability,failure,failures,"#!/usr/bin/env python; # -*- coding: utf-8 -*-; """"""Methods for reporting bugs.""""""; #; """"""Generic exception for failures in bug reporting.""""""; # Collect information about a bug.; # Reporter interfaces.; # ===------------------------------------------------------------------------===#; # ReporterParameter; # ===------------------------------------------------------------------------===#; """"""\; <tr>; <td class=""form_clabel"">%s:</td>; <td class=""form_value""><input type=""text"" name=""%s_%s"" value=""%s""></td>; </tr>""""""; """"""\; <tr>; <td class=""form_clabel"">%s:</td><td class=""form_value""><select name=""%s_%s"">; %s; </select></td>""""""; """"""\; <option value=""%s""%s>%s</option>""""""; # ===------------------------------------------------------------------------===#; # Reporters; # ===------------------------------------------------------------------------===#; # Lifted from python email module examples.; # Guess the content type based on the file's extension. Encoding; # will be ignored, although we should check for simple things like; # gzip'd or compressed files.; # No guess could be made, or the file is encoded (compressed), so; # use a generic bag-of-bits type.; # Note: we should handle calculating the charset; # Encode the payload using Base64; # Set the filename parameter; """"""\; BUG REPORT; ---; Title: %s; Description: %s; """"""; # FIXME: Get config parameters; ###",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-view/share/Reporter.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-view/share/Reporter.py
Integrability,interface,interfaces,"#!/usr/bin/env python; # -*- coding: utf-8 -*-; """"""Methods for reporting bugs.""""""; #; """"""Generic exception for failures in bug reporting.""""""; # Collect information about a bug.; # Reporter interfaces.; # ===------------------------------------------------------------------------===#; # ReporterParameter; # ===------------------------------------------------------------------------===#; """"""\; <tr>; <td class=""form_clabel"">%s:</td>; <td class=""form_value""><input type=""text"" name=""%s_%s"" value=""%s""></td>; </tr>""""""; """"""\; <tr>; <td class=""form_clabel"">%s:</td><td class=""form_value""><select name=""%s_%s"">; %s; </select></td>""""""; """"""\; <option value=""%s""%s>%s</option>""""""; # ===------------------------------------------------------------------------===#; # Reporters; # ===------------------------------------------------------------------------===#; # Lifted from python email module examples.; # Guess the content type based on the file's extension. Encoding; # will be ignored, although we should check for simple things like; # gzip'd or compressed files.; # No guess could be made, or the file is encoded (compressed), so; # use a generic bag-of-bits type.; # Note: we should handle calculating the charset; # Encode the payload using Base64; # Set the filename parameter; """"""\; BUG REPORT; ---; Title: %s; Description: %s; """"""; # FIXME: Get config parameters; ###",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-view/share/Reporter.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-view/share/Reporter.py
Modifiability,config,config,"#!/usr/bin/env python; # -*- coding: utf-8 -*-; """"""Methods for reporting bugs.""""""; #; """"""Generic exception for failures in bug reporting.""""""; # Collect information about a bug.; # Reporter interfaces.; # ===------------------------------------------------------------------------===#; # ReporterParameter; # ===------------------------------------------------------------------------===#; """"""\; <tr>; <td class=""form_clabel"">%s:</td>; <td class=""form_value""><input type=""text"" name=""%s_%s"" value=""%s""></td>; </tr>""""""; """"""\; <tr>; <td class=""form_clabel"">%s:</td><td class=""form_value""><select name=""%s_%s"">; %s; </select></td>""""""; """"""\; <option value=""%s""%s>%s</option>""""""; # ===------------------------------------------------------------------------===#; # Reporters; # ===------------------------------------------------------------------------===#; # Lifted from python email module examples.; # Guess the content type based on the file's extension. Encoding; # will be ignored, although we should check for simple things like; # gzip'd or compressed files.; # No guess could be made, or the file is encoded (compressed), so; # use a generic bag-of-bits type.; # Note: we should handle calculating the charset; # Encode the payload using Base64; # Set the filename parameter; """"""\; BUG REPORT; ---; Title: %s; Description: %s; """"""; # FIXME: Get config parameters; ###",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-view/share/Reporter.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-view/share/Reporter.py
Usability,simpl,simple,"#!/usr/bin/env python; # -*- coding: utf-8 -*-; """"""Methods for reporting bugs.""""""; #; """"""Generic exception for failures in bug reporting.""""""; # Collect information about a bug.; # Reporter interfaces.; # ===------------------------------------------------------------------------===#; # ReporterParameter; # ===------------------------------------------------------------------------===#; """"""\; <tr>; <td class=""form_clabel"">%s:</td>; <td class=""form_value""><input type=""text"" name=""%s_%s"" value=""%s""></td>; </tr>""""""; """"""\; <tr>; <td class=""form_clabel"">%s:</td><td class=""form_value""><select name=""%s_%s"">; %s; </select></td>""""""; """"""\; <option value=""%s""%s>%s</option>""""""; # ===------------------------------------------------------------------------===#; # Reporters; # ===------------------------------------------------------------------------===#; # Lifted from python email module examples.; # Guess the content type based on the file's extension. Encoding; # will be ignored, although we should check for simple things like; # gzip'd or compressed files.; # No guess could be made, or the file is encoded (compressed), so; # use a generic bag-of-bits type.; # Note: we should handle calculating the charset; # Encode the payload using Base64; # Set the filename parameter; """"""\; BUG REPORT; ---; Title: %s; Description: %s; """"""; # FIXME: Get config parameters; ###",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-view/share/Reporter.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-view/share/Reporter.py
Availability,error,errors," file=""crashes/clang_crash_ndSGF9.mi"" stderr=""crashes/clang_crash_ndSGF9.mi.stderr.txt"" info=""crashes/clang_crash_ndSGF9.mi.info"" -->; # Add custom javascript.; """"""\; <script language=""javascript"" type=""text/javascript"">; function load(url) {; if (window.XMLHttpRequest) {; req = new XMLHttpRequest();; } else if (window.ActiveXObject) {; req = new ActiveXObject(""Microsoft.XMLHTTP"");; }; if (req != undefined) {; req.open(""GET"", url, true);; req.send("""");; }; }; </script>""""""; # Insert additional columns.; # Insert report bug and open file links.; # Insert report crashes link.; # Disabled for the time being until we decide exactly when this should; # be enabled. Also the radar reporter needs to be fixed to report; # multiple files.; # kReportReplacements.append((re.compile('<!-- REPORTCRASHES -->'),; # '<br>These files will automatically be attached to ' +; # 'reports filed here: <a href=""report_crashes"">Report Crashes</a>.')); ###; # Other simple parameters; ###; # Add defaults; # Ignore parse errors; # Save on exit; # Ignore errors (only called on exit).; # Ignore socket errors; # Borrowed from Quixote, with simplifications.; """"""Serve a POST request.""""""; # Get the reporter and parameters.; # Update config defaults.; # Create the report.; # Kick off a reporting thread.; # Wait for thread to die...; """"""\; <a href=""/report_crashes"">File Bug</a> > """"""; """"""; <head>; <title>Bug Submission</title>; <link rel=""stylesheet"" type=""text/css"" href=""/scanview.css"" />; </head>; <body>; <h3>; <a href=""/"">Summary</a> > ; %(reportingFor)s; %(fileBug)s; Submit</h3>; <form name=""form"" action="""">; <table class=""form"">; <tr><td>; <table class=""form_group"">; <tr>; <td class=""form_clabel"">Title:</td>; <td class=""form_value"">; <input type=""text"" name=""title"" size=""50"" value=""%(title)s"" disabled>; </td>; </tr>; <tr>; <td class=""form_label"">Description:</td>; <td class=""form_value"">; <textarea rows=""10"" cols=""80"" name=""description"" disabled>; %(description)s; </textarea>; </td>; </table>; </td><",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-view/share/ScanView.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-view/share/ScanView.py
Deployability,update,updateReporterOptions,"""""""; # Just in case something failed, ignore files which don't; # exist.; # Check that this is a valid report.; """"""\; Bug reported by the clang static analyzer. Description: %s; File: %s; Line: %s; """"""; """"""<a href=""/%s"">Report %s</a> > """"""; # report is None is used for crashes; """"""\; <iframe src=""/%s"" width=""100%%"" height=""40%%""; scrolling=""auto"" frameborder=""1"">; <a href=""/%s"">View Bug Report</a>; </iframe>""""""; """"""\; <tr id=""%sReporterOptions"" style=""display:%s"">; <td class=""form_label"">%s Options</td>; <td class=""form_value"">; <table class=""form_inner_group"">; %s; </table>; </td>; </tr>; """"""; """"""\; <option value=""%d"" selected>%s</option>""""""; """"""\; <tr>; <td class=""form_label"">Attach:</td>; <td class=""form_value"">; <select style=""width:100%%"" name=""files"" multiple size=%d>; %s; </select>; </td>; </tr>; """"""; """"""<html>; <head>; <title>File Bug</title>; <link rel=""stylesheet"" type=""text/css"" href=""/scanview.css"" />; </head>; <script language=""javascript"" type=""text/javascript"">; var reporters = %(reportersArray)s;; function updateReporterOptions() {; index = document.getElementById('reporter').selectedIndex;; for (var i=0; i < reporters.length; ++i) {; o = document.getElementById(reporters[i] + ""ReporterOptions"");; if (i == index) {; o.style.display = """";; } else {; o.style.display = ""none"";; }; }; }; </script>; <body onLoad=""updateReporterOptions()"">; <h3>; <a href=""/"">Summary</a> > ; %(reportingFor)s; File Bug</h3>; <form name=""form"" action=""/report_submit"" method=""post"">; <input type=""hidden"" name=""report"" value=""%(report)s"">. <table class=""form"">; <tr><td>; <table class=""form_group"">; <tr>; <td class=""form_clabel"">Title:</td>; <td class=""form_value"">; <input type=""text"" name=""title"" size=""50"" value=""%(title)s"">; </td>; </tr>; <tr>; <td class=""form_label"">Description:</td>; <td class=""form_value"">; <textarea rows=""10"" cols=""80"" name=""description"">; %(description)s; </textarea>; </td>; </tr>. %(attachFileRow)s. </table>; <br>; <table class=""form_group"">; <tr>; <td cl",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-view/share/ScanView.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-view/share/ScanView.py
Integrability,message,message," automatically be attached to ' +; # 'reports filed here: <a href=""report_crashes"">Report Crashes</a>.')); ###; # Other simple parameters; ###; # Add defaults; # Ignore parse errors; # Save on exit; # Ignore errors (only called on exit).; # Ignore socket errors; # Borrowed from Quixote, with simplifications.; """"""Serve a POST request.""""""; # Get the reporter and parameters.; # Update config defaults.; # Create the report.; # Kick off a reporting thread.; # Wait for thread to die...; """"""\; <a href=""/report_crashes"">File Bug</a> > """"""; """"""; <head>; <title>Bug Submission</title>; <link rel=""stylesheet"" type=""text/css"" href=""/scanview.css"" />; </head>; <body>; <h3>; <a href=""/"">Summary</a> > ; %(reportingFor)s; %(fileBug)s; Submit</h3>; <form name=""form"" action="""">; <table class=""form"">; <tr><td>; <table class=""form_group"">; <tr>; <td class=""form_clabel"">Title:</td>; <td class=""form_value"">; <input type=""text"" name=""title"" size=""50"" value=""%(title)s"" disabled>; </td>; </tr>; <tr>; <td class=""form_label"">Description:</td>; <td class=""form_value"">; <textarea rows=""10"" cols=""80"" name=""description"" disabled>; %(description)s; </textarea>; </td>; </table>; </td></tr>; </table>; </form>; <h1 class=""%(statusClass)s"">Submission %(statusName)s</h1>; %(message)s; <p>; <hr>; <a href=""/"">Return to Summary</a>; </body>; </html>""""""; # Don't allow empty reports.; """"""\; The clang static analyzer failed on these inputs:; %s. STDERR Summary; --------------; %s; """"""; # Just in case something failed, ignore files which don't; # exist.; # Check that this is a valid report.; """"""\; Bug reported by the clang static analyzer. Description: %s; File: %s; Line: %s; """"""; """"""<a href=""/%s"">Report %s</a> > """"""; # report is None is used for crashes; """"""\; <iframe src=""/%s"" width=""100%%"" height=""40%%""; scrolling=""auto"" frameborder=""1"">; <a href=""/%s"">View Bug Report</a>; </iframe>""""""; """"""\; <tr id=""%sReporterOptions"" style=""display:%s"">; <td class=""form_label"">%s Options</td>; <td class=""form_value"">; <ta",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-view/share/ScanView.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-view/share/ScanView.py
Modifiability,config,config,""">; function load(url) {; if (window.XMLHttpRequest) {; req = new XMLHttpRequest();; } else if (window.ActiveXObject) {; req = new ActiveXObject(""Microsoft.XMLHTTP"");; }; if (req != undefined) {; req.open(""GET"", url, true);; req.send("""");; }; }; </script>""""""; # Insert additional columns.; # Insert report bug and open file links.; # Insert report crashes link.; # Disabled for the time being until we decide exactly when this should; # be enabled. Also the radar reporter needs to be fixed to report; # multiple files.; # kReportReplacements.append((re.compile('<!-- REPORTCRASHES -->'),; # '<br>These files will automatically be attached to ' +; # 'reports filed here: <a href=""report_crashes"">Report Crashes</a>.')); ###; # Other simple parameters; ###; # Add defaults; # Ignore parse errors; # Save on exit; # Ignore errors (only called on exit).; # Ignore socket errors; # Borrowed from Quixote, with simplifications.; """"""Serve a POST request.""""""; # Get the reporter and parameters.; # Update config defaults.; # Create the report.; # Kick off a reporting thread.; # Wait for thread to die...; """"""\; <a href=""/report_crashes"">File Bug</a> > """"""; """"""; <head>; <title>Bug Submission</title>; <link rel=""stylesheet"" type=""text/css"" href=""/scanview.css"" />; </head>; <body>; <h3>; <a href=""/"">Summary</a> > ; %(reportingFor)s; %(fileBug)s; Submit</h3>; <form name=""form"" action="""">; <table class=""form"">; <tr><td>; <table class=""form_group"">; <tr>; <td class=""form_clabel"">Title:</td>; <td class=""form_value"">; <input type=""text"" name=""title"" size=""50"" value=""%(title)s"" disabled>; </td>; </tr>; <tr>; <td class=""form_label"">Description:</td>; <td class=""form_value"">; <textarea rows=""10"" cols=""80"" name=""description"" disabled>; %(description)s; </textarea>; </td>; </table>; </td></tr>; </table>; </form>; <h1 class=""%(statusClass)s"">Submission %(statusName)s</h1>; %(message)s; <p>; <hr>; <a href=""/"">Return to Summary</a>; </body>; </html>""""""; # Don't allow empty reports.; """"""\; The clang static ",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-view/share/ScanView.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-view/share/ScanView.py
Performance,load,load,"###; # Various patterns matched or replaced by server.; # <!-- REPORTPROBLEM file=""crashes/clang_crash_ndSGF9.mi"" stderr=""crashes/clang_crash_ndSGF9.mi.stderr.txt"" info=""crashes/clang_crash_ndSGF9.mi.info"" -->; # Add custom javascript.; """"""\; <script language=""javascript"" type=""text/javascript"">; function load(url) {; if (window.XMLHttpRequest) {; req = new XMLHttpRequest();; } else if (window.ActiveXObject) {; req = new ActiveXObject(""Microsoft.XMLHTTP"");; }; if (req != undefined) {; req.open(""GET"", url, true);; req.send("""");; }; }; </script>""""""; # Insert additional columns.; # Insert report bug and open file links.; # Insert report crashes link.; # Disabled for the time being until we decide exactly when this should; # be enabled. Also the radar reporter needs to be fixed to report; # multiple files.; # kReportReplacements.append((re.compile('<!-- REPORTCRASHES -->'),; # '<br>These files will automatically be attached to ' +; # 'reports filed here: <a href=""report_crashes"">Report Crashes</a>.')); ###; # Other simple parameters; ###; # Add defaults; # Ignore parse errors; # Save on exit; # Ignore errors (only called on exit).; # Ignore socket errors; # Borrowed from Quixote, with simplifications.; """"""Serve a POST request.""""""; # Get the reporter and parameters.; # Update config defaults.; # Create the report.; # Kick off a reporting thread.; # Wait for thread to die...; """"""\; <a href=""/report_crashes"">File Bug</a> > """"""; """"""; <head>; <title>Bug Submission</title>; <link rel=""stylesheet"" type=""text/css"" href=""/scanview.css"" />; </head>; <body>; <h3>; <a href=""/"">Summary</a> > ; %(reportingFor)s; %(fileBug)s; Submit</h3>; <form name=""form"" action="""">; <table class=""form"">; <tr><td>; <table class=""form_group"">; <tr>; <td class=""form_clabel"">Title:</td>; <td class=""form_value"">; <input type=""text"" name=""title"" size=""50"" value=""%(title)s"" disabled>; </td>; </tr>; <tr>; <td class=""form_label"">Description:</td>; <td class=""form_value"">; <textarea rows=""10"" cols=""80"" name=""d",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-view/share/ScanView.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-view/share/ScanView.py
Usability,simpl,simple," file=""crashes/clang_crash_ndSGF9.mi"" stderr=""crashes/clang_crash_ndSGF9.mi.stderr.txt"" info=""crashes/clang_crash_ndSGF9.mi.info"" -->; # Add custom javascript.; """"""\; <script language=""javascript"" type=""text/javascript"">; function load(url) {; if (window.XMLHttpRequest) {; req = new XMLHttpRequest();; } else if (window.ActiveXObject) {; req = new ActiveXObject(""Microsoft.XMLHTTP"");; }; if (req != undefined) {; req.open(""GET"", url, true);; req.send("""");; }; }; </script>""""""; # Insert additional columns.; # Insert report bug and open file links.; # Insert report crashes link.; # Disabled for the time being until we decide exactly when this should; # be enabled. Also the radar reporter needs to be fixed to report; # multiple files.; # kReportReplacements.append((re.compile('<!-- REPORTCRASHES -->'),; # '<br>These files will automatically be attached to ' +; # 'reports filed here: <a href=""report_crashes"">Report Crashes</a>.')); ###; # Other simple parameters; ###; # Add defaults; # Ignore parse errors; # Save on exit; # Ignore errors (only called on exit).; # Ignore socket errors; # Borrowed from Quixote, with simplifications.; """"""Serve a POST request.""""""; # Get the reporter and parameters.; # Update config defaults.; # Create the report.; # Kick off a reporting thread.; # Wait for thread to die...; """"""\; <a href=""/report_crashes"">File Bug</a> > """"""; """"""; <head>; <title>Bug Submission</title>; <link rel=""stylesheet"" type=""text/css"" href=""/scanview.css"" />; </head>; <body>; <h3>; <a href=""/"">Summary</a> > ; %(reportingFor)s; %(fileBug)s; Submit</h3>; <form name=""form"" action="""">; <table class=""form"">; <tr><td>; <table class=""form_group"">; <tr>; <td class=""form_clabel"">Title:</td>; <td class=""form_value"">; <input type=""text"" name=""title"" size=""50"" value=""%(title)s"" disabled>; </td>; </tr>; <tr>; <td class=""form_label"">Description:</td>; <td class=""form_value"">; <textarea rows=""10"" cols=""80"" name=""description"" disabled>; %(description)s; </textarea>; </td>; </table>; </td><",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/tools/scan-view/share/ScanView.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/scan-view/share/ScanView.py
Availability,avail,available,"langdiag is; enabled, it will use the appropriate diagtool application to determine; the name of the DiagID, and set breakpoints in all locations that; 'diag::name' appears in the source. Since the new breakpoints are set; after they are encountered, users will need to launch the executable a; second time in order to hit the new breakpoints. For in-tree builds, the diagtool application, used to map DiagID's to; names, is found automatically in the same directory as the target; executable. However, out-or-tree builds must use the 'diagtool'; subcommand to set the appropriate path for diagtool in the clang debug; bin directory. Since this mapping is created at build-time, it's; important for users to use the same version that was generated when; clang was compiled, or else the id's won't match. Notes:; - Substrings can be passed for both <warning> and <diag-name>.; - If <warning> is passed, only enable the DiagID(s) for that warning.; - If <diag-name> is passed, only enable that DiagID.; - Rerunning enable clears existing breakpoints.; - diagtool is used in breakpoint callbacks, so it can be changed; without the need to rerun enable.; - Adding this to your ~.lldbinit file makes clangdiag available at startup:; ""command script import /path/to/clangdiag.py"". """"""; # Don't need to test this time, since we did that in enable.; # Make sure we only consider errors, warnings, and extensions.; # FIXME: Make this configurable?; # Always disable existing breakpoints; # Make sure we only consider errors, warnings, and extensions.; # FIXME: Make this configurable?; # Make sure we only consider warnings.; # Remove all diag breakpoints.; # Use the Shell Lexer to properly parse up command options just like a; # shell would; # This initializer is being run from LLDB in the embedded command interpreter; # Make the options so we can generate the help text for the new LLDB; # command line command prior to registering it with LLDB below; # Add any commands contained in this module to LLDB",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/clangdiag.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/clangdiag.py
Modifiability,config,configurable,"langdiag is; enabled, it will use the appropriate diagtool application to determine; the name of the DiagID, and set breakpoints in all locations that; 'diag::name' appears in the source. Since the new breakpoints are set; after they are encountered, users will need to launch the executable a; second time in order to hit the new breakpoints. For in-tree builds, the diagtool application, used to map DiagID's to; names, is found automatically in the same directory as the target; executable. However, out-or-tree builds must use the 'diagtool'; subcommand to set the appropriate path for diagtool in the clang debug; bin directory. Since this mapping is created at build-time, it's; important for users to use the same version that was generated when; clang was compiled, or else the id's won't match. Notes:; - Substrings can be passed for both <warning> and <diag-name>.; - If <warning> is passed, only enable the DiagID(s) for that warning.; - If <diag-name> is passed, only enable that DiagID.; - Rerunning enable clears existing breakpoints.; - diagtool is used in breakpoint callbacks, so it can be changed; without the need to rerun enable.; - Adding this to your ~.lldbinit file makes clangdiag available at startup:; ""command script import /path/to/clangdiag.py"". """"""; # Don't need to test this time, since we did that in enable.; # Make sure we only consider errors, warnings, and extensions.; # FIXME: Make this configurable?; # Always disable existing breakpoints; # Make sure we only consider errors, warnings, and extensions.; # FIXME: Make this configurable?; # Make sure we only consider warnings.; # Remove all diag breakpoints.; # Use the Shell Lexer to properly parse up command options just like a; # shell would; # This initializer is being run from LLDB in the embedded command interpreter; # Make the options so we can generate the help text for the new LLDB; # command line command prior to registering it with LLDB below; # Add any commands contained in this module to LLDB",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/clangdiag.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/clangdiag.py
Testability,test,test,"langdiag is; enabled, it will use the appropriate diagtool application to determine; the name of the DiagID, and set breakpoints in all locations that; 'diag::name' appears in the source. Since the new breakpoints are set; after they are encountered, users will need to launch the executable a; second time in order to hit the new breakpoints. For in-tree builds, the diagtool application, used to map DiagID's to; names, is found automatically in the same directory as the target; executable. However, out-or-tree builds must use the 'diagtool'; subcommand to set the appropriate path for diagtool in the clang debug; bin directory. Since this mapping is created at build-time, it's; important for users to use the same version that was generated when; clang was compiled, or else the id's won't match. Notes:; - Substrings can be passed for both <warning> and <diag-name>.; - If <warning> is passed, only enable the DiagID(s) for that warning.; - If <diag-name> is passed, only enable that DiagID.; - Rerunning enable clears existing breakpoints.; - diagtool is used in breakpoint callbacks, so it can be changed; without the need to rerun enable.; - Adding this to your ~.lldbinit file makes clangdiag available at startup:; ""command script import /path/to/clangdiag.py"". """"""; # Don't need to test this time, since we did that in enable.; # Make sure we only consider errors, warnings, and extensions.; # FIXME: Make this configurable?; # Always disable existing breakpoints; # Make sure we only consider errors, warnings, and extensions.; # FIXME: Make this configurable?; # Make sure we only consider warnings.; # Remove all diag breakpoints.; # Use the Shell Lexer to properly parse up command options just like a; # shell would; # This initializer is being run from LLDB in the embedded command interpreter; # Make the options so we can generate the help text for the new LLDB; # command line command prior to registering it with LLDB below; # Add any commands contained in this module to LLDB",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/clangdiag.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/clangdiag.py
Usability,clear,clears,"langdiag is; enabled, it will use the appropriate diagtool application to determine; the name of the DiagID, and set breakpoints in all locations that; 'diag::name' appears in the source. Since the new breakpoints are set; after they are encountered, users will need to launch the executable a; second time in order to hit the new breakpoints. For in-tree builds, the diagtool application, used to map DiagID's to; names, is found automatically in the same directory as the target; executable. However, out-or-tree builds must use the 'diagtool'; subcommand to set the appropriate path for diagtool in the clang debug; bin directory. Since this mapping is created at build-time, it's; important for users to use the same version that was generated when; clang was compiled, or else the id's won't match. Notes:; - Substrings can be passed for both <warning> and <diag-name>.; - If <warning> is passed, only enable the DiagID(s) for that warning.; - If <diag-name> is passed, only enable that DiagID.; - Rerunning enable clears existing breakpoints.; - diagtool is used in breakpoint callbacks, so it can be changed; without the need to rerun enable.; - Adding this to your ~.lldbinit file makes clangdiag available at startup:; ""command script import /path/to/clangdiag.py"". """"""; # Don't need to test this time, since we did that in enable.; # Make sure we only consider errors, warnings, and extensions.; # FIXME: Make this configurable?; # Always disable existing breakpoints; # Make sure we only consider errors, warnings, and extensions.; # FIXME: Make this configurable?; # Make sure we only consider warnings.; # Remove all diag breakpoints.; # Use the Shell Lexer to properly parse up command options just like a; # shell would; # This initializer is being run from LLDB in the embedded command interpreter; # Make the options so we can generate the help text for the new LLDB; # command line command prior to registering it with LLDB below; # Add any commands contained in this module to LLDB",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/clangdiag.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/clangdiag.py
Deployability,release,release,"#!/usr/bin/env python3; # This script was committed on 20/11/2019 and it would probably make sense to remove; # it after the next release branches.; # This script is pipe based and converts an arm_neon.td (or arm_fp16.td) file; # using the old single-char type modifiers to an equivalent new-style form where; # each modifier is orthogonal and they can be composed.; #; # It was used to directly generate the .td files on main, so if you have any; # local additions I would suggest implementing any modifiers here, and running; # it over your entire pre-merge .td files rather than trying to resolve any; # conflicts manually.; # Conversions like to see the integer type so they know signedness.; # void and pointers make for bad discriminators in CGBuiltin.cpp.; # Otherwise it's a fixed output width modifier.; # y: scalar of half float; # y: scalar of float; # o: scalar of double; # I: scalar of 32-bit signed; # L: scalar of 64-bit signed; # I: scalar of 32-bit unsigned; # O: scalar of 64-bit unsigned; # f: float (int args); # F: double (int args); # H: half (int args); # 0: half (int args), ignore 'Q' size modifier.; # 1: half (int args), force 'Q' size modifier.",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/convert_arm_neon.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/convert_arm_neon.py
Availability,error,error,"#!/usr/bin/env python3; """"""Calls C-Reduce to create a minimal reproducer for clang crashes. Output files:; *.reduced.sh -- crash reproducer with minimal arguments; *.reduced.cpp -- the reduced file; *.test.sh -- interestingness test for C-Reduce; """"""; """"""; Returns absolute path to cmd_path if it is given,; or absolute path to cmd_dir/cmd_name.; """"""; # Make the path absolute so the creduce test can be run from any directory.; # Assume clang call is the first non comment line.; # Remove clang and filename from the command; # Assume the last occurrence of the filename is the clang input file; # Remove color codes; # Look for specific error messages; # Linux assert(); # FreeBSD/Mac assert(); # If no message was found, use the top five stack trace functions,; # ignoring some common functions; # Five is a somewhat arbitrary number; the goal is to get a small number; # of identifying functions with some leeway for common functions; # Disable symbolization if it's not required to avoid slow symbolization.; """"""#!/bin/bash; %s; if %s >& t.log ; then; exit 1; fi; """"""; # Check that the test considers the original file interesting; # Check that an empty file is not interesting; # Instead of modifying the filename in the test file, just run the command; # Heuristic for grouping arguments:; # remove next argument if it doesn't start with ""-""; """"""Simplify clang arguments before running C-Reduce to reduce the time the; interestingness test takes to run.; """"""; # Remove some clang arguments to speed up the interestingness test; # Not suppressing warnings (-w) sometimes prevents the crash from occurring; # after preprocessing; # Try to remove compilation steps; # Try to make implicit int an error for more sensible test output; """"""Minimize the clang arguments after running C-Reduce, to get the smallest; command that reproduces the crash on the reduced file.; """"""; # Remove some often occurring args; # Remove other cases that aren't covered by the heuristic; # Hack to kill C-Reduce because",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/creduce-clang-crash.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/creduce-clang-crash.py
Energy Efficiency,reduce,reduced,"#!/usr/bin/env python3; """"""Calls C-Reduce to create a minimal reproducer for clang crashes. Output files:; *.reduced.sh -- crash reproducer with minimal arguments; *.reduced.cpp -- the reduced file; *.test.sh -- interestingness test for C-Reduce; """"""; """"""; Returns absolute path to cmd_path if it is given,; or absolute path to cmd_dir/cmd_name.; """"""; # Make the path absolute so the creduce test can be run from any directory.; # Assume clang call is the first non comment line.; # Remove clang and filename from the command; # Assume the last occurrence of the filename is the clang input file; # Remove color codes; # Look for specific error messages; # Linux assert(); # FreeBSD/Mac assert(); # If no message was found, use the top five stack trace functions,; # ignoring some common functions; # Five is a somewhat arbitrary number; the goal is to get a small number; # of identifying functions with some leeway for common functions; # Disable symbolization if it's not required to avoid slow symbolization.; """"""#!/bin/bash; %s; if %s >& t.log ; then; exit 1; fi; """"""; # Check that the test considers the original file interesting; # Check that an empty file is not interesting; # Instead of modifying the filename in the test file, just run the command; # Heuristic for grouping arguments:; # remove next argument if it doesn't start with ""-""; """"""Simplify clang arguments before running C-Reduce to reduce the time the; interestingness test takes to run.; """"""; # Remove some clang arguments to speed up the interestingness test; # Not suppressing warnings (-w) sometimes prevents the crash from occurring; # after preprocessing; # Try to remove compilation steps; # Try to make implicit int an error for more sensible test output; """"""Minimize the clang arguments after running C-Reduce, to get the smallest; command that reproduces the crash on the reduced file.; """"""; # Remove some often occurring args; # Remove other cases that aren't covered by the heuristic; # Hack to kill C-Reduce because",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/creduce-clang-crash.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/creduce-clang-crash.py
Integrability,message,messages,"#!/usr/bin/env python3; """"""Calls C-Reduce to create a minimal reproducer for clang crashes. Output files:; *.reduced.sh -- crash reproducer with minimal arguments; *.reduced.cpp -- the reduced file; *.test.sh -- interestingness test for C-Reduce; """"""; """"""; Returns absolute path to cmd_path if it is given,; or absolute path to cmd_dir/cmd_name.; """"""; # Make the path absolute so the creduce test can be run from any directory.; # Assume clang call is the first non comment line.; # Remove clang and filename from the command; # Assume the last occurrence of the filename is the clang input file; # Remove color codes; # Look for specific error messages; # Linux assert(); # FreeBSD/Mac assert(); # If no message was found, use the top five stack trace functions,; # ignoring some common functions; # Five is a somewhat arbitrary number; the goal is to get a small number; # of identifying functions with some leeway for common functions; # Disable symbolization if it's not required to avoid slow symbolization.; """"""#!/bin/bash; %s; if %s >& t.log ; then; exit 1; fi; """"""; # Check that the test considers the original file interesting; # Check that an empty file is not interesting; # Instead of modifying the filename in the test file, just run the command; # Heuristic for grouping arguments:; # remove next argument if it doesn't start with ""-""; """"""Simplify clang arguments before running C-Reduce to reduce the time the; interestingness test takes to run.; """"""; # Remove some clang arguments to speed up the interestingness test; # Not suppressing warnings (-w) sometimes prevents the crash from occurring; # after preprocessing; # Try to remove compilation steps; # Try to make implicit int an error for more sensible test output; """"""Minimize the clang arguments after running C-Reduce, to get the smallest; command that reproduces the crash on the reduced file.; """"""; # Remove some often occurring args; # Remove other cases that aren't covered by the heuristic; # Hack to kill C-Reduce because",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/creduce-clang-crash.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/creduce-clang-crash.py
Safety,avoid,avoid,"#!/usr/bin/env python3; """"""Calls C-Reduce to create a minimal reproducer for clang crashes. Output files:; *.reduced.sh -- crash reproducer with minimal arguments; *.reduced.cpp -- the reduced file; *.test.sh -- interestingness test for C-Reduce; """"""; """"""; Returns absolute path to cmd_path if it is given,; or absolute path to cmd_dir/cmd_name.; """"""; # Make the path absolute so the creduce test can be run from any directory.; # Assume clang call is the first non comment line.; # Remove clang and filename from the command; # Assume the last occurrence of the filename is the clang input file; # Remove color codes; # Look for specific error messages; # Linux assert(); # FreeBSD/Mac assert(); # If no message was found, use the top five stack trace functions,; # ignoring some common functions; # Five is a somewhat arbitrary number; the goal is to get a small number; # of identifying functions with some leeway for common functions; # Disable symbolization if it's not required to avoid slow symbolization.; """"""#!/bin/bash; %s; if %s >& t.log ; then; exit 1; fi; """"""; # Check that the test considers the original file interesting; # Check that an empty file is not interesting; # Instead of modifying the filename in the test file, just run the command; # Heuristic for grouping arguments:; # remove next argument if it doesn't start with ""-""; """"""Simplify clang arguments before running C-Reduce to reduce the time the; interestingness test takes to run.; """"""; # Remove some clang arguments to speed up the interestingness test; # Not suppressing warnings (-w) sometimes prevents the crash from occurring; # after preprocessing; # Try to remove compilation steps; # Try to make implicit int an error for more sensible test output; """"""Minimize the clang arguments after running C-Reduce, to get the smallest; command that reproduces the crash on the reduced file.; """"""; # Remove some often occurring args; # Remove other cases that aren't covered by the heuristic; # Hack to kill C-Reduce because",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/creduce-clang-crash.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/creduce-clang-crash.py
Testability,test,test,"#!/usr/bin/env python3; """"""Calls C-Reduce to create a minimal reproducer for clang crashes. Output files:; *.reduced.sh -- crash reproducer with minimal arguments; *.reduced.cpp -- the reduced file; *.test.sh -- interestingness test for C-Reduce; """"""; """"""; Returns absolute path to cmd_path if it is given,; or absolute path to cmd_dir/cmd_name.; """"""; # Make the path absolute so the creduce test can be run from any directory.; # Assume clang call is the first non comment line.; # Remove clang and filename from the command; # Assume the last occurrence of the filename is the clang input file; # Remove color codes; # Look for specific error messages; # Linux assert(); # FreeBSD/Mac assert(); # If no message was found, use the top five stack trace functions,; # ignoring some common functions; # Five is a somewhat arbitrary number; the goal is to get a small number; # of identifying functions with some leeway for common functions; # Disable symbolization if it's not required to avoid slow symbolization.; """"""#!/bin/bash; %s; if %s >& t.log ; then; exit 1; fi; """"""; # Check that the test considers the original file interesting; # Check that an empty file is not interesting; # Instead of modifying the filename in the test file, just run the command; # Heuristic for grouping arguments:; # remove next argument if it doesn't start with ""-""; """"""Simplify clang arguments before running C-Reduce to reduce the time the; interestingness test takes to run.; """"""; # Remove some clang arguments to speed up the interestingness test; # Not suppressing warnings (-w) sometimes prevents the crash from occurring; # after preprocessing; # Try to remove compilation steps; # Try to make implicit int an error for more sensible test output; """"""Minimize the clang arguments after running C-Reduce, to get the smallest; command that reproduces the crash on the reduced file.; """"""; # Remove some often occurring args; # Remove other cases that aren't covered by the heuristic; # Hack to kill C-Reduce because",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/creduce-clang-crash.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/creduce-clang-crash.py
Deployability,configurat,configuration,#!/usr/bin/env python; # To use:; # 1) Update the 'decls' list below with your fuzzing configuration.; # 2) Run with the clang binary as the command-line argument.; # FIXME: Clean out output directory first.,MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/modfuzz.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/modfuzz.py
Modifiability,config,configuration,#!/usr/bin/env python; # To use:; # 1) Update the 'decls' list below with your fuzzing configuration.; # 2) Run with the clang binary as the command-line argument.; # FIXME: Clean out output directory first.,MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/modfuzz.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/modfuzz.py
Energy Efficiency,reduce,reduce,"#!/usr/bin/env python; ###; ###; # There is no reason to cache successful tests because we will; # always reduce the changeset when we see one.; # Make sure the initial test passes, if not then (a) either; # the user doesn't expect monotonicity, and we may end up; # doing O(N^2) tests, or (b) the test is wrong. Avoid the; # O(N^2) case unless user requests it.; # Check empty set first to quickly find poor test functions.; """"""split(set) -> [sets]. Partition a set into one or two pieces.; """"""; # There are many ways to split, we could do a better job with more; # context information (but then the API becomes grosser).; # assert(reduce(set.union, sets, set()) == c); # If there is nothing left we can remove, we are done.; # Look for a passing subset.; # Otherwise, partition sets if possible; if not we are done.; # If test passes on this subset alone, recurse.; # Otherwise if we have more than two sets, see if test; # pases without this subset.; ###; # Silly programmers refuse to print in simple machine readable; # formats. Whatever.; ###; # Read in the lists of tokens.; # Avoid freeing our giant cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/token-delta.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/token-delta.py
Performance,cache,cache,"#!/usr/bin/env python; ###; ###; # There is no reason to cache successful tests because we will; # always reduce the changeset when we see one.; # Make sure the initial test passes, if not then (a) either; # the user doesn't expect monotonicity, and we may end up; # doing O(N^2) tests, or (b) the test is wrong. Avoid the; # O(N^2) case unless user requests it.; # Check empty set first to quickly find poor test functions.; """"""split(set) -> [sets]. Partition a set into one or two pieces.; """"""; # There are many ways to split, we could do a better job with more; # context information (but then the API becomes grosser).; # assert(reduce(set.union, sets, set()) == c); # If there is nothing left we can remove, we are done.; # Look for a passing subset.; # Otherwise, partition sets if possible; if not we are done.; # If test passes on this subset alone, recurse.; # Otherwise if we have more than two sets, see if test; # pases without this subset.; ###; # Silly programmers refuse to print in simple machine readable; # formats. Whatever.; ###; # Read in the lists of tokens.; # Avoid freeing our giant cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/token-delta.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/token-delta.py
Testability,test,tests,"#!/usr/bin/env python; ###; ###; # There is no reason to cache successful tests because we will; # always reduce the changeset when we see one.; # Make sure the initial test passes, if not then (a) either; # the user doesn't expect monotonicity, and we may end up; # doing O(N^2) tests, or (b) the test is wrong. Avoid the; # O(N^2) case unless user requests it.; # Check empty set first to quickly find poor test functions.; """"""split(set) -> [sets]. Partition a set into one or two pieces.; """"""; # There are many ways to split, we could do a better job with more; # context information (but then the API becomes grosser).; # assert(reduce(set.union, sets, set()) == c); # If there is nothing left we can remove, we are done.; # Look for a passing subset.; # Otherwise, partition sets if possible; if not we are done.; # If test passes on this subset alone, recurse.; # Otherwise if we have more than two sets, see if test; # pases without this subset.; ###; # Silly programmers refuse to print in simple machine readable; # formats. Whatever.; ###; # Read in the lists of tokens.; # Avoid freeing our giant cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/token-delta.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/token-delta.py
Usability,simpl,simple,"#!/usr/bin/env python; ###; ###; # There is no reason to cache successful tests because we will; # always reduce the changeset when we see one.; # Make sure the initial test passes, if not then (a) either; # the user doesn't expect monotonicity, and we may end up; # doing O(N^2) tests, or (b) the test is wrong. Avoid the; # O(N^2) case unless user requests it.; # Check empty set first to quickly find poor test functions.; """"""split(set) -> [sets]. Partition a set into one or two pieces.; """"""; # There are many ways to split, we could do a better job with more; # context information (but then the API becomes grosser).; # assert(reduce(set.union, sets, set()) == c); # If there is nothing left we can remove, we are done.; # Look for a passing subset.; # Otherwise, partition sets if possible; if not we are done.; # If test passes on this subset alone, recurse.; # Otherwise if we have more than two sets, see if test; # pases without this subset.; ###; # Silly programmers refuse to print in simple machine readable; # formats. Whatever.; ###; # Read in the lists of tokens.; # Avoid freeing our giant cache.",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/token-delta.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/token-delta.py
Availability,down,downstream,"#!/usr/bin/env python3; """"""Update Options.td for the flags changes in https://reviews.llvm.org/Dxyz. This script translates Options.td from using Flags to control option visibility; to using Vis instead. It is meant to be idempotent and usable to help update; downstream forks if they have their own changes to Options.td. Usage:; ```sh; % update_options_td_flags.py path/to/Options.td > Options.td.new; % mv Options.td.new path/to/Options.td; ```. This script will be removed after the next LLVM release.; """"""; # We only deal with one thing per line. If multiple things can be; # on the same line (like NegFlag and PosFlag), please preprocess; # that first.; # Make a minimal attempt at reasonable line lengths; # Avoid wrapping the , or ; to the new line",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/update_options_td_flags.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/update_options_td_flags.py
Deployability,update,update,"#!/usr/bin/env python3; """"""Update Options.td for the flags changes in https://reviews.llvm.org/Dxyz. This script translates Options.td from using Flags to control option visibility; to using Vis instead. It is meant to be idempotent and usable to help update; downstream forks if they have their own changes to Options.td. Usage:; ```sh; % update_options_td_flags.py path/to/Options.td > Options.td.new; % mv Options.td.new path/to/Options.td; ```. This script will be removed after the next LLVM release.; """"""; # We only deal with one thing per line. If multiple things can be; # on the same line (like NegFlag and PosFlag), please preprocess; # that first.; # Make a minimal attempt at reasonable line lengths; # Avoid wrapping the , or ; to the new line",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/update_options_td_flags.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/update_options_td_flags.py
Integrability,wrap,wrapping,"#!/usr/bin/env python3; """"""Update Options.td for the flags changes in https://reviews.llvm.org/Dxyz. This script translates Options.td from using Flags to control option visibility; to using Vis instead. It is meant to be idempotent and usable to help update; downstream forks if they have their own changes to Options.td. Usage:; ```sh; % update_options_td_flags.py path/to/Options.td > Options.td.new; % mv Options.td.new path/to/Options.td; ```. This script will be removed after the next LLVM release.; """"""; # We only deal with one thing per line. If multiple things can be; # on the same line (like NegFlag and PosFlag), please preprocess; # that first.; # Make a minimal attempt at reasonable line lengths; # Avoid wrapping the , or ; to the new line",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/update_options_td_flags.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/update_options_td_flags.py
Usability,usab,usable,"#!/usr/bin/env python3; """"""Update Options.td for the flags changes in https://reviews.llvm.org/Dxyz. This script translates Options.td from using Flags to control option visibility; to using Vis instead. It is meant to be idempotent and usable to help update; downstream forks if they have their own changes to Options.td. Usage:; ```sh; % update_options_td_flags.py path/to/Options.td > Options.td.new; % mv Options.td.new path/to/Options.td; ```. This script will be removed after the next LLVM release.; """"""; # We only deal with one thing per line. If multiple things can be; # on the same line (like NegFlag and PosFlag), please preprocess; # that first.; # Make a minimal attempt at reasonable line lengths; # Avoid wrapping the , or ; to the new line",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/update_options_td_flags.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/update_options_td_flags.py
Modifiability,portab,portably,"#!/usr/bin/env python; ####; # Reserve slot; # FIXME: Use designated initializers to access non-first; # fields of unions.; # Hack to work around PR5579.; # Access in this fashion as a hackish way to portably; # access vectors.; # Access in this fashion as a hackish way to portably; # access vectors.; # Builtins - Ints; # Other builtins; # Enumerations; # Derived types; # Tuning; # Construct type generator; # FIXME: Wrong size.; # FIXME: Wrong size.; # FIXME: Allow overriding builtins here; # Fully recursive, just avoid top-level arrays.; # Make a chain of type generators, each builds smaller; # structures.; # Override max,min,count if finite",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/ABITest/ABITestGen.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/ABITest/ABITestGen.py
Safety,avoid,avoid,"#!/usr/bin/env python; ####; # Reserve slot; # FIXME: Use designated initializers to access non-first; # fields of unions.; # Hack to work around PR5579.; # Access in this fashion as a hackish way to portably; # access vectors.; # Access in this fashion as a hackish way to portably; # access vectors.; # Builtins - Ints; # Other builtins; # Enumerations; # Derived types; # Tuning; # Construct type generator; # FIXME: Wrong size.; # FIXME: Wrong size.; # FIXME: Allow overriding builtins here; # Fully recursive, just avoid top-level arrays.; # Make a chain of type generators, each builds smaller; # structures.; # Override max,min,count if finite",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/ABITest/ABITestGen.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/ABITest/ABITestGen.py
Security,access,access,"#!/usr/bin/env python; ####; # Reserve slot; # FIXME: Use designated initializers to access non-first; # fields of unions.; # Hack to work around PR5579.; # Access in this fashion as a hackish way to portably; # access vectors.; # Access in this fashion as a hackish way to portably; # access vectors.; # Builtins - Ints; # Other builtins; # Enumerations; # Derived types; # Tuning; # Construct type generator; # FIXME: Wrong size.; # FIXME: Wrong size.; # FIXME: Allow overriding builtins here; # Fully recursive, just avoid top-level arrays.; # Make a chain of type generators, each builds smaller; # structures.; # Override max,min,count if finite",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/ABITest/ABITestGen.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/ABITest/ABITestGen.py
Integrability,wrap,wrap,"""""""Utilities for enumeration of finite and countably infinite sets.; """"""; ###; # Countable iteration; # Simplifies some calculations; # Avoid various singularities; # Gallop to find bounds for line; # Binary search for starting line; # assert base(lo) <= N < base(hi); """"""getNthPairBounded(N, W, H) -> (x, y). Return the N-th pair such that 0 <= x < W and 0 <= y < H.""""""; # Simple case...; # Otherwise simplify by assuming W < H; # Conceptually we want to slide a diagonal line across a; # rectangle. This gives more interesting results for large; # bounds than using divmod.; # If in lower left, just return as usual; # Otherwise if in upper right, subtract from corner; # Otherwise, compile line and index from number of times we; # wrap.; # p = (W-1, 1+offset) + (-1,1)*index; """"""getNthNTuple(N, W, H) -> (x_0, x_1, ..., x_W). Return the N-th W-tuple, where for 0 <= x_i < H.""""""; """"""getNthTuple(N, maxSize, maxElement) -> x. Return the N-th tuple where len(x) < maxSize and for y in x, 0 <=; y < maxElement.""""""; # All zero sized tuples are isomorphic, don't ya know.; # FIXME: maxsize is inclusive; """"""getNthPairVariableBounds(N, bounds) -> (x, y). Given a finite list of bounds (which may be finite or aleph0),; return the N-th pair such that 0 <= x < len(bounds) and 0 <= y <; bounds[x].""""""; # Found the level; ###; ###; # Toggle to use checked versions of enumeration routines.",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/ABITest/Enumeration.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/ABITest/Enumeration.py
Testability,assert,assert,"""""""Utilities for enumeration of finite and countably infinite sets.; """"""; ###; # Countable iteration; # Simplifies some calculations; # Avoid various singularities; # Gallop to find bounds for line; # Binary search for starting line; # assert base(lo) <= N < base(hi); """"""getNthPairBounded(N, W, H) -> (x, y). Return the N-th pair such that 0 <= x < W and 0 <= y < H.""""""; # Simple case...; # Otherwise simplify by assuming W < H; # Conceptually we want to slide a diagonal line across a; # rectangle. This gives more interesting results for large; # bounds than using divmod.; # If in lower left, just return as usual; # Otherwise if in upper right, subtract from corner; # Otherwise, compile line and index from number of times we; # wrap.; # p = (W-1, 1+offset) + (-1,1)*index; """"""getNthNTuple(N, W, H) -> (x_0, x_1, ..., x_W). Return the N-th W-tuple, where for 0 <= x_i < H.""""""; """"""getNthTuple(N, maxSize, maxElement) -> x. Return the N-th tuple where len(x) < maxSize and for y in x, 0 <=; y < maxElement.""""""; # All zero sized tuples are isomorphic, don't ya know.; # FIXME: maxsize is inclusive; """"""getNthPairVariableBounds(N, bounds) -> (x, y). Given a finite list of bounds (which may be finite or aleph0),; return the N-th pair such that 0 <= x < len(bounds) and 0 <= y <; bounds[x].""""""; # Found the level; ###; ###; # Toggle to use checked versions of enumeration routines.",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/ABITest/Enumeration.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/ABITest/Enumeration.py
Usability,simpl,simplify,"""""""Utilities for enumeration of finite and countably infinite sets.; """"""; ###; # Countable iteration; # Simplifies some calculations; # Avoid various singularities; # Gallop to find bounds for line; # Binary search for starting line; # assert base(lo) <= N < base(hi); """"""getNthPairBounded(N, W, H) -> (x, y). Return the N-th pair such that 0 <= x < W and 0 <= y < H.""""""; # Simple case...; # Otherwise simplify by assuming W < H; # Conceptually we want to slide a diagonal line across a; # rectangle. This gives more interesting results for large; # bounds than using divmod.; # If in lower left, just return as usual; # Otherwise if in upper right, subtract from corner; # Otherwise, compile line and index from number of times we; # wrap.; # p = (W-1, 1+offset) + (-1,1)*index; """"""getNthNTuple(N, W, H) -> (x_0, x_1, ..., x_W). Return the N-th W-tuple, where for 0 <= x_i < H.""""""; """"""getNthTuple(N, maxSize, maxElement) -> x. Return the N-th tuple where len(x) < maxSize and for y in x, 0 <=; y < maxElement.""""""; # All zero sized tuples are isomorphic, don't ya know.; # FIXME: maxsize is inclusive; """"""getNthPairVariableBounds(N, bounds) -> (x, y). Given a finite list of bounds (which may be finite or aleph0),; return the N-th pair such that 0 <= x < len(bounds) and 0 <= y <; bounds[x].""""""; # Found the level; ###; ###; # Toggle to use checked versions of enumeration routines.",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/ABITest/Enumeration.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/ABITest/Enumeration.py
Modifiability,flexible,flexible,"""""""Flexible enumeration of C types.""""""; # TODO:; # - struct improvements (flexible arrays, packed &; # unpacked, alignment); # - objective-c qualified id; # - anonymous / transparent unions; # - VLAs; # - block types; # - K&R functions; # - pass arguments of different types (test extension, transparent union); # - varargs; ###; # Actual type types; # Name the struct for more readable LLVM IR.; # Note that for vectors, this is the size in bytes.; ###; # Type enumerators; # Factorial; # Compute the number of combinations (n choose k); # Enumerate the combinations choosing k elements from the list of values; # From ActiveState Recipe 190465: Generator for permutations,; # combinations, selections of a sequence; # Figure out the number of enumerators in this type; # Find the requested combination of enumerators and build a; # type from it.; # Skip the empty tuple; # fields0.addGenerator( RecordTypeGenerator(fields1, False, 4) )",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/ABITest/TypeGen.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/ABITest/TypeGen.py
Testability,test,test,"""""""Flexible enumeration of C types.""""""; # TODO:; # - struct improvements (flexible arrays, packed &; # unpacked, alignment); # - objective-c qualified id; # - anonymous / transparent unions; # - VLAs; # - block types; # - K&R functions; # - pass arguments of different types (test extension, transparent union); # - varargs; ###; # Actual type types; # Name the struct for more readable LLVM IR.; # Note that for vectors, this is the size in bytes.; ###; # Type enumerators; # Factorial; # Compute the number of combinations (n choose k); # Enumerate the combinations choosing k elements from the list of values; # From ActiveState Recipe 190465: Generator for permutations,; # combinations, selections of a sequence; # Figure out the number of enumerators in this type; # Find the requested combination of enumerators and build a; # type from it.; # Skip the empty tuple; # fields0.addGenerator( RecordTypeGenerator(fields1, False, 4) )",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/ABITest/TypeGen.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/ABITest/TypeGen.py
Deployability,integrat,integrate,"#!/usr/bin/env python; """"""; CmpRuns - A simple tool for comparing two static analyzer runs to determine; which reports have been added, removed, or changed. This is designed to support automated testing using the static analyzer, from; two perspectives:; 1. To monitor changes in the static analyzer's reports on real code bases,; for regression testing. 2. For use by end users who want to integrate regular static analyzer testing; into a buildbot like environment. Usage:. # Load the results of both runs, to obtain lists of the corresponding; # AnalysisDiagnostic objects.; #; resultsA = load_results_from_single_run(singleRunInfoA, delete_empty); resultsB = load_results_from_single_run(singleRunInfoB, delete_empty). # Generate a relation from diagnostics in run A to diagnostics in run B; # to obtain a list of triples (a, b, confidence).; diff = compare_results(resultsA, resultsB). """"""; # Diff in a form: field -> (before, after); # Type for generics; """"""; Color for terminal highlight.; """"""; """"""; Information about analysis run:; path - the analysis output directory; root - the name of the root directory, which will be disregarded when; determining the source file name; """"""; # control edge; # We consider two diagnostics similar only if at least one; # of the key fields is the same in both diagnostics.; # Note, the data format is not an API and may change from one analyzer; # version to another.; # Cumulative list of all diagnostics from all the reports.; # We want to retrieve the clang version even if there are no; # reports. Assume that all reports were created using the same; # clang version (this is always true and is more efficient).; # Ignore/delete empty reports.; # Extract the HTML reports, if they exists.; # FIXME: Why is this named files, when does it have multiple; # files?; # Python 3.10 offers zip(..., strict=True). The following assertion; # mimics it.; """"""; Backwards compatibility API.; """"""; """"""; # Load results of the analyzes from a given output folder.; # -",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/analyzer/CmpRuns.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/analyzer/CmpRuns.py
Energy Efficiency,monitor,monitor,"#!/usr/bin/env python; """"""; CmpRuns - A simple tool for comparing two static analyzer runs to determine; which reports have been added, removed, or changed. This is designed to support automated testing using the static analyzer, from; two perspectives:; 1. To monitor changes in the static analyzer's reports on real code bases,; for regression testing. 2. For use by end users who want to integrate regular static analyzer testing; into a buildbot like environment. Usage:. # Load the results of both runs, to obtain lists of the corresponding; # AnalysisDiagnostic objects.; #; resultsA = load_results_from_single_run(singleRunInfoA, delete_empty); resultsB = load_results_from_single_run(singleRunInfoB, delete_empty). # Generate a relation from diagnostics in run A to diagnostics in run B; # to obtain a list of triples (a, b, confidence).; diff = compare_results(resultsA, resultsB). """"""; # Diff in a form: field -> (before, after); # Type for generics; """"""; Color for terminal highlight.; """"""; """"""; Information about analysis run:; path - the analysis output directory; root - the name of the root directory, which will be disregarded when; determining the source file name; """"""; # control edge; # We consider two diagnostics similar only if at least one; # of the key fields is the same in both diagnostics.; # Note, the data format is not an API and may change from one analyzer; # version to another.; # Cumulative list of all diagnostics from all the reports.; # We want to retrieve the clang version even if there are no; # reports. Assume that all reports were created using the same; # clang version (this is always true and is more efficient).; # Ignore/delete empty reports.; # Extract the HTML reports, if they exists.; # FIXME: Why is this named files, when does it have multiple; # files?; # Python 3.10 offers zip(..., strict=True). The following assertion; # mimics it.; """"""; Backwards compatibility API.; """"""; """"""; # Load results of the analyzes from a given output folder.; # -",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/analyzer/CmpRuns.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/analyzer/CmpRuns.py
Integrability,integrat,integrate,"#!/usr/bin/env python; """"""; CmpRuns - A simple tool for comparing two static analyzer runs to determine; which reports have been added, removed, or changed. This is designed to support automated testing using the static analyzer, from; two perspectives:; 1. To monitor changes in the static analyzer's reports on real code bases,; for regression testing. 2. For use by end users who want to integrate regular static analyzer testing; into a buildbot like environment. Usage:. # Load the results of both runs, to obtain lists of the corresponding; # AnalysisDiagnostic objects.; #; resultsA = load_results_from_single_run(singleRunInfoA, delete_empty); resultsB = load_results_from_single_run(singleRunInfoB, delete_empty). # Generate a relation from diagnostics in run A to diagnostics in run B; # to obtain a list of triples (a, b, confidence).; diff = compare_results(resultsA, resultsB). """"""; # Diff in a form: field -> (before, after); # Type for generics; """"""; Color for terminal highlight.; """"""; """"""; Information about analysis run:; path - the analysis output directory; root - the name of the root directory, which will be disregarded when; determining the source file name; """"""; # control edge; # We consider two diagnostics similar only if at least one; # of the key fields is the same in both diagnostics.; # Note, the data format is not an API and may change from one analyzer; # version to another.; # Cumulative list of all diagnostics from all the reports.; # We want to retrieve the clang version even if there are no; # reports. Assume that all reports were created using the same; # clang version (this is always true and is more efficient).; # Ignore/delete empty reports.; # Extract the HTML reports, if they exists.; # FIXME: Why is this named files, when does it have multiple; # files?; # Python 3.10 offers zip(..., strict=True). The following assertion; # mimics it.; """"""; Backwards compatibility API.; """"""; """"""; # Load results of the analyzes from a given output folder.; # -",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/analyzer/CmpRuns.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/analyzer/CmpRuns.py
Testability,test,testing,"#!/usr/bin/env python; """"""; CmpRuns - A simple tool for comparing two static analyzer runs to determine; which reports have been added, removed, or changed. This is designed to support automated testing using the static analyzer, from; two perspectives:; 1. To monitor changes in the static analyzer's reports on real code bases,; for regression testing. 2. For use by end users who want to integrate regular static analyzer testing; into a buildbot like environment. Usage:. # Load the results of both runs, to obtain lists of the corresponding; # AnalysisDiagnostic objects.; #; resultsA = load_results_from_single_run(singleRunInfoA, delete_empty); resultsB = load_results_from_single_run(singleRunInfoB, delete_empty). # Generate a relation from diagnostics in run A to diagnostics in run B; # to obtain a list of triples (a, b, confidence).; diff = compare_results(resultsA, resultsB). """"""; # Diff in a form: field -> (before, after); # Type for generics; """"""; Color for terminal highlight.; """"""; """"""; Information about analysis run:; path - the analysis output directory; root - the name of the root directory, which will be disregarded when; determining the source file name; """"""; # control edge; # We consider two diagnostics similar only if at least one; # of the key fields is the same in both diagnostics.; # Note, the data format is not an API and may change from one analyzer; # version to another.; # Cumulative list of all diagnostics from all the reports.; # We want to retrieve the clang version even if there are no; # reports. Assume that all reports were created using the same; # clang version (this is always true and is more efficient).; # Ignore/delete empty reports.; # Extract the HTML reports, if they exists.; # FIXME: Why is this named files, when does it have multiple; # files?; # Python 3.10 offers zip(..., strict=True). The following assertion; # mimics it.; """"""; Backwards compatibility API.; """"""; """"""; # Load results of the analyzes from a given output folder.; # -",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/analyzer/CmpRuns.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/analyzer/CmpRuns.py
Usability,simpl,simple,"#!/usr/bin/env python; """"""; CmpRuns - A simple tool for comparing two static analyzer runs to determine; which reports have been added, removed, or changed. This is designed to support automated testing using the static analyzer, from; two perspectives:; 1. To monitor changes in the static analyzer's reports on real code bases,; for regression testing. 2. For use by end users who want to integrate regular static analyzer testing; into a buildbot like environment. Usage:. # Load the results of both runs, to obtain lists of the corresponding; # AnalysisDiagnostic objects.; #; resultsA = load_results_from_single_run(singleRunInfoA, delete_empty); resultsB = load_results_from_single_run(singleRunInfoB, delete_empty). # Generate a relation from diagnostics in run A to diagnostics in run B; # to obtain a list of triples (a, b, confidence).; diff = compare_results(resultsA, resultsB). """"""; # Diff in a form: field -> (before, after); # Type for generics; """"""; Color for terminal highlight.; """"""; """"""; Information about analysis run:; path - the analysis output directory; root - the name of the root directory, which will be disregarded when; determining the source file name; """"""; # control edge; # We consider two diagnostics similar only if at least one; # of the key fields is the same in both diagnostics.; # Note, the data format is not an API and may change from one analyzer; # version to another.; # Cumulative list of all diagnostics from all the reports.; # We want to retrieve the clang version even if there are no; # reports. Assume that all reports were created using the same; # clang version (this is always true and is more efficient).; # Ignore/delete empty reports.; # Extract the HTML reports, if they exists.; # FIXME: Why is this named files, when does it have multiple; # files?; # Python 3.10 offers zip(..., strict=True). The following assertion; # mimics it.; """"""; Backwards compatibility API.; """"""; """"""; # Load results of the analyzes from a given output folder.; # -",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/analyzer/CmpRuns.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/analyzer/CmpRuns.py
Deployability,install,installed,"es specific delimiters `\`; # and a directory or file may starts with the letter `l`.; # Find all `\l` (like `,\l`, `}\l`, `[\l`) except `\\l`,; # because the literal as a rule contains multiple `\` before `\l`.; # ===-----------------------------------------------------------------------===#; # Visitors traverse a deserialized ExplodedGraph and do different things; # with every node and edge.; # ===-----------------------------------------------------------------------===#; # A visitor that dumps the ExplodedGraph into a DOT file with fancy HTML-based; # syntax highlighing.; # This avoids pretty-printing huge statements such as CompoundStmt.; # Such statements show up only at [Pre|Post]StmtPurgeDeadSymbols; # TODO: Print more stuff for other kinds of points.; # Do diffs only when we have a unique predecessor.; # Don't do diffs on the leaf nodes because they're; # the important ones.; # The fallback behavior if graphviz is not installed!; # ===-----------------------------------------------------------------------===#; # Explorers know how to traverse the ExplodedGraph in a certain order.; # They would invoke a Visitor on every node or edge they encounter.; # ===-----------------------------------------------------------------------===#; # BasicExplorer explores the whole graph in no particular order.; # ===-----------------------------------------------------------------------===#; # Trimmers cut out parts of the ExplodedGraph so that to focus on other parts.; # Trimmers can be combined together by applying them sequentially.; # ===-----------------------------------------------------------------------===#; # SinglePathTrimmer keeps only a single path - the leftmost path from the root.; # Useful when the trimmed graph is still too large.; # TargetedTrimmer keeps paths that lead to specific nodes and discards all; # other paths. Useful when you cannot use -trim-egraph (e.g. when debugging; # a crash).; # ===------------------------------------------------------------",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/analyzer/exploded-graph-rewriter.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/analyzer/exploded-graph-rewriter.py
Integrability,message,messages,"between two dictionaries.; # Represents any program state trait that is a dictionary of key-value pairs.; # A deserialized source location.; # A deserialized program point.; # A single expression acting as a key in a deserialized Environment.; # CXXCtorInitializer is not a Stmt!; # Deserialized description of a location context.; # A group of deserialized Environment bindings that correspond to a specific; # location context.; # A deserialized Environment. This class can also hold other entities that; # are similar to Environment, such as Objects Under Construction or; # Indices Of Elements Under Construction.; # TODO: It's difficult to display a good diff when frame numbers shift.; # We have the whole frame replaced with another frame.; # TODO: Produce a nice diff.; # TODO: Add support for added/removed.; # A single binding key in a deserialized RegionStore cluster.; # A single cluster of the deserialized RegionStore.; # A deserialized RegionStore.; # Deserialized messages from a single checker in a single program state.; # Basically a list of raw strings.; # Deserialized messages of all checkers, separated by checker.; # A deserialized program state.; # State traits; #; # For traits we always check if a key exists because if a trait; # has no imformation, nothing will be printed in the .dot file; # we parse.; # A deserialized exploded graph node. Has a default constructor because it; # may be referenced as part of an edge before its contents are deserialized,; # and in this moment we already need a room for predecessors and successors.; # A deserialized ExplodedGraph. Constructed by consuming a .dot file; # line-by-line.; # Parse .dot files with regular expressions.; # Allow line breaks by waiting for ';'. This is not valid in; # a .dot file, but it is useful for writing tests.; # Apply regexps one by one to see if it's a node or an edge; # and extract contents if necessary.; # Note: when writing tests you don't need to escape everything,; # even though in a valid",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/analyzer/exploded-graph-rewriter.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/analyzer/exploded-graph-rewriter.py
Modifiability,rewrite,rewriter,"#!/usr/bin/env python; #; # ===- exploded-graph-rewriter.py - ExplodedGraph dump tool -----*- python -*--#; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ===-----------------------------------------------------------------------===#; # ===-----------------------------------------------------------------------===#; # These data structures represent a deserialized ExplodedGraph.; # ===-----------------------------------------------------------------------===#; # A helper function for finding the difference between two dictionaries.; # Represents any program state trait that is a dictionary of key-value pairs.; # A deserialized source location.; # A deserialized program point.; # A single expression acting as a key in a deserialized Environment.; # CXXCtorInitializer is not a Stmt!; # Deserialized description of a location context.; # A group of deserialized Environment bindings that correspond to a specific; # location context.; # A deserialized Environment. This class can also hold other entities that; # are similar to Environment, such as Objects Under Construction or; # Indices Of Elements Under Construction.; # TODO: It's difficult to display a good diff when frame numbers shift.; # We have the whole frame replaced with another frame.; # TODO: Produce a nice diff.; # TODO: Add support for added/removed.; # A single binding key in a deserialized RegionStore cluster.; # A single cluster of the deserialized RegionStore.; # A deserialized RegionStore.; # Deserialized messages from a single checker in a single program state.; # Basically a list of raw strings.; # Deserialized messages of all checkers, separated by checker.; # A deserialized program state.; # State traits; #; # For traits we always check if a key exists because if a trait; # has no imformation, nothing will be printed in the .dot file; # we par",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/analyzer/exploded-graph-rewriter.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/analyzer/exploded-graph-rewriter.py
Safety,avoid,avoids," tests.; # Apply regexps one by one to see if it's a node or an edge; # and extract contents if necessary.; # Note: when writing tests you don't need to escape everything,; # even though in a valid dot file everything is escaped.; # Handle `\l` separately because a string literal can be in code; # like ""string\\literal"" with the `\l` inside.; # Also on Windows macros __FILE__ produces specific delimiters `\`; # and a directory or file may starts with the letter `l`.; # Find all `\l` (like `,\l`, `}\l`, `[\l`) except `\\l`,; # because the literal as a rule contains multiple `\` before `\l`.; # ===-----------------------------------------------------------------------===#; # Visitors traverse a deserialized ExplodedGraph and do different things; # with every node and edge.; # ===-----------------------------------------------------------------------===#; # A visitor that dumps the ExplodedGraph into a DOT file with fancy HTML-based; # syntax highlighing.; # This avoids pretty-printing huge statements such as CompoundStmt.; # Such statements show up only at [Pre|Post]StmtPurgeDeadSymbols; # TODO: Print more stuff for other kinds of points.; # Do diffs only when we have a unique predecessor.; # Don't do diffs on the leaf nodes because they're; # the important ones.; # The fallback behavior if graphviz is not installed!; # ===-----------------------------------------------------------------------===#; # Explorers know how to traverse the ExplodedGraph in a certain order.; # They would invoke a Visitor on every node or edge they encounter.; # ===-----------------------------------------------------------------------===#; # BasicExplorer explores the whole graph in no particular order.; # ===-----------------------------------------------------------------------===#; # Trimmers cut out parts of the ExplodedGraph so that to focus on other parts.; # Trimmers can be combined together by applying them sequentially.; # ===---------------------------------------------------------",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/analyzer/exploded-graph-rewriter.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/analyzer/exploded-graph-rewriter.py
Testability,test,tests,"dd support for added/removed.; # A single binding key in a deserialized RegionStore cluster.; # A single cluster of the deserialized RegionStore.; # A deserialized RegionStore.; # Deserialized messages from a single checker in a single program state.; # Basically a list of raw strings.; # Deserialized messages of all checkers, separated by checker.; # A deserialized program state.; # State traits; #; # For traits we always check if a key exists because if a trait; # has no imformation, nothing will be printed in the .dot file; # we parse.; # A deserialized exploded graph node. Has a default constructor because it; # may be referenced as part of an edge before its contents are deserialized,; # and in this moment we already need a room for predecessors and successors.; # A deserialized ExplodedGraph. Constructed by consuming a .dot file; # line-by-line.; # Parse .dot files with regular expressions.; # Allow line breaks by waiting for ';'. This is not valid in; # a .dot file, but it is useful for writing tests.; # Apply regexps one by one to see if it's a node or an edge; # and extract contents if necessary.; # Note: when writing tests you don't need to escape everything,; # even though in a valid dot file everything is escaped.; # Handle `\l` separately because a string literal can be in code; # like ""string\\literal"" with the `\l` inside.; # Also on Windows macros __FILE__ produces specific delimiters `\`; # and a directory or file may starts with the letter `l`.; # Find all `\l` (like `,\l`, `}\l`, `[\l`) except `\\l`,; # because the literal as a rule contains multiple `\` before `\l`.; # ===-----------------------------------------------------------------------===#; # Visitors traverse a deserialized ExplodedGraph and do different things; # with every node and edge.; # ===-----------------------------------------------------------------------===#; # A visitor that dumps the ExplodedGraph into a DOT file with fancy HTML-based; # syntax highlighing.; # This avoids pre",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/analyzer/exploded-graph-rewriter.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/analyzer/exploded-graph-rewriter.py
Deployability,update,update,"#!/usr/bin/env python; # validate that given projects are present in the project map file; # First we need to start the docker container in a waiting mode,; # so it doesn't do anything, but most importantly keeps working; # while the shell session is in progress.; # Since the docker container is running, we can actually connect to it; # add subcommand; # TODO: Add an option not to build.; # TODO: Set the path to the Repository directory.; # build subcommand; # compare subcommand; # update subcommand; # docker subcommand; # benchmark subcommand",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/analyzer/SATest.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/analyzer/SATest.py
Security,validat,validate,"#!/usr/bin/env python; # validate that given projects are present in the project map file; # First we need to start the docker container in a waiting mode,; # so it doesn't do anything, but most importantly keeps working; # while the shell session is in progress.; # Since the docker container is running, we can actually connect to it; # add subcommand; # TODO: Add an option not to build.; # TODO: Set the path to the Repository directory.; # build subcommand; # compare subcommand; # update subcommand; # docker subcommand; # benchmark subcommand",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/analyzer/SATest.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/analyzer/SATest.py
Testability,benchmark,benchmark,"#!/usr/bin/env python; # validate that given projects are present in the project map file; # First we need to start the docker container in a waiting mode,; # so it doesn't do anything, but most importantly keeps working; # while the shell session is in progress.; # Since the docker container is running, we can actually connect to it; # add subcommand; # TODO: Add an option not to build.; # TODO: Set the path to the Repository directory.; # build subcommand; # compare subcommand; # update subcommand; # docker subcommand; # benchmark subcommand",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/analyzer/SATest.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/analyzer/SATest.py
Availability,down,download,"#!/usr/bin/env python; """"""; Static Analyzer qualification infrastructure: adding a new project to; the Repository Directory. Add a new project for testing: build it and add to the Project Map file.; Assumes it's being run from the Repository Directory.; The project directory should be added inside the Repository Directory and; have the same name as the project ID. The project should use the following files for set up:; - cleanup_run_static_analyzer.sh - prepare the build environment.; Ex: make clean can be a part of it.; - run_static_analyzer.cmd - a list of commands to run through scan-build.; Each command should be on a separate line.; Choose from: configure, make, xcodebuild; - download_project.sh - download the project into the CachedSource/; directory. For example, download a zip of; the project source from GitHub, unzip it,; and rename the unzipped directory to; 'CachedSource'. This script is not called; when 'CachedSource' is already present,; so an alternative is to check the; 'CachedSource' directory into the; repository directly.; - CachedSource/ - An optional directory containing the source of the; project being analyzed. If present,; download_project.sh will not be called.; - changes_for_analyzer.patch - An optional patch file for any local; changes; (e.g., to adapt to newer version of clang); that should be applied to CachedSource; before analysis. To construct this patch,; run the download script to download; the project to CachedSource, copy the; CachedSource to another directory (for; example, PatchedSource) and make any; needed modifications to the copied; source.; Then run:; diff -ur CachedSource PatchedSource \; > changes_for_analyzer.patch; """"""; """"""; Add a new project for testing: build it and add to the Project Map file.; :param name: is a short string used to identify a project.; """"""; # Build the project.; # Add the project name to the project map.",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/analyzer/SATestAdd.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/analyzer/SATestAdd.py
Deployability,patch,patch,"#!/usr/bin/env python; """"""; Static Analyzer qualification infrastructure: adding a new project to; the Repository Directory. Add a new project for testing: build it and add to the Project Map file.; Assumes it's being run from the Repository Directory.; The project directory should be added inside the Repository Directory and; have the same name as the project ID. The project should use the following files for set up:; - cleanup_run_static_analyzer.sh - prepare the build environment.; Ex: make clean can be a part of it.; - run_static_analyzer.cmd - a list of commands to run through scan-build.; Each command should be on a separate line.; Choose from: configure, make, xcodebuild; - download_project.sh - download the project into the CachedSource/; directory. For example, download a zip of; the project source from GitHub, unzip it,; and rename the unzipped directory to; 'CachedSource'. This script is not called; when 'CachedSource' is already present,; so an alternative is to check the; 'CachedSource' directory into the; repository directly.; - CachedSource/ - An optional directory containing the source of the; project being analyzed. If present,; download_project.sh will not be called.; - changes_for_analyzer.patch - An optional patch file for any local; changes; (e.g., to adapt to newer version of clang); that should be applied to CachedSource; before analysis. To construct this patch,; run the download script to download; the project to CachedSource, copy the; CachedSource to another directory (for; example, PatchedSource) and make any; needed modifications to the copied; source.; Then run:; diff -ur CachedSource PatchedSource \; > changes_for_analyzer.patch; """"""; """"""; Add a new project for testing: build it and add to the Project Map file.; :param name: is a short string used to identify a project.; """"""; # Build the project.; # Add the project name to the project map.",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/analyzer/SATestAdd.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/analyzer/SATestAdd.py
Energy Efficiency,adapt,adapt,"#!/usr/bin/env python; """"""; Static Analyzer qualification infrastructure: adding a new project to; the Repository Directory. Add a new project for testing: build it and add to the Project Map file.; Assumes it's being run from the Repository Directory.; The project directory should be added inside the Repository Directory and; have the same name as the project ID. The project should use the following files for set up:; - cleanup_run_static_analyzer.sh - prepare the build environment.; Ex: make clean can be a part of it.; - run_static_analyzer.cmd - a list of commands to run through scan-build.; Each command should be on a separate line.; Choose from: configure, make, xcodebuild; - download_project.sh - download the project into the CachedSource/; directory. For example, download a zip of; the project source from GitHub, unzip it,; and rename the unzipped directory to; 'CachedSource'. This script is not called; when 'CachedSource' is already present,; so an alternative is to check the; 'CachedSource' directory into the; repository directly.; - CachedSource/ - An optional directory containing the source of the; project being analyzed. If present,; download_project.sh will not be called.; - changes_for_analyzer.patch - An optional patch file for any local; changes; (e.g., to adapt to newer version of clang); that should be applied to CachedSource; before analysis. To construct this patch,; run the download script to download; the project to CachedSource, copy the; CachedSource to another directory (for; example, PatchedSource) and make any; needed modifications to the copied; source.; Then run:; diff -ur CachedSource PatchedSource \; > changes_for_analyzer.patch; """"""; """"""; Add a new project for testing: build it and add to the Project Map file.; :param name: is a short string used to identify a project.; """"""; # Build the project.; # Add the project name to the project map.",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/analyzer/SATestAdd.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/analyzer/SATestAdd.py
Modifiability,config,configure,"#!/usr/bin/env python; """"""; Static Analyzer qualification infrastructure: adding a new project to; the Repository Directory. Add a new project for testing: build it and add to the Project Map file.; Assumes it's being run from the Repository Directory.; The project directory should be added inside the Repository Directory and; have the same name as the project ID. The project should use the following files for set up:; - cleanup_run_static_analyzer.sh - prepare the build environment.; Ex: make clean can be a part of it.; - run_static_analyzer.cmd - a list of commands to run through scan-build.; Each command should be on a separate line.; Choose from: configure, make, xcodebuild; - download_project.sh - download the project into the CachedSource/; directory. For example, download a zip of; the project source from GitHub, unzip it,; and rename the unzipped directory to; 'CachedSource'. This script is not called; when 'CachedSource' is already present,; so an alternative is to check the; 'CachedSource' directory into the; repository directly.; - CachedSource/ - An optional directory containing the source of the; project being analyzed. If present,; download_project.sh will not be called.; - changes_for_analyzer.patch - An optional patch file for any local; changes; (e.g., to adapt to newer version of clang); that should be applied to CachedSource; before analysis. To construct this patch,; run the download script to download; the project to CachedSource, copy the; CachedSource to another directory (for; example, PatchedSource) and make any; needed modifications to the copied; source.; Then run:; diff -ur CachedSource PatchedSource \; > changes_for_analyzer.patch; """"""; """"""; Add a new project for testing: build it and add to the Project Map file.; :param name: is a short string used to identify a project.; """"""; # Build the project.; # Add the project name to the project map.",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/analyzer/SATestAdd.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/analyzer/SATestAdd.py
Testability,test,testing,"#!/usr/bin/env python; """"""; Static Analyzer qualification infrastructure: adding a new project to; the Repository Directory. Add a new project for testing: build it and add to the Project Map file.; Assumes it's being run from the Repository Directory.; The project directory should be added inside the Repository Directory and; have the same name as the project ID. The project should use the following files for set up:; - cleanup_run_static_analyzer.sh - prepare the build environment.; Ex: make clean can be a part of it.; - run_static_analyzer.cmd - a list of commands to run through scan-build.; Each command should be on a separate line.; Choose from: configure, make, xcodebuild; - download_project.sh - download the project into the CachedSource/; directory. For example, download a zip of; the project source from GitHub, unzip it,; and rename the unzipped directory to; 'CachedSource'. This script is not called; when 'CachedSource' is already present,; so an alternative is to check the; 'CachedSource' directory into the; repository directly.; - CachedSource/ - An optional directory containing the source of the; project being analyzed. If present,; download_project.sh will not be called.; - changes_for_analyzer.patch - An optional patch file for any local; changes; (e.g., to adapt to newer version of clang); that should be applied to CachedSource; before analysis. To construct this patch,; run the download script to download; the project to CachedSource, copy the; CachedSource to another directory (for; example, PatchedSource) and make any; needed modifications to the copied; source.; Then run:; diff -ur CachedSource PatchedSource \; > changes_for_analyzer.patch; """"""; """"""; Add a new project for testing: build it and add to the Project Map file.; :param name: is a short string used to identify a project.; """"""; # Build the project.; # Add the project name to the project map.",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/analyzer/SATestAdd.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/analyzer/SATestAdd.py
Integrability,rout,routine,"""""""; Static Analyzer qualification infrastructure. This source file contains all the functionality related to benchmarking; the analyzer on a set projects. Right now, this includes measuring; execution time and peak memory usage. Benchmark runs analysis on every; project multiple times to get a better picture about the distribution; of measured values. Additionally, this file includes a comparison routine for two benchmarking; results that plots the result together on one chart.; """"""; """"""; Becnhmark class encapsulates one functionality: it runs the analysis; multiple times for the given set of projects and stores results in the; specified file.; """"""; """"""; Compare two benchmarking results stored as .csv files; and produce a plot in the specified file.; """"""; # Leave only rows for projects common to both dataframes.; # Seaborn prefers all the data to be in one dataframe.; # TODO: compare data in old and new dataframes using statistical tests; # to check if they belong to the same distribution; # This creates a dataframe with all numerical data averaged.; # Right now 'means' has one row corresponding to one project,; # while 'data' has N rows for each project (one for each iteration).; #; # In order for us to work easier with this data, we duplicate; # 'means' data to match the size of the 'data' dataframe.; #; # All the columns from 'data' will maintain their names, while; # new columns coming from 'means' will have ""_mean"" suffix.; # We want to have time and memory charts one above the other.; # No need to have xlabels on both top and bottom charts.; # The legend on the top chart is enough.",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/analyzer/SATestBenchmark.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/analyzer/SATestBenchmark.py
Testability,benchmark,benchmarking,"""""""; Static Analyzer qualification infrastructure. This source file contains all the functionality related to benchmarking; the analyzer on a set projects. Right now, this includes measuring; execution time and peak memory usage. Benchmark runs analysis on every; project multiple times to get a better picture about the distribution; of measured values. Additionally, this file includes a comparison routine for two benchmarking; results that plots the result together on one chart.; """"""; """"""; Becnhmark class encapsulates one functionality: it runs the analysis; multiple times for the given set of projects and stores results in the; specified file.; """"""; """"""; Compare two benchmarking results stored as .csv files; and produce a plot in the specified file.; """"""; # Leave only rows for projects common to both dataframes.; # Seaborn prefers all the data to be in one dataframe.; # TODO: compare data in old and new dataframes using statistical tests; # to check if they belong to the same distribution; # This creates a dataframe with all numerical data averaged.; # Right now 'means' has one row corresponding to one project,; # while 'data' has N rows for each project (one for each iteration).; #; # In order for us to work easier with this data, we duplicate; # 'means' data to match the size of the 'data' dataframe.; #; # All the columns from 'data' will maintain their names, while; # new columns coming from 'means' will have ""_mean"" suffix.; # We want to have time and memory charts one above the other.; # No need to have xlabels on both top and bottom charts.; # The legend on the top chart is enough.",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/analyzer/SATestBenchmark.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/analyzer/SATestBenchmark.py
Availability,failure,failures,"#!/usr/bin/env python; """"""; Static Analyzer qualification infrastructure. The goal is to test the analyzer against different projects,; check for failures, compare results, and measure performance. Repository Directory will contain sources of the projects as well as the; information on how to build them and the expected output.; Repository Directory structure:; - ProjectMap file; - Historical Performance Data; - Project Dir1; - ReferenceOutput; - Project Dir2; - ReferenceOutput; ..; Note that the build tree must be inside the project dir. To test the build of the analyzer one would:; - Copy over a copy of the Repository Directory. (TODO: Prefer to ensure that; the build directory does not pollute the repository to min network; traffic).; - Build all projects, until error. Produce logs to report errors.; - Compare results. The files which should be kept around for failure investigations:; RepositoryCopy/Project DirI/ScanBuildResults; RepositoryCopy/Project DirI/run_static_analyzer.log. Assumptions (TODO: shouldn't need to assume these.):; The script is being run from the Repository Directory.; The compiler for scan-build and scan-build are in the PATH.; export PATH=/Users/zaks/workspace/c2llvm/build/Release+Asserts/bin:$PATH. For more logging, set the env variables:; zaks:TI zaks$ export CCC_ANALYZER_LOG=1; zaks:TI zaks$ export CCC_ANALYZER_VERBOSE=1. The list of checkers tested are hardcoded in the Checkers variable.; For testing additional checkers, use the SA_ADDITIONAL_CHECKERS environment; variable. It should contain a comma separated list.; """"""; # mypy has problems finding InvalidFileException in the module; # and this is we can shush that false positive; # type:ignore; ###############################################################################; # Helper functions.; ###############################################################################; # Rstrip in order not to write an extra newline.; # TODO: use debug levels for VERBOSE messages; #################",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/analyzer/SATestBuild.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/analyzer/SATestBuild.py
Deployability,patch,patches,"################################################################; # Find Clang for static analysis.; # Number of jobs.; # Names of the project specific scripts.; # The script that downloads the project.; # The script that needs to be executed before the build can start.; # This is a file containing commands for scan-build.; # A comment in a build script which disables wrapping.; # The log file name.; # Summary file - contains the summary of the failures. Ex: This info can be be; # displayed when buildbot detects a build failure.; # The scan-build result directory.; # The name of the directory storing the cached project source. If this; # directory does not exist, the download script will be executed.; # That script should create the ""CachedSource"" directory and download the; # project source into it.; # The name of the directory containing the source code that will be analyzed.; # Each time a project is analyzed, a fresh copy of its CachedSource directory; # will be copied to the PatchedSource directory and then the local patches; # in PATCHFILE_NAME will be applied (if PATCHFILE_NAME exists).; # The name of the patchfile specifying any changes that should be applied; # to the CachedSource before analyzing.; # The list of checkers used during analyzes.; # Currently, consists of all the non-experimental checkers, plus a few alpha; # checkers we don't want to regress on.; ###############################################################################; # Test harness logic.; ###############################################################################; """"""; Run pre-processing script if any.; """"""; """"""; Information about a project and settings for its analysis.; """"""; # typing package doesn't have a separate type for Queue, but has a generic stub; # We still want to have a type-safe checked project queue, for this reason,; # we specify generic type for mypy.; #; # It is a common workaround for this situation:; # https://mypy.readthedocs.io/en/stable/common_issues.html#u",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/analyzer/SATestBuild.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/analyzer/SATestBuild.py
Integrability,message,messages," (TODO: shouldn't need to assume these.):; The script is being run from the Repository Directory.; The compiler for scan-build and scan-build are in the PATH.; export PATH=/Users/zaks/workspace/c2llvm/build/Release+Asserts/bin:$PATH. For more logging, set the env variables:; zaks:TI zaks$ export CCC_ANALYZER_LOG=1; zaks:TI zaks$ export CCC_ANALYZER_VERBOSE=1. The list of checkers tested are hardcoded in the Checkers variable.; For testing additional checkers, use the SA_ADDITIONAL_CHECKERS environment; variable. It should contain a comma separated list.; """"""; # mypy has problems finding InvalidFileException in the module; # and this is we can shush that false positive; # type:ignore; ###############################################################################; # Helper functions.; ###############################################################################; # Rstrip in order not to write an extra newline.; # TODO: use debug levels for VERBOSE messages; ###############################################################################; # Configuration setup.; ###############################################################################; # Find Clang for static analysis.; # Number of jobs.; # Names of the project specific scripts.; # The script that downloads the project.; # The script that needs to be executed before the build can start.; # This is a file containing commands for scan-build.; # A comment in a build script which disables wrapping.; # The log file name.; # Summary file - contains the summary of the failures. Ex: This info can be be; # displayed when buildbot detects a build failure.; # The scan-build result directory.; # The name of the directory storing the cached project source. If this; # directory does not exist, the download script will be executed.; # That script should create the ""CachedSource"" directory and download the; # project source into it.; # The name of the directory containing the source code that will be analyzed.; # Each time a p",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/analyzer/SATestBuild.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/analyzer/SATestBuild.py
Modifiability,variab,variables," the expected output.; Repository Directory structure:; - ProjectMap file; - Historical Performance Data; - Project Dir1; - ReferenceOutput; - Project Dir2; - ReferenceOutput; ..; Note that the build tree must be inside the project dir. To test the build of the analyzer one would:; - Copy over a copy of the Repository Directory. (TODO: Prefer to ensure that; the build directory does not pollute the repository to min network; traffic).; - Build all projects, until error. Produce logs to report errors.; - Compare results. The files which should be kept around for failure investigations:; RepositoryCopy/Project DirI/ScanBuildResults; RepositoryCopy/Project DirI/run_static_analyzer.log. Assumptions (TODO: shouldn't need to assume these.):; The script is being run from the Repository Directory.; The compiler for scan-build and scan-build are in the PATH.; export PATH=/Users/zaks/workspace/c2llvm/build/Release+Asserts/bin:$PATH. For more logging, set the env variables:; zaks:TI zaks$ export CCC_ANALYZER_LOG=1; zaks:TI zaks$ export CCC_ANALYZER_VERBOSE=1. The list of checkers tested are hardcoded in the Checkers variable.; For testing additional checkers, use the SA_ADDITIONAL_CHECKERS environment; variable. It should contain a comma separated list.; """"""; # mypy has problems finding InvalidFileException in the module; # and this is we can shush that false positive; # type:ignore; ###############################################################################; # Helper functions.; ###############################################################################; # Rstrip in order not to write an extra newline.; # TODO: use debug levels for VERBOSE messages; ###############################################################################; # Configuration setup.; ###############################################################################; # Find Clang for static analysis.; # Number of jobs.; # Names of the project specific scripts.; # The script that downloads the project.; ",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/analyzer/SATestBuild.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/analyzer/SATestBuild.py
Performance,perform,performance,"#!/usr/bin/env python; """"""; Static Analyzer qualification infrastructure. The goal is to test the analyzer against different projects,; check for failures, compare results, and measure performance. Repository Directory will contain sources of the projects as well as the; information on how to build them and the expected output.; Repository Directory structure:; - ProjectMap file; - Historical Performance Data; - Project Dir1; - ReferenceOutput; - Project Dir2; - ReferenceOutput; ..; Note that the build tree must be inside the project dir. To test the build of the analyzer one would:; - Copy over a copy of the Repository Directory. (TODO: Prefer to ensure that; the build directory does not pollute the repository to min network; traffic).; - Build all projects, until error. Produce logs to report errors.; - Compare results. The files which should be kept around for failure investigations:; RepositoryCopy/Project DirI/ScanBuildResults; RepositoryCopy/Project DirI/run_static_analyzer.log. Assumptions (TODO: shouldn't need to assume these.):; The script is being run from the Repository Directory.; The compiler for scan-build and scan-build are in the PATH.; export PATH=/Users/zaks/workspace/c2llvm/build/Release+Asserts/bin:$PATH. For more logging, set the env variables:; zaks:TI zaks$ export CCC_ANALYZER_LOG=1; zaks:TI zaks$ export CCC_ANALYZER_VERBOSE=1. The list of checkers tested are hardcoded in the Checkers variable.; For testing additional checkers, use the SA_ADDITIONAL_CHECKERS environment; variable. It should contain a comma separated list.; """"""; # mypy has problems finding InvalidFileException in the module; # and this is we can shush that false positive; # type:ignore; ###############################################################################; # Helper functions.; ###############################################################################; # Rstrip in order not to write an extra newline.; # TODO: use debug levels for VERBOSE messages; #################",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/analyzer/SATestBuild.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/analyzer/SATestBuild.py
Safety,detect,detects,"ding InvalidFileException in the module; # and this is we can shush that false positive; # type:ignore; ###############################################################################; # Helper functions.; ###############################################################################; # Rstrip in order not to write an extra newline.; # TODO: use debug levels for VERBOSE messages; ###############################################################################; # Configuration setup.; ###############################################################################; # Find Clang for static analysis.; # Number of jobs.; # Names of the project specific scripts.; # The script that downloads the project.; # The script that needs to be executed before the build can start.; # This is a file containing commands for scan-build.; # A comment in a build script which disables wrapping.; # The log file name.; # Summary file - contains the summary of the failures. Ex: This info can be be; # displayed when buildbot detects a build failure.; # The scan-build result directory.; # The name of the directory storing the cached project source. If this; # directory does not exist, the download script will be executed.; # That script should create the ""CachedSource"" directory and download the; # project source into it.; # The name of the directory containing the source code that will be analyzed.; # Each time a project is analyzed, a fresh copy of its CachedSource directory; # will be copied to the PatchedSource directory and then the local patches; # in PATCHFILE_NAME will be applied (if PATCHFILE_NAME exists).; # The name of the patchfile specifying any changes that should be applied; # to the CachedSource before analyzing.; # The list of checkers used during analyzes.; # Currently, consists of all the non-experimental checkers, plus a few alpha; # checkers we don't want to regress on.; ###############################################################################; # Test harness logic.;",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/analyzer/SATestBuild.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/analyzer/SATestBuild.py
Testability,test,test,"#!/usr/bin/env python; """"""; Static Analyzer qualification infrastructure. The goal is to test the analyzer against different projects,; check for failures, compare results, and measure performance. Repository Directory will contain sources of the projects as well as the; information on how to build them and the expected output.; Repository Directory structure:; - ProjectMap file; - Historical Performance Data; - Project Dir1; - ReferenceOutput; - Project Dir2; - ReferenceOutput; ..; Note that the build tree must be inside the project dir. To test the build of the analyzer one would:; - Copy over a copy of the Repository Directory. (TODO: Prefer to ensure that; the build directory does not pollute the repository to min network; traffic).; - Build all projects, until error. Produce logs to report errors.; - Compare results. The files which should be kept around for failure investigations:; RepositoryCopy/Project DirI/ScanBuildResults; RepositoryCopy/Project DirI/run_static_analyzer.log. Assumptions (TODO: shouldn't need to assume these.):; The script is being run from the Repository Directory.; The compiler for scan-build and scan-build are in the PATH.; export PATH=/Users/zaks/workspace/c2llvm/build/Release+Asserts/bin:$PATH. For more logging, set the env variables:; zaks:TI zaks$ export CCC_ANALYZER_LOG=1; zaks:TI zaks$ export CCC_ANALYZER_VERBOSE=1. The list of checkers tested are hardcoded in the Checkers variable.; For testing additional checkers, use the SA_ADDITIONAL_CHECKERS environment; variable. It should contain a comma separated list.; """"""; # mypy has problems finding InvalidFileException in the module; # and this is we can shush that false positive; # type:ignore; ###############################################################################; # Helper functions.; ###############################################################################; # Rstrip in order not to write an extra newline.; # TODO: use debug levels for VERBOSE messages; #################",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/analyzer/SATestBuild.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/analyzer/SATestBuild.py
Deployability,install,installed,"""""""which(command, [paths]) - Look up the given command in the paths string; (or the PATH environment variable, if unspecified).""""""; # Check for absolute match first.; # Would be nice if Python had a lib function for this.; # Get suffixes to search.; # On Cygwin, 'PATHEXT' may exist but it should not be used.; # Search the paths...; """"""; Convert given time in seconds into a human-readable string.; """"""; """"""; Convert given number of bytes into a human-readable string.; """"""; # no formatter installed, let's keep it in bytes; # If memory is 0, we didn't succeed measuring it.; """"""; Run command with arguments. Wait for command to complete and measure; execution time and peak memory consumption.; If the exit code was zero then return, otherwise raise; CalledProcessError. The CalledProcessError object will have the; return code in the returncode attribute. The arguments are the same as for the call and check_call functions. Return a tuple of execution time and peak memory.; """"""; # we want to gather memory usage from all of the child processes; # while the process is running calculate resource utilization.; # track the peak utilization of the process; # back off to subprocess if we don't have psutil installed; """"""; Run the provided script if it exists.; """"""; """"""; Treat CSV lines starting with a '#' as a comment.; """"""",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/analyzer/SATestUtils.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/analyzer/SATestUtils.py
Energy Efficiency,consumption,consumption,"""""""which(command, [paths]) - Look up the given command in the paths string; (or the PATH environment variable, if unspecified).""""""; # Check for absolute match first.; # Would be nice if Python had a lib function for this.; # Get suffixes to search.; # On Cygwin, 'PATHEXT' may exist but it should not be used.; # Search the paths...; """"""; Convert given time in seconds into a human-readable string.; """"""; """"""; Convert given number of bytes into a human-readable string.; """"""; # no formatter installed, let's keep it in bytes; # If memory is 0, we didn't succeed measuring it.; """"""; Run command with arguments. Wait for command to complete and measure; execution time and peak memory consumption.; If the exit code was zero then return, otherwise raise; CalledProcessError. The CalledProcessError object will have the; return code in the returncode attribute. The arguments are the same as for the call and check_call functions. Return a tuple of execution time and peak memory.; """"""; # we want to gather memory usage from all of the child processes; # while the process is running calculate resource utilization.; # track the peak utilization of the process; # back off to subprocess if we don't have psutil installed; """"""; Run the provided script if it exists.; """"""; """"""; Treat CSV lines starting with a '#' as a comment.; """"""",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/analyzer/SATestUtils.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/analyzer/SATestUtils.py
Modifiability,variab,variable,"""""""which(command, [paths]) - Look up the given command in the paths string; (or the PATH environment variable, if unspecified).""""""; # Check for absolute match first.; # Would be nice if Python had a lib function for this.; # Get suffixes to search.; # On Cygwin, 'PATHEXT' may exist but it should not be used.; # Search the paths...; """"""; Convert given time in seconds into a human-readable string.; """"""; """"""; Convert given number of bytes into a human-readable string.; """"""; # no formatter installed, let's keep it in bytes; # If memory is 0, we didn't succeed measuring it.; """"""; Run command with arguments. Wait for command to complete and measure; execution time and peak memory consumption.; If the exit code was zero then return, otherwise raise; CalledProcessError. The CalledProcessError object will have the; return code in the returncode attribute. The arguments are the same as for the call and check_call functions. Return a tuple of execution time and peak memory.; """"""; # we want to gather memory usage from all of the child processes; # while the process is running calculate resource utilization.; # track the peak utilization of the process; # back off to subprocess if we don't have psutil installed; """"""; Run the provided script if it exists.; """"""; """"""; Treat CSV lines starting with a '#' as a comment.; """"""",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/analyzer/SATestUtils.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/analyzer/SATestUtils.py
Availability,failure,failure,"le is specified in args.""""""; """"""Replaces the specified name of an output file with the specified name.; Assumes that the output file name is specified in the command line args.""""""; """"""Append an output file to args, presuming not already specified.""""""; """"""Set the output file within the arguments. Appends or replaces as; appropriate.""""""; """"""Return the input file string if it can be found (and there is only; one).""""""; # Test if it is a source file; """"""Replaces the input file with that specified.""""""; # Could not find input file; """"""Check if this is a normal compile which will output an object file rather; than a preprocess or link. args is a list of command line arguments.""""""; # Bitcode cannot be disassembled in the same way; # Version and help are queries of the compiler and override -c if specified; # Options to output dependency files for make; # Check if the input is recognised as a source file (this may be too; # strong a restriction); """"""Runs a step of the compilation. Reports failure as exception.""""""; # Need to use shell=True on Windows as Popen won't use PATH otherwise.; """"""Get a temporary file name with a particular suffix. Let the caller be; responsible for deleting it.""""""; """"""Base class for a check. Subclass this to add a check.""""""; """"""Record the base output file that will be compared against.""""""; """"""Override this to perform the modified compilation and required; checks.""""""; """"""Check if different code is generated with/without the -g flag.""""""; # Compare disassembly (returns first diff if differs); # Clean up temp file if comparison okay; """"""Check if compiling to asm then assembling in separate steps results; in different code than compiling to object directly.""""""; # Compare if object files are exactly the same; # Compare disassembly (returns first diff if differs); # Code is identical, compare debug info; # Clean up temp file if comparison okay; # Create configuration defaults from list of checks; """"""; [Checks]; """"""; # Find all subclasses of WrapperCheck; # P",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/check_cfc/check_cfc.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/check_cfc/check_cfc.py
Deployability,configurat,configuration,"ed; # Options to output dependency files for make; # Check if the input is recognised as a source file (this may be too; # strong a restriction); """"""Runs a step of the compilation. Reports failure as exception.""""""; # Need to use shell=True on Windows as Popen won't use PATH otherwise.; """"""Get a temporary file name with a particular suffix. Let the caller be; responsible for deleting it.""""""; """"""Base class for a check. Subclass this to add a check.""""""; """"""Record the base output file that will be compared against.""""""; """"""Override this to perform the modified compilation and required; checks.""""""; """"""Check if different code is generated with/without the -g flag.""""""; # Compare disassembly (returns first diff if differs); # Clean up temp file if comparison okay; """"""Check if compiling to asm then assembling in separate steps results; in different code than compiling to object directly.""""""; # Compare if object files are exactly the same; # Compare disassembly (returns first diff if differs); # Code is identical, compare debug info; # Clean up temp file if comparison okay; # Create configuration defaults from list of checks; """"""; [Checks]; """"""; # Find all subclasses of WrapperCheck; # Prevent infinite loop if called with absolute path.; # Basic correctness check; # A - original compilation; # Bail out here if we can't apply checks in this case.; # Does not indicate an error.; # Maybe not straight compilation (e.g. -S or --version or -flto); # or maybe > 1 input files.; # Sometimes we generate files which have very long names which can't be; # read/disassembled. This will exit early if we can't find the file we; # expected to be output.; # Copy output file to a temp file; # Run checks, if they are enabled in config and if they are appropriate for; # this command line.; # Check failure; # Remove file to comply with build system expectations (no; # output file if failed); # Compile step failure; # Remove file to comply with build system expectations (no; # output file if failed)",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/check_cfc/check_cfc.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/check_cfc/check_cfc.py
Integrability,wrap,wrapper,"#!/usr/bin/env python; """"""Check CFC - Check Compile Flow Consistency. This is a compiler wrapper for testing that code generation is consistent with; different compilation processes. It checks that code is not unduly affected by; compiler options or other changes which should not have side effects. To use:; -Ensure that the compiler under test (i.e. clang, clang++) is on the PATH; -On Linux copy this script to the name of the compiler; e.g. cp check_cfc.py clang && cp check_cfc.py clang++; -On Windows use setup.py to generate check_cfc.exe and copy that to clang.exe; and clang++.exe; -Enable the desired checks in check_cfc.cfg (in the same directory as the; wrapper); e.g.; [Checks]; dash_g_no_change = true; dash_s_no_change = false. -The wrapper can be run using its absolute path or added to PATH before the; compiler under test; e.g. export PATH=<path to check_cfc>:$PATH; -Compile as normal. The wrapper intercepts normal -c compiles and will return; non-zero if the check fails.; e.g.; $ clang -c test.cpp; Code difference detected with -g; --- /tmp/tmp5nv893.o; +++ /tmp/tmp6Vwjnc.o; @@ -1 +1 @@; - 0: 48 8b 05 51 0b 20 00 mov 0x200b51(%rip),%rax; + 0: 48 39 3d 51 0b 20 00 cmp %rdi,0x200b51(%rip). -To run LNT with Check CFC specify the absolute path to the wrapper to the --cc; and --cxx options; e.g.; lnt runtest nt --cc <path to check_cfc>/clang \\; --cxx <path to check_cfc>/clang++ ... To add a new check:; -Create a new subclass of WrapperCheck; -Implement the perform_check() method. This should perform the alternate compile; and do the comparison.; -Add the new check to check_cfc.cfg. The check has the same name as the; subclass.; """"""; """"""Returns True if running on Windows.""""""; """"""Exception type to be used when a step other than the original compile; fails.""""""; """"""Exception type to be used when a comparison check fails.""""""; """"""Returns True when running as a py2exe executable.""""""; # new py2exe; # old py2exe; # tools/freeze; """"""Get the directory that the script or exec",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/check_cfc/check_cfc.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/check_cfc/check_cfc.py
Modifiability,variab,variable," -1 +1 @@; - 0: 48 8b 05 51 0b 20 00 mov 0x200b51(%rip),%rax; + 0: 48 39 3d 51 0b 20 00 cmp %rdi,0x200b51(%rip). -To run LNT with Check CFC specify the absolute path to the wrapper to the --cc; and --cxx options; e.g.; lnt runtest nt --cc <path to check_cfc>/clang \\; --cxx <path to check_cfc>/clang++ ... To add a new check:; -Create a new subclass of WrapperCheck; -Implement the perform_check() method. This should perform the alternate compile; and do the comparison.; -Add the new check to check_cfc.cfg. The check has the same name as the; subclass.; """"""; """"""Returns True if running on Windows.""""""; """"""Exception type to be used when a step other than the original compile; fails.""""""; """"""Exception type to be used when a comparison check fails.""""""; """"""Returns True when running as a py2exe executable.""""""; # new py2exe; # old py2exe; # tools/freeze; """"""Get the directory that the script or executable is located in.""""""; """"""Remove the specified directory from path_var, a string representing; PATH""""""; """"""Returns the PATH variable modified to remove the path to this program.""""""; """"""Search for -g in args. If it exists then return args without. If not then; add it.""""""; # Return args without any -g; # No -g, add one; """"""Derive output file from the input file (if just one) or None; otherwise.""""""; """"""Return the output file specified by this command or None if not; specified.""""""; # Specified as a separate arg; # Specified conjoined with -o; """"""Return true is output file is specified in args.""""""; """"""Replaces the specified name of an output file with the specified name.; Assumes that the output file name is specified in the command line args.""""""; """"""Append an output file to args, presuming not already specified.""""""; """"""Set the output file within the arguments. Appends or replaces as; appropriate.""""""; """"""Return the input file string if it can be found (and there is only; one).""""""; # Test if it is a source file; """"""Replaces the input file with that specified.""""""; # Could not find input f",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/check_cfc/check_cfc.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/check_cfc/check_cfc.py
Performance,perform,perform,"c.exe and copy that to clang.exe; and clang++.exe; -Enable the desired checks in check_cfc.cfg (in the same directory as the; wrapper); e.g.; [Checks]; dash_g_no_change = true; dash_s_no_change = false. -The wrapper can be run using its absolute path or added to PATH before the; compiler under test; e.g. export PATH=<path to check_cfc>:$PATH; -Compile as normal. The wrapper intercepts normal -c compiles and will return; non-zero if the check fails.; e.g.; $ clang -c test.cpp; Code difference detected with -g; --- /tmp/tmp5nv893.o; +++ /tmp/tmp6Vwjnc.o; @@ -1 +1 @@; - 0: 48 8b 05 51 0b 20 00 mov 0x200b51(%rip),%rax; + 0: 48 39 3d 51 0b 20 00 cmp %rdi,0x200b51(%rip). -To run LNT with Check CFC specify the absolute path to the wrapper to the --cc; and --cxx options; e.g.; lnt runtest nt --cc <path to check_cfc>/clang \\; --cxx <path to check_cfc>/clang++ ... To add a new check:; -Create a new subclass of WrapperCheck; -Implement the perform_check() method. This should perform the alternate compile; and do the comparison.; -Add the new check to check_cfc.cfg. The check has the same name as the; subclass.; """"""; """"""Returns True if running on Windows.""""""; """"""Exception type to be used when a step other than the original compile; fails.""""""; """"""Exception type to be used when a comparison check fails.""""""; """"""Returns True when running as a py2exe executable.""""""; # new py2exe; # old py2exe; # tools/freeze; """"""Get the directory that the script or executable is located in.""""""; """"""Remove the specified directory from path_var, a string representing; PATH""""""; """"""Returns the PATH variable modified to remove the path to this program.""""""; """"""Search for -g in args. If it exists then return args without. If not then; add it.""""""; # Return args without any -g; # No -g, add one; """"""Derive output file from the input file (if just one) or None; otherwise.""""""; """"""Return the output file specified by this command or None if not; specified.""""""; # Specified as a separate arg; # Specified conjoined ",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/check_cfc/check_cfc.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/check_cfc/check_cfc.py
Safety,detect,detected,"ompile Flow Consistency. This is a compiler wrapper for testing that code generation is consistent with; different compilation processes. It checks that code is not unduly affected by; compiler options or other changes which should not have side effects. To use:; -Ensure that the compiler under test (i.e. clang, clang++) is on the PATH; -On Linux copy this script to the name of the compiler; e.g. cp check_cfc.py clang && cp check_cfc.py clang++; -On Windows use setup.py to generate check_cfc.exe and copy that to clang.exe; and clang++.exe; -Enable the desired checks in check_cfc.cfg (in the same directory as the; wrapper); e.g.; [Checks]; dash_g_no_change = true; dash_s_no_change = false. -The wrapper can be run using its absolute path or added to PATH before the; compiler under test; e.g. export PATH=<path to check_cfc>:$PATH; -Compile as normal. The wrapper intercepts normal -c compiles and will return; non-zero if the check fails.; e.g.; $ clang -c test.cpp; Code difference detected with -g; --- /tmp/tmp5nv893.o; +++ /tmp/tmp6Vwjnc.o; @@ -1 +1 @@; - 0: 48 8b 05 51 0b 20 00 mov 0x200b51(%rip),%rax; + 0: 48 39 3d 51 0b 20 00 cmp %rdi,0x200b51(%rip). -To run LNT with Check CFC specify the absolute path to the wrapper to the --cc; and --cxx options; e.g.; lnt runtest nt --cc <path to check_cfc>/clang \\; --cxx <path to check_cfc>/clang++ ... To add a new check:; -Create a new subclass of WrapperCheck; -Implement the perform_check() method. This should perform the alternate compile; and do the comparison.; -Add the new check to check_cfc.cfg. The check has the same name as the; subclass.; """"""; """"""Returns True if running on Windows.""""""; """"""Exception type to be used when a step other than the original compile; fails.""""""; """"""Exception type to be used when a comparison check fails.""""""; """"""Returns True when running as a py2exe executable.""""""; # new py2exe; # old py2exe; # tools/freeze; """"""Get the directory that the script or executable is located in.""""""; """"""Remove the spec",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/check_cfc/check_cfc.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/check_cfc/check_cfc.py
Testability,test,testing,"#!/usr/bin/env python; """"""Check CFC - Check Compile Flow Consistency. This is a compiler wrapper for testing that code generation is consistent with; different compilation processes. It checks that code is not unduly affected by; compiler options or other changes which should not have side effects. To use:; -Ensure that the compiler under test (i.e. clang, clang++) is on the PATH; -On Linux copy this script to the name of the compiler; e.g. cp check_cfc.py clang && cp check_cfc.py clang++; -On Windows use setup.py to generate check_cfc.exe and copy that to clang.exe; and clang++.exe; -Enable the desired checks in check_cfc.cfg (in the same directory as the; wrapper); e.g.; [Checks]; dash_g_no_change = true; dash_s_no_change = false. -The wrapper can be run using its absolute path or added to PATH before the; compiler under test; e.g. export PATH=<path to check_cfc>:$PATH; -Compile as normal. The wrapper intercepts normal -c compiles and will return; non-zero if the check fails.; e.g.; $ clang -c test.cpp; Code difference detected with -g; --- /tmp/tmp5nv893.o; +++ /tmp/tmp6Vwjnc.o; @@ -1 +1 @@; - 0: 48 8b 05 51 0b 20 00 mov 0x200b51(%rip),%rax; + 0: 48 39 3d 51 0b 20 00 cmp %rdi,0x200b51(%rip). -To run LNT with Check CFC specify the absolute path to the wrapper to the --cc; and --cxx options; e.g.; lnt runtest nt --cc <path to check_cfc>/clang \\; --cxx <path to check_cfc>/clang++ ... To add a new check:; -Create a new subclass of WrapperCheck; -Implement the perform_check() method. This should perform the alternate compile; and do the comparison.; -Add the new check to check_cfc.cfg. The check has the same name as the; subclass.; """"""; """"""Returns True if running on Windows.""""""; """"""Exception type to be used when a step other than the original compile; fails.""""""; """"""Exception type to be used when a comparison check fails.""""""; """"""Returns True when running as a py2exe executable.""""""; # new py2exe; # old py2exe; # tools/freeze; """"""Get the directory that the script or exec",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/check_cfc/check_cfc.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/check_cfc/check_cfc.py
Integrability,depend,dependency,"#!/usr/bin/env python; """"""Test internal functions within check_cfc.py.""""""; # Test removing last thing in path; # Test removing one entry and leaving others; # Also tests removing repeated path; # Test removing non-canonical path; # Windows is case insensitive so should remove a different case; # path; # Case sensitive so will not remove different case path; # Not specified for implied output file name; # Can't get output file if more than one input file; # No output file specified; # Test getting implicit output file; # Outputting bitcode is not a normal compile; # Outputting preprocessed output or assembly is not a normal compile; # Input of preprocessed or assembly is not a ""normal compile""; # Specifying --version and -c is not a normal compile; # Outputting dependency files is not a normal compile; # Creating a dependency file as a side effect still outputs an object file; # Test output not specified; # Test output is specified; # No input file; # Input C file; # Input C++ file; # Multiple input files; # Don't handle preprocessed files; # Test identifying input file with quotes; # Test multiple quotes",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/check_cfc/test_check_cfc.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/check_cfc/test_check_cfc.py
Testability,test,tests,"#!/usr/bin/env python; """"""Test internal functions within check_cfc.py.""""""; # Test removing last thing in path; # Test removing one entry and leaving others; # Also tests removing repeated path; # Test removing non-canonical path; # Windows is case insensitive so should remove a different case; # path; # Case sensitive so will not remove different case path; # Not specified for implied output file name; # Can't get output file if more than one input file; # No output file specified; # Test getting implicit output file; # Outputting bitcode is not a normal compile; # Outputting preprocessed output or assembly is not a normal compile; # Input of preprocessed or assembly is not a ""normal compile""; # Specifying --version and -c is not a normal compile; # Outputting dependency files is not a normal compile; # Creating a dependency file as a side effect still outputs an object file; # Test output not specified; # Test output is specified; # No input file; # Input C file; # Input C++ file; # Multiple input files; # Don't handle preprocessed files; # Test identifying input file with quotes; # Test multiple quotes",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/check_cfc/test_check_cfc.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/check_cfc/test_check_cfc.py
Integrability,message,messages,#!/usr/bin/env python; # Create socket and bind to address; # Open the logging file.; # Receive messages; # Close socket,MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/CIndex/completion_logger_server.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/CIndex/completion_logger_server.py
Testability,log,logging,#!/usr/bin/env python; # Create socket and bind to address; # Open the logging file.; # Receive messages; # Close socket,MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/CIndex/completion_logger_server.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/CIndex/completion_logger_server.py
Deployability,patch,patched,"--------------------------------------===#; # Use python's arg parser to handle all leading option arguments, but pass; # everything else through to dtrace; # Find the cc1 command used by the compiler. To do this we execute the; # compiler with '-###' to figure out what it wants to do.; # Filter out known garbage.; # Use python's arg parser to handle all leading option arguments, but pass; # everything else through to dtrace; # clear the profile file env, so that we don't generate profdata; # when capturing the cc1 command; # Extract the list of symbols from the given file, which is assumed to be; # the output of a dtrace run logging either probefunc or ustack(1) and; # nothing else. The dtrace -xdemangle option needs to be used.; #; # This is particular to OS X at the moment, because of the '_' handling.; # Drop leading and trailing whitespace.; # If this is a timestamp specifier, extract it.; # If there is a '`' in the line, assume it is a ustack(1) entry in; # the form of <modulename>`<modulefunc>, where <modulefunc> is never; # truncated (but does need the mangling patched).; # Otherwise, assume this is a probefunc printout. DTrace on OS X; # seems to have a bug where it prints the mangled version of symbols; # which aren't C++ mangled. We just add a '_' to anything but start; # which doesn't already have a '_'.; # If we don't know all the symbols, or the symbol is one of them,; # just return it.; # Otherwise, we have a symbol name which isn't present in the; # binary. We assume it is truncated, and try to extend it.; # Get all the symbols with this prefix.; # If we found too many possible symbols, ignore this as a prefix.; # Report that we resolved a missing symbol.; # Otherwise, treat all the possible matches as having occurred. This; # is an over-approximation, but it should be ok in practice.; # Simply strategy, just return symbols in order of occurrence, even across; # multiple runs.; # More complicated strategy that tries to respect the call order across al",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/perf-training/perf-helper.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/perf-training/perf-helper.py
Modifiability,extend,extend,"command; # Extract the list of symbols from the given file, which is assumed to be; # the output of a dtrace run logging either probefunc or ustack(1) and; # nothing else. The dtrace -xdemangle option needs to be used.; #; # This is particular to OS X at the moment, because of the '_' handling.; # Drop leading and trailing whitespace.; # If this is a timestamp specifier, extract it.; # If there is a '`' in the line, assume it is a ustack(1) entry in; # the form of <modulename>`<modulefunc>, where <modulefunc> is never; # truncated (but does need the mangling patched).; # Otherwise, assume this is a probefunc printout. DTrace on OS X; # seems to have a bug where it prints the mangled version of symbols; # which aren't C++ mangled. We just add a '_' to anything but start; # which doesn't already have a '_'.; # If we don't know all the symbols, or the symbol is one of them,; # just return it.; # Otherwise, we have a symbol name which isn't present in the; # binary. We assume it is truncated, and try to extend it.; # Get all the symbols with this prefix.; # If we found too many possible symbols, ignore this as a prefix.; # Report that we resolved a missing symbol.; # Otherwise, treat all the possible matches as having occurred. This; # is an over-approximation, but it should be ok in practice.; # Simply strategy, just return symbols in order of occurrence, even across; # multiple runs.; # More complicated strategy that tries to respect the call order across all; # of the test cases, instead of giving a huge preference to the first test; # case.; # First, uniq all the lists.; # Compute the successors for each list.; # Emit all the symbols, but make sure to always emit all successors from any; # call list whenever we see a symbol.; #; # There isn't much science here, but this sometimes works better than the; # more naive strategy. Then again, sometimes it doesn't so more research is; # probably needed.; # Form the order file by just putting the most commonly occurring symb",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/perf-training/perf-helper.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/perf-training/perf-helper.py
Testability,log,logging,"# ===- perf-helper.py - Clang Python Bindings -----------------*- python -*--===#; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ===------------------------------------------------------------------------===#; # Use python's arg parser to handle all leading option arguments, but pass; # everything else through to dtrace; # Find the cc1 command used by the compiler. To do this we execute the; # compiler with '-###' to figure out what it wants to do.; # Filter out known garbage.; # Use python's arg parser to handle all leading option arguments, but pass; # everything else through to dtrace; # clear the profile file env, so that we don't generate profdata; # when capturing the cc1 command; # Extract the list of symbols from the given file, which is assumed to be; # the output of a dtrace run logging either probefunc or ustack(1) and; # nothing else. The dtrace -xdemangle option needs to be used.; #; # This is particular to OS X at the moment, because of the '_' handling.; # Drop leading and trailing whitespace.; # If this is a timestamp specifier, extract it.; # If there is a '`' in the line, assume it is a ustack(1) entry in; # the form of <modulename>`<modulefunc>, where <modulefunc> is never; # truncated (but does need the mangling patched).; # Otherwise, assume this is a probefunc printout. DTrace on OS X; # seems to have a bug where it prints the mangled version of symbols; # which aren't C++ mangled. We just add a '_' to anything but start; # which doesn't already have a '_'.; # If we don't know all the symbols, or the symbol is one of them,; # just return it.; # Otherwise, we have a symbol name which isn't present in the; # binary. We assume it is truncated, and try to extend it.; # Get all the symbols with this prefix.; # If we found too many possible symbols, ignore this as a prefix.; # Report that we",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/perf-training/perf-helper.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/perf-training/perf-helper.py
Usability,clear,clear,"# ===- perf-helper.py - Clang Python Bindings -----------------*- python -*--===#; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ===------------------------------------------------------------------------===#; # Use python's arg parser to handle all leading option arguments, but pass; # everything else through to dtrace; # Find the cc1 command used by the compiler. To do this we execute the; # compiler with '-###' to figure out what it wants to do.; # Filter out known garbage.; # Use python's arg parser to handle all leading option arguments, but pass; # everything else through to dtrace; # clear the profile file env, so that we don't generate profdata; # when capturing the cc1 command; # Extract the list of symbols from the given file, which is assumed to be; # the output of a dtrace run logging either probefunc or ustack(1) and; # nothing else. The dtrace -xdemangle option needs to be used.; #; # This is particular to OS X at the moment, because of the '_' handling.; # Drop leading and trailing whitespace.; # If this is a timestamp specifier, extract it.; # If there is a '`' in the line, assume it is a ustack(1) entry in; # the form of <modulename>`<modulefunc>, where <modulefunc> is never; # truncated (but does need the mangling patched).; # Otherwise, assume this is a probefunc printout. DTrace on OS X; # seems to have a bug where it prints the mangled version of symbols; # which aren't C++ mangled. We just add a '_' to anything but start; # which doesn't already have a '_'.; # If we don't know all the symbols, or the symbol is one of them,; # just return it.; # Otherwise, we have a symbol name which isn't present in the; # binary. We assume it is truncated, and try to extend it.; # Get all the symbols with this prefix.; # If we found too many possible symbols, ignore this as a prefix.; # Report that we",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/utils/perf-training/perf-helper.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/perf-training/perf-helper.py
Integrability,depend,depending,"#!/usr/bin/env python; # Giant associative set of builtin->intrinsic mappings where clang doesn't; # implement the builtin since the vector operation works by default.; # Special unhandled cases:; # __builtin_ia32_vec_ext_*(__P, idx) -> _mm_store_sd/_mm_storeh_pd; # depending on index. No abstract insert/extract for these oddly.",MatchSource.CODE_COMMENT,interpreter/llvm-project/clang/www/builtins.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/builtins.py
Availability,avail,available,"ch files and; # directories to ignore when looking for source files.; # The reST default role (used for this markup: `text`) to use for all documents.; # default_role = None; # If true, '()' will be appended to :func: etc. cross-reference text.; # add_function_parentheses = True; # If true, the current module name will be prepended to all description; # unit titles (such as .. function::).; # add_module_names = True; # If true, sectionauthor and moduleauthor directives will be shown in the; # output. They are ignored by default.; # The name of the Pygments (syntax highlighting) style to use.; # A list of ignored prefixes for module index sorting.; # modindex_common_prefix = []; # -- Options for HTML output ---------------------------------------------------; # The theme to use for HTML and HTML Help pages. See the documentation for; # a list of builtin themes.; # Theme options are theme-specific and customize the look and feel of a theme; # further. For a list of options available for each theme, see the; # documentation.; # Add any paths that contain custom themes here, relative to this directory.; # The name for this set of Sphinx documents. If None, it defaults to; # ""<project> v<release> documentation"".; # html_title = None; # A shorter title for the navigation bar. Default is the same as html_title.; # html_short_title = None; # The name of an image file (relative to this directory) to place at the top; # of the sidebar.; # html_logo = None; # The name of an image file (within the static path) to use as favicon of the; # docs. This file should be a Windows icon file (.ico) being 16x16 or 32x32; # pixels large.; # html_favicon = None; # Add any paths that contain custom static files (such as style sheets) here,; # relative to this directory. They are copied after the builtin static files,; # so a file named ""default.css"" will overwrite the builtin ""default.css"".; # If not '', a 'Last updated on:' timestamp is inserted at every page bottom,; # using the given str",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/docs/conf.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/conf.py
Deployability,configurat,configuration,"# -*- coding: utf-8 -*-; #; # LLVM documentation build configuration file.; #; # This file is execfile()d with the current directory set to its containing dir.; #; # Note that not all possible configuration values are present in this; # autogenerated file.; #; # All configuration values have a default; values that are commented out; # serve to show the default.; # If extensions (or modules to document with autodoc) are in another directory,; # add these directories to sys.path here. If the directory is relative to the; # documentation root, use os.path.abspath to make it absolute, like shown here.; # -- General configuration -----------------------------------------------------; # If your documentation needs a minimal Sphinx version, state it here.; # needs_sphinx = '1.0'; # Add any Sphinx extension module names here, as strings. They can be extensions; # coming with Sphinx (named 'sphinx.ext.*') or your custom ones.; # Automatic anchors for markdown titles; # Add any paths that contain templates here, relative to this directory.; # The encoding of source files.; # source_encoding = 'utf-8-sig'; # The master toctree document.; # General information about the project.; # The language for content autogenerated by Sphinx. Refer to documentation; # for a list of supported languages.; # language = None; # There are two options for replacing |today|: either, you set today to some; # non-false value, then it is used:; # today = ''; # Else, today_fmt is used as the format for a strftime call.; # List of patterns, relative to source directory, that match files and; # directories to ignore when looking for source files.; # The reST default role (used for this markup: `text`) to use for all documents.; # default_role = None; # If true, '()' will be appended to :func: etc. cross-reference text.; # add_function_parentheses = True; # If true, the current module name will be prepended to all description; # unit titles (such as .. function::).; # add_module_names = True; # If true, ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/docs/conf.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/conf.py
Modifiability,config,configuration,"# -*- coding: utf-8 -*-; #; # LLVM documentation build configuration file.; #; # This file is execfile()d with the current directory set to its containing dir.; #; # Note that not all possible configuration values are present in this; # autogenerated file.; #; # All configuration values have a default; values that are commented out; # serve to show the default.; # If extensions (or modules to document with autodoc) are in another directory,; # add these directories to sys.path here. If the directory is relative to the; # documentation root, use os.path.abspath to make it absolute, like shown here.; # -- General configuration -----------------------------------------------------; # If your documentation needs a minimal Sphinx version, state it here.; # needs_sphinx = '1.0'; # Add any Sphinx extension module names here, as strings. They can be extensions; # coming with Sphinx (named 'sphinx.ext.*') or your custom ones.; # Automatic anchors for markdown titles; # Add any paths that contain templates here, relative to this directory.; # The encoding of source files.; # source_encoding = 'utf-8-sig'; # The master toctree document.; # General information about the project.; # The language for content autogenerated by Sphinx. Refer to documentation; # for a list of supported languages.; # language = None; # There are two options for replacing |today|: either, you set today to some; # non-false value, then it is used:; # today = ''; # Else, today_fmt is used as the format for a strftime call.; # List of patterns, relative to source directory, that match files and; # directories to ignore when looking for source files.; # The reST default role (used for this markup: `text`) to use for all documents.; # default_role = None; # If true, '()' will be appended to :func: etc. cross-reference text.; # add_function_parentheses = True; # If true, the current module name will be prepended to all description; # unit titles (such as .. function::).; # add_module_names = True; # If true, ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/docs/conf.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/conf.py
Safety,abort,abort,"file will be output, and all pages will; # contain a <link> tag referring to it. The value of this option must be the; # base URL from which the finished HTML is served.; # html_use_opensearch = ''; # This is the file name suffix for HTML files (e.g. "".xhtml"").; # html_file_suffix = None; # Output file base name for HTML help builder.; # -- Options for LaTeX output --------------------------------------------------; # The paper size ('letterpaper' or 'a4paper').; #'papersize': 'letterpaper',; # The font size ('10pt', '11pt' or '12pt').; #'pointsize': '10pt',; # Additional stuff for the LaTeX preamble.; #'preamble': '',; # Grouping the document tree into LaTeX files. List of tuples; # (source start file, target name, title, author, documentclass [howto/manual]).; # The name of an image file (relative to this directory) to place at the top of; # the title page.; # latex_logo = None; # For ""manual"" documents, if this is true, then toplevel headings are parts,; # not chapters.; # latex_use_parts = False; # If true, show page references after internal links.; # latex_show_pagerefs = False; # If true, show URL addresses after external links.; # latex_show_urls = False; # Documents to append as an appendix to all manuals.; # latex_appendices = []; # If false, no module index is generated.; # latex_domain_indices = True; # -- Options for manual page output --------------------------------------------; # One entry per manual page. List of tuples; # (source start file, name, description, authors, manual section).; # Automatically derive the list of man pages from the contents of the command; # guide subdirectory.; # Split the name out of the title.; # Process Markdown files; # Process ReST files apart from the index page.; # If true, show URL addresses after external links.; # man_show_urls = False; # FIXME: Define intersphinx configuration.; # Pygment lexer are sometimes out of date (when parsing LLVM for example) or; # wrong. Suppress the warning so the build doesn't abort.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/docs/conf.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/conf.py
Usability,guid,guide,"file will be output, and all pages will; # contain a <link> tag referring to it. The value of this option must be the; # base URL from which the finished HTML is served.; # html_use_opensearch = ''; # This is the file name suffix for HTML files (e.g. "".xhtml"").; # html_file_suffix = None; # Output file base name for HTML help builder.; # -- Options for LaTeX output --------------------------------------------------; # The paper size ('letterpaper' or 'a4paper').; #'papersize': 'letterpaper',; # The font size ('10pt', '11pt' or '12pt').; #'pointsize': '10pt',; # Additional stuff for the LaTeX preamble.; #'preamble': '',; # Grouping the document tree into LaTeX files. List of tuples; # (source start file, target name, title, author, documentclass [howto/manual]).; # The name of an image file (relative to this directory) to place at the top of; # the title page.; # latex_logo = None; # For ""manual"" documents, if this is true, then toplevel headings are parts,; # not chapters.; # latex_use_parts = False; # If true, show page references after internal links.; # latex_show_pagerefs = False; # If true, show URL addresses after external links.; # latex_show_urls = False; # Documents to append as an appendix to all manuals.; # latex_appendices = []; # If false, no module index is generated.; # latex_domain_indices = True; # -- Options for manual page output --------------------------------------------; # One entry per manual page. List of tuples; # (source start file, name, description, authors, manual section).; # Automatically derive the list of man pages from the contents of the command; # guide subdirectory.; # Split the name out of the title.; # Process Markdown files; # Process ReST files apart from the index page.; # If true, show URL addresses after external links.; # man_show_urls = False; # FIXME: Define intersphinx configuration.; # Pygment lexer are sometimes out of date (when parsing LLVM for example) or; # wrong. Suppress the warning so the build doesn't abort.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/docs/conf.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/conf.py
Modifiability,variab,variable,"#!/usr/bin/env python; """"""Used to generate a bash script which will invoke the toy and time it""""""; """"""Echo some comments and invoke both versions of toy""""""; """"""Used to generate random Kaleidoscope code""""""; # A mapping of calls within functions with no duplicates; # A list of function calls which will actually be executed; # A comprehensive mapping of calls within functions; # used for computing the total number of calls; # Count this call; # Then count all the functions it calls; """"""Maintains a map of functions that are called from other functions""""""; """"""Maintains a list of functions that will actually be called""""""; # Update the total call count; # If this function is already in the list, don't do anything else; # Add this function to the list of those that will be called.; # If this function calls other functions, add them too; """"""Sets the probably of generating a function call""""""; # Don't let our intermediate value become zero; # This is complicated by the fact that '<' is our only comparison operator; # Initialize the variable names to be rotated; # Write some random operations; # Rotate the variables; """"""Generate a random Kaleidoscope script based on the given parameters""""""; # Always end with a function call; # Execution begins here; # Generate the code",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/cached/genk-timing.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/cached/genk-timing.py
Modifiability,variab,variable,"#!/usr/bin/env python; """"""Used to generate a bash script which will invoke the toy and time it""""""; """"""Echo some comments and invoke both versions of toy""""""; """"""Used to generate random Kaleidoscope code""""""; # A mapping of calls within functions with no duplicates; # A list of function calls which will actually be executed; # A comprehensive mapping of calls within functions; # used for computing the total number of calls; # Count this call; # Then count all the functions it calls; """"""Maintains a map of functions that are called from other functions""""""; """"""Maintains a list of functions that will actually be called""""""; # Update the total call count; # If this function is already in the list, don't do anything else; # Add this function to the list of those that will be called.; # If this function calls other functions, add them too; """"""Sets the probably of generating a function call""""""; # Don't let our intermediate value become zero; # This is complicated by the fact that '<' is our only comparison operator; # Initialize the variable names to be rotated; # Write some random operations; # Rotate the variables; """"""Generate a random Kaleidoscope script based on the given parameters""""""; # Always end with a function call; # Execution begins here; # Generate the code",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/complete/genk-timing.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/complete/genk-timing.py
Modifiability,variab,variable,"#!/usr/bin/env python; """"""Used to generate a bash script which will invoke the toy and time it""""""; """"""Echo some comments and invoke both versions of toy""""""; """"""Used to generate random Kaleidoscope code""""""; # A mapping of calls within functions with no duplicates; # A list of function calls which will actually be executed; # A comprehensive mapping of calls within functions; # used for computing the total number of calls; # Count this call; # Then count all the functions it calls; """"""Maintains a map of functions that are called from other functions""""""; """"""Maintains a list of functions that will actually be called""""""; # Update the total call count; # If this function is already in the list, don't do anything else; # Add this function to the list of those that will be called.; # If this function calls other functions, add them too; """"""Sets the probably of generating a function call""""""; # Don't let our intermediate value become zero; # This is complicated by the fact that '<' is our only comparison operator; # Initialize the variable names to be rotated; # Write some random operations; # Rotate the variables; """"""Generate a random Kaleidoscope script based on the given parameters""""""; # Always end with a function call; # Execution begins here; # Generate the code",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/lazy/genk-timing.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/lazy/genk-timing.py
Testability,mock,mock,"""""""Generate a mock model for LLVM tests. The generated model is not a neural net - it is just a tf.function with the; correct input and output parameters. By construction, the mock model will always; output 1.; """"""; """"""; [; {; ""logging_name"": ""inlining_decision"",; ""tensor_spec"": {; ""name"": ""StatefulPartitionedCall"",; ""port"": 0,; ""type"": ""int64_t"",; ""shape"": [; 1; ]; }; }; ]; """"""; # pylint: disable=g-complex-comprehension; """"""Returns the list of features for LLVM inlining.""""""; # int64 features; # float32 features; # int32 features; """"""Build and save the mock model with the given signature""""""",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Analysis/models/gen-inline-oz-test-model.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/models/gen-inline-oz-test-model.py
Modifiability,variab,variable,"""""""Generate a mock model for LLVM tests for Register Allocation.; The generated model is not a neural net - it is just a tf.function with the; correct input and output parameters. By construction, the mock model will always; output the first liverange that can be evicted.; """"""; """"""; [; {; ""logging_name"": ""index_to_evict"",; ""tensor_spec"": {; ""name"": ""StatefulPartitionedCall"",; ""port"": 0,; ""type"": ""int64_t"",; ""shape"": [; 1; ]; }; }; ]; """"""; """"""Returns (time_step_spec, action_spec) for LLVM register allocation.""""""; """"""Build and save the mock model with the given signature.""""""; # We have to set this useless variable in order for the TF C API to correctly; # intake it",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Analysis/models/gen-regalloc-eviction-test-model.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/models/gen-regalloc-eviction-test-model.py
Testability,mock,mock,"""""""Generate a mock model for LLVM tests for Register Allocation.; The generated model is not a neural net - it is just a tf.function with the; correct input and output parameters. By construction, the mock model will always; output the first liverange that can be evicted.; """"""; """"""; [; {; ""logging_name"": ""index_to_evict"",; ""tensor_spec"": {; ""name"": ""StatefulPartitionedCall"",; ""port"": 0,; ""type"": ""int64_t"",; ""shape"": [; 1; ]; }; }; ]; """"""; """"""Returns (time_step_spec, action_spec) for LLVM register allocation.""""""; """"""Build and save the mock model with the given signature.""""""; # We have to set this useless variable in order for the TF C API to correctly; # intake it",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Analysis/models/gen-regalloc-eviction-test-model.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/models/gen-regalloc-eviction-test-model.py
Modifiability,variab,variable,"""""""Generate a mock model for LLVM tests for Register Allocation.; The generated model is not a neural net - it is just a tf.function with the; correct input and output parameters. ; """"""; ## By construction, the mock model will always output the first liverange that can be evicted.; """"""; [; {; ""logging_name"": ""priority"", ; ""tensor_spec"": {; ""name"": ""StatefulPartitionedCall"", ; ""port"": 0, ; ""type"": ""float"", ; ""shape"": [; 1; ]; }; }; ]; """"""; """"""Returns (time_step_spec, action_spec) for LLVM register allocation.""""""; """"""Build and save the mock model with the given signature.""""""; # We have to set this useless variable in order for the TF C API to correctly; # intake it; # Add a large number so s won't be 0.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Analysis/models/gen-regalloc-priority-test-model.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/models/gen-regalloc-priority-test-model.py
Testability,mock,mock,"""""""Generate a mock model for LLVM tests for Register Allocation.; The generated model is not a neural net - it is just a tf.function with the; correct input and output parameters. ; """"""; ## By construction, the mock model will always output the first liverange that can be evicted.; """"""; [; {; ""logging_name"": ""priority"", ; ""tensor_spec"": {; ""name"": ""StatefulPartitionedCall"", ; ""port"": 0, ; ""type"": ""float"", ; ""shape"": [; 1; ]; }; }; ]; """"""; """"""Returns (time_step_spec, action_spec) for LLVM register allocation.""""""; """"""Build and save the mock model with the given signature.""""""; # We have to set this useless variable in order for the TF C API to correctly; # intake it; # Add a large number so s won't be 0.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Analysis/models/gen-regalloc-priority-test-model.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/models/gen-regalloc-priority-test-model.py
Testability,test,testing,"""""""Utility for testing InteractiveModelRunner. Use it from pass-specific tests by providing a main .py which calls this library's; `run_interactive` with an appropriate callback to provide advice. From .ll tests, just call the above-mentioned main as a prefix to the opt/llc; invocation (with the appropriate flags enabling the interactive mode). Examples:; test/Transforms/Inline/ML/interactive-mode.ll; test/CodeGen/MLRegAlloc/interactive-mode.ll; """"""; """"""Send the `value` - currently just a scalar - formatted as per `spec`.""""""; # just int64 for now; """"""Host the compiler.; Args:; temp_rootname: the base file name from which to construct the 2 pipes for; communicating with the compiler.; make_response: a function that, given the current tensor values, provides a; response.; process_and_args: the full commandline for the compiler. It it assumed it; contains a flag poiting to `temp_rootname` so that the InteractiveModeRunner; would attempt communication on the same pair as this function opens. This function sets up the communication with the compiler - via 2 files named; `temp_rootname`.in and `temp_rootname`.out - prints out the received features,; and sends back to the compiler an advice (which it gets from `make_response`).; It's used for testing, and also to showcase how to set up communication in an; interactive ML (""gym"") environment.; """"""",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Analysis/models/interactive_host.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/models/interactive_host.py
Testability,log,log,"""""""Reader for training log. See lib/Analysis/TrainingLogger.cpp for a description of the format.; """"""",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/lib/Analysis/models/log_reader.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/models/log_reader.py
Integrability,depend,depending,"#!/usr/bin/env python; """"""Generate the difference of two YAML files into a new YAML file (works on; pair of directories too). A new attribute 'Added' is set to True or False; depending whether the entry is added or removed from the first input to the; next. The tools requires PyYAML.""""""; # Try to use the C parser.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/tools/opt-viewer/opt-diff.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/opt-viewer/opt-diff.py
Performance,optimiz,optimization,"#!/usr/bin/env python; """"""Generate statistics about optimization records from the YAML files; generated with -fsave-optimization-record and -fdiagnostics-show-hotness. The tools requires PyYAML and Pygments Python packages.""""""",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/tools/opt-viewer/opt-stats.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/opt-viewer/opt-stats.py
Deployability,toggle,toggleExpandedMessage,"ss=""highlight""><pre>{html_line}</pre></div></td>; </tr>""""""; # Column is the number of characters *including* tabs, keep those and; # replace everything else with spaces.; # Create expanded message and link if we have a multiline message.; """"""; <div class=""full-info"" style=""display:none;"">; <div class=""col-left""><pre style=""display:inline"">{}</pre></div>; <div class=""expanded col-left""><pre>{}</pre></div>; </div>""""""; """"""; <tr>; <td></td>; <td>{r.RelativeHotness}</td>; <td class=\""column-entry-{r.color}\"">{r.PassWithDiffPrefix}</td>; <td><pre style=""display:inline"">{indent}</pre><span class=\""column-entry-yellow\"">{expand_link} {message}&nbsp;</span>{expand_message}</td>; <td class=\""column-entry-yellow\"">{inlining_context}</td>; </tr>""""""; """"""; <html>; <title>{}</title>; <meta charset=""utf-8"" />; <head>; <link rel='stylesheet' type='text/css' href='style.css'>; <script type=""text/javascript"">; /* Simple helper to show/hide the expanded message of a remark. */; function toggleExpandedMessage(e) {{; var FullTextElems = e.parentElement.parentElement.getElementsByClassName(""full-info"");; if (!FullTextElems || FullTextElems.length < 1) {{; return false;; }}; var FullText = FullTextElems[0];; if (FullText.style.display == 'none') {{; e.innerHTML = '-';; FullText.style.display = 'block';; }} else {{; e.innerHTML = '+';; FullText.style.display = 'none';; }}; }}; </script>; </head>; <body>; <div class=""centered"">; <table class=""source"">; <thead>; <tr>; <th style=""width: 2%"">Line</td>; <th style=""width: 3%"">Hotness</td>; <th style=""width: 10%"">Optimization</td>; <th style=""width: 70%"">Source</td>; <th style=""width: 15%"">Inline Context</td>; </tr>; </thead>; <tbody>""""""; """"""; </tbody>; </table>; </body>; </html>""""""; """"""; <tr>; <td class=\""column-entry-{odd}\""><a href={r.Link}>{r.DebugLocString}</a></td>; <td class=\""column-entry-{odd}\"">{r.RelativeHotness}</td>; <td class=\""column-entry-{odd}\"">{escaped_name}</td>; <td class=\""column-entry-{r.color}\"">{r.PassWithDiffPrefix}</td>; ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/tools/opt-viewer/opt-viewer.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/opt-viewer/opt-viewer.py
Integrability,message,message,"with -fsave-optimization-record and -fdiagnostics-show-hotness. The tools requires PyYAML and Pygments Python packages.""""""; # This allows passing the global context to the child processes.; # Map function names to their source location for function where inlining happened; """"""; <html>; <h1>Unable to locate file {}</h1>; </html>; """"""; # Note that the API is different between Python 2 and 3. On; # Python 3, pygments.highlight() returns a bytes object, so we; # have to decode. On Python 2, the output is str but since we; # support unicode characters and the output streams is unicode we; # decode too.; # Take off the header and footer, these must be; # reapplied line-wise, within the page structure; """"""; <tr>; <td><a name=\""L{linenum}\"">{linenum}</a></td>; <td></td>; <td></td>; <td><div class=""highlight""><pre>{html_line}</pre></div></td>; </tr>""""""; # Column is the number of characters *including* tabs, keep those and; # replace everything else with spaces.; # Create expanded message and link if we have a multiline message.; """"""; <div class=""full-info"" style=""display:none;"">; <div class=""col-left""><pre style=""display:inline"">{}</pre></div>; <div class=""expanded col-left""><pre>{}</pre></div>; </div>""""""; """"""; <tr>; <td></td>; <td>{r.RelativeHotness}</td>; <td class=\""column-entry-{r.color}\"">{r.PassWithDiffPrefix}</td>; <td><pre style=""display:inline"">{indent}</pre><span class=\""column-entry-yellow\"">{expand_link} {message}&nbsp;</span>{expand_message}</td>; <td class=\""column-entry-yellow\"">{inlining_context}</td>; </tr>""""""; """"""; <html>; <title>{}</title>; <meta charset=""utf-8"" />; <head>; <link rel='stylesheet' type='text/css' href='style.css'>; <script type=""text/javascript"">; /* Simple helper to show/hide the expanded message of a remark. */; function toggleExpandedMessage(e) {{; var FullTextElems = e.parentElement.parentElement.getElementsByClassName(""full-info"");; if (!FullTextElems || FullTextElems.length < 1) {{; return false;; }}; var FullText = FullTextElems[0];;",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/tools/opt-viewer/opt-viewer.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/opt-viewer/opt-viewer.py
Modifiability,variab,variable,"an class=\""column-entry-yellow\"">{expand_link} {message}&nbsp;</span>{expand_message}</td>; <td class=\""column-entry-yellow\"">{inlining_context}</td>; </tr>""""""; """"""; <html>; <title>{}</title>; <meta charset=""utf-8"" />; <head>; <link rel='stylesheet' type='text/css' href='style.css'>; <script type=""text/javascript"">; /* Simple helper to show/hide the expanded message of a remark. */; function toggleExpandedMessage(e) {{; var FullTextElems = e.parentElement.parentElement.getElementsByClassName(""full-info"");; if (!FullTextElems || FullTextElems.length < 1) {{; return false;; }}; var FullText = FullTextElems[0];; if (FullText.style.display == 'none') {{; e.innerHTML = '-';; FullText.style.display = 'block';; }} else {{; e.innerHTML = '+';; FullText.style.display = 'none';; }}; }}; </script>; </head>; <body>; <div class=""centered"">; <table class=""source"">; <thead>; <tr>; <th style=""width: 2%"">Line</td>; <th style=""width: 3%"">Hotness</td>; <th style=""width: 10%"">Optimization</td>; <th style=""width: 70%"">Source</td>; <th style=""width: 15%"">Inline Context</td>; </tr>; </thead>; <tbody>""""""; """"""; </tbody>; </table>; </body>; </html>""""""; """"""; <tr>; <td class=\""column-entry-{odd}\""><a href={r.Link}>{r.DebugLocString}</a></td>; <td class=\""column-entry-{odd}\"">{r.RelativeHotness}</td>; <td class=\""column-entry-{odd}\"">{escaped_name}</td>; <td class=\""column-entry-{r.color}\"">{r.PassWithDiffPrefix}</td>; </tr>""""""; """"""; <html>; <meta charset=""utf-8"" />; <head>; <link rel='stylesheet' type='text/css' href='style.css'>; </head>; <body>; <div class=""centered"">; <table>; <tr>; <td>Source Location</td>; <td>Hotness</td>; <td>Function</td>; <td>Pass</td>; </tr>""""""; """"""; </table>; </body>; </html>""""""; # Set up a map between function names and their source location for; # function where inlining happened; # Do not make this a global variable. Values needed to be propagated through; # to individual classes and functions to be portable with multiprocessing across; # Windows and non-Windows.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/tools/opt-viewer/opt-viewer.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/opt-viewer/opt-viewer.py
Performance,optimiz,optimization,"#!/usr/bin/env python; """"""Generate HTML output to visualize optimization records from the YAML files; generated with -fsave-optimization-record and -fdiagnostics-show-hotness. The tools requires PyYAML and Pygments Python packages.""""""; # This allows passing the global context to the child processes.; # Map function names to their source location for function where inlining happened; """"""; <html>; <h1>Unable to locate file {}</h1>; </html>; """"""; # Note that the API is different between Python 2 and 3. On; # Python 3, pygments.highlight() returns a bytes object, so we; # have to decode. On Python 2, the output is str but since we; # support unicode characters and the output streams is unicode we; # decode too.; # Take off the header and footer, these must be; # reapplied line-wise, within the page structure; """"""; <tr>; <td><a name=\""L{linenum}\"">{linenum}</a></td>; <td></td>; <td></td>; <td><div class=""highlight""><pre>{html_line}</pre></div></td>; </tr>""""""; # Column is the number of characters *including* tabs, keep those and; # replace everything else with spaces.; # Create expanded message and link if we have a multiline message.; """"""; <div class=""full-info"" style=""display:none;"">; <div class=""col-left""><pre style=""display:inline"">{}</pre></div>; <div class=""expanded col-left""><pre>{}</pre></div>; </div>""""""; """"""; <tr>; <td></td>; <td>{r.RelativeHotness}</td>; <td class=\""column-entry-{r.color}\"">{r.PassWithDiffPrefix}</td>; <td><pre style=""display:inline"">{indent}</pre><span class=\""column-entry-yellow\"">{expand_link} {message}&nbsp;</span>{expand_message}</td>; <td class=\""column-entry-yellow\"">{inlining_context}</td>; </tr>""""""; """"""; <html>; <title>{}</title>; <meta charset=""utf-8"" />; <head>; <link rel='stylesheet' type='text/css' href='style.css'>; <script type=""text/javascript"">; /* Simple helper to show/hide the expanded message of a remark. */; function toggleExpandedMessage(e) {{; var FullTextElems = e.parentElement.parentElement.getElementsByClassName(""full-in",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/tools/opt-viewer/opt-viewer.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/opt-viewer/opt-viewer.py
Integrability,depend,depend,"#!/usr/bin/env python; # Try to use the C parser.; # The previously builtin function `intern()` was moved; # to the `sys` module in Python 3.; # Python 3; # Python 2; # Work-around for http://pyyaml.org/ticket/154.; # Intern all strings since we have lot of duplication across filenames,; # remark text.; #; # Change Args from a list of dicts to a tuple of tuples. This saves; # memory in two ways. One, a small tuple is significantly smaller than a; # small dict. Two, using tuple instead of list allows Args to be directly; # used as part of the key (in Python only immutable types are hashable).; # Can't intern unicode strings.; # This handles [{'Caller': ..., 'DebugLoc': { 'File': ... }}]; # The inverse operation of the dictonary-related memory optimization in; # _reduce_memory_dict. E.g.; # (('DebugLoc', (('File', ...) ... ))) -> [{'DebugLoc': {'File': ...} ....}]; # Return a cached dictionary for the arguments. The key for each entry is; # the argument key (e.g. 'Callee' for inlining remarks. The value is a; # list containing the value (e.g. for 'Callee' the function) and; # optionally a DebugLoc.; # Args is a list of mappings (dictionaries); # Avoid remarks withoug debug location or if they are duplicated; # If we're reading a back a diff yaml file, max_hotness is already; # captured which may actually be less than the max hotness found; # in the file.; # Bring max_hotness into the remarks so that; # RelativeHotness does not depend on an external global.; # Exclude mounted directories and symlinks (os.walk default).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/tools/opt-viewer/optrecord.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/opt-viewer/optrecord.py
Performance,optimiz,optimization,"#!/usr/bin/env python; # Try to use the C parser.; # The previously builtin function `intern()` was moved; # to the `sys` module in Python 3.; # Python 3; # Python 2; # Work-around for http://pyyaml.org/ticket/154.; # Intern all strings since we have lot of duplication across filenames,; # remark text.; #; # Change Args from a list of dicts to a tuple of tuples. This saves; # memory in two ways. One, a small tuple is significantly smaller than a; # small dict. Two, using tuple instead of list allows Args to be directly; # used as part of the key (in Python only immutable types are hashable).; # Can't intern unicode strings.; # This handles [{'Caller': ..., 'DebugLoc': { 'File': ... }}]; # The inverse operation of the dictonary-related memory optimization in; # _reduce_memory_dict. E.g.; # (('DebugLoc', (('File', ...) ... ))) -> [{'DebugLoc': {'File': ...} ....}]; # Return a cached dictionary for the arguments. The key for each entry is; # the argument key (e.g. 'Callee' for inlining remarks. The value is a; # list containing the value (e.g. for 'Callee' the function) and; # optionally a DebugLoc.; # Args is a list of mappings (dictionaries); # Avoid remarks withoug debug location or if they are duplicated; # If we're reading a back a diff yaml file, max_hotness is already; # captured which may actually be less than the max hotness found; # in the file.; # Bring max_hotness into the remarks so that; # RelativeHotness does not depend on an external global.; # Exclude mounted directories and symlinks (os.walk default).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/tools/opt-viewer/optrecord.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/opt-viewer/optrecord.py
Security,hash,hashable,"#!/usr/bin/env python; # Try to use the C parser.; # The previously builtin function `intern()` was moved; # to the `sys` module in Python 3.; # Python 3; # Python 2; # Work-around for http://pyyaml.org/ticket/154.; # Intern all strings since we have lot of duplication across filenames,; # remark text.; #; # Change Args from a list of dicts to a tuple of tuples. This saves; # memory in two ways. One, a small tuple is significantly smaller than a; # small dict. Two, using tuple instead of list allows Args to be directly; # used as part of the key (in Python only immutable types are hashable).; # Can't intern unicode strings.; # This handles [{'Caller': ..., 'DebugLoc': { 'File': ... }}]; # The inverse operation of the dictonary-related memory optimization in; # _reduce_memory_dict. E.g.; # (('DebugLoc', (('File', ...) ... ))) -> [{'DebugLoc': {'File': ...} ....}]; # Return a cached dictionary for the arguments. The key for each entry is; # the argument key (e.g. 'Callee' for inlining remarks. The value is a; # list containing the value (e.g. for 'Callee' the function) and; # optionally a DebugLoc.; # Args is a list of mappings (dictionaries); # Avoid remarks withoug debug location or if they are duplicated; # If we're reading a back a diff yaml file, max_hotness is already; # captured which may actually be less than the max hotness found; # in the file.; # Bring max_hotness into the remarks so that; # RelativeHotness does not depend on an external global.; # Exclude mounted directories and symlinks (os.walk default).",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/tools/opt-viewer/optrecord.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/opt-viewer/optrecord.py
Availability,down,down,"#!/usr/bin/env python; #; # Given a previous good compile narrow down miscompiles.; # Expects two directories named ""before"" and ""after"" each containing a set of; # assembly or object files where the ""after"" version is assumed to be broken.; # You also have to provide a script called ""link_test"". It is called with a; # list of files which should be linked together and result tested. ""link_test""; # should returns with exitcode 0 if the linking and testing succeeded.; #; # If a response file is provided, only the object files that are listed in the; # file are inspected. In addition, the ""link_test"" is called with a temporary; # response file representing one iteration of bisection.; #; # abtest.py operates by taking all files from the ""before"" directory and; # in each step replacing one of them with a file from the ""bad"" directory.; #; # Additionally you can perform the same steps with a single .s file. In this; # mode functions are identified by "" -- Begin function FunctionName"" and; # "" -- End function"" markers. The abtest.py then takes all; # function from the file in the ""before"" directory and replaces one function; # with the corresponding function from the ""bad"" file in each step.; #; # Example usage to identify miscompiled files:; # 1. Create a link_test script, make it executable. Simple Example:; # clang ""$@"" -o /tmp/test && /tmp/test || echo ""PROBLEM""; # 2. Run the script to figure out which files are miscompiled:; # > ./abtest.py; # somefile.s: ok; # someotherfile.s: skipped: same content; # anotherfile.s: failed: './link_test' exitcode != 0; # ...; # Example usage to identify miscompiled functions inside a file:; # 3. Run the tests on a single file (assuming before/file.s and; # after/file.s exist); # > ./abtest.py file.s; # funcname1 [0/XX]: ok; # funcname2 [1/XX]: ok; # funcname3 [2/XX]: skipped: same content; # funcname4 [3/XX]: failed: './link_test' exitcode != 0; # ...; # Specify LINKTEST via `--test`. Default value is './link_test'.; # Compute the ma",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/abtest.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/abtest.py
Performance,perform,perform,"#!/usr/bin/env python; #; # Given a previous good compile narrow down miscompiles.; # Expects two directories named ""before"" and ""after"" each containing a set of; # assembly or object files where the ""after"" version is assumed to be broken.; # You also have to provide a script called ""link_test"". It is called with a; # list of files which should be linked together and result tested. ""link_test""; # should returns with exitcode 0 if the linking and testing succeeded.; #; # If a response file is provided, only the object files that are listed in the; # file are inspected. In addition, the ""link_test"" is called with a temporary; # response file representing one iteration of bisection.; #; # abtest.py operates by taking all files from the ""before"" directory and; # in each step replacing one of them with a file from the ""bad"" directory.; #; # Additionally you can perform the same steps with a single .s file. In this; # mode functions are identified by "" -- Begin function FunctionName"" and; # "" -- End function"" markers. The abtest.py then takes all; # function from the file in the ""before"" directory and replaces one function; # with the corresponding function from the ""bad"" file in each step.; #; # Example usage to identify miscompiled files:; # 1. Create a link_test script, make it executable. Simple Example:; # clang ""$@"" -o /tmp/test && /tmp/test || echo ""PROBLEM""; # 2. Run the script to figure out which files are miscompiled:; # > ./abtest.py; # somefile.s: ok; # someotherfile.s: skipped: same content; # anotherfile.s: failed: './link_test' exitcode != 0; # ...; # Example usage to identify miscompiled functions inside a file:; # 3. Run the tests on a single file (assuming before/file.s and; # after/file.s exist); # > ./abtest.py file.s; # funcname1 [0/XX]: ok; # funcname2 [1/XX]: ok; # funcname3 [2/XX]: skipped: same content; # funcname4 [3/XX]: failed: './link_test' exitcode != 0; # ...; # Specify LINKTEST via `--test`. Default value is './link_test'.; # Compute the ma",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/abtest.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/abtest.py
Safety,sanity check,sanity check,"rectory.; #; # Additionally you can perform the same steps with a single .s file. In this; # mode functions are identified by "" -- Begin function FunctionName"" and; # "" -- End function"" markers. The abtest.py then takes all; # function from the file in the ""before"" directory and replaces one function; # with the corresponding function from the ""bad"" file in each step.; #; # Example usage to identify miscompiled files:; # 1. Create a link_test script, make it executable. Simple Example:; # clang ""$@"" -o /tmp/test && /tmp/test || echo ""PROBLEM""; # 2. Run the script to figure out which files are miscompiled:; # > ./abtest.py; # somefile.s: ok; # someotherfile.s: skipped: same content; # anotherfile.s: failed: './link_test' exitcode != 0; # ...; # Example usage to identify miscompiled functions inside a file:; # 3. Run the tests on a single file (assuming before/file.s and; # after/file.s exist); # > ./abtest.py file.s; # funcname1 [0/XX]: ok; # funcname2 [1/XX]: ok; # funcname3 [2/XX]: skipped: same content; # funcname4 [3/XX]: failed: './link_test' exitcode != 0; # ...; # Specify LINKTEST via `--test`. Default value is './link_test'.; # Compute the maximum number of checks we have to do in the worst case.; # TODO:; # - We could optimize based on the knowledge that when splitting a failed; # partition into two and one side checks out okay then we can deduce that; # the other partition must be a failure.; # remove prefix; # Note that we iterate over files_a so we don't change the order; # (cannot use `picks` as it is a dictionary without order); # If response file is used, create a temporary response file for the; # picked files.; # Preparation phase: Creates a dictionary mapping names to a list of two; # choices each. The bisection algorithm will pick one choice for each name; # and then run the perform_test function on it.; # ""Checking whether build environment is sane ...""; # This shouldn't happen when the sanity check works...; # Maybe link_test isn't deterministic?",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/abtest.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/abtest.py
Testability,test,tested,"#!/usr/bin/env python; #; # Given a previous good compile narrow down miscompiles.; # Expects two directories named ""before"" and ""after"" each containing a set of; # assembly or object files where the ""after"" version is assumed to be broken.; # You also have to provide a script called ""link_test"". It is called with a; # list of files which should be linked together and result tested. ""link_test""; # should returns with exitcode 0 if the linking and testing succeeded.; #; # If a response file is provided, only the object files that are listed in the; # file are inspected. In addition, the ""link_test"" is called with a temporary; # response file representing one iteration of bisection.; #; # abtest.py operates by taking all files from the ""before"" directory and; # in each step replacing one of them with a file from the ""bad"" directory.; #; # Additionally you can perform the same steps with a single .s file. In this; # mode functions are identified by "" -- Begin function FunctionName"" and; # "" -- End function"" markers. The abtest.py then takes all; # function from the file in the ""before"" directory and replaces one function; # with the corresponding function from the ""bad"" file in each step.; #; # Example usage to identify miscompiled files:; # 1. Create a link_test script, make it executable. Simple Example:; # clang ""$@"" -o /tmp/test && /tmp/test || echo ""PROBLEM""; # 2. Run the script to figure out which files are miscompiled:; # > ./abtest.py; # somefile.s: ok; # someotherfile.s: skipped: same content; # anotherfile.s: failed: './link_test' exitcode != 0; # ...; # Example usage to identify miscompiled functions inside a file:; # 3. Run the tests on a single file (assuming before/file.s and; # after/file.s exist); # > ./abtest.py file.s; # funcname1 [0/XX]: ok; # funcname2 [1/XX]: ok; # funcname3 [2/XX]: skipped: same content; # funcname4 [3/XX]: failed: './link_test' exitcode != 0; # ...; # Specify LINKTEST via `--test`. Default value is './link_test'.; # Compute the ma",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/abtest.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/abtest.py
Availability,failure,failures,"#!/usr/bin/env python; """"""Reduces GlobalISel failures. This script is a utility to reduce tests that GlobalISel; fails to compile. It runs llc to get the error message using a regex and creates; a custom command to check that specific error. Then, it runs bugpoint; with the custom command. """"""; # Check if this is called by bugpoint.; # Parse arguments.; # Check if the binaries exist.; # Run llc to see if GlobalISel fails.; # Run bugpoint.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/bugpoint_gisel_reducer.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/bugpoint_gisel_reducer.py
Energy Efficiency,reduce,reduce,"#!/usr/bin/env python; """"""Reduces GlobalISel failures. This script is a utility to reduce tests that GlobalISel; fails to compile. It runs llc to get the error message using a regex and creates; a custom command to check that specific error. Then, it runs bugpoint; with the custom command. """"""; # Check if this is called by bugpoint.; # Parse arguments.; # Check if the binaries exist.; # Run llc to see if GlobalISel fails.; # Run bugpoint.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/bugpoint_gisel_reducer.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/bugpoint_gisel_reducer.py
Integrability,message,message,"#!/usr/bin/env python; """"""Reduces GlobalISel failures. This script is a utility to reduce tests that GlobalISel; fails to compile. It runs llc to get the error message using a regex and creates; a custom command to check that specific error. Then, it runs bugpoint; with the custom command. """"""; # Check if this is called by bugpoint.; # Parse arguments.; # Check if the binaries exist.; # Run llc to see if GlobalISel fails.; # Run bugpoint.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/bugpoint_gisel_reducer.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/bugpoint_gisel_reducer.py
Testability,test,tests,"#!/usr/bin/env python; """"""Reduces GlobalISel failures. This script is a utility to reduce tests that GlobalISel; fails to compile. It runs llc to get the error message using a regex and creates; a custom command to check that specific error. Then, it runs bugpoint; with the custom command. """"""; # Check if this is called by bugpoint.; # Parse arguments.; # Check if the binaries exist.; # Run llc to see if GlobalISel fails.; # Run bugpoint.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/bugpoint_gisel_reducer.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/bugpoint_gisel_reducer.py
Availability,failure,failure,"#!/usr/bin/env python3; #; # ======- check-ninja-deps - build debugging script ----*- python -*--========#; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ==------------------------------------------------------------------------==#; """"""Script to find missing formal dependencies in a build.ninja file. Suppose you have a header file that's autogenerated by (for example) Tablegen.; If a C++ compilation step needs to include that header, then it must be; executed after the Tablegen build step that generates the header. So the; dependency graph in build.ninja should have the Tablegen build step as an; ancestor of the C++ one. If it does not, then there's a latent build-failure; bug, because depending on the order that ninja chooses to schedule its build; steps, the C++ build step could run first, and fail because the header it needs; does not exist yet. But because that kind of bug can easily be latent or intermittent, you might; not notice, if your local test build happens to succeed. What you'd like is a; way to detect problems of this kind reliably, even if they _didn't_ cause a; failure on your first test. This script tries to do that. It's specific to the 'ninja' build tool, because; ninja has useful auxiliary output modes that produce the necessary data:. - 'ninja -t graph' emits the full DAG of formal dependencies derived from; build.ninja (in Graphviz format). - 'ninja -t deps' dumps the database of dependencies discovered at build time; by finding out which headers each source file actually included. By cross-checking these two sources of data against each other, you can find; true dependencies shown by 'deps' that are not reflected as formal dependencies; in 'graph', i.e. a generated header that is required by a given source file but; not forced to be built first. To run it:. - set up a build directory",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/check_ninja_deps.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/check_ninja_deps.py
Energy Efficiency,schedul,schedule,"#!/usr/bin/env python3; #; # ======- check-ninja-deps - build debugging script ----*- python -*--========#; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ==------------------------------------------------------------------------==#; """"""Script to find missing formal dependencies in a build.ninja file. Suppose you have a header file that's autogenerated by (for example) Tablegen.; If a C++ compilation step needs to include that header, then it must be; executed after the Tablegen build step that generates the header. So the; dependency graph in build.ninja should have the Tablegen build step as an; ancestor of the C++ one. If it does not, then there's a latent build-failure; bug, because depending on the order that ninja chooses to schedule its build; steps, the C++ build step could run first, and fail because the header it needs; does not exist yet. But because that kind of bug can easily be latent or intermittent, you might; not notice, if your local test build happens to succeed. What you'd like is a; way to detect problems of this kind reliably, even if they _didn't_ cause a; failure on your first test. This script tries to do that. It's specific to the 'ninja' build tool, because; ninja has useful auxiliary output modes that produce the necessary data:. - 'ninja -t graph' emits the full DAG of formal dependencies derived from; build.ninja (in Graphviz format). - 'ninja -t deps' dumps the database of dependencies discovered at build time; by finding out which headers each source file actually included. By cross-checking these two sources of data against each other, you can find; true dependencies shown by 'deps' that are not reflected as formal dependencies; in 'graph', i.e. a generated header that is required by a given source file but; not forced to be built first. To run it:. - set up a build directory",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/check_ninja_deps.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/check_ninja_deps.py
Integrability,depend,dependencies,"#!/usr/bin/env python3; #; # ======- check-ninja-deps - build debugging script ----*- python -*--========#; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ==------------------------------------------------------------------------==#; """"""Script to find missing formal dependencies in a build.ninja file. Suppose you have a header file that's autogenerated by (for example) Tablegen.; If a C++ compilation step needs to include that header, then it must be; executed after the Tablegen build step that generates the header. So the; dependency graph in build.ninja should have the Tablegen build step as an; ancestor of the C++ one. If it does not, then there's a latent build-failure; bug, because depending on the order that ninja chooses to schedule its build; steps, the C++ build step could run first, and fail because the header it needs; does not exist yet. But because that kind of bug can easily be latent or intermittent, you might; not notice, if your local test build happens to succeed. What you'd like is a; way to detect problems of this kind reliably, even if they _didn't_ cause a; failure on your first test. This script tries to do that. It's specific to the 'ninja' build tool, because; ninja has useful auxiliary output modes that produce the necessary data:. - 'ninja -t graph' emits the full DAG of formal dependencies derived from; build.ninja (in Graphviz format). - 'ninja -t deps' dumps the database of dependencies discovered at build time; by finding out which headers each source file actually included. By cross-checking these two sources of data against each other, you can find; true dependencies shown by 'deps' that are not reflected as formal dependencies; in 'graph', i.e. a generated header that is required by a given source file but; not forced to be built first. To run it:. - set up a build directory",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/check_ninja_deps.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/check_ninja_deps.py
Performance,perform,perform,"; not notice, if your local test build happens to succeed. What you'd like is a; way to detect problems of this kind reliably, even if they _didn't_ cause a; failure on your first test. This script tries to do that. It's specific to the 'ninja' build tool, because; ninja has useful auxiliary output modes that produce the necessary data:. - 'ninja -t graph' emits the full DAG of formal dependencies derived from; build.ninja (in Graphviz format). - 'ninja -t deps' dumps the database of dependencies discovered at build time; by finding out which headers each source file actually included. By cross-checking these two sources of data against each other, you can find; true dependencies shown by 'deps' that are not reflected as formal dependencies; in 'graph', i.e. a generated header that is required by a given source file but; not forced to be built first. To run it:. - set up a build directory using ninja as the build tool (cmake -G Ninja). - in that build directory, run ninja to perform an actual build (populating; the dependency database). - then, in the same build directory, run this script. No arguments are needed; (but -C and -f are accepted, and propagated to ninja for convenience). Requirements outside core Python: the 'pygraphviz' module, available via pip or; as the 'python3-pygraphviz' package in Debian and Ubuntu. """"""; """"""Topologically sort a graph. The input g is a pygraphviz graph object representing a DAG. The function; yields the vertices of g in an arbitrary order consistent with the edges,; so that for any edge v->w, v is output before w.""""""; # Count the number of immediate predecessors *not yet output* for each; # vertex. Initially this is simply their in-degrees.; # Set of vertices which can be output next, which is true if they have no; # immediate predecessor that has not already been output.; # Keep outputting vertices while we have any to output.; # Having output v, find each immediate successor w, and decrement its; # 'ideg' value by 1, to indicate",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/check_ninja_deps.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/check_ninja_deps.py
Safety,detect,detect,"E.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ==------------------------------------------------------------------------==#; """"""Script to find missing formal dependencies in a build.ninja file. Suppose you have a header file that's autogenerated by (for example) Tablegen.; If a C++ compilation step needs to include that header, then it must be; executed after the Tablegen build step that generates the header. So the; dependency graph in build.ninja should have the Tablegen build step as an; ancestor of the C++ one. If it does not, then there's a latent build-failure; bug, because depending on the order that ninja chooses to schedule its build; steps, the C++ build step could run first, and fail because the header it needs; does not exist yet. But because that kind of bug can easily be latent or intermittent, you might; not notice, if your local test build happens to succeed. What you'd like is a; way to detect problems of this kind reliably, even if they _didn't_ cause a; failure on your first test. This script tries to do that. It's specific to the 'ninja' build tool, because; ninja has useful auxiliary output modes that produce the necessary data:. - 'ninja -t graph' emits the full DAG of formal dependencies derived from; build.ninja (in Graphviz format). - 'ninja -t deps' dumps the database of dependencies discovered at build time; by finding out which headers each source file actually included. By cross-checking these two sources of data against each other, you can find; true dependencies shown by 'deps' that are not reflected as formal dependencies; in 'graph', i.e. a generated header that is required by a given source file but; not forced to be built first. To run it:. - set up a build directory using ninja as the build tool (cmake -G Ninja). - in that build directory, run ninja to perform an actual build (populating; the dependency database). - then, in the same build directory, run this script. No arguments ar",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/check_ninja_deps.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/check_ninja_deps.py
Testability,test,test,"on -*--========#; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ==------------------------------------------------------------------------==#; """"""Script to find missing formal dependencies in a build.ninja file. Suppose you have a header file that's autogenerated by (for example) Tablegen.; If a C++ compilation step needs to include that header, then it must be; executed after the Tablegen build step that generates the header. So the; dependency graph in build.ninja should have the Tablegen build step as an; ancestor of the C++ one. If it does not, then there's a latent build-failure; bug, because depending on the order that ninja chooses to schedule its build; steps, the C++ build step could run first, and fail because the header it needs; does not exist yet. But because that kind of bug can easily be latent or intermittent, you might; not notice, if your local test build happens to succeed. What you'd like is a; way to detect problems of this kind reliably, even if they _didn't_ cause a; failure on your first test. This script tries to do that. It's specific to the 'ninja' build tool, because; ninja has useful auxiliary output modes that produce the necessary data:. - 'ninja -t graph' emits the full DAG of formal dependencies derived from; build.ninja (in Graphviz format). - 'ninja -t deps' dumps the database of dependencies discovered at build time; by finding out which headers each source file actually included. By cross-checking these two sources of data against each other, you can find; true dependencies shown by 'deps' that are not reflected as formal dependencies; in 'graph', i.e. a generated header that is required by a given source file but; not forced to be built first. To run it:. - set up a build directory using ninja as the build tool (cmake -G Ninja). - in that build directory, run ninja to p",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/check_ninja_deps.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/check_ninja_deps.py
Usability,simpl,simply,"ncies shown by 'deps' that are not reflected as formal dependencies; in 'graph', i.e. a generated header that is required by a given source file but; not forced to be built first. To run it:. - set up a build directory using ninja as the build tool (cmake -G Ninja). - in that build directory, run ninja to perform an actual build (populating; the dependency database). - then, in the same build directory, run this script. No arguments are needed; (but -C and -f are accepted, and propagated to ninja for convenience). Requirements outside core Python: the 'pygraphviz' module, available via pip or; as the 'python3-pygraphviz' package in Debian and Ubuntu. """"""; """"""Topologically sort a graph. The input g is a pygraphviz graph object representing a DAG. The function; yields the vertices of g in an arbitrary order consistent with the edges,; so that for any edge v->w, v is output before w.""""""; # Count the number of immediate predecessors *not yet output* for each; # vertex. Initially this is simply their in-degrees.; # Set of vertices which can be output next, which is true if they have no; # immediate predecessor that has not already been output.; # Keep outputting vertices while we have any to output.; # Having output v, find each immediate successor w, and decrement its; # 'ideg' value by 1, to indicate that one more of its predecessors has; # now been output.; # If that counter reaches zero, w is ready to output.; """"""Form the set of ancestors for each vertex of a graph. The input g is a pygraphviz graph object representing a DAG. The function; yields a sequence of pairs (vertex, set of proper ancestors). The vertex names are all mapped through 'translate' before output. This; allows us to produce output referring to the label rather than the; identifier of every vertex.; """"""; # Store the set of (translated) ancestors for each vertex so far. a[v]; # includes (the translation of) v itself.; # Make up a[v], based on a[predecessors of v].; # include v itself; # Remove v itsel",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/check_ninja_deps.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/check_ninja_deps.py
Testability,log,log,"#!/usr/bin/env python; # Given a -print-before-all and/or -print-after-all -print-module-scope log from; # an opt invocation, chunk it into a series of individual IR files, one for each; # pass invocation. If the log ends with an obvious stack trace, try to split off; # a separate ""crashinfo.txt"" file leaving only the valid input IR in the last; # chunk. Files are written to current working directory.; # This function gets the pass name from the following line:; # *** IR Dump Before/After PASS_NAME... ***",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/chunk-print-before-all.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/chunk-print-before-all.py
Deployability,update,updated,"#!/usr/bin/env python3; """"""; This script:; - Builds clang with user-defined flags; - Uses that clang to build an instrumented clang, which can be used to collect; PGO samples; - Builds a user-defined set of sources (default: clang) to act as a; ""benchmark"" to generate a PGO profile; - Builds clang once more with the PGO profile generated above. This is a total of four clean builds of clang (by default). This may take a; while. :). This scripts duplicates https://llvm.org/docs/AdvancedBuilds.html#multi-stage-pgo; Eventually, it will be updated to instead call the cmake cache mentioned there.; """"""; ### User configuration; # If you want to use a different 'benchmark' than building clang, make this; # function do what you want. out_dir is the build directory for clang, so all; # of the clang binaries will live under ""${out_dir}/bin/"". Using clang in; # ${out_dir} will magically have the profiles go to the right place.; #; # You may assume that out_dir is a freshly-built directory that you can reach; # in to build more things, if you'd like.; """"""The 'benchmark' we run to generate profile data.""""""; # `check-llvm` and `check-clang` are cheap ways to increase coverage. The; # former lets us touch on the non-x86 backends a bit if configured, and the; # latter gives us more C to chew on (and will send us through diagnostic; # paths a fair amount, though the `if (stuff_is_broken) { diag() ... }`; # branches should still heavily be weighted in the not-taken direction,; # since we built all of LLVM/etc).; # Building tblgen gets us coverage; don't skip it. (out_dir may also not; # have them anyway, but that's less of an issue); # Just build all the things. The more data we have, the better.; ### Script; # Map of str -> (list|str).; # No, I didn't intend to append ['-', 'O', '2'] to my flags, thanks :); # We preload all of the list-y values (cflags, ...). If we've; # nothing to add, don't.; # Note that we don't allow capturing stdout/stderr. This works quite nicely; # with dry_run.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/collect_and_build_with_pgo.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/collect_and_build_with_pgo.py
Modifiability,config,configuration,"#!/usr/bin/env python3; """"""; This script:; - Builds clang with user-defined flags; - Uses that clang to build an instrumented clang, which can be used to collect; PGO samples; - Builds a user-defined set of sources (default: clang) to act as a; ""benchmark"" to generate a PGO profile; - Builds clang once more with the PGO profile generated above. This is a total of four clean builds of clang (by default). This may take a; while. :). This scripts duplicates https://llvm.org/docs/AdvancedBuilds.html#multi-stage-pgo; Eventually, it will be updated to instead call the cmake cache mentioned there.; """"""; ### User configuration; # If you want to use a different 'benchmark' than building clang, make this; # function do what you want. out_dir is the build directory for clang, so all; # of the clang binaries will live under ""${out_dir}/bin/"". Using clang in; # ${out_dir} will magically have the profiles go to the right place.; #; # You may assume that out_dir is a freshly-built directory that you can reach; # in to build more things, if you'd like.; """"""The 'benchmark' we run to generate profile data.""""""; # `check-llvm` and `check-clang` are cheap ways to increase coverage. The; # former lets us touch on the non-x86 backends a bit if configured, and the; # latter gives us more C to chew on (and will send us through diagnostic; # paths a fair amount, though the `if (stuff_is_broken) { diag() ... }`; # branches should still heavily be weighted in the not-taken direction,; # since we built all of LLVM/etc).; # Building tblgen gets us coverage; don't skip it. (out_dir may also not; # have them anyway, but that's less of an issue); # Just build all the things. The more data we have, the better.; ### Script; # Map of str -> (list|str).; # No, I didn't intend to append ['-', 'O', '2'] to my flags, thanks :); # We preload all of the list-y values (cflags, ...). If we've; # nothing to add, don't.; # Note that we don't allow capturing stdout/stderr. This works quite nicely; # with dry_run.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/collect_and_build_with_pgo.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/collect_and_build_with_pgo.py
Performance,cache,cache,"#!/usr/bin/env python3; """"""; This script:; - Builds clang with user-defined flags; - Uses that clang to build an instrumented clang, which can be used to collect; PGO samples; - Builds a user-defined set of sources (default: clang) to act as a; ""benchmark"" to generate a PGO profile; - Builds clang once more with the PGO profile generated above. This is a total of four clean builds of clang (by default). This may take a; while. :). This scripts duplicates https://llvm.org/docs/AdvancedBuilds.html#multi-stage-pgo; Eventually, it will be updated to instead call the cmake cache mentioned there.; """"""; ### User configuration; # If you want to use a different 'benchmark' than building clang, make this; # function do what you want. out_dir is the build directory for clang, so all; # of the clang binaries will live under ""${out_dir}/bin/"". Using clang in; # ${out_dir} will magically have the profiles go to the right place.; #; # You may assume that out_dir is a freshly-built directory that you can reach; # in to build more things, if you'd like.; """"""The 'benchmark' we run to generate profile data.""""""; # `check-llvm` and `check-clang` are cheap ways to increase coverage. The; # former lets us touch on the non-x86 backends a bit if configured, and the; # latter gives us more C to chew on (and will send us through diagnostic; # paths a fair amount, though the `if (stuff_is_broken) { diag() ... }`; # branches should still heavily be weighted in the not-taken direction,; # since we built all of LLVM/etc).; # Building tblgen gets us coverage; don't skip it. (out_dir may also not; # have them anyway, but that's less of an issue); # Just build all the things. The more data we have, the better.; ### Script; # Map of str -> (list|str).; # No, I didn't intend to append ['-', 'O', '2'] to my flags, thanks :); # We preload all of the list-y values (cflags, ...). If we've; # nothing to add, don't.; # Note that we don't allow capturing stdout/stderr. This works quite nicely; # with dry_run.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/collect_and_build_with_pgo.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/collect_and_build_with_pgo.py
Security,hash,hash,"ou may assume that out_dir is a freshly-built directory that you can reach; # in to build more things, if you'd like.; """"""The 'benchmark' we run to generate profile data.""""""; # `check-llvm` and `check-clang` are cheap ways to increase coverage. The; # former lets us touch on the non-x86 backends a bit if configured, and the; # latter gives us more C to chew on (and will send us through diagnostic; # paths a fair amount, though the `if (stuff_is_broken) { diag() ... }`; # branches should still heavily be weighted in the not-taken direction,; # since we built all of LLVM/etc).; # Building tblgen gets us coverage; don't skip it. (out_dir may also not; # have them anyway, but that's less of an issue); # Just build all the things. The more data we have, the better.; ### Script; # Map of str -> (list|str).; # No, I didn't intend to append ['-', 'O', '2'] to my flags, thanks :); # We preload all of the list-y values (cflags, ...). If we've; # nothing to add, don't.; # Note that we don't allow capturing stdout/stderr. This works quite nicely; # with dry_run.; # Don't use subprocess.run because it's >= py3.5 only, and it's not too; # much extra effort to get what it gives us anyway.; # We often get no value out of building new tblgens; the previous build; # should have them. It's still correct to build them, just slower.; # Check that this exists, since the user's allowed to specify their own; # stage1 directory (which is generally where we'll source everything; # from). Dry runs should hope for the best from our user, as well.; # libcxx's configure step messes with our link order: we'll link; # libclang_rt.profile after libgcc, and the former requires atexit from the; # latter. So, configure checks fail.; #; # Since we don't need libcxx or compiler-rt anyway, just disable them.; # We'll get complaints about hash mismatches in `main` in tools/etc. Ignore; # it.; """"""Arbitrary set of heuristics to determine if `directory` is an llvm dir. Errs on the side of false-positives.""""""",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/collect_and_build_with_pgo.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/collect_and_build_with_pgo.py
Testability,benchmark,benchmark,"#!/usr/bin/env python3; """"""; This script:; - Builds clang with user-defined flags; - Uses that clang to build an instrumented clang, which can be used to collect; PGO samples; - Builds a user-defined set of sources (default: clang) to act as a; ""benchmark"" to generate a PGO profile; - Builds clang once more with the PGO profile generated above. This is a total of four clean builds of clang (by default). This may take a; while. :). This scripts duplicates https://llvm.org/docs/AdvancedBuilds.html#multi-stage-pgo; Eventually, it will be updated to instead call the cmake cache mentioned there.; """"""; ### User configuration; # If you want to use a different 'benchmark' than building clang, make this; # function do what you want. out_dir is the build directory for clang, so all; # of the clang binaries will live under ""${out_dir}/bin/"". Using clang in; # ${out_dir} will magically have the profiles go to the right place.; #; # You may assume that out_dir is a freshly-built directory that you can reach; # in to build more things, if you'd like.; """"""The 'benchmark' we run to generate profile data.""""""; # `check-llvm` and `check-clang` are cheap ways to increase coverage. The; # former lets us touch on the non-x86 backends a bit if configured, and the; # latter gives us more C to chew on (and will send us through diagnostic; # paths a fair amount, though the `if (stuff_is_broken) { diag() ... }`; # branches should still heavily be weighted in the not-taken direction,; # since we built all of LLVM/etc).; # Building tblgen gets us coverage; don't skip it. (out_dir may also not; # have them anyway, but that's less of an issue); # Just build all the things. The more data we have, the better.; ### Script; # Map of str -> (list|str).; # No, I didn't intend to append ['-', 'O', '2'] to my flags, thanks :); # We preload all of the list-y values (cflags, ...). If we've; # nothing to add, don't.; # Note that we don't allow capturing stdout/stderr. This works quite nicely; # with dry_run.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/collect_and_build_with_pgo.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/collect_and_build_with_pgo.py
Testability,log,log,"#!/usr/bin/env python; """"""; Helper script to convert the log generated by '-debug-only=constraint-system'; to a Python script that uses Z3 to verify the decisions using Z3's Python API. Example usage:. > cat path/to/file.log; ---; x6 + -1 * x7 <= -1; x6 + -1 * x7 <= -2; sat. > ./convert-constraint-log-to-z3.py path/to/file.log > check.py && python ./check.py. > cat check.py; from z3 import *; x3 = Int(""x3""); x1 = Int(""x1""); x2 = Int(""x2""); s = Solver(); s.add(x1 + -1 * x2 <= 0); s.add(x2 + -1 * x3 <= 0); s.add(-1 * x1 + x3 <= -1); assert(s.check() == unsat); print('all checks passed'); """"""",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/convert-constraint-log-to-z3.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/convert-constraint-log-to-z3.py
Testability,test,test,"#!/usr/bin/env python; """"""A ladder graph creation program. This is a python program that creates c source code that will generate; CFGs that are ladder graphs. Ladder graphs are generally the worst case; for a lot of dominance related algorithms (Dominance frontiers, etc),; and often generate N^2 or worse behavior. One good use of this program is to test whether your linear time algorithm is; really behaving linearly.; """"""",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/create_ladder_graph.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/create_ladder_graph.py
Testability,test,testing,"# Given a path to llvm-objdump and a directory tree, spider the directory tree; # dumping every object file encountered with correct options needed to demangle; # symbols in the object file, and collect statistics about failed / crashed; # demanglings. Useful for stress testing the demangler against a large corpus; # of inputs.; # It's only possible that a single item is incomplete, and it has to be the; # last item.; # Now ordered_dirs contains a list of all directories which *did* complete.; # If this directory had no object files, just print a default; # status line and continue with the next dir; # Drain the tasks, `pool_size` at a time, until we have less than; # `pool_size` tasks remaining.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/demangle_tree.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/demangle_tree.py
Availability,error,error,"#!/usr/bin/env python; # changelog:; # 10/13/2005b: replaced the # in tmp(.#*)* with alphanumeric and _, this will then remove; # nodes such as %tmp.1.i and %tmp._i.3; # 10/13/2005: exntended to remove variables of the form %tmp(.#)* rather than just; #%tmp.#, i.e. it now will remove %tmp.12.3.15 etc, additionally fixed a spelling error in; # the comments; # 10/12/2005: now it only removes nodes and edges for which the label is %tmp.# rather; # than removing all lines for which the lable CONTAINS %tmp.#; # get a file object; # we'll get this one line at a time...while we could just put the whole thing in a string; # it would kill old computers; # skip next line, write neither this line nor the next; # this isn't a tmp Node, we can write it; # prepare for the next iteration",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/DSAclean.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/DSAclean.py
Modifiability,variab,variables,"#!/usr/bin/env python; # changelog:; # 10/13/2005b: replaced the # in tmp(.#*)* with alphanumeric and _, this will then remove; # nodes such as %tmp.1.i and %tmp._i.3; # 10/13/2005: exntended to remove variables of the form %tmp(.#)* rather than just; #%tmp.#, i.e. it now will remove %tmp.12.3.15 etc, additionally fixed a spelling error in; # the comments; # 10/12/2005: now it only removes nodes and edges for which the label is %tmp.# rather; # than removing all lines for which the lable CONTAINS %tmp.#; # get a file object; # we'll get this one line at a time...while we could just put the whole thing in a string; # it would kill old computers; # skip next line, write neither this line nor the next; # this isn't a tmp Node, we can write it; # prepare for the next iteration",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/DSAclean.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/DSAclean.py
Availability,robust,robust," if a line contains '->' and is not an edge line; # problems will occur. If node labels do not begin with; # Node this also will not work. Since this is designed to work; # on DSA dot output and not general dot files this is ok.; # If you want to use this on other files rename the node labels; # to Node[.*] with a script or something. This also relies on; # the length of a node name being 13 characters (as it is in all; # DSA dot output files); # Note that the name of the node can be any substring of the actual; # name in the dot file. Thus if you say specify COLLAPSED; # as a parameter this script will pull out all COLLAPSED; # nodes in the file; # Specifying escape characters in the name like \n also will not work,; # as Python; # will make it \\n, I'm not really sure how to fix this; # currently the script prints the names it is searching for; # to STDOUT, so you can check to see if they are what you intend; # open the input file; # construct a set of node names; # construct a list of compiled regular expressions from the; # node_name_set; # used to see what kind of line we are on; # used to check to see if the current line is an edge line; # read the file one line at a time; # filter out the unnecessary checks on all the edge lines; # check to see if this is a node we are looking for; # if this name is for the current node, add the dot variable name; # for the node (it will be Node(hex number)) to our set of nodes; # test code; # print '\n'; # print node_set; # open the output file; # start the second pass over the file; # there are three types of lines we are looking for; # 1) node lines, 2) edge lines 3) support lines (like page size, etc); # is this an edge line?; # note that this is no completely robust, if a none edge line; # for some reason contains -> it will be missidentified; # hand edit the file if this happens; # check to make sure that both nodes are in the node list; # if they are print this to output; # this is a node line; # this is a support line",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/DSAextract.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/DSAextract.py
Modifiability,variab,variable," if a line contains '->' and is not an edge line; # problems will occur. If node labels do not begin with; # Node this also will not work. Since this is designed to work; # on DSA dot output and not general dot files this is ok.; # If you want to use this on other files rename the node labels; # to Node[.*] with a script or something. This also relies on; # the length of a node name being 13 characters (as it is in all; # DSA dot output files); # Note that the name of the node can be any substring of the actual; # name in the dot file. Thus if you say specify COLLAPSED; # as a parameter this script will pull out all COLLAPSED; # nodes in the file; # Specifying escape characters in the name like \n also will not work,; # as Python; # will make it \\n, I'm not really sure how to fix this; # currently the script prints the names it is searching for; # to STDOUT, so you can check to see if they are what you intend; # open the input file; # construct a set of node names; # construct a list of compiled regular expressions from the; # node_name_set; # used to see what kind of line we are on; # used to check to see if the current line is an edge line; # read the file one line at a time; # filter out the unnecessary checks on all the edge lines; # check to see if this is a node we are looking for; # if this name is for the current node, add the dot variable name; # for the node (it will be Node(hex number)) to our set of nodes; # test code; # print '\n'; # print node_set; # open the output file; # start the second pass over the file; # there are three types of lines we are looking for; # 1) node lines, 2) edge lines 3) support lines (like page size, etc); # is this an edge line?; # note that this is no completely robust, if a none edge line; # for some reason contains -> it will be missidentified; # hand edit the file if this happens; # check to make sure that both nodes are in the node list; # if they are print this to output; # this is a node line; # this is a support line",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/DSAextract.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/DSAextract.py
Testability,test,test," if a line contains '->' and is not an edge line; # problems will occur. If node labels do not begin with; # Node this also will not work. Since this is designed to work; # on DSA dot output and not general dot files this is ok.; # If you want to use this on other files rename the node labels; # to Node[.*] with a script or something. This also relies on; # the length of a node name being 13 characters (as it is in all; # DSA dot output files); # Note that the name of the node can be any substring of the actual; # name in the dot file. Thus if you say specify COLLAPSED; # as a parameter this script will pull out all COLLAPSED; # nodes in the file; # Specifying escape characters in the name like \n also will not work,; # as Python; # will make it \\n, I'm not really sure how to fix this; # currently the script prints the names it is searching for; # to STDOUT, so you can check to see if they are what you intend; # open the input file; # construct a set of node names; # construct a list of compiled regular expressions from the; # node_name_set; # used to see what kind of line we are on; # used to check to see if the current line is an edge line; # read the file one line at a time; # filter out the unnecessary checks on all the edge lines; # check to see if this is a node we are looking for; # if this name is for the current node, add the dot variable name; # for the node (it will be Node(hex number)) to our set of nodes; # test code; # print '\n'; # print node_set; # open the output file; # start the second pass over the file; # there are three types of lines we are looking for; # 1) node lines, 2) edge lines 3) support lines (like page size, etc); # is this an edge line?; # note that this is no completely robust, if a none edge line; # for some reason contains -> it will be missidentified; # hand edit the file if this happens; # check to make sure that both nodes are in the node list; # if they are print this to output; # this is a node line; # this is a support line",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/DSAextract.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/DSAextract.py
Integrability,wrap,wrapper,"#!/usr/bin/env python; """"""; Helper script to print out the raw content of an ELF section.; Example usages:; ```; # print out as bits by default; extract-section.py .text --input-file=foo.o; ```; ```; # read from stdin and print out in hex; cat foo.o | extract-section.py -h .text; ```; This is merely a wrapper around `llvm-readobj` that focuses on the binary; content as well as providing more formatting options.; """"""; # Unfortunately reading binary from stdin is not so trivial in Python...; # Windows will always read as string so we need some; # special handling; # From stdin; # The default '-h' (--help) will conflict with our '-h' (hex) format; # Output format; # exclude any non-hex dump string; # divided into bytes first",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/extract-section.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/extract-section.py
Availability,error,error,"e; # Check for a template name; # Some other kind of name that we can't handle; # Parse the mangling into a list of (name, is_template); # If any component is a template then return it; # Not a template; # Close std streams as we don't want any output and we don't; # want the process to wait for something on stdin.; # How we determine which symbols to keep and which to discard depends on; # the mangling scheme; # Get the list of libraries to extract symbols from; # When invoked by cmake the arguments are the cmake target names of the; # libraries, so we need to add .lib/.a to the end and maybe lib to the; # start to get the filename. Also allow objects.; # Check if calling convention decoration is used by inspecting the first; # library in the list; # Extract symbols from libraries in parallel. This is a huge time saver when; # doing a debug build, as there are hundreds of thousands of symbols in each; # library.; # FIXME: On AIX, the default pool size can be too big for a logical; # partition's allocated memory, and can lead to an out of memory; # IO error. We are setting the pool size to 8 to avoid such; # errors at the moment, and will look for a graceful solution later.; # Only one argument can be passed to the mapping function, and we can't; # use a lambda or local function definition as that doesn't work on; # windows, so create a list of tuples which duplicates the arguments; # that are the same in all calls.; # Do an async map then wait for the result to make sure that; # KeyboardInterrupt gets caught correctly (see; # http://bugs.python.org/issue8296); # On Ctrl-C terminate everything and exit; # Merge everything into a single dict; # Find which template instantiations are referenced at least once.; # Print symbols which both:; # * Appear in exactly one input, as symbols defined in multiple; # objects/libraries are assumed to have public definitions.; # * Are not a template instantiation that isn't referenced anywhere. This; # is because we need to export a",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/extract_symbols.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/extract_symbols.py
Energy Efficiency,allocate,allocated,"e; # Check for a template name; # Some other kind of name that we can't handle; # Parse the mangling into a list of (name, is_template); # If any component is a template then return it; # Not a template; # Close std streams as we don't want any output and we don't; # want the process to wait for something on stdin.; # How we determine which symbols to keep and which to discard depends on; # the mangling scheme; # Get the list of libraries to extract symbols from; # When invoked by cmake the arguments are the cmake target names of the; # libraries, so we need to add .lib/.a to the end and maybe lib to the; # start to get the filename. Also allow objects.; # Check if calling convention decoration is used by inspecting the first; # library in the list; # Extract symbols from libraries in parallel. This is a huge time saver when; # doing a debug build, as there are hundreds of thousands of symbols in each; # library.; # FIXME: On AIX, the default pool size can be too big for a logical; # partition's allocated memory, and can lead to an out of memory; # IO error. We are setting the pool size to 8 to avoid such; # errors at the moment, and will look for a graceful solution later.; # Only one argument can be passed to the mapping function, and we can't; # use a lambda or local function definition as that doesn't work on; # windows, so create a list of tuples which duplicates the arguments; # that are the same in all calls.; # Do an async map then wait for the result to make sure that; # KeyboardInterrupt gets caught correctly (see; # http://bugs.python.org/issue8296); # On Ctrl-C terminate everything and exit; # Merge everything into a single dict; # Find which template instantiations are referenced at least once.; # Print symbols which both:; # * Appear in exactly one input, as symbols defined in multiple; # objects/libraries are assumed to have public definitions.; # * Are not a template instantiation that isn't referenced anywhere. This; # is because we need to export a",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/extract_symbols.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/extract_symbols.py
Integrability,depend,depending,"nd yield a symbol at a time instead of using; # subprocess.check_output and returning a list as, especially on Windows, waiting; # for the entire output to be ready can take a significant amount of time.; # '-P' means the output is in portable format,; # '-g' means we only get global symbols,; # '-Xany' enforce handling both 32- and 64-bit objects on AIX,; # '--no-demangle' ensure that C++ symbol names are not demangled; note; # that llvm-nm do not demangle by default, but the system nm on AIX does; # that, so the behavior may change in the future,; # '-p' do not waste time sorting the symbols.; # Look for external symbols that are defined in some section; # The POSIX format is:; # name type value size; # The -P flag displays the size field for symbols only when applicable,; # so the last field is optional. There's no space after the value field,; # but \s+ match newline also, so \s+\S* will match the optional size field.; # Look for undefined symbols, which have type U and may or may not; # (depending on which nm is being used) have value and size.; # Define a function which determines if the target is 32-bit Windows (as that's; # where calling convention name decoration happens).; # MSVC mangles names to ?<identifier_mangling>@<type_mangling>. By examining the; # identifier/type mangling we can decide which symbols could possibly be; # required and which we can discard.; # Keep unmangled (i.e. extern ""C"") names; # Remove calling convention decoration from names; # Discard floating point/SIMD constants.; # Deleting destructors start with ?_G or ?_E and can be discarded because; # link.exe gives you a warning telling you they can't be exported if you; # don't; # An anonymous namespace is mangled as ?A(maybe hex number)@. Any symbol; # that mentions an anonymous namespace can be discarded, as the anonymous; # namespace doesn't exist outside of that translation unit.; # Skip X86GenMnemonicTables functions, they are not exposed from llvm/include/.; # Keep mangled llvm:",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/extract_symbols.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/extract_symbols.py
Modifiability,portab,portable,"t all the defined symbols, as there's a limit of 65535; exported symbols and in clang we go way over that, particularly in a debug; build. Therefore a large part of the work is pruning symbols either which can't; be imported, or which we think are things that have definitions in public header; files (i.e. template instantiations) and we would get defined in the thing; importing these symbols anyway.; """"""; # Define a function which extracts a list of pairs of (symbols, is_def) from a; # library using llvm-nm becuase it can work both with regular and bitcode files.; # We use subprocess.Popen and yield a symbol at a time instead of using; # subprocess.check_output and returning a list as, especially on Windows, waiting; # for the entire output to be ready can take a significant amount of time.; # '-P' means the output is in portable format,; # '-g' means we only get global symbols,; # '-Xany' enforce handling both 32- and 64-bit objects on AIX,; # '--no-demangle' ensure that C++ symbol names are not demangled; note; # that llvm-nm do not demangle by default, but the system nm on AIX does; # that, so the behavior may change in the future,; # '-p' do not waste time sorting the symbols.; # Look for external symbols that are defined in some section; # The POSIX format is:; # name type value size; # The -P flag displays the size field for symbols only when applicable,; # so the last field is optional. There's no space after the value field,; # but \s+ match newline also, so \s+\S* will match the optional size field.; # Look for undefined symbols, which have type U and may or may not; # (depending on which nm is being used) have value and size.; # Define a function which determines if the target is 32-bit Windows (as that's; # where calling convention name decoration happens).; # MSVC mangles names to ?<identifier_mangling>@<type_mangling>. By examining the; # identifier/type mangling we can decide which symbols could possibly be; # required and which we can discard.; # Keep",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/extract_symbols.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/extract_symbols.py
Safety,detect,detect," determines if the target is 32-bit Windows (as that's; # where calling convention name decoration happens).; # MSVC mangles names to ?<identifier_mangling>@<type_mangling>. By examining the; # identifier/type mangling we can decide which symbols could possibly be; # required and which we can discard.; # Keep unmangled (i.e. extern ""C"") names; # Remove calling convention decoration from names; # Discard floating point/SIMD constants.; # Deleting destructors start with ?_G or ?_E and can be discarded because; # link.exe gives you a warning telling you they can't be exported if you; # don't; # An anonymous namespace is mangled as ?A(maybe hex number)@. Any symbol; # that mentions an anonymous namespace can be discarded, as the anonymous; # namespace doesn't exist outside of that translation unit.; # Skip X86GenMnemonicTables functions, they are not exposed from llvm/include/.; # Keep mangled llvm:: and clang:: function symbols. How we detect these is a; # bit of a mess and imprecise, but that avoids having to completely demangle; # the symbol name. The outermost namespace is at the end of the identifier; # mangling, and the identifier mangling is followed by the type mangling, so; # we look for (llvm|clang)@@ followed by something that looks like a; # function type mangling. To spot a function type we use (this is derived; # from clang/lib/AST/MicrosoftMangle.cpp):; # <function-type> ::= <function-class> <this-cvr-qualifiers>; # <calling-convention> <return-type>; # <argument-list> <throw-spec>; # <function-class> ::= [A-Z]; # <this-cvr-qualifiers> ::= [A-Z0-9_]*; # <calling-convention> ::= [A-JQ]; # <return-type> ::= .+; # <argument-list> ::= X (void); # ::= .+@ (list of types); # ::= .*Z (list of types, varargs); # <throw-spec> ::= exceptions are not allowed; # Itanium manglings are of the form _Z<identifier_mangling><type_mangling>. We; # demangle the identifier mangling to identify symbols that can be safely; # discarded.; # Start by removing any calling convention",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/extract_symbols.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/extract_symbols.py
Security,expose,exposed,"# Look for undefined symbols, which have type U and may or may not; # (depending on which nm is being used) have value and size.; # Define a function which determines if the target is 32-bit Windows (as that's; # where calling convention name decoration happens).; # MSVC mangles names to ?<identifier_mangling>@<type_mangling>. By examining the; # identifier/type mangling we can decide which symbols could possibly be; # required and which we can discard.; # Keep unmangled (i.e. extern ""C"") names; # Remove calling convention decoration from names; # Discard floating point/SIMD constants.; # Deleting destructors start with ?_G or ?_E and can be discarded because; # link.exe gives you a warning telling you they can't be exported if you; # don't; # An anonymous namespace is mangled as ?A(maybe hex number)@. Any symbol; # that mentions an anonymous namespace can be discarded, as the anonymous; # namespace doesn't exist outside of that translation unit.; # Skip X86GenMnemonicTables functions, they are not exposed from llvm/include/.; # Keep mangled llvm:: and clang:: function symbols. How we detect these is a; # bit of a mess and imprecise, but that avoids having to completely demangle; # the symbol name. The outermost namespace is at the end of the identifier; # mangling, and the identifier mangling is followed by the type mangling, so; # we look for (llvm|clang)@@ followed by something that looks like a; # function type mangling. To spot a function type we use (this is derived; # from clang/lib/AST/MicrosoftMangle.cpp):; # <function-type> ::= <function-class> <this-cvr-qualifiers>; # <calling-convention> <return-type>; # <argument-list> <throw-spec>; # <function-class> ::= [A-Z]; # <this-cvr-qualifiers> ::= [A-Z0-9_]*; # <calling-convention> ::= [A-JQ]; # <return-type> ::= .+; # <argument-list> ::= X (void); # ::= .+@ (list of types); # ::= .*Z (list of types, varargs); # <throw-spec> ::= exceptions are not allowed; # Itanium manglings are of the form _Z<identifier_mangli",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/extract_symbols.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/extract_symbols.py
Testability,log,logical,"e; # Check for a template name; # Some other kind of name that we can't handle; # Parse the mangling into a list of (name, is_template); # If any component is a template then return it; # Not a template; # Close std streams as we don't want any output and we don't; # want the process to wait for something on stdin.; # How we determine which symbols to keep and which to discard depends on; # the mangling scheme; # Get the list of libraries to extract symbols from; # When invoked by cmake the arguments are the cmake target names of the; # libraries, so we need to add .lib/.a to the end and maybe lib to the; # start to get the filename. Also allow objects.; # Check if calling convention decoration is used by inspecting the first; # library in the list; # Extract symbols from libraries in parallel. This is a huge time saver when; # doing a debug build, as there are hundreds of thousands of symbols in each; # library.; # FIXME: On AIX, the default pool size can be too big for a logical; # partition's allocated memory, and can lead to an out of memory; # IO error. We are setting the pool size to 8 to avoid such; # errors at the moment, and will look for a graceful solution later.; # Only one argument can be passed to the mapping function, and we can't; # use a lambda or local function definition as that doesn't work on; # windows, so create a list of tuples which duplicates the arguments; # that are the same in all calls.; # Do an async map then wait for the result to make sure that; # KeyboardInterrupt gets caught correctly (see; # http://bugs.python.org/issue8296); # On Ctrl-C terminate everything and exit; # Merge everything into a single dict; # Find which template instantiations are referenced at least once.; # Print symbols which both:; # * Appear in exactly one input, as symbols defined in multiple; # objects/libraries are assumed to have public definitions.; # * Are not a template instantiation that isn't referenced anywhere. This; # is because we need to export a",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/extract_symbols.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/extract_symbols.py
Usability,simpl,simple,"nto a list of pairs of; # (name, is_template), returning (list, rest of string).; # A nested name starts with N; # Skip past the N, and possibly a substitution; # Skip past CV-qualifiers and ref qualifiers; # Repeatedly parse names from the string until we reach the end of the; # nested name; # An E ends the nested name; # Parse a name; # If we failed then we don't know how to demangle this; # If this name is a template record that, then skip the template; # arguments; # Add the name to the list; # If we get here then something went wrong; # Parse a microsoft mangled symbol and return a list of pairs of; # (name, is_template). This is very rudimentary and does just enough; # in order to determine if the first or second component is a template.; # If the name doesn't start with ? this isn't a mangled name; # If we see an empty component we've reached the end; # Check for a simple name; # Check for a special function name; # Check for a template name; # Some other kind of name that we can't handle; # Parse the mangling into a list of (name, is_template); # If any component is a template then return it; # Not a template; # Close std streams as we don't want any output and we don't; # want the process to wait for something on stdin.; # How we determine which symbols to keep and which to discard depends on; # the mangling scheme; # Get the list of libraries to extract symbols from; # When invoked by cmake the arguments are the cmake target names of the; # libraries, so we need to add .lib/.a to the end and maybe lib to the; # start to get the filename. Also allow objects.; # Check if calling convention decoration is used by inspecting the first; # library in the list; # Extract symbols from libraries in parallel. This is a huge time saver when; # doing a debug build, as there are hundreds of thousands of symbols in each; # library.; # FIXME: On AIX, the default pool size can be too big for a logical; # partition's allocated memory, and can lead to an out of memory; # IO e",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/extract_symbols.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/extract_symbols.py
Deployability,install,installed,"#!/usr/bin/env python; # This script extracts the VPlan digraphs from the vectoriser debug messages; # and saves them in individual dot files (one for each plan). Optionally, and; # providing 'dot' is installed, it can also render the dot into a PNG file.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/extract_vplan.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/extract_vplan.py
Integrability,message,messages,"#!/usr/bin/env python; # This script extracts the VPlan digraphs from the vectoriser debug messages; # and saves them in individual dot files (one for each plan). Optionally, and; # providing 'dot' is installed, it can also render the dot into a PNG file.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/extract_vplan.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/extract_vplan.py
Security,expose,expose,"# debugger.HandleCommand(; # ""type synthetic add -w llvm ""; # f""-l {__name__}.PointerIntPairSynthProvider ""; # '-x ""^llvm::PointerIntPair<.+>$""'; # ); # debugger.HandleCommand(; # ""type synthetic add -w llvm ""; # f""-l {__name__}.PointerUnionSynthProvider ""; # '-x ""^llvm::PointerUnion<.+>$""'; # ); # Pretty printer for llvm::SmallVector/llvm::SmallVectorImpl; # initialize this provider; # Do bounds checking.; # If this is a reference type we have to dereference it to get to the; # template parameter.; """"""Provider for llvm::ArrayRef""""""; # initialize this provider; """"""Provides deref support to llvm::Optional<T>""""""; # The underlying SmallVector base class is the first child.; # StringRef's are also used to point at binary blobs in memory,; # so filter out suspiciously long strings.; # json.dumps conveniently escapes the string for us.; """"""; LLDB doesn't support template parameter packs, so let's parse them manually.; """"""; # The heuristic to identify valid entries does not handle the case of a; # single tombstone. The summary calls attention to this.; # The indexes into `Buckets` that contain valid map entries.; # By default, DenseMap instances use DenseMapPair to hold key-value; # entries. When the entry is a DenseMapPair, unwrap it to expose the; # children as simple std::pair values.; #; # This entry type is customizable (a template parameter). For other; # types, expose the entry type as is.; # Bucket entries contain one of the following:; # 1. Valid key-value; # 2. Empty key; # 3. Tombstone key (a deleted entry); #; # NumBuckets is always greater than NumEntries. The empty key, and; # potentially the tombstone key, will occur multiple times. A key that; # is repeated is either the empty key or the tombstone key.; # For each key, collect a list of buckets it appears in.; # Heuristic: This is not a multi-map, any repeated (non-unique) keys are; # either the the empty key or the tombstone key. Populate child_buckets; # with the indexes of entries containing unique keys.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lldbDataFormatters.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lldbDataFormatters.py
Usability,simpl,simple,"# debugger.HandleCommand(; # ""type synthetic add -w llvm ""; # f""-l {__name__}.PointerIntPairSynthProvider ""; # '-x ""^llvm::PointerIntPair<.+>$""'; # ); # debugger.HandleCommand(; # ""type synthetic add -w llvm ""; # f""-l {__name__}.PointerUnionSynthProvider ""; # '-x ""^llvm::PointerUnion<.+>$""'; # ); # Pretty printer for llvm::SmallVector/llvm::SmallVectorImpl; # initialize this provider; # Do bounds checking.; # If this is a reference type we have to dereference it to get to the; # template parameter.; """"""Provider for llvm::ArrayRef""""""; # initialize this provider; """"""Provides deref support to llvm::Optional<T>""""""; # The underlying SmallVector base class is the first child.; # StringRef's are also used to point at binary blobs in memory,; # so filter out suspiciously long strings.; # json.dumps conveniently escapes the string for us.; """"""; LLDB doesn't support template parameter packs, so let's parse them manually.; """"""; # The heuristic to identify valid entries does not handle the case of a; # single tombstone. The summary calls attention to this.; # The indexes into `Buckets` that contain valid map entries.; # By default, DenseMap instances use DenseMapPair to hold key-value; # entries. When the entry is a DenseMapPair, unwrap it to expose the; # children as simple std::pair values.; #; # This entry type is customizable (a template parameter). For other; # types, expose the entry type as is.; # Bucket entries contain one of the following:; # 1. Valid key-value; # 2. Empty key; # 3. Tombstone key (a deleted entry); #; # NumBuckets is always greater than NumEntries. The empty key, and; # potentially the tombstone key, will occur multiple times. A key that; # is repeated is either the empty key or the tombstone key.; # For each key, collect a list of buckets it appears in.; # Heuristic: This is not a multi-map, any repeated (non-unique) keys are; # either the the empty key or the tombstone key. Populate child_buckets; # with the indexes of entries containing unique keys.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lldbDataFormatters.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lldbDataFormatters.py
Integrability,depend,depending,"#!/usr/bin/env python; """"""; Summarize the information in the given coverage files. Emits the number of rules covered or the percentage of rules covered depending; on whether --num-rules has been used to specify the total number of rules.; """"""",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/llvm-gisel-cov.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/llvm-gisel-cov.py
Modifiability,variab,variable,"; # Print the summary.; """"""<tr>; <td colspan='2'> No bugs found </td>; </tr>; """"""; # Create the table for SP bugs.; """"""<table>; <caption><b>{}</b></caption>; <tr>; """"""; # Handle fn bugs.; # No SP bugs for the pass.; # Get the bugs info.; # Dump the bugs info into the table.; # The same file-pass pair can have multiple bugs.; """"""<tr>; <td colspan='4'> No bugs found </td>; </tr>; """"""; # Create the summary table for the sp bugs.; """"""<table>; <caption><b>{}</b></caption>; <tr>; """"""; # Print the summary.; """"""<tr>; <td colspan='2'> No bugs found </td>; </tr>; """"""; # Create the table for Variable bugs.; """"""<table>; <caption><b>{}</b></caption>; <tr>; """"""; # Handle var bugs.; # No SP bugs for the pass.; # Get the bugs info.; # Dump the bugs info into the table.; # The same file-pass pair can have multiple bugs.; """"""<tr>; <td colspan='4'> No bugs found </td>; </tr>; """"""; # Create the summary table for the sp bugs.; """"""<table>; <caption><b>{}</b></caption>; <tr>; """"""; # Print the summary.; """"""<tr>; <td colspan='2'> No bugs found </td>; </tr>; """"""; # Finish the html page.; """"""</body>; </html>""""""; # Read the JSON file in chunks.; # The file contains json object per line.; # An example of the line (formatted json):; # {; # ""file"": ""simple.c"",; # ""pass"": ""Deduce function attributes in RPO"",; # ""bugs"": [; # [; # {; # ""action"": ""drop"",; # ""metadata"": ""DISubprogram"",; # ""name"": ""fn2""; # },; # {; # ""action"": ""drop"",; # ""metadata"": ""DISubprogram"",; # ""name"": ""fn1""; # }; # ]; # ]; # }; # Parse the program arguments.; # Use the defaultdict in order to make multidim dicts.; # Use the ordered dict to make a summary.; # Compress similar bugs.; # DILocBugs with same pass & instruction name.; # DISPBugs with same pass & function name.; # DIVarBugs with same pass & variable name.; # Process each chunk of 1 million JSON lines.; # Map the bugs into the file-pass pairs.; # Omit duplicated bugs.; # Fill the summary dict.; # Fill the summary dict.; # Fill the summary dict.; # Unsupported metadata.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/llvm-original-di-preservation.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/llvm-original-di-preservation.py
Testability,test,testing,"#!/usr/bin/env python; #; # Debugify summary for the original debug info testing.; #; # Report the bugs in form of html.; """""" <html>; <head>; <style>; table, th, td {; border: 1px solid black;; }; table.center {; margin-left: auto;; margin-right: auto;; }; </style>; </head>; <body>; """"""; # Create the table for Location bugs.; """"""<table>; <caption><b>{}</b></caption>; <tr>; """"""; # Handle loction bugs.; # No location bugs for the pass.; # Get the bugs info.; # Dump the bugs info into the table.; # The same file-pass pair can have multiple bugs.; """""" <tr>; <td colspan='7'> No bugs found </td>; </tr>; """"""; # Create the summary table for the loc bugs.; """"""<table>; <caption><b>{}</b></caption>; <tr>; """"""; # Print the summary.; """"""<tr>; <td colspan='2'> No bugs found </td>; </tr>; """"""; # Create the table for SP bugs.; """"""<table>; <caption><b>{}</b></caption>; <tr>; """"""; # Handle fn bugs.; # No SP bugs for the pass.; # Get the bugs info.; # Dump the bugs info into the table.; # The same file-pass pair can have multiple bugs.; """"""<tr>; <td colspan='4'> No bugs found </td>; </tr>; """"""; # Create the summary table for the sp bugs.; """"""<table>; <caption><b>{}</b></caption>; <tr>; """"""; # Print the summary.; """"""<tr>; <td colspan='2'> No bugs found </td>; </tr>; """"""; # Create the table for Variable bugs.; """"""<table>; <caption><b>{}</b></caption>; <tr>; """"""; # Handle var bugs.; # No SP bugs for the pass.; # Get the bugs info.; # Dump the bugs info into the table.; # The same file-pass pair can have multiple bugs.; """"""<tr>; <td colspan='4'> No bugs found </td>; </tr>; """"""; # Create the summary table for the sp bugs.; """"""<table>; <caption><b>{}</b></caption>; <tr>; """"""; # Print the summary.; """"""<tr>; <td colspan='2'> No bugs found </td>; </tr>; """"""; # Finish the html page.; """"""</body>; </html>""""""; # Read the JSON file in chunks.; # The file contains json object per line.; # An example of the line (formatted json):; # {; # ""file"": ""simple.c"",; # ""pass"": ""Deduce function attributes in RP",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/llvm-original-di-preservation.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/llvm-original-di-preservation.py
Usability,simpl,simple,"; # Print the summary.; """"""<tr>; <td colspan='2'> No bugs found </td>; </tr>; """"""; # Create the table for SP bugs.; """"""<table>; <caption><b>{}</b></caption>; <tr>; """"""; # Handle fn bugs.; # No SP bugs for the pass.; # Get the bugs info.; # Dump the bugs info into the table.; # The same file-pass pair can have multiple bugs.; """"""<tr>; <td colspan='4'> No bugs found </td>; </tr>; """"""; # Create the summary table for the sp bugs.; """"""<table>; <caption><b>{}</b></caption>; <tr>; """"""; # Print the summary.; """"""<tr>; <td colspan='2'> No bugs found </td>; </tr>; """"""; # Create the table for Variable bugs.; """"""<table>; <caption><b>{}</b></caption>; <tr>; """"""; # Handle var bugs.; # No SP bugs for the pass.; # Get the bugs info.; # Dump the bugs info into the table.; # The same file-pass pair can have multiple bugs.; """"""<tr>; <td colspan='4'> No bugs found </td>; </tr>; """"""; # Create the summary table for the sp bugs.; """"""<table>; <caption><b>{}</b></caption>; <tr>; """"""; # Print the summary.; """"""<tr>; <td colspan='2'> No bugs found </td>; </tr>; """"""; # Finish the html page.; """"""</body>; </html>""""""; # Read the JSON file in chunks.; # The file contains json object per line.; # An example of the line (formatted json):; # {; # ""file"": ""simple.c"",; # ""pass"": ""Deduce function attributes in RPO"",; # ""bugs"": [; # [; # {; # ""action"": ""drop"",; # ""metadata"": ""DISubprogram"",; # ""name"": ""fn2""; # },; # {; # ""action"": ""drop"",; # ""metadata"": ""DISubprogram"",; # ""name"": ""fn1""; # }; # ]; # ]; # }; # Parse the program arguments.; # Use the defaultdict in order to make multidim dicts.; # Use the ordered dict to make a summary.; # Compress similar bugs.; # DILocBugs with same pass & instruction name.; # DISPBugs with same pass & function name.; # DIVarBugs with same pass & variable name.; # Process each chunk of 1 million JSON lines.; # Map the bugs into the file-pass pairs.; # Omit duplicated bugs.; # Fill the summary dict.; # Fill the summary dict.; # Fill the summary dict.; # Unsupported metadata.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/llvm-original-di-preservation.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/llvm-original-di-preservation.py
Deployability,pipeline,pipeline,"# Automatically formatted with yapf (https://github.com/google/yapf); """"""Utility functions for creating and manipulating LLVM 'opt' NPM pipeline objects.""""""; """"""Create pipeline object from string representation.""""""; """"""Create string representation of pipeline object.""""""; """"""Count number of passes (pass-managers excluded) in pipeline object.""""""; """"""Create two new pipeline objects by splitting pipeObj in two directly after pass with index splitIndex.""""""; """"""Create new pipeline object by removing pass with index removeIndex from pipeObj.""""""; """"""Create copy of pipeline object srcPipeObj.""""""; """"""Create new pipeline object by removing empty pass-managers (those with count = 0) from srcPipeObj.""""""",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/pipeline.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/pipeline.py
Availability,error,error,"#!/usr/bin/env python3; # Automatically formatted with yapf (https://github.com/google/yapf); # Script for automatic 'opt' pipeline reduction for when using the new; # pass-manager (NPM). Based around the '-print-pipeline-passes' option.; #; # The reduction algorithm consists of several phases (steps).; #; # Step #0: Verify that input fails with the given pipeline and make note of the; # error code.; #; # Step #1: Split pipeline in two starting from front and move forward as long as; # first pipeline exits normally and the second pipeline fails with the expected; # error code. Move on to step #2 with the IR from the split point and the; # pipeline from the second invocation.; #; # Step #2: Remove passes from end of the pipeline as long as the pipeline fails; # with the expected error code.; #; # Step #3: Make several sweeps over the remaining pipeline trying to remove one; # pass at a time. Repeat sweeps until unable to remove any more passes.; #; # Usage example:; # reduce_pipeline.py --opt-binary=./build-all-Debug/bin/opt --input=input.ll --output=output.ll --passes=PIPELINE [EXTRA-OPT-ARGS ...]; # Step #-1; # Launch 'opt' once with '-print-pipeline-passes' to expand pipeline before; # starting reduction. Allows specifying a default pipelines (e.g.; # '-passes=default<O3>').; # Step #0; # Confirm that the given input, passes and options result in failure.; # Step #1; # Try to narrow down the failing pass sequence by splitting the pipeline in two; # opt invocations (A and B) starting with invocation A only running the first; # pipeline pass and invocation B the remaining. Keep moving the split point; # forward as long as invocation A exits normally and invocation B fails with; # the expected error. This will accomplish two things first the input IR will be; # further reduced and second, with that IR, the reduced pipeline for invocation; # B will be sufficient to reproduce.; # Step #2; # Try removing passes from the end of the remaining pipeline while still; # reprod",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/reduce_pipeline.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/reduce_pipeline.py
Deployability,pipeline,pipeline,"#!/usr/bin/env python3; # Automatically formatted with yapf (https://github.com/google/yapf); # Script for automatic 'opt' pipeline reduction for when using the new; # pass-manager (NPM). Based around the '-print-pipeline-passes' option.; #; # The reduction algorithm consists of several phases (steps).; #; # Step #0: Verify that input fails with the given pipeline and make note of the; # error code.; #; # Step #1: Split pipeline in two starting from front and move forward as long as; # first pipeline exits normally and the second pipeline fails with the expected; # error code. Move on to step #2 with the IR from the split point and the; # pipeline from the second invocation.; #; # Step #2: Remove passes from end of the pipeline as long as the pipeline fails; # with the expected error code.; #; # Step #3: Make several sweeps over the remaining pipeline trying to remove one; # pass at a time. Repeat sweeps until unable to remove any more passes.; #; # Usage example:; # reduce_pipeline.py --opt-binary=./build-all-Debug/bin/opt --input=input.ll --output=output.ll --passes=PIPELINE [EXTRA-OPT-ARGS ...]; # Step #-1; # Launch 'opt' once with '-print-pipeline-passes' to expand pipeline before; # starting reduction. Allows specifying a default pipelines (e.g.; # '-passes=default<O3>').; # Step #0; # Confirm that the given input, passes and options result in failure.; # Step #1; # Try to narrow down the failing pass sequence by splitting the pipeline in two; # opt invocations (A and B) starting with invocation A only running the first; # pipeline pass and invocation B the remaining. Keep moving the split point; # forward as long as invocation A exits normally and invocation B fails with; # the expected error. This will accomplish two things first the input IR will be; # further reduced and second, with that IR, the reduced pipeline for invocation; # B will be sufficient to reproduce.; # Step #2; # Try removing passes from the end of the remaining pipeline while still; # reprod",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/reduce_pipeline.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/reduce_pipeline.py
Energy Efficiency,reduce,reduced,"passes' option.; #; # The reduction algorithm consists of several phases (steps).; #; # Step #0: Verify that input fails with the given pipeline and make note of the; # error code.; #; # Step #1: Split pipeline in two starting from front and move forward as long as; # first pipeline exits normally and the second pipeline fails with the expected; # error code. Move on to step #2 with the IR from the split point and the; # pipeline from the second invocation.; #; # Step #2: Remove passes from end of the pipeline as long as the pipeline fails; # with the expected error code.; #; # Step #3: Make several sweeps over the remaining pipeline trying to remove one; # pass at a time. Repeat sweeps until unable to remove any more passes.; #; # Usage example:; # reduce_pipeline.py --opt-binary=./build-all-Debug/bin/opt --input=input.ll --output=output.ll --passes=PIPELINE [EXTRA-OPT-ARGS ...]; # Step #-1; # Launch 'opt' once with '-print-pipeline-passes' to expand pipeline before; # starting reduction. Allows specifying a default pipelines (e.g.; # '-passes=default<O3>').; # Step #0; # Confirm that the given input, passes and options result in failure.; # Step #1; # Try to narrow down the failing pass sequence by splitting the pipeline in two; # opt invocations (A and B) starting with invocation A only running the first; # pipeline pass and invocation B the remaining. Keep moving the split point; # forward as long as invocation A exits normally and invocation B fails with; # the expected error. This will accomplish two things first the input IR will be; # further reduced and second, with that IR, the reduced pipeline for invocation; # B will be sufficient to reproduce.; # Step #2; # Try removing passes from the end of the remaining pipeline while still; # reproducing the error.; # Step #3; # Now that we have a pipeline that is reduced both front and back we do; # exhaustive sweeps over the remainder trying to remove one pass at a time.; # Repeat as long as reduction is possible.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/reduce_pipeline.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/reduce_pipeline.py
Testability,test,tests,"#!/usr/bin/env python3; """"""Replaces absolute line numbers in lit-tests with relative line numbers. Writing line numbers like 152 in 'RUN: or CHECK:' makes tests hard to maintain:; inserting lines in the middle of the test means updating all the line numbers. Encoding them relative to the current line helps, and tools support it:; Lit will substitute %(line+2) with the actual line number; FileCheck supports [[@LINE+2]]. This tool takes a regex which captures a line number, and a list of test files.; It searches for line numbers in the files and replaces them with a relative; line number reference.; """"""; """"""Example usage:; find -type f clang/test/CodeCompletion | grep -v /Inputs/ | \\; xargs relative_lines.py --dry-run --verbose --near=100 \\; --pattern='-code-completion-at[ =]%s:(\d+)' \\; --pattern='requires fix-it: {(\d+):\d+-(\d+):\d+}'; """"""; """"""Text to replace a capture group, e.g. 42 => %(line+1)""""""; """"""Text to replace a whole match, e.g. --at=42:3 => --at=%(line+2):3""""""; # re groups are conventionally 1-indexed",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/relative_lines.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/relative_lines.py
Integrability,depend,dependencies,"es; tests on a target.; """"""; # Note: The default value is for the backward compatibility with a hack in; # libcxx test suite.; # If an argument is a file that ends in `.tmp.exe`, assume it is the name; # of an executable generated by a test file. We call these test-executables; # below. This allows us to do custom processing like codesigning test-executables; # and changing their path when running on the remote host. It's also possible; # for there to be no such executable, for example in the case of a .sh.cpp; # test.; # Retrieve the exec directory from the command line.; # Get the current directory in that case.; # Create a temporary directory where the test will be run.; # That is effectively the value of %T on the remote host.; # Do any necessary codesigning of test-executables found in the command line.; # tar up the execution directory (which contains everything that's needed; # to run the test), and copy the tarball over to the remote host.; # Make sure we close the file before we scp it, because accessing; # the temporary file while still open doesn't work on Windows.; # Make sure we close the file in case an exception happens before; # we've closed it above -- otherwise close() is idempotent.; # Untar the dependencies in the temporary directory and remove the tarball.; # Copy only the files, which are specified in the command line.; # Copy them to remote host one by one.; # Make sure all executables in the remote command line have 'execute'; # permissions on the remote host. The host that compiled the test-executable; # might not have a notion of 'executable' permissions.; # Execute the command through SSH in the temporary directory, with the; # correct environment. We tweak the command line to run it on the remote; # host by transforming the path of test-executables to their path in the; # temporary directory on the remote host.; # Finally, SSH to the remote host and execute all the commands.; # Make sure the temporary directory is removed when we're done.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/remote-exec.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/remote-exec.py
Security,access,accessing,"es; tests on a target.; """"""; # Note: The default value is for the backward compatibility with a hack in; # libcxx test suite.; # If an argument is a file that ends in `.tmp.exe`, assume it is the name; # of an executable generated by a test file. We call these test-executables; # below. This allows us to do custom processing like codesigning test-executables; # and changing their path when running on the remote host. It's also possible; # for there to be no such executable, for example in the case of a .sh.cpp; # test.; # Retrieve the exec directory from the command line.; # Get the current directory in that case.; # Create a temporary directory where the test will be run.; # That is effectively the value of %T on the remote host.; # Do any necessary codesigning of test-executables found in the command line.; # tar up the execution directory (which contains everything that's needed; # to run the test), and copy the tarball over to the remote host.; # Make sure we close the file before we scp it, because accessing; # the temporary file while still open doesn't work on Windows.; # Make sure we close the file in case an exception happens before; # we've closed it above -- otherwise close() is idempotent.; # Untar the dependencies in the temporary directory and remove the tarball.; # Copy only the files, which are specified in the command line.; # Copy them to remote host one by one.; # Make sure all executables in the remote command line have 'execute'; # permissions on the remote host. The host that compiled the test-executable; # might not have a notion of 'executable' permissions.; # Execute the command through SSH in the temporary directory, with the; # correct environment. We tweak the command line to run it on the remote; # host by transforming the path of test-executables to their path in the; # temporary directory on the remote host.; # Finally, SSH to the remote host and execute all the commands.; # Make sure the temporary directory is removed when we're done.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/remote-exec.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/remote-exec.py
Testability,test,tests,"#!/usr/bin/env python; # ===----------------------------------------------------------------------===##; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ===----------------------------------------------------------------------===##; """"""; Runs an executable on a remote host. This is meant to be used as an executor when running the LLVM and the Libraries; tests on a target.; """"""; # Note: The default value is for the backward compatibility with a hack in; # libcxx test suite.; # If an argument is a file that ends in `.tmp.exe`, assume it is the name; # of an executable generated by a test file. We call these test-executables; # below. This allows us to do custom processing like codesigning test-executables; # and changing their path when running on the remote host. It's also possible; # for there to be no such executable, for example in the case of a .sh.cpp; # test.; # Retrieve the exec directory from the command line.; # Get the current directory in that case.; # Create a temporary directory where the test will be run.; # That is effectively the value of %T on the remote host.; # Do any necessary codesigning of test-executables found in the command line.; # tar up the execution directory (which contains everything that's needed; # to run the test), and copy the tarball over to the remote host.; # Make sure we close the file before we scp it, because accessing; # the temporary file while still open doesn't work on Windows.; # Make sure we close the file in case an exception happens before; # we've closed it above -- otherwise close() is idempotent.; # Untar the dependencies in the temporary directory and remove the tarball.; # Copy only the files, which are specified in the command line.; # Copy them to remote host one by one.; # Make sure all executables in the remote command line have 'execute'; # permissions",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/remote-exec.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/remote-exec.py
Deployability,release,release,"' with an example, if we had the following; commit history (where `a -> b` notes that `b` is a direct child of `a`):. 123abc -> 223abc -> 323abc -> 423abc -> 523abc. And where 423abc is a revert of 223abc, this revert is considered to be 'across'; 323abc. More generally, a revert A of a parent commit B is considered to be; 'across' a commit C if C is a parent of A and B is a parent of C. Please note that revert detection in general is really difficult, since merge; conflicts/etc always introduce _some_ amount of fuzziness. This script just; uses a bundle of heuristics, and is bound to ignore / incorrectly flag some; reverts. The hope is that it'll easily catch the vast majority (>90%) of them,; though. This is designed to be used in one of two ways: an import in Python, or run; directly from a shell. If you want to import this, the `find_reverts`; function is the thing to look at. If you'd rather use this from a shell, have a; usage example:. ```; ./revert_checker.py c47f97169 origin/main origin/release/12.x; ```. This checks for all reverts from the tip of origin/main to c47f97169, which are; across the latter. It then does the same for origin/release/12.x to c47f97169.; Duplicate reverts discovered when walking both roots (origin/main and; origin/release/12.x) are deduplicated in output.; """"""; # People are creative with their reverts, and heuristics are a bit difficult.; # Like 90% of of reverts have ""This reverts commit ${full_sha}"".; # Some lack that entirely, while others have many of them specified in ad-hoc; # ways, while others use short SHAs and whatever.; #; # The 90% case is trivial to handle (and 100% free + automatic). The extra 10%; # starts involving human intervention, which is probably not worth it for now.; # for mypy's happiness.; # Find the next separator line. If there's nothing to log, it may not exist.; # It might not be the first line if git feels complainy.; """"""Finds the closest common parent commit between `ref_a` and `ref_b`.""""""; """"""Finds r",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/revert_checker.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/revert_checker.py
Safety,detect,detection,"#!/usr/bin/env python3; # -*- coding: utf-8 -*-; # ===----------------------------------------------------------------------===##; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ===----------------------------------------------------------------------===##; """"""Checks for reverts of commits across a given git commit. To clarify the meaning of 'across' with an example, if we had the following; commit history (where `a -> b` notes that `b` is a direct child of `a`):. 123abc -> 223abc -> 323abc -> 423abc -> 523abc. And where 423abc is a revert of 223abc, this revert is considered to be 'across'; 323abc. More generally, a revert A of a parent commit B is considered to be; 'across' a commit C if C is a parent of A and B is a parent of C. Please note that revert detection in general is really difficult, since merge; conflicts/etc always introduce _some_ amount of fuzziness. This script just; uses a bundle of heuristics, and is bound to ignore / incorrectly flag some; reverts. The hope is that it'll easily catch the vast majority (>90%) of them,; though. This is designed to be used in one of two ways: an import in Python, or run; directly from a shell. If you want to import this, the `find_reverts`; function is the thing to look at. If you'd rather use this from a shell, have a; usage example:. ```; ./revert_checker.py c47f97169 origin/main origin/release/12.x; ```. This checks for all reverts from the tip of origin/main to c47f97169, which are; across the latter. It then does the same for origin/release/12.x to c47f97169.; Duplicate reverts discovered when walking both roots (origin/main and; origin/release/12.x) are deduplicated in output.; """"""; # People are creative with their reverts, and heuristics are a bit difficult.; # Like 90% of of reverts have ""This reverts commit ${full_sha}"".; # Some lack that entirely,",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/revert_checker.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/revert_checker.py
Testability,log,log," uses a bundle of heuristics, and is bound to ignore / incorrectly flag some; reverts. The hope is that it'll easily catch the vast majority (>90%) of them,; though. This is designed to be used in one of two ways: an import in Python, or run; directly from a shell. If you want to import this, the `find_reverts`; function is the thing to look at. If you'd rather use this from a shell, have a; usage example:. ```; ./revert_checker.py c47f97169 origin/main origin/release/12.x; ```. This checks for all reverts from the tip of origin/main to c47f97169, which are; across the latter. It then does the same for origin/release/12.x to c47f97169.; Duplicate reverts discovered when walking both roots (origin/main and; origin/release/12.x) are deduplicated in output.; """"""; # People are creative with their reverts, and heuristics are a bit difficult.; # Like 90% of of reverts have ""This reverts commit ${full_sha}"".; # Some lack that entirely, while others have many of them specified in ad-hoc; # ways, while others use short SHAs and whatever.; #; # The 90% case is trivial to handle (and 100% free + automatic). The extra 10%; # starts involving human intervention, which is probably not worth it for now.; # for mypy's happiness.; # Find the next separator line. If there's nothing to log, it may not exist.; # It might not be the first line if git feels complainy.; """"""Finds the closest common parent commit between `ref_a` and `ref_b`.""""""; """"""Finds reverts across `across_ref` in `git_dir`, starting from `root`. These reverts are returned in order of oldest reverts first.; """"""; # Since `all_reverts` contains reverts in log order (e.g., newer comes before; # older), we need to reverse this to keep with our guarantee of older =; # earlier in the result.; # `root`s can have related history, so we want to filter duplicate commits; # out. The overwhelmingly common case is also to have one root, and it's way; # easier to reason about output that comes in an order that's meaningful to; # git.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/revert_checker.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/revert_checker.py
Availability,avail,available,"#!/usr/bin/env python3; # -*- coding: utf-8 -*-; # ===----------------------------------------------------------------------===##; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ===----------------------------------------------------------------------===##; """"""Tests for revert_checker. Note that these tests require having LLVM's git history available, since our; repository has a few interesting instances of edge-cases.; """"""; # pylint: disable=protected-access; """"""Returns the path to llvm-project's root.""""""; """"""Silences all log messages. Also collects info about log messages that would've been emitted.; """"""; """"""Tests for revert_checker.""""""; # c9944df916e41b1014dff5f6f75d52297b48ecdc mentions reverting a non-commit; # object. It sits between the given base_ref and root.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/revert_checker_test.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/revert_checker_test.py
Integrability,message,messages,"#!/usr/bin/env python3; # -*- coding: utf-8 -*-; # ===----------------------------------------------------------------------===##; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ===----------------------------------------------------------------------===##; """"""Tests for revert_checker. Note that these tests require having LLVM's git history available, since our; repository has a few interesting instances of edge-cases.; """"""; # pylint: disable=protected-access; """"""Returns the path to llvm-project's root.""""""; """"""Silences all log messages. Also collects info about log messages that would've been emitted.; """"""; """"""Tests for revert_checker.""""""; # c9944df916e41b1014dff5f6f75d52297b48ecdc mentions reverting a non-commit; # object. It sits between the given base_ref and root.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/revert_checker_test.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/revert_checker_test.py
Security,access,access,"#!/usr/bin/env python3; # -*- coding: utf-8 -*-; # ===----------------------------------------------------------------------===##; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ===----------------------------------------------------------------------===##; """"""Tests for revert_checker. Note that these tests require having LLVM's git history available, since our; repository has a few interesting instances of edge-cases.; """"""; # pylint: disable=protected-access; """"""Returns the path to llvm-project's root.""""""; """"""Silences all log messages. Also collects info about log messages that would've been emitted.; """"""; """"""Tests for revert_checker.""""""; # c9944df916e41b1014dff5f6f75d52297b48ecdc mentions reverting a non-commit; # object. It sits between the given base_ref and root.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/revert_checker_test.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/revert_checker_test.py
Testability,test,tests,"#!/usr/bin/env python3; # -*- coding: utf-8 -*-; # ===----------------------------------------------------------------------===##; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ===----------------------------------------------------------------------===##; """"""Tests for revert_checker. Note that these tests require having LLVM's git history available, since our; repository has a few interesting instances of edge-cases.; """"""; # pylint: disable=protected-access; """"""Returns the path to llvm-project's root.""""""; """"""Silences all log messages. Also collects info about log messages that would've been emitted.; """"""; """"""Tests for revert_checker.""""""; # c9944df916e41b1014dff5f6f75d52297b48ecdc mentions reverting a non-commit; # object. It sits between the given base_ref and root.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/revert_checker_test.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/revert_checker_test.py
Availability,failure,failure,"------------------===##; """"""Script to bisect over files in an rsp file. This is mostly used for detecting which file contains a miscompile between two; compiler revisions. It does this by bisecting over an rsp file. Between two; build directories, this script will make the rsp file reference the current; build directory's version of some set of the rsp's object files/libraries, and; reference the other build directory's version of the same files for the; remaining set of object files/libraries. Build the target in two separate directories with the two compiler revisions,; keeping the rsp file around since ninja by default deletes the rsp file after; building.; $ ninja -d keeprsp mytarget. Create a script to build the target and run an interesting test. Get the; command to build the target via; $ ninja -t commands | grep mytarget; The command to build the target should reference the rsp file.; This script doesn't care if the test script returns 0 or 1 for specifically the; successful or failing test, just that the test script returns a different; return code for success vs failure.; Since the command that `ninja -t commands` is run from the build directory,; usually the test script cd's to the build directory. $ rsp_bisect.py --test=path/to/test_script --rsp=path/to/build/target.rsp; --other_rel_path=../Other; where --other_rel_path is the relative path from the first build directory to; the other build directory. This is prepended to files in the rsp. For a full example, if the foo target is suspected to contain a miscompile in; some file, have two different build directories, buildgood/ and buildbad/ and; run; $ ninja -d keeprsp foo; in both so we have two versions of all relevant object files that may contain a; miscompile, one built by a good compiler and one by a bad compiler. In buildgood/, run; $ ninja -t commands | grep '-o .*foo'; to get the command to link the files together. It may look something like; clang -o foo @foo.rsp. Now create a test script that ru",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/rsp_bisect.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/rsp_bisect.py
Safety,detect,detecting,"#!/usr/bin/env python3; # ===----------------------------------------------------------------------===##; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ===----------------------------------------------------------------------===##; """"""Script to bisect over files in an rsp file. This is mostly used for detecting which file contains a miscompile between two; compiler revisions. It does this by bisecting over an rsp file. Between two; build directories, this script will make the rsp file reference the current; build directory's version of some set of the rsp's object files/libraries, and; reference the other build directory's version of the same files for the; remaining set of object files/libraries. Build the target in two separate directories with the two compiler revisions,; keeping the rsp file around since ninja by default deletes the rsp file after; building.; $ ninja -d keeprsp mytarget. Create a script to build the target and run an interesting test. Get the; command to build the target via; $ ninja -t commands | grep mytarget; The command to build the target should reference the rsp file.; This script doesn't care if the test script returns 0 or 1 for specifically the; successful or failing test, just that the test script returns a different; return code for success vs failure.; Since the command that `ninja -t commands` is run from the build directory,; usually the test script cd's to the build directory. $ rsp_bisect.py --test=path/to/test_script --rsp=path/to/build/target.rsp; --other_rel_path=../Other; where --other_rel_path is the relative path from the first build directory to; the other build directory. This is prepended to files in the rsp. For a full example, if the foo target is suspected to contain a miscompile in; some file, have two different build directories, buildgood/ and buildbad/ an",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/rsp_bisect.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/rsp_bisect.py
Testability,test,test,"===##; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ===----------------------------------------------------------------------===##; """"""Script to bisect over files in an rsp file. This is mostly used for detecting which file contains a miscompile between two; compiler revisions. It does this by bisecting over an rsp file. Between two; build directories, this script will make the rsp file reference the current; build directory's version of some set of the rsp's object files/libraries, and; reference the other build directory's version of the same files for the; remaining set of object files/libraries. Build the target in two separate directories with the two compiler revisions,; keeping the rsp file around since ninja by default deletes the rsp file after; building.; $ ninja -d keeprsp mytarget. Create a script to build the target and run an interesting test. Get the; command to build the target via; $ ninja -t commands | grep mytarget; The command to build the target should reference the rsp file.; This script doesn't care if the test script returns 0 or 1 for specifically the; successful or failing test, just that the test script returns a different; return code for success vs failure.; Since the command that `ninja -t commands` is run from the build directory,; usually the test script cd's to the build directory. $ rsp_bisect.py --test=path/to/test_script --rsp=path/to/build/target.rsp; --other_rel_path=../Other; where --other_rel_path is the relative path from the first build directory to; the other build directory. This is prepended to files in the rsp. For a full example, if the foo target is suspected to contain a miscompile in; some file, have two different build directories, buildgood/ and buildbad/ and; run; $ ninja -d keeprsp foo; in both so we have two versions of all relevant object files that m",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/rsp_bisect.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/rsp_bisect.py
Availability,error,errors,"'A' be 'len(shuffle_range)', 'C' be 'max_shuffle_height',; # and 'B' be the bias we use to compensate for; # C '((A+1)*A^(1/C))/(A*(A+1)^(1/C))':; #; # 1 - (B * A)/(A + 1)^C = 1 - A/(A + 1); #; # So at each node we use:; #; # 1 - (B * A)/(A + 1); # = 1 - ((A + 1) * A * A^(1/C))/(A * (A + 1) * (A + 1)^(1/C)); # = 1 - ((A + 1) * A^((C + 1)/C))/(A * (A + 1)^((C + 1)/C)); #; # This is the formula we use to select undef lanes in the shuffle.; # Print out the shuffle sequence in a compact form.; # Symbolically evaluate the shuffle tree.; # The IR uses silly names for floating point types. We also need a same-size; # integer type.; # Now we need to generate IR for the shuffle function.; """"""; define internal fastcc <%(N)d x %(T)s> @test(%(arguments)s) noinline nounwind {; entry:""""""; """"""; %%s.%(next_i)d.%(j)d = shufflevector <%(N)d x %(T)s> %%s.%(i)d.%(j)d, <%(N)d x %(T)s> %%s.%(i)d.%(next_j)d, <%(N)d x i32> <%(S)s>; """"""; """"""; ret <%(N)d x %(T)s> %%s.%(i)d.0; }; """"""; # Generate some string constants that we can use to report errors.; """"""; @error.%(i)d = private unnamed_addr global [128 x i8] c""%(s)s""; """"""; # Define a wrapper function which is marked 'optnone' to prevent; # interprocedural optimizations from deleting the test.; """"""; define internal fastcc <%(N)d x %(T)s> @test_wrapper(%(arguments)s) optnone noinline {; %%result = call fastcc <%(N)d x %(T)s> @test(%(arguments)s); ret <%(N)d x %(T)s> %%result; }; """"""; # Finally, generate a main function which will trap if any lanes are mapped; # incorrectly (in an observable way).; """"""; define i32 @main() {; entry:; ; Create a scratch space to print error messages.; %%str = alloca [128 x i8]; %%str.ptr = getelementptr inbounds [128 x i8], [128 x i8]* %%str, i32 0, i32 0. ; Build the input vector and call the test function.; %%v = call fastcc <%(N)d x %(T)s> @test_wrapper(%(inputs)s); ; We need to cast this back to an integer type vector to easily check the; ; result.; %%v.cast = bitcast <%(N)d x %(T)s> %%v to <%(N)d x %(IT)s>; b",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/shuffle_fuzz.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/shuffle_fuzz.py
Energy Efficiency,power,power,"evectors, maintaining the; element mapping accumulated across the function. It then generates a main; function which calls it with a different value in each element and checks that; the result matches the expected mapping. Take the output IR printed to stdout, compile it to an executable using whatever; set of transforms you want to test, and run the program. If it crashes, it found; a bug.; """"""; # Checked above by argument parsing.; # Because undef (-1) saturates and is indistinguishable when testing the; # correctness of a shuffle, we want to bias our fuzz toward having a decent; # mixture of non-undef lanes in the end. With a deep shuffle tree, the; # probabilies aren't good so we need to bias things. The math here is that if; # we uniformly select between -1 and the other inputs, each element of the; # result will have the following probability of being undef:; #; # 1 - (shuffle_range/(shuffle_range+1))^max_shuffle_height; #; # More generally, for any probability P of selecting a defined element in; # a single shuffle, the end result is:; #; # 1 - P^max_shuffle_height; #; # The power of the shuffle height is the real problem, as we want:; #; # 1 - shuffle_range/(shuffle_range+1); #; # So we bias the selection of undef at any given node based on the tree; # height. Below, let 'A' be 'len(shuffle_range)', 'C' be 'max_shuffle_height',; # and 'B' be the bias we use to compensate for; # C '((A+1)*A^(1/C))/(A*(A+1)^(1/C))':; #; # 1 - (B * A)/(A + 1)^C = 1 - A/(A + 1); #; # So at each node we use:; #; # 1 - (B * A)/(A + 1); # = 1 - ((A + 1) * A * A^(1/C))/(A * (A + 1) * (A + 1)^(1/C)); # = 1 - ((A + 1) * A^((C + 1)/C))/(A * (A + 1)^((C + 1)/C)); #; # This is the formula we use to select undef lanes in the shuffle.; # Print out the shuffle sequence in a compact form.; # Symbolically evaluate the shuffle tree.; # The IR uses silly names for floating point types. We also need a same-size; # integer type.; # Now we need to generate IR for the shuffle function.; """"""; define ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/shuffle_fuzz.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/shuffle_fuzz.py
Integrability,wrap,wrapper,":; #; # 1 - (B * A)/(A + 1)^C = 1 - A/(A + 1); #; # So at each node we use:; #; # 1 - (B * A)/(A + 1); # = 1 - ((A + 1) * A * A^(1/C))/(A * (A + 1) * (A + 1)^(1/C)); # = 1 - ((A + 1) * A^((C + 1)/C))/(A * (A + 1)^((C + 1)/C)); #; # This is the formula we use to select undef lanes in the shuffle.; # Print out the shuffle sequence in a compact form.; # Symbolically evaluate the shuffle tree.; # The IR uses silly names for floating point types. We also need a same-size; # integer type.; # Now we need to generate IR for the shuffle function.; """"""; define internal fastcc <%(N)d x %(T)s> @test(%(arguments)s) noinline nounwind {; entry:""""""; """"""; %%s.%(next_i)d.%(j)d = shufflevector <%(N)d x %(T)s> %%s.%(i)d.%(j)d, <%(N)d x %(T)s> %%s.%(i)d.%(next_j)d, <%(N)d x i32> <%(S)s>; """"""; """"""; ret <%(N)d x %(T)s> %%s.%(i)d.0; }; """"""; # Generate some string constants that we can use to report errors.; """"""; @error.%(i)d = private unnamed_addr global [128 x i8] c""%(s)s""; """"""; # Define a wrapper function which is marked 'optnone' to prevent; # interprocedural optimizations from deleting the test.; """"""; define internal fastcc <%(N)d x %(T)s> @test_wrapper(%(arguments)s) optnone noinline {; %%result = call fastcc <%(N)d x %(T)s> @test(%(arguments)s); ret <%(N)d x %(T)s> %%result; }; """"""; # Finally, generate a main function which will trap if any lanes are mapped; # incorrectly (in an observable way).; """"""; define i32 @main() {; entry:; ; Create a scratch space to print error messages.; %%str = alloca [128 x i8]; %%str.ptr = getelementptr inbounds [128 x i8], [128 x i8]* %%str, i32 0, i32 0. ; Build the input vector and call the test function.; %%v = call fastcc <%(N)d x %(T)s> @test_wrapper(%(inputs)s); ; We need to cast this back to an integer type vector to easily check the; ; result.; %%v.cast = bitcast <%(N)d x %(T)s> %%v to <%(N)d x %(IT)s>; br label %%test.0; """"""; # Test that each non-undef result lane contains the expected value.; """"""; test.%(i)d:; ; Skip this lane, its value is und",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/shuffle_fuzz.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/shuffle_fuzz.py
Performance,optimiz,optimizations,":; #; # 1 - (B * A)/(A + 1)^C = 1 - A/(A + 1); #; # So at each node we use:; #; # 1 - (B * A)/(A + 1); # = 1 - ((A + 1) * A * A^(1/C))/(A * (A + 1) * (A + 1)^(1/C)); # = 1 - ((A + 1) * A^((C + 1)/C))/(A * (A + 1)^((C + 1)/C)); #; # This is the formula we use to select undef lanes in the shuffle.; # Print out the shuffle sequence in a compact form.; # Symbolically evaluate the shuffle tree.; # The IR uses silly names for floating point types. We also need a same-size; # integer type.; # Now we need to generate IR for the shuffle function.; """"""; define internal fastcc <%(N)d x %(T)s> @test(%(arguments)s) noinline nounwind {; entry:""""""; """"""; %%s.%(next_i)d.%(j)d = shufflevector <%(N)d x %(T)s> %%s.%(i)d.%(j)d, <%(N)d x %(T)s> %%s.%(i)d.%(next_j)d, <%(N)d x i32> <%(S)s>; """"""; """"""; ret <%(N)d x %(T)s> %%s.%(i)d.0; }; """"""; # Generate some string constants that we can use to report errors.; """"""; @error.%(i)d = private unnamed_addr global [128 x i8] c""%(s)s""; """"""; # Define a wrapper function which is marked 'optnone' to prevent; # interprocedural optimizations from deleting the test.; """"""; define internal fastcc <%(N)d x %(T)s> @test_wrapper(%(arguments)s) optnone noinline {; %%result = call fastcc <%(N)d x %(T)s> @test(%(arguments)s); ret <%(N)d x %(T)s> %%result; }; """"""; # Finally, generate a main function which will trap if any lanes are mapped; # incorrectly (in an observable way).; """"""; define i32 @main() {; entry:; ; Create a scratch space to print error messages.; %%str = alloca [128 x i8]; %%str.ptr = getelementptr inbounds [128 x i8], [128 x i8]* %%str, i32 0, i32 0. ; Build the input vector and call the test function.; %%v = call fastcc <%(N)d x %(T)s> @test_wrapper(%(inputs)s); ; We need to cast this back to an integer type vector to easily check the; ; result.; %%v.cast = bitcast <%(N)d x %(T)s> %%v to <%(N)d x %(IT)s>; br label %%test.0; """"""; # Test that each non-undef result lane contains the expected value.; """"""; test.%(i)d:; ; Skip this lane, its value is und",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/shuffle_fuzz.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/shuffle_fuzz.py
Testability,test,tester,"#!/usr/bin/env python; """"""A shuffle vector fuzz tester. This is a python program to fuzz test the LLVM shufflevector instruction. It; generates a function with a random sequnece of shufflevectors, maintaining the; element mapping accumulated across the function. It then generates a main; function which calls it with a different value in each element and checks that; the result matches the expected mapping. Take the output IR printed to stdout, compile it to an executable using whatever; set of transforms you want to test, and run the program. If it crashes, it found; a bug.; """"""; # Checked above by argument parsing.; # Because undef (-1) saturates and is indistinguishable when testing the; # correctness of a shuffle, we want to bias our fuzz toward having a decent; # mixture of non-undef lanes in the end. With a deep shuffle tree, the; # probabilies aren't good so we need to bias things. The math here is that if; # we uniformly select between -1 and the other inputs, each element of the; # result will have the following probability of being undef:; #; # 1 - (shuffle_range/(shuffle_range+1))^max_shuffle_height; #; # More generally, for any probability P of selecting a defined element in; # a single shuffle, the end result is:; #; # 1 - P^max_shuffle_height; #; # The power of the shuffle height is the real problem, as we want:; #; # 1 - shuffle_range/(shuffle_range+1); #; # So we bias the selection of undef at any given node based on the tree; # height. Below, let 'A' be 'len(shuffle_range)', 'C' be 'max_shuffle_height',; # and 'B' be the bias we use to compensate for; # C '((A+1)*A^(1/C))/(A*(A+1)^(1/C))':; #; # 1 - (B * A)/(A + 1)^C = 1 - A/(A + 1); #; # So at each node we use:; #; # 1 - (B * A)/(A + 1); # = 1 - ((A + 1) * A * A^(1/C))/(A * (A + 1) * (A + 1)^(1/C)); # = 1 - ((A + 1) * A^((C + 1)/C))/(A * (A + 1)^((C + 1)/C)); #; # This is the formula we use to select undef lanes in the shuffle.; # Print out the shuffle sequence in a compact form.; # Symbolically eval",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/shuffle_fuzz.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/shuffle_fuzz.py
Availability,error,error,"#!/usr/bin/env python; """"""A shuffle-select vector fuzz tester. This is a python program to fuzz test the LLVM shufflevector and select; instructions. It generates a function with a random sequnece of shufflevectors; while optionally attaching it with a select instruction (regular or zero merge),; maintaining the element mapping accumulated across the function. It then; generates a main function which calls it with a different value in each element; and checks that the result matches the expected mapping. Take the output IR printed to stdout, compile it to an executable using whatever; set of transforms you want to test, and run the program. If it crashes, it found; a bug (an error message with the expected and actual result is printed).; """"""; # Possibility of one undef index in generated mask for shufflevector instruction; # Possibility of one undef index in generated mask for select instruction; # Possibility of adding a select instruction to the result of a shufflevector; # If we are adding a select instruction, this is the possibility of a; # merge-select instruction (1 - MERGE_SEL_POS = possibility of zero-merge-select; # instruction.; # Boolean; # Integer; # Integer; # Class to represent any value (variable) that can be used.; # Type; # String; # list of integers or floating points; # Class to represent an IR instruction (shuffle/select).; # Value; # Value; # list of integers; # Class to represent an IR shuffle instruction; # -1 => undef; # Class to represent an IR select instruction; # -1 => undef; # Returns a list of Values initialized with actual numbers according to the; # provided type; # Returns a random vector type to be tested; # In case one of the dimensions (scalar type/number of elements) is provided,; # fill the blank dimension and return appropriate Type object.; # 1 for integer type, 0 for floating-point; # Generate mask for shufflevector IR instruction, with SHUF_UNDEF_POS possibility; # of one undef index.; # Generate mask for select IR instructi",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/shuffle_select_fuzz_tester.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/shuffle_select_fuzz_tester.py
Integrability,message,message,"#!/usr/bin/env python; """"""A shuffle-select vector fuzz tester. This is a python program to fuzz test the LLVM shufflevector and select; instructions. It generates a function with a random sequnece of shufflevectors; while optionally attaching it with a select instruction (regular or zero merge),; maintaining the element mapping accumulated across the function. It then; generates a main function which calls it with a different value in each element; and checks that the result matches the expected mapping. Take the output IR printed to stdout, compile it to an executable using whatever; set of transforms you want to test, and run the program. If it crashes, it found; a bug (an error message with the expected and actual result is printed).; """"""; # Possibility of one undef index in generated mask for shufflevector instruction; # Possibility of one undef index in generated mask for select instruction; # Possibility of adding a select instruction to the result of a shufflevector; # If we are adding a select instruction, this is the possibility of a; # merge-select instruction (1 - MERGE_SEL_POS = possibility of zero-merge-select; # instruction.; # Boolean; # Integer; # Integer; # Class to represent any value (variable) that can be used.; # Type; # String; # list of integers or floating points; # Class to represent an IR instruction (shuffle/select).; # Value; # Value; # list of integers; # Class to represent an IR shuffle instruction; # -1 => undef; # Class to represent an IR select instruction; # -1 => undef; # Returns a list of Values initialized with actual numbers according to the; # provided type; # Returns a random vector type to be tested; # In case one of the dimensions (scalar type/number of elements) is provided,; # fill the blank dimension and return appropriate Type object.; # 1 for integer type, 0 for floating-point; # Generate mask for shufflevector IR instruction, with SHUF_UNDEF_POS possibility; # of one undef index.; # Generate mask for select IR instructi",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/shuffle_select_fuzz_tester.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/shuffle_select_fuzz_tester.py
Modifiability,variab,variable,"fflevectors; while optionally attaching it with a select instruction (regular or zero merge),; maintaining the element mapping accumulated across the function. It then; generates a main function which calls it with a different value in each element; and checks that the result matches the expected mapping. Take the output IR printed to stdout, compile it to an executable using whatever; set of transforms you want to test, and run the program. If it crashes, it found; a bug (an error message with the expected and actual result is printed).; """"""; # Possibility of one undef index in generated mask for shufflevector instruction; # Possibility of one undef index in generated mask for select instruction; # Possibility of adding a select instruction to the result of a shufflevector; # If we are adding a select instruction, this is the possibility of a; # merge-select instruction (1 - MERGE_SEL_POS = possibility of zero-merge-select; # instruction.; # Boolean; # Integer; # Integer; # Class to represent any value (variable) that can be used.; # Type; # String; # list of integers or floating points; # Class to represent an IR instruction (shuffle/select).; # Value; # Value; # list of integers; # Class to represent an IR shuffle instruction; # -1 => undef; # Class to represent an IR select instruction; # -1 => undef; # Returns a list of Values initialized with actual numbers according to the; # provided type; # Returns a random vector type to be tested; # In case one of the dimensions (scalar type/number of elements) is provided,; # fill the blank dimension and return appropriate Type object.; # 1 for integer type, 0 for floating-point; # Generate mask for shufflevector IR instruction, with SHUF_UNDEF_POS possibility; # of one undef index.; # Generate mask for select IR instruction, with SEL_UNDEF_POS possibility; # of one undef index.; # Generate shuffle instructions with optional select instruction after.; # Choose 2 available Values - remove them from inputs list.; # Create ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/shuffle_select_fuzz_tester.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/shuffle_select_fuzz_tester.py
Testability,test,tester,"#!/usr/bin/env python; """"""A shuffle-select vector fuzz tester. This is a python program to fuzz test the LLVM shufflevector and select; instructions. It generates a function with a random sequnece of shufflevectors; while optionally attaching it with a select instruction (regular or zero merge),; maintaining the element mapping accumulated across the function. It then; generates a main function which calls it with a different value in each element; and checks that the result matches the expected mapping. Take the output IR printed to stdout, compile it to an executable using whatever; set of transforms you want to test, and run the program. If it crashes, it found; a bug (an error message with the expected and actual result is printed).; """"""; # Possibility of one undef index in generated mask for shufflevector instruction; # Possibility of one undef index in generated mask for select instruction; # Possibility of adding a select instruction to the result of a shufflevector; # If we are adding a select instruction, this is the possibility of a; # merge-select instruction (1 - MERGE_SEL_POS = possibility of zero-merge-select; # instruction.; # Boolean; # Integer; # Integer; # Class to represent any value (variable) that can be used.; # Type; # String; # list of integers or floating points; # Class to represent an IR instruction (shuffle/select).; # Value; # Value; # list of integers; # Class to represent an IR shuffle instruction; # -1 => undef; # Class to represent an IR select instruction; # -1 => undef; # Returns a list of Values initialized with actual numbers according to the; # provided type; # Returns a random vector type to be tested; # In case one of the dimensions (scalar type/number of elements) is provided,; # fill the blank dimension and return appropriate Type object.; # 1 for integer type, 0 for floating-point; # Generate mask for shufflevector IR instruction, with SHUF_UNDEF_POS possibility; # of one undef index.; # Generate mask for select IR instructi",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/shuffle_select_fuzz_tester.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/shuffle_select_fuzz_tester.py
Testability,test,test,"#!/usr/bin/env python; """"""Script to sort the top-most block of #include lines. Assumes the LLVM coding conventions. Currently, this script only bothers sorting the llvm/... headers. Patches; welcome for more functionality, and sorting other header groups.; """"""; """"""Sort the #include lines of a specific file.""""""; # Skip files which are under INPUTS trees or test trees.; # Only allow comments and #defines prior to any includes. If either are; # mixed with includes, the order might be sensitive.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/sort_includes.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/sort_includes.py
Deployability,update,updated,"#!/usr/bin/env python3; """"""Helps manage sysroots.""""""; # Not all MSVC versions ship the DIA SDK, so the junction destination; # might not exist. That's fine.; # The SDKs used by default in compiler-rt/cmake/base-config-ix.cmake.; # COMPILER_RT_ENABLE_IOS defaults to on.; # COMPILER_RT_ENABLE_WATCHOS and COMPILER_RT_ENABLE_TV default to off.; # compiler-rt/cmake/config-ix.cmake sets DARWIN_EMBEDDED_PLATFORMS; # depending on these.; # sdkpath is something like /.../SDKs/MacOSX11.1.sdk, which is a; # symlink to MacOSX.sdk in the same directory. Resolve the symlink,; # to make the symlink in out_dir less likely to break when the SDK; # is updated (which will bump the number on xcrun's output, but not; # on the symlink destination).; # CMake doesn't like backslashes in commandline args.; # For find_darwin_sdk_dir() in; # compiler-rt/cmake/Modules/CompilerRTDarwinUtils.cmake",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/sysroot.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/sysroot.py
Integrability,depend,depending,"#!/usr/bin/env python3; """"""Helps manage sysroots.""""""; # Not all MSVC versions ship the DIA SDK, so the junction destination; # might not exist. That's fine.; # The SDKs used by default in compiler-rt/cmake/base-config-ix.cmake.; # COMPILER_RT_ENABLE_IOS defaults to on.; # COMPILER_RT_ENABLE_WATCHOS and COMPILER_RT_ENABLE_TV default to off.; # compiler-rt/cmake/config-ix.cmake sets DARWIN_EMBEDDED_PLATFORMS; # depending on these.; # sdkpath is something like /.../SDKs/MacOSX11.1.sdk, which is a; # symlink to MacOSX.sdk in the same directory. Resolve the symlink,; # to make the symlink in out_dir less likely to break when the SDK; # is updated (which will bump the number on xcrun's output, but not; # on the symlink destination).; # CMake doesn't like backslashes in commandline args.; # For find_darwin_sdk_dir() in; # compiler-rt/cmake/Modules/CompilerRTDarwinUtils.cmake",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/sysroot.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/sysroot.py
Modifiability,config,config-ix,"#!/usr/bin/env python3; """"""Helps manage sysroots.""""""; # Not all MSVC versions ship the DIA SDK, so the junction destination; # might not exist. That's fine.; # The SDKs used by default in compiler-rt/cmake/base-config-ix.cmake.; # COMPILER_RT_ENABLE_IOS defaults to on.; # COMPILER_RT_ENABLE_WATCHOS and COMPILER_RT_ENABLE_TV default to off.; # compiler-rt/cmake/config-ix.cmake sets DARWIN_EMBEDDED_PLATFORMS; # depending on these.; # sdkpath is something like /.../SDKs/MacOSX11.1.sdk, which is a; # symlink to MacOSX.sdk in the same directory. Resolve the symlink,; # to make the symlink in out_dir less likely to break when the SDK; # is updated (which will bump the number on xcrun's output, but not; # on the symlink destination).; # CMake doesn't like backslashes in commandline args.; # For find_darwin_sdk_dir() in; # compiler-rt/cmake/Modules/CompilerRTDarwinUtils.cmake",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/sysroot.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/sysroot.py
Modifiability,variab,variable,"#!/usr/bin/env python; """"""; Unicode case folding database conversion utility. Parses the database and generates a C++ function which implements the case; folding algorithm. The database entries are of the form:. <code>; <status>; <mapping>; # <name>. <status> can be one of four characters:; C - Common mappings; S - mappings for Simple case folding; F - mappings for Full case folding; T - special case for Turkish I characters. Right now this generates a function which implements simple case folding (C+S; entries).; """"""; # This variable will body of the mappings function; # Reads file line-by-line, extracts Common and Simple case fold mappings and; # returns a (from_char, to_char, from_name) tuple.; # Computes the shift (to_char - from_char) in a mapping.; # Computes the stride (from_char2 - from_char1) of two mappings.; # Computes the stride of a list of mappings. The list should have at least two; # mappings. All mappings in the list are assumed to have the same stride.; # b is a list of mappings. All the mappings are assumed to have the same; # shift and the stride between adjecant mappings (if any) is constant.; # Special case for handling blocks of length 1. We don't even need to; # emit the ""if (C < X) return C"" check below as all characters in this; # range will be caught by the ""C < X"" check emitted by the first; # non-trivial block.; # All characters before this block map to themselves.; # Generic pattern: check upper bound (lower bound is checked by the ""if""; # above) and modulo of C, return C+shift.; # Special case:; # We can elide the modulo-check because the expression ""C|1"" will map; # the intervening characters to themselves.; # Another special case: X % 1 is always zero, so don't emit the; # modulo-check.; # Incompatible shift, start a new block.; # Incompatible stride, start a new block.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/unicode-case-fold.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/unicode-case-fold.py
Usability,simpl,simple,"#!/usr/bin/env python; """"""; Unicode case folding database conversion utility. Parses the database and generates a C++ function which implements the case; folding algorithm. The database entries are of the form:. <code>; <status>; <mapping>; # <name>. <status> can be one of four characters:; C - Common mappings; S - mappings for Simple case folding; F - mappings for Full case folding; T - special case for Turkish I characters. Right now this generates a function which implements simple case folding (C+S; entries).; """"""; # This variable will body of the mappings function; # Reads file line-by-line, extracts Common and Simple case fold mappings and; # returns a (from_char, to_char, from_name) tuple.; # Computes the shift (to_char - from_char) in a mapping.; # Computes the stride (from_char2 - from_char1) of two mappings.; # Computes the stride of a list of mappings. The list should have at least two; # mappings. All mappings in the list are assumed to have the same stride.; # b is a list of mappings. All the mappings are assumed to have the same; # shift and the stride between adjecant mappings (if any) is constant.; # Special case for handling blocks of length 1. We don't even need to; # emit the ""if (C < X) return C"" check below as all characters in this; # range will be caught by the ""C < X"" check emitted by the first; # non-trivial block.; # All characters before this block map to themselves.; # Generic pattern: check upper bound (lower bound is checked by the ""if""; # above) and modulo of C, return C+shift.; # Special case:; # We can elide the modulo-check because the expression ""C|1"" will map; # the intervening characters to themselves.; # Another special case: X % 1 is always zero, so don't emit the; # modulo-check.; # Incompatible shift, start a new block.; # Incompatible stride, start a new block.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/unicode-case-fold.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/unicode-case-fold.py
Availability,down,down,"#!/usr/bin/env python3; """"""A script to generate FileCheck statements for 'opt' analysis tests. This script is a utility to update LLVM opt analysis test cases with new; FileCheck patterns. It can either update all of the tests in the file or; a single test function. Example usage:; $ update_analyze_test_checks.py --opt=../bin/opt test/foo.ll. Workflow:; 1. Make a compiler patch that requires updating some number of FileCheck lines; in regression test files.; 2. Save the patch and revert it from your local work area.; 3. Update the RUN-lines in the affected regression tests to look canonical.; Example: ""; RUN: opt < %s -passes='print<cost-model>' -disable-output 2>&1 | FileCheck %s""; 4. Refresh the FileCheck lines for either the entire file or select functions by; running this script.; 5. Commit the fresh baseline of checks.; 6. Apply your patch from step 1 and rebuild your local binaries.; 7. Re-run this script on affected regression tests.; 8. Check the diffs to ensure the script has done something reasonable.; 9. Submit a patch including the regression test diffs for review. A common pattern is to have the script insert complete checking of every; instruction. Then, edit it down to only check the relevant instructions.; The script is designed to make adding checks to a test case fast, it is *not*; designed to be authoratitive about what constitutes a good test!; """"""; # Used to advertise this file's name (""autogenerated_note"").; # FIXME: We should use multiple check prefixes to common check lines. For; # now, we just ignore all but the last.; # Split analysis outputs by ""Printing analysis "" declarations.; # Split analysis outputs by ""Printing analysis "" declarations.; # Print out the various check lines here.; # This input line of the function body will go as-is into the output.; # If it's outside a function, it just gets copied to the output.; # When filtering on a specific function, skip all others.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/update_analyze_test_checks.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/update_analyze_test_checks.py
Deployability,update,update,"#!/usr/bin/env python3; """"""A script to generate FileCheck statements for 'opt' analysis tests. This script is a utility to update LLVM opt analysis test cases with new; FileCheck patterns. It can either update all of the tests in the file or; a single test function. Example usage:; $ update_analyze_test_checks.py --opt=../bin/opt test/foo.ll. Workflow:; 1. Make a compiler patch that requires updating some number of FileCheck lines; in regression test files.; 2. Save the patch and revert it from your local work area.; 3. Update the RUN-lines in the affected regression tests to look canonical.; Example: ""; RUN: opt < %s -passes='print<cost-model>' -disable-output 2>&1 | FileCheck %s""; 4. Refresh the FileCheck lines for either the entire file or select functions by; running this script.; 5. Commit the fresh baseline of checks.; 6. Apply your patch from step 1 and rebuild your local binaries.; 7. Re-run this script on affected regression tests.; 8. Check the diffs to ensure the script has done something reasonable.; 9. Submit a patch including the regression test diffs for review. A common pattern is to have the script insert complete checking of every; instruction. Then, edit it down to only check the relevant instructions.; The script is designed to make adding checks to a test case fast, it is *not*; designed to be authoratitive about what constitutes a good test!; """"""; # Used to advertise this file's name (""autogenerated_note"").; # FIXME: We should use multiple check prefixes to common check lines. For; # now, we just ignore all but the last.; # Split analysis outputs by ""Printing analysis "" declarations.; # Split analysis outputs by ""Printing analysis "" declarations.; # Print out the various check lines here.; # This input line of the function body will go as-is into the output.; # If it's outside a function, it just gets copied to the output.; # When filtering on a specific function, skip all others.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/update_analyze_test_checks.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/update_analyze_test_checks.py
Testability,test,tests,"#!/usr/bin/env python3; """"""A script to generate FileCheck statements for 'opt' analysis tests. This script is a utility to update LLVM opt analysis test cases with new; FileCheck patterns. It can either update all of the tests in the file or; a single test function. Example usage:; $ update_analyze_test_checks.py --opt=../bin/opt test/foo.ll. Workflow:; 1. Make a compiler patch that requires updating some number of FileCheck lines; in regression test files.; 2. Save the patch and revert it from your local work area.; 3. Update the RUN-lines in the affected regression tests to look canonical.; Example: ""; RUN: opt < %s -passes='print<cost-model>' -disable-output 2>&1 | FileCheck %s""; 4. Refresh the FileCheck lines for either the entire file or select functions by; running this script.; 5. Commit the fresh baseline of checks.; 6. Apply your patch from step 1 and rebuild your local binaries.; 7. Re-run this script on affected regression tests.; 8. Check the diffs to ensure the script has done something reasonable.; 9. Submit a patch including the regression test diffs for review. A common pattern is to have the script insert complete checking of every; instruction. Then, edit it down to only check the relevant instructions.; The script is designed to make adding checks to a test case fast, it is *not*; designed to be authoratitive about what constitutes a good test!; """"""; # Used to advertise this file's name (""autogenerated_note"").; # FIXME: We should use multiple check prefixes to common check lines. For; # now, we just ignore all but the last.; # Split analysis outputs by ""Printing analysis "" declarations.; # Split analysis outputs by ""Printing analysis "" declarations.; # Print out the various check lines here.; # This input line of the function body will go as-is into the output.; # If it's outside a function, it just gets copied to the output.; # When filtering on a specific function, skip all others.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/update_analyze_test_checks.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/update_analyze_test_checks.py
Testability,test,test,"#!/usr/bin/env python3; """"""Dispatch to update_*_test_checks.py scripts automatically in bulk. Given a list of test files, this script will invoke the correct; update_test_checks-style script, skipping any tests which have not previously; had assertions autogenerated.; """"""; """"""; Return the path to the given UTC tool in the search path, or None if not; found.; """"""",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/update_any_test_checks.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/update_any_test_checks.py
Availability,error,error,"ion decls:; # Specializations must use the loc from the specialization, not the; # template, and search for the class's spelling as the specialization; # does not mention the method names in the source.; # Otherwise we ignore everything except functions:; # If there is no line it is probably a builtin function -> skip; # If there is no 'inner' object, it is a function declaration and we can; # skip it. However, function declarations may also contain an 'inner' list,; # but in that case it will only contains ParmVarDecls. If we find an entry; # that is not a ParmVarDecl, we know that this is a function definition.; # Determine the builtin includes directory so that we can update tests that; # depend on the builtin headers. See get_clang_builtin_include_dir() and; # use_clang() in llvm/utils/lit/lit/llvm/config.py.; # Many uses of this tool will not need an opt binary, because it's only; # needed for updating a test that runs clang | opt | FileCheck. So we; # defer this error message until we find that opt is actually needed.; # TODO Clean up duplication of asm/common build_function_body_dictionary; # Invoke external tool and extract function bodies.; # Build a list of filechecked and non-filechecked RUN lines.; # Parse executable args.; # Execute non-clang runline.; # Do lit-like substitutions.; # This is a clang runline, apply %clang substitution rule, do lit-like substitutions,; # and append args.clang_args; # Extract -check-prefix in FileCheck args; # Execute non-filechecked clang runline.; # Execute clang, generate LLVM IR, and extract functions.; # Store only filechecked runlines.; # Execute non-filechecked runline.; # Invoke clang -Xclang -ast-dump=json to get mapping from start lines to; # mangled names. Forward all clang args for now.; # Generate the appropriate checks for each function. We need to emit; # these in the order according to the generated output so that CHECK-LABEL; # works properly. func_order provides that.; # It turns out that when clang genera",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/update_cc_test_checks.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/update_cc_test_checks.py
Deployability,update,update,"#!/usr/bin/env python3; """"""A utility to update LLVM IR CHECK lines in C/C++ FileCheck test files. Example RUN lines in .c/.cc test files:. // RUN: %clang -emit-llvm -S %s -o - -O2 | FileCheck %s; // RUN: %clangxx -emit-llvm -S %s -o - -O2 | FileCheck -check-prefix=CHECK-A %s. Usage:. % utils/update_cc_test_checks.py --llvm-bin=release/bin test/a.cc; % utils/update_cc_test_checks.py --clang=release/bin/clang /tmp/c/a.cc; """"""; # Use clang's JSON AST dump to get the mangled name; # For tests that invoke %clang instead if %clang_cc1 we have to use; # -Xclang -ast-dump=json instead:; # Parse the clang JSON and add all children of type FunctionDecl.; # TODO: Should we add checks for global variables being emitted?; # Recurse for the following nodes that can contain nested function decls:; # Specializations must use the loc from the specialization, not the; # template, and search for the class's spelling as the specialization; # does not mention the method names in the source.; # Otherwise we ignore everything except functions:; # If there is no line it is probably a builtin function -> skip; # If there is no 'inner' object, it is a function declaration and we can; # skip it. However, function declarations may also contain an 'inner' list,; # but in that case it will only contains ParmVarDecls. If we find an entry; # that is not a ParmVarDecl, we know that this is a function definition.; # Determine the builtin includes directory so that we can update tests that; # depend on the builtin headers. See get_clang_builtin_include_dir() and; # use_clang() in llvm/utils/lit/lit/llvm/config.py.; # Many uses of this tool will not need an opt binary, because it's only; # needed for updating a test that runs clang | opt | FileCheck. So we; # defer this error message until we find that opt is actually needed.; # TODO Clean up duplication of asm/common build_function_body_dictionary; # Invoke external tool and extract function bodies.; # Build a list of filechecked and non-filechecked R",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/update_cc_test_checks.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/update_cc_test_checks.py
Integrability,depend,depend,"to get the mangled name; # For tests that invoke %clang instead if %clang_cc1 we have to use; # -Xclang -ast-dump=json instead:; # Parse the clang JSON and add all children of type FunctionDecl.; # TODO: Should we add checks for global variables being emitted?; # Recurse for the following nodes that can contain nested function decls:; # Specializations must use the loc from the specialization, not the; # template, and search for the class's spelling as the specialization; # does not mention the method names in the source.; # Otherwise we ignore everything except functions:; # If there is no line it is probably a builtin function -> skip; # If there is no 'inner' object, it is a function declaration and we can; # skip it. However, function declarations may also contain an 'inner' list,; # but in that case it will only contains ParmVarDecls. If we find an entry; # that is not a ParmVarDecl, we know that this is a function definition.; # Determine the builtin includes directory so that we can update tests that; # depend on the builtin headers. See get_clang_builtin_include_dir() and; # use_clang() in llvm/utils/lit/lit/llvm/config.py.; # Many uses of this tool will not need an opt binary, because it's only; # needed for updating a test that runs clang | opt | FileCheck. So we; # defer this error message until we find that opt is actually needed.; # TODO Clean up duplication of asm/common build_function_body_dictionary; # Invoke external tool and extract function bodies.; # Build a list of filechecked and non-filechecked RUN lines.; # Parse executable args.; # Execute non-clang runline.; # Do lit-like substitutions.; # This is a clang runline, apply %clang substitution rule, do lit-like substitutions,; # and append args.clang_args; # Extract -check-prefix in FileCheck args; # Execute non-filechecked clang runline.; # Execute clang, generate LLVM IR, and extract functions.; # Store only filechecked runlines.; # Execute non-filechecked runline.; # Invoke clang -Xclang -ast",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/update_cc_test_checks.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/update_cc_test_checks.py
Modifiability,variab,variables,"#!/usr/bin/env python3; """"""A utility to update LLVM IR CHECK lines in C/C++ FileCheck test files. Example RUN lines in .c/.cc test files:. // RUN: %clang -emit-llvm -S %s -o - -O2 | FileCheck %s; // RUN: %clangxx -emit-llvm -S %s -o - -O2 | FileCheck -check-prefix=CHECK-A %s. Usage:. % utils/update_cc_test_checks.py --llvm-bin=release/bin test/a.cc; % utils/update_cc_test_checks.py --clang=release/bin/clang /tmp/c/a.cc; """"""; # Use clang's JSON AST dump to get the mangled name; # For tests that invoke %clang instead if %clang_cc1 we have to use; # -Xclang -ast-dump=json instead:; # Parse the clang JSON and add all children of type FunctionDecl.; # TODO: Should we add checks for global variables being emitted?; # Recurse for the following nodes that can contain nested function decls:; # Specializations must use the loc from the specialization, not the; # template, and search for the class's spelling as the specialization; # does not mention the method names in the source.; # Otherwise we ignore everything except functions:; # If there is no line it is probably a builtin function -> skip; # If there is no 'inner' object, it is a function declaration and we can; # skip it. However, function declarations may also contain an 'inner' list,; # but in that case it will only contains ParmVarDecls. If we find an entry; # that is not a ParmVarDecl, we know that this is a function definition.; # Determine the builtin includes directory so that we can update tests that; # depend on the builtin headers. See get_clang_builtin_include_dir() and; # use_clang() in llvm/utils/lit/lit/llvm/config.py.; # Many uses of this tool will not need an opt binary, because it's only; # needed for updating a test that runs clang | opt | FileCheck. So we; # defer this error message until we find that opt is actually needed.; # TODO Clean up duplication of asm/common build_function_body_dictionary; # Invoke external tool and extract function bodies.; # Build a list of filechecked and non-filechecked R",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/update_cc_test_checks.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/update_cc_test_checks.py
Testability,test,test,"#!/usr/bin/env python3; """"""A utility to update LLVM IR CHECK lines in C/C++ FileCheck test files. Example RUN lines in .c/.cc test files:. // RUN: %clang -emit-llvm -S %s -o - -O2 | FileCheck %s; // RUN: %clangxx -emit-llvm -S %s -o - -O2 | FileCheck -check-prefix=CHECK-A %s. Usage:. % utils/update_cc_test_checks.py --llvm-bin=release/bin test/a.cc; % utils/update_cc_test_checks.py --clang=release/bin/clang /tmp/c/a.cc; """"""; # Use clang's JSON AST dump to get the mangled name; # For tests that invoke %clang instead if %clang_cc1 we have to use; # -Xclang -ast-dump=json instead:; # Parse the clang JSON and add all children of type FunctionDecl.; # TODO: Should we add checks for global variables being emitted?; # Recurse for the following nodes that can contain nested function decls:; # Specializations must use the loc from the specialization, not the; # template, and search for the class's spelling as the specialization; # does not mention the method names in the source.; # Otherwise we ignore everything except functions:; # If there is no line it is probably a builtin function -> skip; # If there is no 'inner' object, it is a function declaration and we can; # skip it. However, function declarations may also contain an 'inner' list,; # but in that case it will only contains ParmVarDecls. If we find an entry; # that is not a ParmVarDecl, we know that this is a function definition.; # Determine the builtin includes directory so that we can update tests that; # depend on the builtin headers. See get_clang_builtin_include_dir() and; # use_clang() in llvm/utils/lit/lit/llvm/config.py.; # Many uses of this tool will not need an opt binary, because it's only; # needed for updating a test that runs clang | opt | FileCheck. So we; # defer this error message until we find that opt is actually needed.; # TODO Clean up duplication of asm/common build_function_body_dictionary; # Invoke external tool and extract function bodies.; # Build a list of filechecked and non-filechecked R",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/update_cc_test_checks.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/update_cc_test_checks.py
Usability,simpl,simply," we find that opt is actually needed.; # TODO Clean up duplication of asm/common build_function_body_dictionary; # Invoke external tool and extract function bodies.; # Build a list of filechecked and non-filechecked RUN lines.; # Parse executable args.; # Execute non-clang runline.; # Do lit-like substitutions.; # This is a clang runline, apply %clang substitution rule, do lit-like substitutions,; # and append args.clang_args; # Extract -check-prefix in FileCheck args; # Execute non-filechecked clang runline.; # Execute clang, generate LLVM IR, and extract functions.; # Store only filechecked runlines.; # Execute non-filechecked runline.; # Invoke clang -Xclang -ast-dump=json to get mapping from start lines to; # mangled names. Forward all clang args for now.; # Generate the appropriate checks for each function. We need to emit; # these in the order according to the generated output so that CHECK-LABEL; # works properly. func_order provides that.; # It turns out that when clang generates functions (for example, with; # -fopenmp), it can sometimes cause functions to be re-ordered in the; # output, even functions that exist in the source file. Therefore we; # can't insert check lines before each source function and instead have to; # put them at the end. So the first thing to do is dump out the source; # lines.; # Now generate all the checks.; # Normal mode. Put checks before each source function.; # Don't append the existing CHECK lines; # Skip special separator comments added by commmon.add_global_checks.; # One line may contain multiple function declarations.; # Skip if the mangled name has been added before.; # The line number may come from an included file, we simply require; # the search string (normally the function's spelling name, but is; # the class's spelling name for class specializations) to appear on; # the line to exclude functions from other files.; # Remove the comment line since we will generate a new comment; # line as part of common.add_ir_checks()",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/update_cc_test_checks.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/update_cc_test_checks.py
Availability,down,downstream,"#!/usr/bin/env python3; """"""A test case update script. This script is a utility to update LLVM 'llc' based test cases with new; FileCheck patterns. It can either update all of the tests in the file or; a single test function.; """"""; # Used to advertise this file's name (""autogenerated_note"").; # llc is the only llc-like in the LLVM tree but downstream forks can add; # additional ones here if they have them.; # FIXME: We should use multiple check prefixes to common check lines. For; # now, we just ignore all but the last.; # Generate the appropriate checks for each function. We need to emit; # these in the order according to the generated output so that CHECK-LABEL; # works properly. func_order provides that.; # We can't predict where various passes might insert functions so we can't; # be sure the input function order is maintained. Therefore, first spit; # out all the source lines.; # Now generate all the checks.; # Print out the various check lines here.; # This input line of the function body will go as-is into the output.; # If it's outside a function, it just gets copied to the output.; # When filtering on a specific function, skip all others.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/update_llc_test_checks.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/update_llc_test_checks.py
Deployability,update,update,"#!/usr/bin/env python3; """"""A test case update script. This script is a utility to update LLVM 'llc' based test cases with new; FileCheck patterns. It can either update all of the tests in the file or; a single test function.; """"""; # Used to advertise this file's name (""autogenerated_note"").; # llc is the only llc-like in the LLVM tree but downstream forks can add; # additional ones here if they have them.; # FIXME: We should use multiple check prefixes to common check lines. For; # now, we just ignore all but the last.; # Generate the appropriate checks for each function. We need to emit; # these in the order according to the generated output so that CHECK-LABEL; # works properly. func_order provides that.; # We can't predict where various passes might insert functions so we can't; # be sure the input function order is maintained. Therefore, first spit; # out all the source lines.; # Now generate all the checks.; # Print out the various check lines here.; # This input line of the function body will go as-is into the output.; # If it's outside a function, it just gets copied to the output.; # When filtering on a specific function, skip all others.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/update_llc_test_checks.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/update_llc_test_checks.py
Safety,predict,predict,"#!/usr/bin/env python3; """"""A test case update script. This script is a utility to update LLVM 'llc' based test cases with new; FileCheck patterns. It can either update all of the tests in the file or; a single test function.; """"""; # Used to advertise this file's name (""autogenerated_note"").; # llc is the only llc-like in the LLVM tree but downstream forks can add; # additional ones here if they have them.; # FIXME: We should use multiple check prefixes to common check lines. For; # now, we just ignore all but the last.; # Generate the appropriate checks for each function. We need to emit; # these in the order according to the generated output so that CHECK-LABEL; # works properly. func_order provides that.; # We can't predict where various passes might insert functions so we can't; # be sure the input function order is maintained. Therefore, first spit; # out all the source lines.; # Now generate all the checks.; # Print out the various check lines here.; # This input line of the function body will go as-is into the output.; # If it's outside a function, it just gets copied to the output.; # When filtering on a specific function, skip all others.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/update_llc_test_checks.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/update_llc_test_checks.py
Testability,test,test,"#!/usr/bin/env python3; """"""A test case update script. This script is a utility to update LLVM 'llc' based test cases with new; FileCheck patterns. It can either update all of the tests in the file or; a single test function.; """"""; # Used to advertise this file's name (""autogenerated_note"").; # llc is the only llc-like in the LLVM tree but downstream forks can add; # additional ones here if they have them.; # FIXME: We should use multiple check prefixes to common check lines. For; # now, we just ignore all but the last.; # Generate the appropriate checks for each function. We need to emit; # these in the order according to the generated output so that CHECK-LABEL; # works properly. func_order provides that.; # We can't predict where various passes might insert functions so we can't; # be sure the input function order is maintained. Therefore, first spit; # out all the source lines.; # Now generate all the checks.; # Print out the various check lines here.; # This input line of the function body will go as-is into the output.; # If it's outside a function, it just gets copied to the output.; # When filtering on a specific function, skip all others.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/update_llc_test_checks.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/update_llc_test_checks.py
Availability,down,down,"#!/usr/bin/env python3; """"""A test case update script. This script is a utility to update LLVM 'llvm-mca' based test cases with new; FileCheck patterns.; """"""; """"""Generic Error that can be raised without printing a traceback.""""""; """"""Log a user warning to stderr.""""""; """"""Version of warnings.showwarning that won't attempt to print out the; line at the location of the warning if the line text is not explicitly; specified.; """"""; """"""Given a block_info, see if we can analyze it further to let us break it; down by prefix per-line rather than per-block.; """"""; # Split the lines from each of the incoming block_texts and zip them so that; # each element contains the corresponding lines from each text. E.g.; #; # block_text_1: A # line 1; # B # line 2; #; # block_text_2: A # line 1; # C # line 2; #; # would become:; #; # [(A, A), # line 1; # (B, C)] # line 2; #; # To simplify output, we'll only proceed if the very first line of the block; # texts is common to each of them.; # We're about to output a line with the common prefix. This is a sync; # point so flush any batched-up lines one prefix at a time to the output; # first.; # The line is common to each block so output with the common prefix.; # The line is not common to each block, or we don't have a common prefix.; # If there are no prefixes available, warn and bail out.; # Iterate through the line from each of the blocks and add the line with; # the corresponding prefix to the current batch of results so that we can; # later output them per-prefix.; # Flush any remaining batched-up lines one prefix at a time to the output.; """"""Given the run_infos, calculate any prefixes that are common to every one,; and the length of the longest prefix string.; """"""; """"""Some sub-sequences of blocks may be common to multiple lists of blocks,; but at different indexes in each one. For example, in the following case, A,B,E,F, and H are common to both; sets, but only A and B would be identified as such due to the indexes; matching:. index | 0 1 2 3",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/update_mca_test_checks.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/update_mca_test_checks.py
Deployability,update,update,"#!/usr/bin/env python3; """"""A test case update script. This script is a utility to update LLVM 'llvm-mca' based test cases with new; FileCheck patterns.; """"""; """"""Generic Error that can be raised without printing a traceback.""""""; """"""Log a user warning to stderr.""""""; """"""Version of warnings.showwarning that won't attempt to print out the; line at the location of the warning if the line text is not explicitly; specified.; """"""; """"""Given a block_info, see if we can analyze it further to let us break it; down by prefix per-line rather than per-block.; """"""; # Split the lines from each of the incoming block_texts and zip them so that; # each element contains the corresponding lines from each text. E.g.; #; # block_text_1: A # line 1; # B # line 2; #; # block_text_2: A # line 1; # C # line 2; #; # would become:; #; # [(A, A), # line 1; # (B, C)] # line 2; #; # To simplify output, we'll only proceed if the very first line of the block; # texts is common to each of them.; # We're about to output a line with the common prefix. This is a sync; # point so flush any batched-up lines one prefix at a time to the output; # first.; # The line is common to each block so output with the common prefix.; # The line is not common to each block, or we don't have a common prefix.; # If there are no prefixes available, warn and bail out.; # Iterate through the line from each of the blocks and add the line with; # the corresponding prefix to the current batch of results so that we can; # later output them per-prefix.; # Flush any remaining batched-up lines one prefix at a time to the output.; """"""Given the run_infos, calculate any prefixes that are common to every one,; and the length of the longest prefix string.; """"""; """"""Some sub-sequences of blocks may be common to multiple lists of blocks,; but at different indexes in each one. For example, in the following case, A,B,E,F, and H are common to both; sets, but only A and B would be identified as such due to the indexes; matching:. index | 0 1 2 3",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/update_mca_test_checks.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/update_mca_test_checks.py
Performance,cache,cache,") then insert empty blocks until the index; # matches the farthest index identified for that block.; # Bail out. We'll need to re-do the farthest block analysis now that; # we've inserted some blocks.; # noqa; """"""For each run line, run the tool with the specified args and collect the; output. We use the concept of 'blocks' for uniquing, where a block is; a series of lines of text with no more than one newline character between; each one. For example:. This; is; one; block. This is; another block. This is yet another block. We then build up a 'block_infos' structure containing a dict where the; text of each block is the key and a list of the sets of prefixes that may; generate that particular block. This then goes through a series of; transformations to minimise the amount of CHECK lines that need to be; written by taking advantage of common prefixes.; """"""; """"""Get a hashable key based on the current tool_args and prefixes.""""""; # A cache of the furthest-back position in any block list of the first; # instance of each block, indexed by the block itself.; # Run the tool for each run line to generate all of the blocks.; # Replace any lines consisting of purely whitespace with empty lines.; # Split blocks, stripping all trailing whitespace, but keeping preceding; # whitespace except for newlines so that columns will line up visually.; # Attempt to align matching blocks until no more changes can be made.; # If necessary, pad the lists of blocks with empty blocks so that they are; # all the same length.; # Create the block_infos structure where it is a nested dict in the form of:; # block number -> block text -> list of prefix sets; # Now go through the block_infos structure and attempt to smartly prune the; # number of prefixes per block to the minimal set possible to output.; # When there are multiple block texts for a block num, remove any; # prefixes that are common to more than one of them.; # E.g. [ [{ALL,FOO}] , [{ALL,BAR}] ] -> [ [{FOO}] , [{BAR}] ]; # When a block t",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/update_mca_test_checks.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/update_mca_test_checks.py
Safety,avoid,avoid,"ad the lists of blocks with empty blocks so that they are; # all the same length.; # Create the block_infos structure where it is a nested dict in the form of:; # block number -> block text -> list of prefix sets; # Now go through the block_infos structure and attempt to smartly prune the; # number of prefixes per block to the minimal set possible to output.; # When there are multiple block texts for a block num, remove any; # prefixes that are common to more than one of them.; # E.g. [ [{ALL,FOO}] , [{ALL,BAR}] ] -> [ [{FOO}] , [{BAR}] ]; # When a block text matches multiple sets of prefixes, try removing any; # prefixes that aren't common to all of them.; # E.g. [ {ALL,FOO} , {ALL,BAR} ] -> [{ALL}]; # Everything should be uniqued as much as possible by now. Apply the; # newly pruned sets to the block_infos structure.; # If there are any blocks of text that still match multiple prefixes,; # output a warning.; # If we have multiple block_texts, try to break them down further to avoid; # the case where we have very similar block_texts repeated after each; # other.; # We'll only attempt this if each of the block_texts have the same number; # of lines as each other.; """"""Write an individual block, with correct padding on the prefixes.; Returns a set of all of the prefixes that it has written.; """"""; # If the previous line isn't already blank and we're writing more than one; # line for the current prefix output a blank line first, unless either the; # current of previous prefix is common to all.; # noqa; # This input line of the function body will go as-is into the output.; # Except make leading whitespace uniform: 2 spaces.; # Skip empty lines if the previous output line is also empty.; # Add a blank line before the new checks if required.; # The block is of the type output from _break_down_block().; # _break_down_block() was unable to do do anything so output the block; # as-is.; # Rather than writing out each block as soon we encounter it, save it; # indexed by prefix s",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/update_mca_test_checks.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/update_mca_test_checks.py
Security,hash,hashable,"; # If the block has not already been subject to alignment (i.e. if the; # previous block is not empty) then insert empty blocks until the index; # matches the farthest index identified for that block.; # Bail out. We'll need to re-do the farthest block analysis now that; # we've inserted some blocks.; # noqa; """"""For each run line, run the tool with the specified args and collect the; output. We use the concept of 'blocks' for uniquing, where a block is; a series of lines of text with no more than one newline character between; each one. For example:. This; is; one; block. This is; another block. This is yet another block. We then build up a 'block_infos' structure containing a dict where the; text of each block is the key and a list of the sets of prefixes that may; generate that particular block. This then goes through a series of; transformations to minimise the amount of CHECK lines that need to be; written by taking advantage of common prefixes.; """"""; """"""Get a hashable key based on the current tool_args and prefixes.""""""; # A cache of the furthest-back position in any block list of the first; # instance of each block, indexed by the block itself.; # Run the tool for each run line to generate all of the blocks.; # Replace any lines consisting of purely whitespace with empty lines.; # Split blocks, stripping all trailing whitespace, but keeping preceding; # whitespace except for newlines so that columns will line up visually.; # Attempt to align matching blocks until no more changes can be made.; # If necessary, pad the lists of blocks with empty blocks so that they are; # all the same length.; # Create the block_infos structure where it is a nested dict in the form of:; # block number -> block text -> list of prefix sets; # Now go through the block_infos structure and attempt to smartly prune the; # number of prefixes per block to the minimal set possible to output.; # When there are multiple block texts for a block num, remove any; # prefixes that are common to m",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/update_mca_test_checks.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/update_mca_test_checks.py
Testability,test,test,"#!/usr/bin/env python3; """"""A test case update script. This script is a utility to update LLVM 'llvm-mca' based test cases with new; FileCheck patterns.; """"""; """"""Generic Error that can be raised without printing a traceback.""""""; """"""Log a user warning to stderr.""""""; """"""Version of warnings.showwarning that won't attempt to print out the; line at the location of the warning if the line text is not explicitly; specified.; """"""; """"""Given a block_info, see if we can analyze it further to let us break it; down by prefix per-line rather than per-block.; """"""; # Split the lines from each of the incoming block_texts and zip them so that; # each element contains the corresponding lines from each text. E.g.; #; # block_text_1: A # line 1; # B # line 2; #; # block_text_2: A # line 1; # C # line 2; #; # would become:; #; # [(A, A), # line 1; # (B, C)] # line 2; #; # To simplify output, we'll only proceed if the very first line of the block; # texts is common to each of them.; # We're about to output a line with the common prefix. This is a sync; # point so flush any batched-up lines one prefix at a time to the output; # first.; # The line is common to each block so output with the common prefix.; # The line is not common to each block, or we don't have a common prefix.; # If there are no prefixes available, warn and bail out.; # Iterate through the line from each of the blocks and add the line with; # the corresponding prefix to the current batch of results so that we can; # later output them per-prefix.; # Flush any remaining batched-up lines one prefix at a time to the output.; """"""Given the run_infos, calculate any prefixes that are common to every one,; and the length of the longest prefix string.; """"""; """"""Some sub-sequences of blocks may be common to multiple lists of blocks,; but at different indexes in each one. For example, in the following case, A,B,E,F, and H are common to both; sets, but only A and B would be identified as such due to the indexes; matching:. index | 0 1 2 3",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/update_mca_test_checks.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/update_mca_test_checks.py
Usability,simpl,simplify,"#!/usr/bin/env python3; """"""A test case update script. This script is a utility to update LLVM 'llvm-mca' based test cases with new; FileCheck patterns.; """"""; """"""Generic Error that can be raised without printing a traceback.""""""; """"""Log a user warning to stderr.""""""; """"""Version of warnings.showwarning that won't attempt to print out the; line at the location of the warning if the line text is not explicitly; specified.; """"""; """"""Given a block_info, see if we can analyze it further to let us break it; down by prefix per-line rather than per-block.; """"""; # Split the lines from each of the incoming block_texts and zip them so that; # each element contains the corresponding lines from each text. E.g.; #; # block_text_1: A # line 1; # B # line 2; #; # block_text_2: A # line 1; # C # line 2; #; # would become:; #; # [(A, A), # line 1; # (B, C)] # line 2; #; # To simplify output, we'll only proceed if the very first line of the block; # texts is common to each of them.; # We're about to output a line with the common prefix. This is a sync; # point so flush any batched-up lines one prefix at a time to the output; # first.; # The line is common to each block so output with the common prefix.; # The line is not common to each block, or we don't have a common prefix.; # If there are no prefixes available, warn and bail out.; # Iterate through the line from each of the blocks and add the line with; # the corresponding prefix to the current batch of results so that we can; # later output them per-prefix.; # Flush any remaining batched-up lines one prefix at a time to the output.; """"""Given the run_infos, calculate any prefixes that are common to every one,; and the length of the longest prefix string.; """"""; """"""Some sub-sequences of blocks may be common to multiple lists of blocks,; but at different indexes in each one. For example, in the following case, A,B,E,F, and H are common to both; sets, but only A and B would be identified as such due to the indexes; matching:. index | 0 1 2 3",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/update_mca_test_checks.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/update_mca_test_checks.py
Deployability,update,update,"#!/usr/bin/env python3; """"""Updates FileCheck checks in MIR tests. This script is a utility to update MIR based tests with new FileCheck; patterns. The checks added by this script will cover the entire body of each; function it handles. Virtual registers used are given names via; FileCheck patterns, so if you do want to check a subset of the body it; should be straightforward to trim out the irrelevant parts. None of; the YAML metadata will be checked, other than function names, and fixedStack; if the --print-fixed-stack option is used. If there are multiple llc commands in a test, the full set of checks; will be repeated for each different check pattern. Checks for patterns; that are common between different commands will be left as-is by; default, or removed if the --remove-common-prefixes flag is provided.; """"""; # Fix line endings to unix CR style.; # If we find -march but not -mtriple, use that.; # Sort prefixes that are shared between run lines before unshared prefixes.; # This causes us to prefer printing shared prefixes.; # Vreg mangling; # Add some space between different check prefixes.; # Don't bother checking the basic block label for a single BB; # A check comment, indented the appropriate amount; # The mir printer prints leading whitespace so we can't use CHECK-EMPTY:; # Simplify some common prefixes and suffixes; # Shorten some common opcodes with long-ish names; # Avoid ambiguity when opcodes end in numbers; # Skip any check lines that we're handling as well as comments; # If there's only one block, put the checks inside it",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/update_mir_test_checks.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/update_mir_test_checks.py
Testability,test,tests,"#!/usr/bin/env python3; """"""Updates FileCheck checks in MIR tests. This script is a utility to update MIR based tests with new FileCheck; patterns. The checks added by this script will cover the entire body of each; function it handles. Virtual registers used are given names via; FileCheck patterns, so if you do want to check a subset of the body it; should be straightforward to trim out the irrelevant parts. None of; the YAML metadata will be checked, other than function names, and fixedStack; if the --print-fixed-stack option is used. If there are multiple llc commands in a test, the full set of checks; will be repeated for each different check pattern. Checks for patterns; that are common between different commands will be left as-is by; default, or removed if the --remove-common-prefixes flag is provided.; """"""; # Fix line endings to unix CR style.; # If we find -march but not -mtriple, use that.; # Sort prefixes that are shared between run lines before unshared prefixes.; # This causes us to prefer printing shared prefixes.; # Vreg mangling; # Add some space between different check prefixes.; # Don't bother checking the basic block label for a single BB; # A check comment, indented the appropriate amount; # The mir printer prints leading whitespace so we can't use CHECK-EMPTY:; # Simplify some common prefixes and suffixes; # Shorten some common opcodes with long-ish names; # Avoid ambiguity when opcodes end in numbers; # Skip any check lines that we're handling as well as comments; # If there's only one block, put the checks inside it",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/update_mir_test_checks.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/update_mir_test_checks.py
Deployability,update,update,"#!/usr/bin/env python3; """"""A script to generate FileCheck statements for 'opt' regression tests. This script is a utility to update LLVM opt test cases with new; FileCheck patterns. It can either update all of the tests in the file or; a single test function. Example usage:. # Default to using `opt` as found in your PATH.; $ update_test_checks.py test/foo.ll. # Override the path lookup.; $ update_test_checks.py --tool-binary=../bin/opt test/foo.ll. # Use a custom tool instead of `opt`.; $ update_test_checks.py --tool=yourtool test/foo.ll. Workflow:; 1. Make a compiler patch that requires updating some number of FileCheck lines; in regression test files.; 2. Save the patch and revert it from your local work area.; 3. Update the RUN-lines in the affected regression tests to look canonical.; Example: ""; RUN: opt < %s -instcombine -S | FileCheck %s""; 4. Refresh the FileCheck lines for either the entire file or select functions by; running this script.; 5. Commit the fresh baseline of checks.; 6. Apply your patch from step 1 and rebuild your local binaries.; 7. Re-run this script on affected regression tests.; 8. Check the diffs to ensure the script has done something reasonable.; 9. Submit a patch including the regression test diffs for review.; """"""; # Used to advertise this file's name (""autogenerated_note"").; # If requested we scrub trailing attribute annotations, e.g., '#0', together with whitespaces; # FIXME: We should use multiple check prefixes to common check lines. For; # now, we just ignore all but the last.; # Generate the appropriate checks for each function. We need to emit; # these in the order according to the generated output so that CHECK-LABEL; # works properly. func_order provides that.; # We can't predict where various passes might insert functions so we can't; # be sure the input function order is maintained. Therefore, first spit; # out all the source lines.; # Now generate all the checks.; # ""Normal"" mode.; # Print out the various check lines here.;",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/update_test_checks.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/update_test_checks.py
Safety,predict,predict,"t can either update all of the tests in the file or; a single test function. Example usage:. # Default to using `opt` as found in your PATH.; $ update_test_checks.py test/foo.ll. # Override the path lookup.; $ update_test_checks.py --tool-binary=../bin/opt test/foo.ll. # Use a custom tool instead of `opt`.; $ update_test_checks.py --tool=yourtool test/foo.ll. Workflow:; 1. Make a compiler patch that requires updating some number of FileCheck lines; in regression test files.; 2. Save the patch and revert it from your local work area.; 3. Update the RUN-lines in the affected regression tests to look canonical.; Example: ""; RUN: opt < %s -instcombine -S | FileCheck %s""; 4. Refresh the FileCheck lines for either the entire file or select functions by; running this script.; 5. Commit the fresh baseline of checks.; 6. Apply your patch from step 1 and rebuild your local binaries.; 7. Re-run this script on affected regression tests.; 8. Check the diffs to ensure the script has done something reasonable.; 9. Submit a patch including the regression test diffs for review.; """"""; # Used to advertise this file's name (""autogenerated_note"").; # If requested we scrub trailing attribute annotations, e.g., '#0', together with whitespaces; # FIXME: We should use multiple check prefixes to common check lines. For; # now, we just ignore all but the last.; # Generate the appropriate checks for each function. We need to emit; # these in the order according to the generated output so that CHECK-LABEL; # works properly. func_order provides that.; # We can't predict where various passes might insert functions so we can't; # be sure the input function order is maintained. Therefore, first spit; # out all the source lines.; # Now generate all the checks.; # ""Normal"" mode.; # Print out the various check lines here.; # This input line of the function body will go as-is into the output.; # Except make leading whitespace uniform: 2 spaces.; # When filtering on a specific function, skip all others.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/update_test_checks.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/update_test_checks.py
Testability,test,tests,"#!/usr/bin/env python3; """"""A script to generate FileCheck statements for 'opt' regression tests. This script is a utility to update LLVM opt test cases with new; FileCheck patterns. It can either update all of the tests in the file or; a single test function. Example usage:. # Default to using `opt` as found in your PATH.; $ update_test_checks.py test/foo.ll. # Override the path lookup.; $ update_test_checks.py --tool-binary=../bin/opt test/foo.ll. # Use a custom tool instead of `opt`.; $ update_test_checks.py --tool=yourtool test/foo.ll. Workflow:; 1. Make a compiler patch that requires updating some number of FileCheck lines; in regression test files.; 2. Save the patch and revert it from your local work area.; 3. Update the RUN-lines in the affected regression tests to look canonical.; Example: ""; RUN: opt < %s -instcombine -S | FileCheck %s""; 4. Refresh the FileCheck lines for either the entire file or select functions by; running this script.; 5. Commit the fresh baseline of checks.; 6. Apply your patch from step 1 and rebuild your local binaries.; 7. Re-run this script on affected regression tests.; 8. Check the diffs to ensure the script has done something reasonable.; 9. Submit a patch including the regression test diffs for review.; """"""; # Used to advertise this file's name (""autogenerated_note"").; # If requested we scrub trailing attribute annotations, e.g., '#0', together with whitespaces; # FIXME: We should use multiple check prefixes to common check lines. For; # now, we just ignore all but the last.; # Generate the appropriate checks for each function. We need to emit; # these in the order according to the generated output so that CHECK-LABEL; # works properly. func_order provides that.; # We can't predict where various passes might insert functions so we can't; # be sure the input function order is maintained. Therefore, first spit; # out all the source lines.; # Now generate all the checks.; # ""Normal"" mode.; # Print out the various check lines here.;",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/update_test_checks.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/update_test_checks.py
Usability,simpl,simplistic,"#!/usr/bin/env python; """"""; wciia - Whose Code Is It Anyway. Determines code owner of the file/folder relative to the llvm source root.; Code owner is determined from the content of the CODE_OWNERS.TXT ; by parsing the D: field. usage:. utils/wciia.py path. limitations:; - must be run from llvm source root; - very simplistic algorithm; - only handles * as a wildcard; - not very user friendly ; - does not handle the proposed F: field. """"""; # paths must be in ( ... ) so strip them; # give up; # split paths; # 		print ""F: field missing, using D: field""; # process CODE_OWNERS.TXT first; # reset the values; # very simplistic way of findning the best match; # 				print ""searching ("" + path + "")""; # try exact match; # see if path ends with a *; # try the longest match,; # now lest try to find the owner of the file or folder; # the path we are checking; # check if this is real path; # be grammatically correct; # bottom up walk of the current .; # not yet used",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/wciia.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/wciia.py
Security,checksum,checksums,"#!/usr/bin/env python; """""" A small program to compute checksums of LLVM checkout.; """"""; """"""Compute checksums for LLVM sources checked out using svn. Args:; root_path: a directory of llvm checkout.; projects: a list of LLVMProject instances, which describe checkout paths,; relative to root_path. Returns:; A dict mapping from project name to project checksum.; """"""; # Replace svn substitutions for $Date$ and $LastChangedDate$.; # Unfortunately, these are locale-specific.; # Hash each project.; # Compute final checksum.; """"""Writes checksums to a text file. Args:; checksums: a dict mapping from project name to project checksum (result of; ComputeLLVMChecksums).; f: a file object to write into.; """"""; """"""Reads checksums from a text file, produced by WriteLLVMChecksums. Returns:; A dict, mapping from project name to project checksum.; """"""; """"""Validates that reference_checksums and new_checksums match. Args:; reference_checksums: a dict of reference checksums, mapping from a project; name to a project checksum.; new_checksums: a dict of checksums to be checked, mapping from a project; name to a project checksum.; allow_missing_projects:; When True, reference_checksums may contain more projects than; new_checksums. Projects missing from new_checksums are ignored.; When False, new_checksums and reference_checksums must contain checksums; for the same set of projects. If there is a project in; reference_checksums, missing from new_checksums, ValidateChecksums; will return False. Returns:; True, if checksums match with regards to allow_missing_projects flag value.; False, otherwise.; """"""; # We never computed a checksum for this project.; # Checksum did not match.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/docker/scripts/llvm_checksum/llvm_checksum.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/docker/scripts/llvm_checksum/llvm_checksum.py
Security,checksum,checksums,"""""""Contains helper functions to compute checksums for LLVM checkouts.; """"""; """"""An LLVM project with a descriptive name and a relative checkout path.""""""; """"""Check if self is checked out as a subdirectory of other_project.""""""; """"""Walk over all files inside a project without recursing into subprojects, '.git' and '.svn' subfolders. checkout_root: root of the LLVM checkout.; all_projects: projects in the LLVM checkout.; project: a project to walk the files of. Must be inside all_projects.; visitor: a function called on each visited file.; """"""; """"""Returns a list of LLVMProject instances, describing relative paths of a typical LLVM checkout. Args:; single_tree_checkout:; When True, relative paths for each project points to a typical single; source tree checkout.; When False, relative paths for each projects points to a separate; directory. However, clang-tools-extra is an exception, its relative path; will always be 'clang/tools/extra'.; """"""; # FIXME: cover all of llvm projects.; # Projects that reside inside 'projects/' in a single source tree checkout.; # Projects that reside inside 'tools/' in a single source tree checkout.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/docker/scripts/llvm_checksum/project_tree.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/docker/scripts/llvm_checksum/project_tree.py
Safety,detect,detects,"# ===----------------------------------------------------------------------===##; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ===----------------------------------------------------------------------===##; """"""A linter that detects potential typos in FileCheck directive names. Consider a broken test foo.cpp:. // RUN: clang -cc1 -ast-dump %s | FileCheck %s --check-prefix=NEW; // RUN: clang -cc1 -ast-dump %s -std=c++98 | FileCheck %s --check-prefix=OLD; auto x = 42;; // NEWW: auto is a c++11 extension; // ODL-NOT: auto is a c++11 extension. We first detect the locally valid FileCheck directive prefixes by parsing the; --check-prefix flags. Here we get {CHECK, NEW, OLD}, so our directive names are; {CHECK, NEW, OLD, CHECK-NOT, NEW-NOT, ...}. Then we look for lines that look like directives. These are of the form 'FOO:',; usually at the beginning of a line or a comment. If any of these are a; ""near-miss"" for a directive name, then we suspect this is a typo and report it. Usage: filecheck_lint path/to/test/file/1 ... path/to/test/file/n; """"""; # 'NOTE' and 'TODO' are not directives, but are likely to be false positives; # if encountered and to generate noise as a result. We filter them out also to; # avoid this.; # 'COM' and 'RUN' are default comment prefixes for FileCheck.; # pylint: disable=g-doc-args; """"""Computes the edit distance between two strings. Additions, deletions, and substitutions all count as a single operation.; """"""; """"""Stores the coordinates of a span on a single line within a file. Attributes:; line: the line number; start_column: the (inclusive) column where the span starts; end_column: the (inclusive) column where the span ends; """"""; # pylint: disable=g-doc-args; """"""Derives a span's coordinates based on a string and start/end bytes. `start_byte` and `end_byte` are assumed to be on the same lin",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/filecheck_lint/filecheck_lint.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/filecheck_lint/filecheck_lint.py
Testability,test,test,"# ===----------------------------------------------------------------------===##; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ===----------------------------------------------------------------------===##; """"""A linter that detects potential typos in FileCheck directive names. Consider a broken test foo.cpp:. // RUN: clang -cc1 -ast-dump %s | FileCheck %s --check-prefix=NEW; // RUN: clang -cc1 -ast-dump %s -std=c++98 | FileCheck %s --check-prefix=OLD; auto x = 42;; // NEWW: auto is a c++11 extension; // ODL-NOT: auto is a c++11 extension. We first detect the locally valid FileCheck directive prefixes by parsing the; --check-prefix flags. Here we get {CHECK, NEW, OLD}, so our directive names are; {CHECK, NEW, OLD, CHECK-NOT, NEW-NOT, ...}. Then we look for lines that look like directives. These are of the form 'FOO:',; usually at the beginning of a line or a comment. If any of these are a; ""near-miss"" for a directive name, then we suspect this is a typo and report it. Usage: filecheck_lint path/to/test/file/1 ... path/to/test/file/n; """"""; # 'NOTE' and 'TODO' are not directives, but are likely to be false positives; # if encountered and to generate noise as a result. We filter them out also to; # avoid this.; # 'COM' and 'RUN' are default comment prefixes for FileCheck.; # pylint: disable=g-doc-args; """"""Computes the edit distance between two strings. Additions, deletions, and substitutions all count as a single operation.; """"""; """"""Stores the coordinates of a span on a single line within a file. Attributes:; line: the line number; start_column: the (inclusive) column where the span starts; end_column: the (inclusive) column where the span ends; """"""; # pylint: disable=g-doc-args; """"""Derives a span's coordinates based on a string and start/end bytes. `start_byte` and `end_byte` are assumed to be on the same lin",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/filecheck_lint/filecheck_lint.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/filecheck_lint/filecheck_lint.py
Availability,error,error,"""""""Print an llvm::SmallString object.""""""; """"""Print an llvm::StringRef object.""""""; """"""Print an llvm::SmallVector object.""""""; """"""Print an llvm::ArrayRef object.""""""; """"""Print an llvm::Expected object.""""""; """"""Print an llvm::Optional object.""""""; # disabled until the comments below can be addressed; # keeping as notes/posterity/hints for future contributors; # the following is invalid, GDB fails with:; # Python Exception <class 'gdb.error'> Attempt to take address of value; # not located in memory.; # because isEqual took parameter (for the unsigned long key I was testing); # by const ref, and GDB; # It's also not entirely general - we should be accessing the ""getFirst()""; # member function, not the 'first' member variable, but I've yet to figure; # out how to find/call member functions (especially (const) overloaded; # ones) on a gdb.Value.; """"""Lookup the default pretty-printer for val and use it. If no pretty-printer is defined for the type of val, print an error and; return a placeholder string.""""""; # The pretty-printer may return a LazyString instead of an actual Python; # string. Convert it to a Python string. However, GDB doesn't seem to; # register the LazyString type, so we can't check; # ""type(s) == gdb.LazyString"".; # apparently some GDB versions add the NodeKind:: namespace; # (happens for me on GDB 7.11); """"""Return the string representation of the Twine::Child child.""""""; """"""Return the string representation of the Twine object twine.""""""; """"""Get tuple from llvm::PointerIntPair.""""""; # Note: this throws a gdb.error if the info type is not used (by means of a; # call to getPointer() or similar) in the current translation unit.; """"""Print a PointerIntPair.""""""; """"""Factory for an llvm::PointerIntPair printer.""""""; # If PointerIntPair cannot be analyzed, print as raw value.; """"""Print a PointerUnion.""""""; """"""Factory for an llvm::PointerUnion printer.""""""; # If PointerIntPair cannot be analyzed, print as raw value.; """"""Print an llvm::ilist_node object.""""""; # One of Prev and P",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/gdb-scripts/prettyprinters.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/gdb-scripts/prettyprinters.py
Modifiability,variab,variable,"""""""Print an llvm::SmallString object.""""""; """"""Print an llvm::StringRef object.""""""; """"""Print an llvm::SmallVector object.""""""; """"""Print an llvm::ArrayRef object.""""""; """"""Print an llvm::Expected object.""""""; """"""Print an llvm::Optional object.""""""; # disabled until the comments below can be addressed; # keeping as notes/posterity/hints for future contributors; # the following is invalid, GDB fails with:; # Python Exception <class 'gdb.error'> Attempt to take address of value; # not located in memory.; # because isEqual took parameter (for the unsigned long key I was testing); # by const ref, and GDB; # It's also not entirely general - we should be accessing the ""getFirst()""; # member function, not the 'first' member variable, but I've yet to figure; # out how to find/call member functions (especially (const) overloaded; # ones) on a gdb.Value.; """"""Lookup the default pretty-printer for val and use it. If no pretty-printer is defined for the type of val, print an error and; return a placeholder string.""""""; # The pretty-printer may return a LazyString instead of an actual Python; # string. Convert it to a Python string. However, GDB doesn't seem to; # register the LazyString type, so we can't check; # ""type(s) == gdb.LazyString"".; # apparently some GDB versions add the NodeKind:: namespace; # (happens for me on GDB 7.11); """"""Return the string representation of the Twine::Child child.""""""; """"""Return the string representation of the Twine object twine.""""""; """"""Get tuple from llvm::PointerIntPair.""""""; # Note: this throws a gdb.error if the info type is not used (by means of a; # call to getPointer() or similar) in the current translation unit.; """"""Print a PointerIntPair.""""""; """"""Factory for an llvm::PointerIntPair printer.""""""; # If PointerIntPair cannot be analyzed, print as raw value.; """"""Print a PointerUnion.""""""; """"""Factory for an llvm::PointerUnion printer.""""""; # If PointerIntPair cannot be analyzed, print as raw value.; """"""Print an llvm::ilist_node object.""""""; # One of Prev and P",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/gdb-scripts/prettyprinters.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/gdb-scripts/prettyprinters.py
Security,access,accessing,"""""""Print an llvm::SmallString object.""""""; """"""Print an llvm::StringRef object.""""""; """"""Print an llvm::SmallVector object.""""""; """"""Print an llvm::ArrayRef object.""""""; """"""Print an llvm::Expected object.""""""; """"""Print an llvm::Optional object.""""""; # disabled until the comments below can be addressed; # keeping as notes/posterity/hints for future contributors; # the following is invalid, GDB fails with:; # Python Exception <class 'gdb.error'> Attempt to take address of value; # not located in memory.; # because isEqual took parameter (for the unsigned long key I was testing); # by const ref, and GDB; # It's also not entirely general - we should be accessing the ""getFirst()""; # member function, not the 'first' member variable, but I've yet to figure; # out how to find/call member functions (especially (const) overloaded; # ones) on a gdb.Value.; """"""Lookup the default pretty-printer for val and use it. If no pretty-printer is defined for the type of val, print an error and; return a placeholder string.""""""; # The pretty-printer may return a LazyString instead of an actual Python; # string. Convert it to a Python string. However, GDB doesn't seem to; # register the LazyString type, so we can't check; # ""type(s) == gdb.LazyString"".; # apparently some GDB versions add the NodeKind:: namespace; # (happens for me on GDB 7.11); """"""Return the string representation of the Twine::Child child.""""""; """"""Return the string representation of the Twine object twine.""""""; """"""Get tuple from llvm::PointerIntPair.""""""; # Note: this throws a gdb.error if the info type is not used (by means of a; # call to getPointer() or similar) in the current translation unit.; """"""Print a PointerIntPair.""""""; """"""Factory for an llvm::PointerIntPair printer.""""""; # If PointerIntPair cannot be analyzed, print as raw value.; """"""Print a PointerUnion.""""""; """"""Factory for an llvm::PointerUnion printer.""""""; # If PointerIntPair cannot be analyzed, print as raw value.; """"""Print an llvm::ilist_node object.""""""; # One of Prev and P",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/gdb-scripts/prettyprinters.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/gdb-scripts/prettyprinters.py
Testability,test,testing,"""""""Print an llvm::SmallString object.""""""; """"""Print an llvm::StringRef object.""""""; """"""Print an llvm::SmallVector object.""""""; """"""Print an llvm::ArrayRef object.""""""; """"""Print an llvm::Expected object.""""""; """"""Print an llvm::Optional object.""""""; # disabled until the comments below can be addressed; # keeping as notes/posterity/hints for future contributors; # the following is invalid, GDB fails with:; # Python Exception <class 'gdb.error'> Attempt to take address of value; # not located in memory.; # because isEqual took parameter (for the unsigned long key I was testing); # by const ref, and GDB; # It's also not entirely general - we should be accessing the ""getFirst()""; # member function, not the 'first' member variable, but I've yet to figure; # out how to find/call member functions (especially (const) overloaded; # ones) on a gdb.Value.; """"""Lookup the default pretty-printer for val and use it. If no pretty-printer is defined for the type of val, print an error and; return a placeholder string.""""""; # The pretty-printer may return a LazyString instead of an actual Python; # string. Convert it to a Python string. However, GDB doesn't seem to; # register the LazyString type, so we can't check; # ""type(s) == gdb.LazyString"".; # apparently some GDB versions add the NodeKind:: namespace; # (happens for me on GDB 7.11); """"""Return the string representation of the Twine::Child child.""""""; """"""Return the string representation of the Twine object twine.""""""; """"""Get tuple from llvm::PointerIntPair.""""""; # Note: this throws a gdb.error if the info type is not used (by means of a; # call to getPointer() or similar) in the current translation unit.; """"""Print a PointerIntPair.""""""; """"""Factory for an llvm::PointerIntPair printer.""""""; # If PointerIntPair cannot be analyzed, print as raw value.; """"""Print a PointerUnion.""""""; """"""Factory for an llvm::PointerUnion printer.""""""; # If PointerIntPair cannot be analyzed, print as raw value.; """"""Print an llvm::ilist_node object.""""""; # One of Prev and P",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/gdb-scripts/prettyprinters.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/gdb-scripts/prettyprinters.py
Availability,failure,failure,"#!/usr/bin/env python3; #; # ====- code-format-helper, runs code formatters from the ci or in a hook --*- python -*--==#; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ==--------------------------------------------------------------------------------------==#; """"""; This script is run by GitHub actions to ensure that the code in PR's conform to; the coding style of LLVM. It can also be installed as a pre-commit git hook to; check the coding style before submitting it. The canonical source of this script; is in the LLVM source tree under llvm/utils/git. For C/C++ code it uses clang-format and for Python code it uses darker (which; in turn invokes black). You can learn more about the LLVM coding style on llvm.org:; https://llvm.org/docs/CodingStandards.html. You can install this script as a git hook by symlinking it to the .git/hooks; directory:. ln -s $(pwd)/llvm/utils/git/code-format-helper.py .git/hooks/pre-commit. You can control the exact path to clang-format or darker with the following; environment variables: $CLANG_FORMAT_PATH and $DARKER_FORMAT_PATH.; """"""; # TODO: any type should be replaced with the correct github type, but it requires refactoring to; # not require the github module to be installed everywhere.; # The formatter failed but didn't output a diff (e.g. some sort of; # infrastructure failure).; # formatting needed, or the command otherwise failed; # Print the diff in the log so that it is viewable there; # formatting needed, or the command otherwise failed; # Print the diff in the log so that it is viewable there; # fill out args; # find the changed files",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/git/code-format-helper.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/git/code-format-helper.py
Deployability,install,installed,"#!/usr/bin/env python3; #; # ====- code-format-helper, runs code formatters from the ci or in a hook --*- python -*--==#; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ==--------------------------------------------------------------------------------------==#; """"""; This script is run by GitHub actions to ensure that the code in PR's conform to; the coding style of LLVM. It can also be installed as a pre-commit git hook to; check the coding style before submitting it. The canonical source of this script; is in the LLVM source tree under llvm/utils/git. For C/C++ code it uses clang-format and for Python code it uses darker (which; in turn invokes black). You can learn more about the LLVM coding style on llvm.org:; https://llvm.org/docs/CodingStandards.html. You can install this script as a git hook by symlinking it to the .git/hooks; directory:. ln -s $(pwd)/llvm/utils/git/code-format-helper.py .git/hooks/pre-commit. You can control the exact path to clang-format or darker with the following; environment variables: $CLANG_FORMAT_PATH and $DARKER_FORMAT_PATH.; """"""; # TODO: any type should be replaced with the correct github type, but it requires refactoring to; # not require the github module to be installed everywhere.; # The formatter failed but didn't output a diff (e.g. some sort of; # infrastructure failure).; # formatting needed, or the command otherwise failed; # Print the diff in the log so that it is viewable there; # formatting needed, or the command otherwise failed; # Print the diff in the log so that it is viewable there; # fill out args; # find the changed files",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/git/code-format-helper.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/git/code-format-helper.py
Modifiability,variab,variables,"#!/usr/bin/env python3; #; # ====- code-format-helper, runs code formatters from the ci or in a hook --*- python -*--==#; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ==--------------------------------------------------------------------------------------==#; """"""; This script is run by GitHub actions to ensure that the code in PR's conform to; the coding style of LLVM. It can also be installed as a pre-commit git hook to; check the coding style before submitting it. The canonical source of this script; is in the LLVM source tree under llvm/utils/git. For C/C++ code it uses clang-format and for Python code it uses darker (which; in turn invokes black). You can learn more about the LLVM coding style on llvm.org:; https://llvm.org/docs/CodingStandards.html. You can install this script as a git hook by symlinking it to the .git/hooks; directory:. ln -s $(pwd)/llvm/utils/git/code-format-helper.py .git/hooks/pre-commit. You can control the exact path to clang-format or darker with the following; environment variables: $CLANG_FORMAT_PATH and $DARKER_FORMAT_PATH.; """"""; # TODO: any type should be replaced with the correct github type, but it requires refactoring to; # not require the github module to be installed everywhere.; # The formatter failed but didn't output a diff (e.g. some sort of; # infrastructure failure).; # formatting needed, or the command otherwise failed; # Print the diff in the log so that it is viewable there; # formatting needed, or the command otherwise failed; # Print the diff in the log so that it is viewable there; # fill out args; # find the changed files",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/git/code-format-helper.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/git/code-format-helper.py
Testability,log,log,"#!/usr/bin/env python3; #; # ====- code-format-helper, runs code formatters from the ci or in a hook --*- python -*--==#; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ==--------------------------------------------------------------------------------------==#; """"""; This script is run by GitHub actions to ensure that the code in PR's conform to; the coding style of LLVM. It can also be installed as a pre-commit git hook to; check the coding style before submitting it. The canonical source of this script; is in the LLVM source tree under llvm/utils/git. For C/C++ code it uses clang-format and for Python code it uses darker (which; in turn invokes black). You can learn more about the LLVM coding style on llvm.org:; https://llvm.org/docs/CodingStandards.html. You can install this script as a git hook by symlinking it to the .git/hooks; directory:. ln -s $(pwd)/llvm/utils/git/code-format-helper.py .git/hooks/pre-commit. You can control the exact path to clang-format or darker with the following; environment variables: $CLANG_FORMAT_PATH and $DARKER_FORMAT_PATH.; """"""; # TODO: any type should be replaced with the correct github type, but it requires refactoring to; # not require the github module to be installed everywhere.; # The formatter failed but didn't output a diff (e.g. some sort of; # infrastructure failure).; # formatting needed, or the command otherwise failed; # Print the diff in the log so that it is viewable there; # formatting needed, or the command otherwise failed; # Print the diff in the log so that it is viewable there; # fill out args; # find the changed files",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/git/code-format-helper.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/git/code-format-helper.py
Usability,learn,learn,"#!/usr/bin/env python3; #; # ====- code-format-helper, runs code formatters from the ci or in a hook --*- python -*--==#; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ==--------------------------------------------------------------------------------------==#; """"""; This script is run by GitHub actions to ensure that the code in PR's conform to; the coding style of LLVM. It can also be installed as a pre-commit git hook to; check the coding style before submitting it. The canonical source of this script; is in the LLVM source tree under llvm/utils/git. For C/C++ code it uses clang-format and for Python code it uses darker (which; in turn invokes black). You can learn more about the LLVM coding style on llvm.org:; https://llvm.org/docs/CodingStandards.html. You can install this script as a git hook by symlinking it to the .git/hooks; directory:. ln -s $(pwd)/llvm/utils/git/code-format-helper.py .git/hooks/pre-commit. You can control the exact path to clang-format or darker with the following; environment variables: $CLANG_FORMAT_PATH and $DARKER_FORMAT_PATH.; """"""; # TODO: any type should be replaced with the correct github type, but it requires refactoring to; # not require the github module to be installed everywhere.; # The formatter failed but didn't output a diff (e.g. some sort of; # infrastructure failure).; # formatting needed, or the command otherwise failed; # Print the diff in the log so that it is viewable there; # formatting needed, or the command otherwise failed; # Print the diff in the log so that it is viewable there; # fill out args; # find the changed files",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/git/code-format-helper.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/git/code-format-helper.py
Availability,failure,failures,"rg/docs/TestingGuide.html#unit-and-regression-tests) locally. Remember that the subdirectories under `test/` create fine-grained testing targets, so you can e.g. use `make check-clang-ast` to only run Clang's AST tests.; 4. Create a Git commit.; 5. Run [`git clang-format HEAD~1`](https://clang.llvm.org/docs/ClangFormat.html#git-integration) to format your changes.; 6. Open a [pull request](https://github.com/llvm/llvm-project/pulls) to the [upstream repository](https://github.com/llvm/llvm-project) on GitHub. Detailed instructions can be found [in GitHub's documentation](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request). If you have any further questions about this issue, don't hesitate to ask via a comment in the thread below.; """"""; # If the description of an issue/pull request is empty, the Github API; # library returns None instead of an empty string. Handle this here to; # avoid failures from trying to manipulate None.; # https://github.com/github/markup/issues/1168#issuecomment-494946168; # '@' followed by alphanum is a user name; # '#' followed by digits is considered an issue number; # GitHub limits comments to 65,536 characters, let's limit the diff; # and the file list to 20kB each.; # Get statistics for each file; # Get the diff; # Note: the comment is in markdown and the code below; # is sensible to line break; # We assume that this is only called for a PR that has just been opened; # by a user new to LLVM and/or GitHub itself.; # This text is using Markdown formatting.; """"""; Configure the git repo in `git_dir` with the llvmbot account so; commits are attributed to llvmbot.; """"""; """"""; Make an API call to the Phabricator web service and return a dictionary; containing the json response.; """"""; """"""; Tries to translate a Phabricator login to a github login by; finding a commit made in Phabricator's Differential.; The commit's SHA1 is then looked up in the github",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/git/github-automation.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/git/github-automation.py
Deployability,integrat,integration,"omation - LLVM GitHub Automation Routines--*- python -*--==#; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ==-------------------------------------------------------------------------==#; # type: ignore; """"""; Hi!. This issue may be a good introductory issue for people new to working on LLVM. If you would like to work on this issue, your first steps are:. 1. In the comments of the issue, request for it to be assigned to you.; 2. Fix the issue locally.; 3. [Run the test suite](https://llvm.org/docs/TestingGuide.html#unit-and-regression-tests) locally. Remember that the subdirectories under `test/` create fine-grained testing targets, so you can e.g. use `make check-clang-ast` to only run Clang's AST tests.; 4. Create a Git commit.; 5. Run [`git clang-format HEAD~1`](https://clang.llvm.org/docs/ClangFormat.html#git-integration) to format your changes.; 6. Open a [pull request](https://github.com/llvm/llvm-project/pulls) to the [upstream repository](https://github.com/llvm/llvm-project) on GitHub. Detailed instructions can be found [in GitHub's documentation](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request). If you have any further questions about this issue, don't hesitate to ask via a comment in the thread below.; """"""; # If the description of an issue/pull request is empty, the Github API; # library returns None instead of an empty string. Handle this here to; # avoid failures from trying to manipulate None.; # https://github.com/github/markup/issues/1168#issuecomment-494946168; # '@' followed by alphanum is a user name; # '#' followed by digits is considered an issue number; # GitHub limits comments to 65,536 characters, let's limit the diff; # and the file list to 20kB each.; # Get statistics for each file; #",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/git/github-automation.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/git/github-automation.py
Integrability,integrat,integration,"omation - LLVM GitHub Automation Routines--*- python -*--==#; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ==-------------------------------------------------------------------------==#; # type: ignore; """"""; Hi!. This issue may be a good introductory issue for people new to working on LLVM. If you would like to work on this issue, your first steps are:. 1. In the comments of the issue, request for it to be assigned to you.; 2. Fix the issue locally.; 3. [Run the test suite](https://llvm.org/docs/TestingGuide.html#unit-and-regression-tests) locally. Remember that the subdirectories under `test/` create fine-grained testing targets, so you can e.g. use `make check-clang-ast` to only run Clang's AST tests.; 4. Create a Git commit.; 5. Run [`git clang-format HEAD~1`](https://clang.llvm.org/docs/ClangFormat.html#git-integration) to format your changes.; 6. Open a [pull request](https://github.com/llvm/llvm-project/pulls) to the [upstream repository](https://github.com/llvm/llvm-project) on GitHub. Detailed instructions can be found [in GitHub's documentation](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request). If you have any further questions about this issue, don't hesitate to ask via a comment in the thread below.; """"""; # If the description of an issue/pull request is empty, the Github API; # library returns None instead of an empty string. Handle this here to; # avoid failures from trying to manipulate None.; # https://github.com/github/markup/issues/1168#issuecomment-494946168; # '@' followed by alphanum is a user name; # '#' followed by digits is considered an issue number; # GitHub limits comments to 65,536 characters, let's limit the diff; # and the file list to 20kB each.; # Get statistics for each file; #",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/git/github-automation.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/git/github-automation.py
Safety,avoid,avoid,"rg/docs/TestingGuide.html#unit-and-regression-tests) locally. Remember that the subdirectories under `test/` create fine-grained testing targets, so you can e.g. use `make check-clang-ast` to only run Clang's AST tests.; 4. Create a Git commit.; 5. Run [`git clang-format HEAD~1`](https://clang.llvm.org/docs/ClangFormat.html#git-integration) to format your changes.; 6. Open a [pull request](https://github.com/llvm/llvm-project/pulls) to the [upstream repository](https://github.com/llvm/llvm-project) on GitHub. Detailed instructions can be found [in GitHub's documentation](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request). If you have any further questions about this issue, don't hesitate to ask via a comment in the thread below.; """"""; # If the description of an issue/pull request is empty, the Github API; # library returns None instead of an empty string. Handle this here to; # avoid failures from trying to manipulate None.; # https://github.com/github/markup/issues/1168#issuecomment-494946168; # '@' followed by alphanum is a user name; # '#' followed by digits is considered an issue number; # GitHub limits comments to 65,536 characters, let's limit the diff; # and the file list to 20kB each.; # Get statistics for each file; # Get the diff; # Note: the comment is in markdown and the code below; # is sensible to line break; # We assume that this is only called for a PR that has just been opened; # by a user new to LLVM and/or GitHub itself.; # This text is using Markdown formatting.; """"""; Configure the git repo in `git_dir` with the llvmbot account so; commits are attributed to llvmbot.; """"""; """"""; Make an API call to the Phabricator web service and return a dictionary; containing the json response.; """"""; """"""; Tries to translate a Phabricator login to a github login by; finding a commit made in Phabricator's Differential.; The commit's SHA1 is then looked up in the github",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/git/github-automation.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/git/github-automation.py
Security,hash,hash,"he github repo and; the committer's login associated with that commit is returned. :param str phab_token: The Conduit API token to use for communication with Pabricator; :param github.Repository.Repository repo: The github repo to use when looking for the SHA1 found in Differential; :param str phab_login: The Phabricator login to be translated.; """"""; # PHID for ""LLVM Github Monorepo"" repository; # API documentation: https://reviews.llvm.org/conduit/method/diffusion.commit.search/; # Can't find any commits associated with this user; # This committer had an email address GitHub could not recognize, so; # it can't link the user to a GitHub account.; # API documentation: https://reviews.llvm.org/conduit/method/differential.parsecommitmessage/; # No Phabricator revision for this commit; # API documentation: https://reviews.llvm.org/conduit/method/differential.revision.search/; # API documentation: https://reviews.llvm.org/conduit/method/user.search/; """"""; Extract the commit hash from the argument passed to /action github; comment actions. We currently only support passing the commit hash; directly or use the github URL, such as; https://github.com/llvm/llvm-project/commit/2832d7941f4207f1fcf813b27cf08cecc3086959; """"""; """"""; This class implements the sub-commands for the release-workflow command.; The current sub-commands are:; * create-branch; * create-pull-request. The execute_command method will automatically choose the correct sub-command; based on the text in stdin.; """"""; """"""; Returns the comment string with a prefix that will cause; a Github workflow to skip parsing this comment. :param str comment: The comment to ignore; """"""; """"""; This function will try to find the best reviewers for `commits` and; then add a comment requesting review of the backport and add them as; reviewers. The reviewers selected are those users who approved the pull request; for the main branch.; """"""; """"""; This function attempts to backport `commits` into the branch associated; with `self.issue",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/git/github-automation.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/git/github-automation.py
Testability,test,test,"#!/usr/bin/env python3; #; # ======- github-automation - LLVM GitHub Automation Routines--*- python -*--==#; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ==-------------------------------------------------------------------------==#; # type: ignore; """"""; Hi!. This issue may be a good introductory issue for people new to working on LLVM. If you would like to work on this issue, your first steps are:. 1. In the comments of the issue, request for it to be assigned to you.; 2. Fix the issue locally.; 3. [Run the test suite](https://llvm.org/docs/TestingGuide.html#unit-and-regression-tests) locally. Remember that the subdirectories under `test/` create fine-grained testing targets, so you can e.g. use `make check-clang-ast` to only run Clang's AST tests.; 4. Create a Git commit.; 5. Run [`git clang-format HEAD~1`](https://clang.llvm.org/docs/ClangFormat.html#git-integration) to format your changes.; 6. Open a [pull request](https://github.com/llvm/llvm-project/pulls) to the [upstream repository](https://github.com/llvm/llvm-project) on GitHub. Detailed instructions can be found [in GitHub's documentation](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request). If you have any further questions about this issue, don't hesitate to ask via a comment in the thread below.; """"""; # If the description of an issue/pull request is empty, the Github API; # library returns None instead of an empty string. Handle this here to; # avoid failures from trying to manipulate None.; # https://github.com/github/markup/issues/1168#issuecomment-494946168; # '@' followed by alphanum is a user name; # '#' followed by digits is considered an issue number; # GitHub limits comments to 65,536 characters, let's limit the diff; # and the file list t",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/git/github-automation.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/git/github-automation.py
Availability,error,errors,"#!/usr/bin/env python3; #; # ======- pre-push - LLVM Git Help Integration ---------*- python -*--========#; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ==------------------------------------------------------------------------==#; """"""; pre-push git hook integration; =============================. This script is intended to be setup as a pre-push hook, from the root of the; repo run:. ln -sf ../../llvm/utils/git/pre-push.py .git/hooks/pre-push. From the git doc:. The pre-push hook runs during git push, after the remote refs have been; updated but before any objects have been transferred. It receives the name; and location of the remote as parameters, and a list of to-be-updated refs; through stdin. You can use it to validate a set of ref updates before a push; occurs (a non-zero exit code will abort the push).; """"""; """"""Lazily create a /dev/null fd for use in shell()""""""; # Escape args when logging for easy repro.; # Silence errors if requested.; # Reverse the order so we print the oldest commit first; """"""Check a single push request (which can include multiple revisions)""""""; # Handle request to delete; # Push a new branch; # Update to existing branch, examine new commits; # Check that the remote commit exists, otherwise let git proceed; # This can happen if someone is force pushing an older revision to a branch; # Print the revision about to be pushed commits",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/git/pre-push.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/git/pre-push.py
Deployability,integrat,integration,"#!/usr/bin/env python3; #; # ======- pre-push - LLVM Git Help Integration ---------*- python -*--========#; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ==------------------------------------------------------------------------==#; """"""; pre-push git hook integration; =============================. This script is intended to be setup as a pre-push hook, from the root of the; repo run:. ln -sf ../../llvm/utils/git/pre-push.py .git/hooks/pre-push. From the git doc:. The pre-push hook runs during git push, after the remote refs have been; updated but before any objects have been transferred. It receives the name; and location of the remote as parameters, and a list of to-be-updated refs; through stdin. You can use it to validate a set of ref updates before a push; occurs (a non-zero exit code will abort the push).; """"""; """"""Lazily create a /dev/null fd for use in shell()""""""; # Escape args when logging for easy repro.; # Silence errors if requested.; # Reverse the order so we print the oldest commit first; """"""Check a single push request (which can include multiple revisions)""""""; # Handle request to delete; # Push a new branch; # Update to existing branch, examine new commits; # Check that the remote commit exists, otherwise let git proceed; # This can happen if someone is force pushing an older revision to a branch; # Print the revision about to be pushed commits",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/git/pre-push.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/git/pre-push.py
Integrability,integrat,integration,"#!/usr/bin/env python3; #; # ======- pre-push - LLVM Git Help Integration ---------*- python -*--========#; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ==------------------------------------------------------------------------==#; """"""; pre-push git hook integration; =============================. This script is intended to be setup as a pre-push hook, from the root of the; repo run:. ln -sf ../../llvm/utils/git/pre-push.py .git/hooks/pre-push. From the git doc:. The pre-push hook runs during git push, after the remote refs have been; updated but before any objects have been transferred. It receives the name; and location of the remote as parameters, and a list of to-be-updated refs; through stdin. You can use it to validate a set of ref updates before a push; occurs (a non-zero exit code will abort the push).; """"""; """"""Lazily create a /dev/null fd for use in shell()""""""; # Escape args when logging for easy repro.; # Silence errors if requested.; # Reverse the order so we print the oldest commit first; """"""Check a single push request (which can include multiple revisions)""""""; # Handle request to delete; # Push a new branch; # Update to existing branch, examine new commits; # Check that the remote commit exists, otherwise let git proceed; # This can happen if someone is force pushing an older revision to a branch; # Print the revision about to be pushed commits",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/git/pre-push.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/git/pre-push.py
Safety,abort,abort,"#!/usr/bin/env python3; #; # ======- pre-push - LLVM Git Help Integration ---------*- python -*--========#; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ==------------------------------------------------------------------------==#; """"""; pre-push git hook integration; =============================. This script is intended to be setup as a pre-push hook, from the root of the; repo run:. ln -sf ../../llvm/utils/git/pre-push.py .git/hooks/pre-push. From the git doc:. The pre-push hook runs during git push, after the remote refs have been; updated but before any objects have been transferred. It receives the name; and location of the remote as parameters, and a list of to-be-updated refs; through stdin. You can use it to validate a set of ref updates before a push; occurs (a non-zero exit code will abort the push).; """"""; """"""Lazily create a /dev/null fd for use in shell()""""""; # Escape args when logging for easy repro.; # Silence errors if requested.; # Reverse the order so we print the oldest commit first; """"""Check a single push request (which can include multiple revisions)""""""; # Handle request to delete; # Push a new branch; # Update to existing branch, examine new commits; # Check that the remote commit exists, otherwise let git proceed; # This can happen if someone is force pushing an older revision to a branch; # Print the revision about to be pushed commits",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/git/pre-push.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/git/pre-push.py
Security,validat,validate,"#!/usr/bin/env python3; #; # ======- pre-push - LLVM Git Help Integration ---------*- python -*--========#; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ==------------------------------------------------------------------------==#; """"""; pre-push git hook integration; =============================. This script is intended to be setup as a pre-push hook, from the root of the; repo run:. ln -sf ../../llvm/utils/git/pre-push.py .git/hooks/pre-push. From the git doc:. The pre-push hook runs during git push, after the remote refs have been; updated but before any objects have been transferred. It receives the name; and location of the remote as parameters, and a list of to-be-updated refs; through stdin. You can use it to validate a set of ref updates before a push; occurs (a non-zero exit code will abort the push).; """"""; """"""Lazily create a /dev/null fd for use in shell()""""""; # Escape args when logging for easy repro.; # Silence errors if requested.; # Reverse the order so we print the oldest commit first; """"""Check a single push request (which can include multiple revisions)""""""; # Handle request to delete; # Push a new branch; # Update to existing branch, examine new commits; # Check that the remote commit exists, otherwise let git proceed; # This can happen if someone is force pushing an older revision to a branch; # Print the revision about to be pushed commits",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/git/pre-push.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/git/pre-push.py
Testability,log,logging,"#!/usr/bin/env python3; #; # ======- pre-push - LLVM Git Help Integration ---------*- python -*--========#; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ==------------------------------------------------------------------------==#; """"""; pre-push git hook integration; =============================. This script is intended to be setup as a pre-push hook, from the root of the; repo run:. ln -sf ../../llvm/utils/git/pre-push.py .git/hooks/pre-push. From the git doc:. The pre-push hook runs during git push, after the remote refs have been; updated but before any objects have been transferred. It receives the name; and location of the remote as parameters, and a list of to-be-updated refs; through stdin. You can use it to validate a set of ref updates before a push; occurs (a non-zero exit code will abort the push).; """"""; """"""Lazily create a /dev/null fd for use in shell()""""""; # Escape args when logging for easy repro.; # Silence errors if requested.; # Reverse the order so we print the oldest commit first; """"""Check a single push request (which can include multiple revisions)""""""; # Handle request to delete; # Push a new branch; # Update to existing branch, examine new commits; # Check that the remote commit exists, otherwise let git proceed; # This can happen if someone is force pushing an older revision to a branch; # Print the revision about to be pushed commits",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/git/pre-push.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/git/pre-push.py
Deployability,install,install,"# setuptools expects to be invoked from within the directory of setup.py, but it; # is nice to allow:; # python path/to/setup.py install; # to work (for scripts, etc.)",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/setup.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/setup.py
Availability,error,error,"# A simple evaluator of boolean expressions.; #; # Grammar:; # expr :: or_expr; # or_expr :: and_expr ('||' and_expr)*; # and_expr :: not_expr ('&&' not_expr)*; # not_expr :: '!' not_expr; # '(' or_expr ')'; # match_expr; # match_expr :: braced_regex; # identifier; # braced_regex match_expr; # identifier match_expr; # identifier :: [-+=._a-zA-Z0-9]+; # braced_regex :: '{{' python_regex '}}'; # Evaluates `string` as a boolean expression.; # Returns True or False. Throws a ValueError on syntax error.; #; # Variables in `variables` are true.; # Regexes that match any variable in `variables` are true.; # 'true' is true.; # All other identifiers are false.; #####; # Singleton end-of-expression marker.; # Tokenization pattern.; # this is technically the wrong associativity, but it; # doesn't matter for this limited expression grammar; # this is technically the wrong associativity, but it; # doesn't matter for this limited expression grammar; #######; # Tests; # Evaluate boolean expression `expr`.; # Fail if it does not throw a ValueError containing the text `error`.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/BooleanExpression.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/BooleanExpression.py
Modifiability,variab,variables,"# A simple evaluator of boolean expressions.; #; # Grammar:; # expr :: or_expr; # or_expr :: and_expr ('||' and_expr)*; # and_expr :: not_expr ('&&' not_expr)*; # not_expr :: '!' not_expr; # '(' or_expr ')'; # match_expr; # match_expr :: braced_regex; # identifier; # braced_regex match_expr; # identifier match_expr; # identifier :: [-+=._a-zA-Z0-9]+; # braced_regex :: '{{' python_regex '}}'; # Evaluates `string` as a boolean expression.; # Returns True or False. Throws a ValueError on syntax error.; #; # Variables in `variables` are true.; # Regexes that match any variable in `variables` are true.; # 'true' is true.; # All other identifiers are false.; #####; # Singleton end-of-expression marker.; # Tokenization pattern.; # this is technically the wrong associativity, but it; # doesn't matter for this limited expression grammar; # this is technically the wrong associativity, but it; # doesn't matter for this limited expression grammar; #######; # Tests; # Evaluate boolean expression `expr`.; # Fail if it does not throw a ValueError containing the text `error`.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/BooleanExpression.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/BooleanExpression.py
Usability,simpl,simple,"# A simple evaluator of boolean expressions.; #; # Grammar:; # expr :: or_expr; # or_expr :: and_expr ('||' and_expr)*; # and_expr :: not_expr ('&&' not_expr)*; # not_expr :: '!' not_expr; # '(' or_expr ')'; # match_expr; # match_expr :: braced_regex; # identifier; # braced_regex match_expr; # identifier match_expr; # identifier :: [-+=._a-zA-Z0-9]+; # braced_regex :: '{{' python_regex '}}'; # Evaluates `string` as a boolean expression.; # Returns True or False. Throws a ValueError on syntax error.; #; # Variables in `variables` are true.; # Regexes that match any variable in `variables` are true.; # 'true' is true.; # All other identifiers are false.; #####; # Singleton end-of-expression marker.; # Tokenization pattern.; # this is technically the wrong associativity, but it; # doesn't matter for this limited expression grammar; # this is technically the wrong associativity, but it; # doesn't matter for this limited expression grammar; #######; # Tests; # Evaluate boolean expression `expr`.; # Fail if it does not throw a ValueError containing the text `error`.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/BooleanExpression.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/BooleanExpression.py
Modifiability,variab,variables,"# FIXME: I find these names very confusing, although I like the; # functionality.; # Note: this does not generate flags for user-defined result codes.; # LIT is special: environment variables override command line arguments.; # Validate command line options",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/cl_arguments.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/cl_arguments.py
Availability,error,errors,"s in, and its; relative path inside that suite.; """"""; # Check for a site config or a lit config.; # If we didn't find a config file, keep looking.; # This is a private builtin parameter which can be used to perform; # translation of configuration paths. Specifically, this parameter; # can be set to a dictionary that the discovery process will consult; # when it finds a configuration it is about to load. If the given; # path is in the map, the value of that key is a path to the; # configuration to load instead.; # We found a test suite, create a new config for it and load it.; # Check for an already instantiated test suite.; # Canonicalize the path.; # Skip files and virtual components.; # Get the parent config.; # Check if there is a local configuration file.; # If not, just reuse the parent config.; # Otherwise, copy the current config and load the local configuration; # file into it.; # Find the test suite for this input and its relative path.; # Check that the source path exists (errors here are reported by the; # caller).; # Check if the user named a test directly.; # If we don't have a test format or if we are running standalone tests,; # always ""find"" the test itself. Otherwise, we might find no tests at; # all, which is considered an error but isn't an error with standalone; # tests.; # Otherwise we have a directory to search for tests, start by getting the; # local configuration.; # Directory contains tests to be run standalone. Do not try to discover.; # Search for tests.; # Search subdirectories.; # FIXME: This doesn't belong here?; # Ignore non-directories.; # Check for nested test suites, first in the execpath in case there is a; # site configuration and then in the source path.; # If the this directory recursively maps back to the current test suite,; # disregard it (this can happen if the exec root is located inside the; # current test suite, for example).; # Otherwise, load from the nested test suite, if present.; """"""; find_tests_for_inputs(lit_config",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/discovery.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/discovery.py
Deployability,configurat,configuration,"""""""; Test discovery functions.; """"""; """"""getTestSuite(item, litConfig, cache) -> (suite, relative_path). Find the test suite containing @arg item. @retval (None, ...) - Indicates no test suite contains @arg item.; @retval (suite, relative_path) - The suite that @arg item is in, and its; relative path inside that suite.; """"""; # Check for a site config or a lit config.; # If we didn't find a config file, keep looking.; # This is a private builtin parameter which can be used to perform; # translation of configuration paths. Specifically, this parameter; # can be set to a dictionary that the discovery process will consult; # when it finds a configuration it is about to load. If the given; # path is in the map, the value of that key is a path to the; # configuration to load instead.; # We found a test suite, create a new config for it and load it.; # Check for an already instantiated test suite.; # Canonicalize the path.; # Skip files and virtual components.; # Get the parent config.; # Check if there is a local configuration file.; # If not, just reuse the parent config.; # Otherwise, copy the current config and load the local configuration; # file into it.; # Find the test suite for this input and its relative path.; # Check that the source path exists (errors here are reported by the; # caller).; # Check if the user named a test directly.; # If we don't have a test format or if we are running standalone tests,; # always ""find"" the test itself. Otherwise, we might find no tests at; # all, which is considered an error but isn't an error with standalone; # tests.; # Otherwise we have a directory to search for tests, start by getting the; # local configuration.; # Directory contains tests to be run standalone. Do not try to discover.; # Search for tests.; # Search subdirectories.; # FIXME: This doesn't belong here?; # Ignore non-directories.; # Check for nested test suites, first in the execpath in case there is a; # site configuration and then in the source path.; # If the",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/discovery.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/discovery.py
Modifiability,config,config,"""""""; Test discovery functions.; """"""; """"""getTestSuite(item, litConfig, cache) -> (suite, relative_path). Find the test suite containing @arg item. @retval (None, ...) - Indicates no test suite contains @arg item.; @retval (suite, relative_path) - The suite that @arg item is in, and its; relative path inside that suite.; """"""; # Check for a site config or a lit config.; # If we didn't find a config file, keep looking.; # This is a private builtin parameter which can be used to perform; # translation of configuration paths. Specifically, this parameter; # can be set to a dictionary that the discovery process will consult; # when it finds a configuration it is about to load. If the given; # path is in the map, the value of that key is a path to the; # configuration to load instead.; # We found a test suite, create a new config for it and load it.; # Check for an already instantiated test suite.; # Canonicalize the path.; # Skip files and virtual components.; # Get the parent config.; # Check if there is a local configuration file.; # If not, just reuse the parent config.; # Otherwise, copy the current config and load the local configuration; # file into it.; # Find the test suite for this input and its relative path.; # Check that the source path exists (errors here are reported by the; # caller).; # Check if the user named a test directly.; # If we don't have a test format or if we are running standalone tests,; # always ""find"" the test itself. Otherwise, we might find no tests at; # all, which is considered an error but isn't an error with standalone; # tests.; # Otherwise we have a directory to search for tests, start by getting the; # local configuration.; # Directory contains tests to be run standalone. Do not try to discover.; # Search for tests.; # Search subdirectories.; # FIXME: This doesn't belong here?; # Ignore non-directories.; # Check for nested test suites, first in the execpath in case there is a; # site configuration and then in the source path.; # If the",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/discovery.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/discovery.py
Performance,cache,cache,"""""""; Test discovery functions.; """"""; """"""getTestSuite(item, litConfig, cache) -> (suite, relative_path). Find the test suite containing @arg item. @retval (None, ...) - Indicates no test suite contains @arg item.; @retval (suite, relative_path) - The suite that @arg item is in, and its; relative path inside that suite.; """"""; # Check for a site config or a lit config.; # If we didn't find a config file, keep looking.; # This is a private builtin parameter which can be used to perform; # translation of configuration paths. Specifically, this parameter; # can be set to a dictionary that the discovery process will consult; # when it finds a configuration it is about to load. If the given; # path is in the map, the value of that key is a path to the; # configuration to load instead.; # We found a test suite, create a new config for it and load it.; # Check for an already instantiated test suite.; # Canonicalize the path.; # Skip files and virtual components.; # Get the parent config.; # Check if there is a local configuration file.; # If not, just reuse the parent config.; # Otherwise, copy the current config and load the local configuration; # file into it.; # Find the test suite for this input and its relative path.; # Check that the source path exists (errors here are reported by the; # caller).; # Check if the user named a test directly.; # If we don't have a test format or if we are running standalone tests,; # always ""find"" the test itself. Otherwise, we might find no tests at; # all, which is considered an error but isn't an error with standalone; # tests.; # Otherwise we have a directory to search for tests, start by getting the; # local configuration.; # Directory contains tests to be run standalone. Do not try to discover.; # Search for tests.; # Search subdirectories.; # FIXME: This doesn't belong here?; # Ignore non-directories.; # Check for nested test suites, first in the execpath in case there is a; # site configuration and then in the source path.; # If the",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/discovery.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/discovery.py
Testability,test,test,"""""""; Test discovery functions.; """"""; """"""getTestSuite(item, litConfig, cache) -> (suite, relative_path). Find the test suite containing @arg item. @retval (None, ...) - Indicates no test suite contains @arg item.; @retval (suite, relative_path) - The suite that @arg item is in, and its; relative path inside that suite.; """"""; # Check for a site config or a lit config.; # If we didn't find a config file, keep looking.; # This is a private builtin parameter which can be used to perform; # translation of configuration paths. Specifically, this parameter; # can be set to a dictionary that the discovery process will consult; # when it finds a configuration it is about to load. If the given; # path is in the map, the value of that key is a path to the; # configuration to load instead.; # We found a test suite, create a new config for it and load it.; # Check for an already instantiated test suite.; # Canonicalize the path.; # Skip files and virtual components.; # Get the parent config.; # Check if there is a local configuration file.; # If not, just reuse the parent config.; # Otherwise, copy the current config and load the local configuration; # file into it.; # Find the test suite for this input and its relative path.; # Check that the source path exists (errors here are reported by the; # caller).; # Check if the user named a test directly.; # If we don't have a test format or if we are running standalone tests,; # always ""find"" the test itself. Otherwise, we might find no tests at; # all, which is considered an error but isn't an error with standalone; # tests.; # Otherwise we have a directory to search for tests, start by getting the; # local configuration.; # Directory contains tests to be run standalone. Do not try to discover.; # Search for tests.; # Search subdirectories.; # FIXME: This doesn't belong here?; # Ignore non-directories.; # Check for nested test suites, first in the execpath in case there is a; # site configuration and then in the source path.; # If the",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/discovery.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/discovery.py
Availability,failure,failure,"# NOTE: median would be more precise, but might be too slow.; # Show the test result line.; # Show the test failure output, if requested.; # Encode/decode so that, when using Python 3.6.5 in Windows 10,; # print(out) doesn't raise UnicodeEncodeError if out contains; # special characters. However, Python 2 might try to decode; # as part of the encode call if out is already encoded, so skip; # encoding if it raises UnicodeDecodeError.; # Python 2 can raise UnicodeDecodeError here too in cases; # where the stdout encoding is ASCII. Ignore decode errors; # in this case.; # Report test metrics, if present.; # Report micro-tests, if present; # Ensure the output is flushed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/display.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/display.py
Testability,test,test,"# NOTE: median would be more precise, but might be too slow.; # Show the test result line.; # Show the test failure output, if requested.; # Encode/decode so that, when using Python 3.6.5 in Windows 10,; # print(out) doesn't raise UnicodeEncodeError if out contains; # special characters. However, Python 2 might try to decode; # as part of the encode call if out is already encoded, so skip; # encoding if it raises UnicodeDecodeError.; # Python 2 can raise UnicodeDecodeError here too in cases; # where the stdout encoding is ASCII. Ignore decode errors; # in this case.; # Report test metrics, if present.; # Report micro-tests, if present; # Ensure the output is flushed.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/display.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/display.py
Availability,error,error,"# LitConfig must be a new style class for properties to work; """"""LitConfig - Configuration data for a 'lit' test runner instance, shared; across all tests. The LitConfig object is also used to communicate with client configuration; files, it is always passed in as the global variable 'lit' so that; configuration files can access common functionality and internal components; easily.; """"""; # The name of the test runner.; # The items to add to the PATH environment variable.; # Configuration files to look for when discovering test suites.; # The default is 'summary'.; """"""; Interface for getting maximum time to spend executing; a single test; """"""; """"""; Returns a tuple (<supported> , <error message>); where; `<supported>` is True if setting maxIndividualTestTime is supported; on the current host, returns False otherwise.; `<error message>` is an empty string if `<supported>` is True,; otherwise is contains a string describing why setting; maxIndividualTestTime is not supported.; """"""; """"""; Interface for setting maximum time to spend executing; a single test; """"""; # The current implementation needs psutil on some platforms to set; # a timeout per test. Check it's available.; # See lit.util.killProcessAndChildren(); """"""; Interface for getting the per_test_coverage value; """"""; """"""; Interface for setting the per_test_coverage value; """"""; """"""load_config(config, path) - Load a config object from an alternate; path.""""""; """"""getBashPath - Get the path to 'bash'""""""; # Check whether the found version of bash is able to cope with paths in; # the host path format. If not, don't return it as it can't be used to; # run scripts. For example, WSL's bash.exe requires '/mnt/c/foo' rather; # than 'C:\\foo' or 'C:/foo'.; # bash; # Get the file/line where this message was generated.; # Step out of _write_message, and then out of wrapper.; # In a git bash terminal, the writes to sys.stderr aren't visible; # on screen immediately. Flush them here to avoid broken/misoredered; # output.; """"""substitu",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/LitConfig.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/LitConfig.py
Deployability,configurat,configuration,"# LitConfig must be a new style class for properties to work; """"""LitConfig - Configuration data for a 'lit' test runner instance, shared; across all tests. The LitConfig object is also used to communicate with client configuration; files, it is always passed in as the global variable 'lit' so that; configuration files can access common functionality and internal components; easily.; """"""; # The name of the test runner.; # The items to add to the PATH environment variable.; # Configuration files to look for when discovering test suites.; # The default is 'summary'.; """"""; Interface for getting maximum time to spend executing; a single test; """"""; """"""; Returns a tuple (<supported> , <error message>); where; `<supported>` is True if setting maxIndividualTestTime is supported; on the current host, returns False otherwise.; `<error message>` is an empty string if `<supported>` is True,; otherwise is contains a string describing why setting; maxIndividualTestTime is not supported.; """"""; """"""; Interface for setting maximum time to spend executing; a single test; """"""; # The current implementation needs psutil on some platforms to set; # a timeout per test. Check it's available.; # See lit.util.killProcessAndChildren(); """"""; Interface for getting the per_test_coverage value; """"""; """"""; Interface for setting the per_test_coverage value; """"""; """"""load_config(config, path) - Load a config object from an alternate; path.""""""; """"""getBashPath - Get the path to 'bash'""""""; # Check whether the found version of bash is able to cope with paths in; # the host path format. If not, don't return it as it can't be used to; # run scripts. For example, WSL's bash.exe requires '/mnt/c/foo' rather; # than 'C:\\foo' or 'C:/foo'.; # bash; # Get the file/line where this message was generated.; # Step out of _write_message, and then out of wrapper.; # In a git bash terminal, the writes to sys.stderr aren't visible; # on screen immediately. Flush them here to avoid broken/misoredered; # output.; """"""substitu",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/LitConfig.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/LitConfig.py
Integrability,message,message,"# LitConfig must be a new style class for properties to work; """"""LitConfig - Configuration data for a 'lit' test runner instance, shared; across all tests. The LitConfig object is also used to communicate with client configuration; files, it is always passed in as the global variable 'lit' so that; configuration files can access common functionality and internal components; easily.; """"""; # The name of the test runner.; # The items to add to the PATH environment variable.; # Configuration files to look for when discovering test suites.; # The default is 'summary'.; """"""; Interface for getting maximum time to spend executing; a single test; """"""; """"""; Returns a tuple (<supported> , <error message>); where; `<supported>` is True if setting maxIndividualTestTime is supported; on the current host, returns False otherwise.; `<error message>` is an empty string if `<supported>` is True,; otherwise is contains a string describing why setting; maxIndividualTestTime is not supported.; """"""; """"""; Interface for setting maximum time to spend executing; a single test; """"""; # The current implementation needs psutil on some platforms to set; # a timeout per test. Check it's available.; # See lit.util.killProcessAndChildren(); """"""; Interface for getting the per_test_coverage value; """"""; """"""; Interface for setting the per_test_coverage value; """"""; """"""load_config(config, path) - Load a config object from an alternate; path.""""""; """"""getBashPath - Get the path to 'bash'""""""; # Check whether the found version of bash is able to cope with paths in; # the host path format. If not, don't return it as it can't be used to; # run scripts. For example, WSL's bash.exe requires '/mnt/c/foo' rather; # than 'C:\\foo' or 'C:/foo'.; # bash; # Get the file/line where this message was generated.; # Step out of _write_message, and then out of wrapper.; # In a git bash terminal, the writes to sys.stderr aren't visible; # on screen immediately. Flush them here to avoid broken/misoredered; # output.; """"""substitu",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/LitConfig.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/LitConfig.py
Modifiability,config,configuration,"# LitConfig must be a new style class for properties to work; """"""LitConfig - Configuration data for a 'lit' test runner instance, shared; across all tests. The LitConfig object is also used to communicate with client configuration; files, it is always passed in as the global variable 'lit' so that; configuration files can access common functionality and internal components; easily.; """"""; # The name of the test runner.; # The items to add to the PATH environment variable.; # Configuration files to look for when discovering test suites.; # The default is 'summary'.; """"""; Interface for getting maximum time to spend executing; a single test; """"""; """"""; Returns a tuple (<supported> , <error message>); where; `<supported>` is True if setting maxIndividualTestTime is supported; on the current host, returns False otherwise.; `<error message>` is an empty string if `<supported>` is True,; otherwise is contains a string describing why setting; maxIndividualTestTime is not supported.; """"""; """"""; Interface for setting maximum time to spend executing; a single test; """"""; # The current implementation needs psutil on some platforms to set; # a timeout per test. Check it's available.; # See lit.util.killProcessAndChildren(); """"""; Interface for getting the per_test_coverage value; """"""; """"""; Interface for setting the per_test_coverage value; """"""; """"""load_config(config, path) - Load a config object from an alternate; path.""""""; """"""getBashPath - Get the path to 'bash'""""""; # Check whether the found version of bash is able to cope with paths in; # the host path format. If not, don't return it as it can't be used to; # run scripts. For example, WSL's bash.exe requires '/mnt/c/foo' rather; # than 'C:\\foo' or 'C:/foo'.; # bash; # Get the file/line where this message was generated.; # Step out of _write_message, and then out of wrapper.; # In a git bash terminal, the writes to sys.stderr aren't visible; # on screen immediately. Flush them here to avoid broken/misoredered; # output.; """"""substitu",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/LitConfig.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/LitConfig.py
Safety,timeout,timeout," properties to work; """"""LitConfig - Configuration data for a 'lit' test runner instance, shared; across all tests. The LitConfig object is also used to communicate with client configuration; files, it is always passed in as the global variable 'lit' so that; configuration files can access common functionality and internal components; easily.; """"""; # The name of the test runner.; # The items to add to the PATH environment variable.; # Configuration files to look for when discovering test suites.; # The default is 'summary'.; """"""; Interface for getting maximum time to spend executing; a single test; """"""; """"""; Returns a tuple (<supported> , <error message>); where; `<supported>` is True if setting maxIndividualTestTime is supported; on the current host, returns False otherwise.; `<error message>` is an empty string if `<supported>` is True,; otherwise is contains a string describing why setting; maxIndividualTestTime is not supported.; """"""; """"""; Interface for setting maximum time to spend executing; a single test; """"""; # The current implementation needs psutil on some platforms to set; # a timeout per test. Check it's available.; # See lit.util.killProcessAndChildren(); """"""; Interface for getting the per_test_coverage value; """"""; """"""; Interface for setting the per_test_coverage value; """"""; """"""load_config(config, path) - Load a config object from an alternate; path.""""""; """"""getBashPath - Get the path to 'bash'""""""; # Check whether the found version of bash is able to cope with paths in; # the host path format. If not, don't return it as it can't be used to; # run scripts. For example, WSL's bash.exe requires '/mnt/c/foo' rather; # than 'C:\\foo' or 'C:/foo'.; # bash; # Get the file/line where this message was generated.; # Step out of _write_message, and then out of wrapper.; # In a git bash terminal, the writes to sys.stderr aren't visible; # on screen immediately. Flush them here to avoid broken/misoredered; # output.; """"""substitute - Interpolate params into a string""""""",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/LitConfig.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/LitConfig.py
Security,access,access,"# LitConfig must be a new style class for properties to work; """"""LitConfig - Configuration data for a 'lit' test runner instance, shared; across all tests. The LitConfig object is also used to communicate with client configuration; files, it is always passed in as the global variable 'lit' so that; configuration files can access common functionality and internal components; easily.; """"""; # The name of the test runner.; # The items to add to the PATH environment variable.; # Configuration files to look for when discovering test suites.; # The default is 'summary'.; """"""; Interface for getting maximum time to spend executing; a single test; """"""; """"""; Returns a tuple (<supported> , <error message>); where; `<supported>` is True if setting maxIndividualTestTime is supported; on the current host, returns False otherwise.; `<error message>` is an empty string if `<supported>` is True,; otherwise is contains a string describing why setting; maxIndividualTestTime is not supported.; """"""; """"""; Interface for setting maximum time to spend executing; a single test; """"""; # The current implementation needs psutil on some platforms to set; # a timeout per test. Check it's available.; # See lit.util.killProcessAndChildren(); """"""; Interface for getting the per_test_coverage value; """"""; """"""; Interface for setting the per_test_coverage value; """"""; """"""load_config(config, path) - Load a config object from an alternate; path.""""""; """"""getBashPath - Get the path to 'bash'""""""; # Check whether the found version of bash is able to cope with paths in; # the host path format. If not, don't return it as it can't be used to; # run scripts. For example, WSL's bash.exe requires '/mnt/c/foo' rather; # than 'C:\\foo' or 'C:/foo'.; # bash; # Get the file/line where this message was generated.; # Step out of _write_message, and then out of wrapper.; # In a git bash terminal, the writes to sys.stderr aren't visible; # on screen immediately. Flush them here to avoid broken/misoredered; # output.; """"""substitu",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/LitConfig.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/LitConfig.py
Testability,test,test,"# LitConfig must be a new style class for properties to work; """"""LitConfig - Configuration data for a 'lit' test runner instance, shared; across all tests. The LitConfig object is also used to communicate with client configuration; files, it is always passed in as the global variable 'lit' so that; configuration files can access common functionality and internal components; easily.; """"""; # The name of the test runner.; # The items to add to the PATH environment variable.; # Configuration files to look for when discovering test suites.; # The default is 'summary'.; """"""; Interface for getting maximum time to spend executing; a single test; """"""; """"""; Returns a tuple (<supported> , <error message>); where; `<supported>` is True if setting maxIndividualTestTime is supported; on the current host, returns False otherwise.; `<error message>` is an empty string if `<supported>` is True,; otherwise is contains a string describing why setting; maxIndividualTestTime is not supported.; """"""; """"""; Interface for setting maximum time to spend executing; a single test; """"""; # The current implementation needs psutil on some platforms to set; # a timeout per test. Check it's available.; # See lit.util.killProcessAndChildren(); """"""; Interface for getting the per_test_coverage value; """"""; """"""; Interface for setting the per_test_coverage value; """"""; """"""load_config(config, path) - Load a config object from an alternate; path.""""""; """"""getBashPath - Get the path to 'bash'""""""; # Check whether the found version of bash is able to cope with paths in; # the host path format. If not, don't return it as it can't be used to; # run scripts. For example, WSL's bash.exe requires '/mnt/c/foo' rather; # than 'C:\\foo' or 'C:/foo'.; # bash; # Get the file/line where this message was generated.; # Step out of _write_message, and then out of wrapper.; # In a git bash terminal, the writes to sys.stderr aren't visible; # on screen immediately. Flush them here to avoid broken/misoredered; # output.; """"""substitu",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/LitConfig.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/LitConfig.py
Energy Efficiency,adapt,adaptor,"""""""; TestCase adaptor for providing a Python 'unittest' compatible interface to 'lit'; tests.; """"""; # Run the test.; # Adapt the result to unittest.; # Create the global config object.; # Perform test discovery.; # Return a unittest test suite which just runs the tests in order.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/LitTestCase.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/LitTestCase.py
Integrability,interface,interface,"""""""; TestCase adaptor for providing a Python 'unittest' compatible interface to 'lit'; tests.; """"""; # Run the test.; # Adapt the result to unittest.; # Create the global config object.; # Perform test discovery.; # Return a unittest test suite which just runs the tests in order.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/LitTestCase.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/LitTestCase.py
Modifiability,adapt,adaptor,"""""""; TestCase adaptor for providing a Python 'unittest' compatible interface to 'lit'; tests.; """"""; # Run the test.; # Adapt the result to unittest.; # Create the global config object.; # Perform test discovery.; # Return a unittest test suite which just runs the tests in order.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/LitTestCase.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/LitTestCase.py
Testability,test,tests,"""""""; TestCase adaptor for providing a Python 'unittest' compatible interface to 'lit'; tests.; """"""; # Run the test.; # Adapt the result to unittest.; # Create the global config object.; # Perform test discovery.; # Return a unittest test suite which just runs the tests in order.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/LitTestCase.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/LitTestCase.py
Deployability,configurat,configuration,"""""""; lit - LLVM Integrated Tester. See lit.pod for more information.; """"""; # Command line overrides configuration for maxIndividualTestTime.; # `not None` is important (default: 0); # When running multiple shards, don't include skipped tests in the xunit; # output since merging the files will result in duplicates.; # For clarity, generate a preview of the first few test indices in the shard; # to accompany the arithmetic expression.; # Create a temp directory inside the normal temp directory so that we can; # try to avoid temporary test file leaks. The user can avoid this behavior; # by setting LIT_PRESERVES_TMP in the environment, so they can easily use; # their own temp directory to monitor temporary file leaks or handle them at; # the buildbot level.; # z/OS linker does not support '_' in paths, so use '-'.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/main.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/main.py
Energy Efficiency,monitor,monitor,"""""""; lit - LLVM Integrated Tester. See lit.pod for more information.; """"""; # Command line overrides configuration for maxIndividualTestTime.; # `not None` is important (default: 0); # When running multiple shards, don't include skipped tests in the xunit; # output since merging the files will result in duplicates.; # For clarity, generate a preview of the first few test indices in the shard; # to accompany the arithmetic expression.; # Create a temp directory inside the normal temp directory so that we can; # try to avoid temporary test file leaks. The user can avoid this behavior; # by setting LIT_PRESERVES_TMP in the environment, so they can easily use; # their own temp directory to monitor temporary file leaks or handle them at; # the buildbot level.; # z/OS linker does not support '_' in paths, so use '-'.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/main.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/main.py
Modifiability,config,configuration,"""""""; lit - LLVM Integrated Tester. See lit.pod for more information.; """"""; # Command line overrides configuration for maxIndividualTestTime.; # `not None` is important (default: 0); # When running multiple shards, don't include skipped tests in the xunit; # output since merging the files will result in duplicates.; # For clarity, generate a preview of the first few test indices in the shard; # to accompany the arithmetic expression.; # Create a temp directory inside the normal temp directory so that we can; # try to avoid temporary test file leaks. The user can avoid this behavior; # by setting LIT_PRESERVES_TMP in the environment, so they can easily use; # their own temp directory to monitor temporary file leaks or handle them at; # the buildbot level.; # z/OS linker does not support '_' in paths, so use '-'.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/main.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/main.py
Safety,avoid,avoid,"""""""; lit - LLVM Integrated Tester. See lit.pod for more information.; """"""; # Command line overrides configuration for maxIndividualTestTime.; # `not None` is important (default: 0); # When running multiple shards, don't include skipped tests in the xunit; # output since merging the files will result in duplicates.; # For clarity, generate a preview of the first few test indices in the shard; # to accompany the arithmetic expression.; # Create a temp directory inside the normal temp directory so that we can; # try to avoid temporary test file leaks. The user can avoid this behavior; # by setting LIT_PRESERVES_TMP in the environment, so they can easily use; # their own temp directory to monitor temporary file leaks or handle them at; # the buildbot level.; # z/OS linker does not support '_' in paths, so use '-'.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/main.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/main.py
Testability,test,tests,"""""""; lit - LLVM Integrated Tester. See lit.pod for more information.; """"""; # Command line overrides configuration for maxIndividualTestTime.; # `not None` is important (default: 0); # When running multiple shards, don't include skipped tests in the xunit; # output since merging the files will result in duplicates.; # For clarity, generate a preview of the first few test indices in the shard; # to accompany the arithmetic expression.; # Create a temp directory inside the normal temp directory so that we can; # try to avoid temporary test file leaks. The user can avoid this behavior; # by setting LIT_PRESERVES_TMP in the environment, so they can easily use; # their own temp directory to monitor temporary file leaks or handle them at; # the buildbot level.; # z/OS linker does not support '_' in paths, so use '-'.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/main.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/main.py
Availability,down,down," `render()` method can used, which replaces; '${action}' with the string required to perform 'action':. >>> term = TerminalController(); >>> print(term.render('This is ${GREEN}green${NORMAL}')). If the terminal doesn't support a given action, then the value of; the corresponding instance variable will be set to ''. As a; result, the above code will still work on terminals that do not; support color, except that their output will not be colored.; Also, this means that you can test whether the terminal supports a; given action by simply testing the truth value of the; corresponding instance variable:. >>> term = TerminalController(); >>> if term.CLEAR_SCREEN:; ... print('This terminal supports clearning the screen.'). Finally, if the width and height of the terminal are known, then; they will be stored in the `COLS` and `LINES` attributes.; """"""; # Cursor movement:; #: Move the cursor to the beginning of the line; #: Move the cursor up one line; #: Move the cursor down one line; #: Move the cursor left one char; #: Move the cursor right one char; # Deletion:; #: Clear the screen and move to home position; #: Clear to the end of the line.; #: Clear to the beginning of the line.; #: Clear to the end of the screen; # Output modes:; #: Turn on bold mode; #: Turn on blink mode; #: Turn on half-bright mode; #: Turn on reverse-video mode; #: Turn off all modes; # Cursor display:; #: Make the cursor invisible; #: Make the cursor visible; # Terminal size:; #: Width of the terminal (None for unknown); #: Height of the terminal (None for unknown); # Foreground colors:; # Background colors:; """"""; BOL=cr UP=cuu1 DOWN=cud1 LEFT=cub1 RIGHT=cuf1; CLEAR_SCREEN=clear CLEAR_EOL=el CLEAR_BOL=el1 CLEAR_EOS=ed BOLD=bold; BLINK=blink DIM=dim REVERSE=rev UNDERLINE=smul NORMAL=sgr0; HIDE_CURSOR=cinvis SHOW_CURSOR=cnorm""""""; """"""BLACK BLUE GREEN CYAN RED MAGENTA YELLOW WHITE""""""; """"""; Create a `TerminalController` and initialize its attributes; with appropriate values for the current terminal.; `te",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/ProgressBar.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/ProgressBar.py
Deployability,update,update," (None for unknown); # Foreground colors:; # Background colors:; """"""; BOL=cr UP=cuu1 DOWN=cud1 LEFT=cub1 RIGHT=cuf1; CLEAR_SCREEN=clear CLEAR_EOL=el CLEAR_BOL=el1 CLEAR_EOS=ed BOLD=bold; BLINK=blink DIM=dim REVERSE=rev UNDERLINE=smul NORMAL=sgr0; HIDE_CURSOR=cinvis SHOW_CURSOR=cnorm""""""; """"""BLACK BLUE GREEN CYAN RED MAGENTA YELLOW WHITE""""""; """"""; Create a `TerminalController` and initialize its attributes; with appropriate values for the current terminal.; `term_stream` is the stream that will be used for terminal; output; if this stream is not a tty, then the terminal is; assumed to be a dumb terminal (i.e., have no capabilities).; """"""; # Curses isn't available on all platforms; # If the stream isn't a tty, then assume it has no capabilities.; # Check the terminal type. If we fail, then assume that the; # terminal has no capabilities.; # Look up numeric capabilities.; # Look up string capabilities.; # Colors; # String capabilities can include ""delays"" of the form ""$<2>"".; # For any modern terminal, we should be able to just ignore; # these, so strip them out.; """"""; Replace each $-substitutions in the given template string with; the corresponding terminal control string (if it's defined) or; '' (if it's not).; """"""; #######################################################################; # Example use case: progress bar; #######################################################################; """"""; A simple progress bar which doesn't need any terminal support. This prints out a progress bar like:; 'Header: 0.. 10.. 20.. ...'; """"""; # Skip second char; """"""; A 3-line progress bar, which looks like::. Header; 20% [===========----------------------------------]; progress message. The progress bar is colored, if the terminal supports color; output; and adjusts to the width of the terminal.; """"""; # BoL from col#79; # Newline from col#79; # Cursor must be fed to the next line; #: true if we haven't drawn the bar yet.; # self.update(0, ''); # ^C creates extra line. Gobble it up!",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/ProgressBar.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/ProgressBar.py
Energy Efficiency,green,green,"#!/usr/bin/env python; # Source: http://code.activestate.com/recipes/475116/, with; # modifications by Daniel Dunbar.; # Encode to UTF-8 to get binary data.; """"""; A class that can be used to portably generate formatted output to; a terminal. `TerminalController` defines a set of instance variables whose; values are initialized to the control sequence necessary to; perform a given action. These can be simply included in normal; output to the terminal:. >>> term = TerminalController(); >>> print('This is '+term.GREEN+'green'+term.NORMAL). Alternatively, the `render()` method can used, which replaces; '${action}' with the string required to perform 'action':. >>> term = TerminalController(); >>> print(term.render('This is ${GREEN}green${NORMAL}')). If the terminal doesn't support a given action, then the value of; the corresponding instance variable will be set to ''. As a; result, the above code will still work on terminals that do not; support color, except that their output will not be colored.; Also, this means that you can test whether the terminal supports a; given action by simply testing the truth value of the; corresponding instance variable:. >>> term = TerminalController(); >>> if term.CLEAR_SCREEN:; ... print('This terminal supports clearning the screen.'). Finally, if the width and height of the terminal are known, then; they will be stored in the `COLS` and `LINES` attributes.; """"""; # Cursor movement:; #: Move the cursor to the beginning of the line; #: Move the cursor up one line; #: Move the cursor down one line; #: Move the cursor left one char; #: Move the cursor right one char; # Deletion:; #: Clear the screen and move to home position; #: Clear to the end of the line.; #: Clear to the beginning of the line.; #: Clear to the end of the screen; # Output modes:; #: Turn on bold mode; #: Turn on blink mode; #: Turn on half-bright mode; #: Turn on reverse-video mode; #: Turn off all modes; # Cursor display:; #: Make the cursor invisible; #: Make the curso",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/ProgressBar.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/ProgressBar.py
Integrability,message,message," (None for unknown); # Foreground colors:; # Background colors:; """"""; BOL=cr UP=cuu1 DOWN=cud1 LEFT=cub1 RIGHT=cuf1; CLEAR_SCREEN=clear CLEAR_EOL=el CLEAR_BOL=el1 CLEAR_EOS=ed BOLD=bold; BLINK=blink DIM=dim REVERSE=rev UNDERLINE=smul NORMAL=sgr0; HIDE_CURSOR=cinvis SHOW_CURSOR=cnorm""""""; """"""BLACK BLUE GREEN CYAN RED MAGENTA YELLOW WHITE""""""; """"""; Create a `TerminalController` and initialize its attributes; with appropriate values for the current terminal.; `term_stream` is the stream that will be used for terminal; output; if this stream is not a tty, then the terminal is; assumed to be a dumb terminal (i.e., have no capabilities).; """"""; # Curses isn't available on all platforms; # If the stream isn't a tty, then assume it has no capabilities.; # Check the terminal type. If we fail, then assume that the; # terminal has no capabilities.; # Look up numeric capabilities.; # Look up string capabilities.; # Colors; # String capabilities can include ""delays"" of the form ""$<2>"".; # For any modern terminal, we should be able to just ignore; # these, so strip them out.; """"""; Replace each $-substitutions in the given template string with; the corresponding terminal control string (if it's defined) or; '' (if it's not).; """"""; #######################################################################; # Example use case: progress bar; #######################################################################; """"""; A simple progress bar which doesn't need any terminal support. This prints out a progress bar like:; 'Header: 0.. 10.. 20.. ...'; """"""; # Skip second char; """"""; A 3-line progress bar, which looks like::. Header; 20% [===========----------------------------------]; progress message. The progress bar is colored, if the terminal supports color; output; and adjusts to the width of the terminal.; """"""; # BoL from col#79; # Newline from col#79; # Cursor must be fed to the next line; #: true if we haven't drawn the bar yet.; # self.update(0, ''); # ^C creates extra line. Gobble it up!",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/ProgressBar.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/ProgressBar.py
Modifiability,portab,portably,"#!/usr/bin/env python; # Source: http://code.activestate.com/recipes/475116/, with; # modifications by Daniel Dunbar.; # Encode to UTF-8 to get binary data.; """"""; A class that can be used to portably generate formatted output to; a terminal. `TerminalController` defines a set of instance variables whose; values are initialized to the control sequence necessary to; perform a given action. These can be simply included in normal; output to the terminal:. >>> term = TerminalController(); >>> print('This is '+term.GREEN+'green'+term.NORMAL). Alternatively, the `render()` method can used, which replaces; '${action}' with the string required to perform 'action':. >>> term = TerminalController(); >>> print(term.render('This is ${GREEN}green${NORMAL}')). If the terminal doesn't support a given action, then the value of; the corresponding instance variable will be set to ''. As a; result, the above code will still work on terminals that do not; support color, except that their output will not be colored.; Also, this means that you can test whether the terminal supports a; given action by simply testing the truth value of the; corresponding instance variable:. >>> term = TerminalController(); >>> if term.CLEAR_SCREEN:; ... print('This terminal supports clearning the screen.'). Finally, if the width and height of the terminal are known, then; they will be stored in the `COLS` and `LINES` attributes.; """"""; # Cursor movement:; #: Move the cursor to the beginning of the line; #: Move the cursor up one line; #: Move the cursor down one line; #: Move the cursor left one char; #: Move the cursor right one char; # Deletion:; #: Clear the screen and move to home position; #: Clear to the end of the line.; #: Clear to the beginning of the line.; #: Clear to the end of the screen; # Output modes:; #: Turn on bold mode; #: Turn on blink mode; #: Turn on half-bright mode; #: Turn on reverse-video mode; #: Turn off all modes; # Cursor display:; #: Make the cursor invisible; #: Make the curso",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/ProgressBar.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/ProgressBar.py
Performance,perform,perform,"#!/usr/bin/env python; # Source: http://code.activestate.com/recipes/475116/, with; # modifications by Daniel Dunbar.; # Encode to UTF-8 to get binary data.; """"""; A class that can be used to portably generate formatted output to; a terminal. `TerminalController` defines a set of instance variables whose; values are initialized to the control sequence necessary to; perform a given action. These can be simply included in normal; output to the terminal:. >>> term = TerminalController(); >>> print('This is '+term.GREEN+'green'+term.NORMAL). Alternatively, the `render()` method can used, which replaces; '${action}' with the string required to perform 'action':. >>> term = TerminalController(); >>> print(term.render('This is ${GREEN}green${NORMAL}')). If the terminal doesn't support a given action, then the value of; the corresponding instance variable will be set to ''. As a; result, the above code will still work on terminals that do not; support color, except that their output will not be colored.; Also, this means that you can test whether the terminal supports a; given action by simply testing the truth value of the; corresponding instance variable:. >>> term = TerminalController(); >>> if term.CLEAR_SCREEN:; ... print('This terminal supports clearning the screen.'). Finally, if the width and height of the terminal are known, then; they will be stored in the `COLS` and `LINES` attributes.; """"""; # Cursor movement:; #: Move the cursor to the beginning of the line; #: Move the cursor up one line; #: Move the cursor down one line; #: Move the cursor left one char; #: Move the cursor right one char; # Deletion:; #: Clear the screen and move to home position; #: Clear to the end of the line.; #: Clear to the beginning of the line.; #: Clear to the end of the screen; # Output modes:; #: Turn on bold mode; #: Turn on blink mode; #: Turn on half-bright mode; #: Turn on reverse-video mode; #: Turn off all modes; # Cursor display:; #: Make the cursor invisible; #: Make the curso",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/ProgressBar.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/ProgressBar.py
Testability,test,test,"difications by Daniel Dunbar.; # Encode to UTF-8 to get binary data.; """"""; A class that can be used to portably generate formatted output to; a terminal. `TerminalController` defines a set of instance variables whose; values are initialized to the control sequence necessary to; perform a given action. These can be simply included in normal; output to the terminal:. >>> term = TerminalController(); >>> print('This is '+term.GREEN+'green'+term.NORMAL). Alternatively, the `render()` method can used, which replaces; '${action}' with the string required to perform 'action':. >>> term = TerminalController(); >>> print(term.render('This is ${GREEN}green${NORMAL}')). If the terminal doesn't support a given action, then the value of; the corresponding instance variable will be set to ''. As a; result, the above code will still work on terminals that do not; support color, except that their output will not be colored.; Also, this means that you can test whether the terminal supports a; given action by simply testing the truth value of the; corresponding instance variable:. >>> term = TerminalController(); >>> if term.CLEAR_SCREEN:; ... print('This terminal supports clearning the screen.'). Finally, if the width and height of the terminal are known, then; they will be stored in the `COLS` and `LINES` attributes.; """"""; # Cursor movement:; #: Move the cursor to the beginning of the line; #: Move the cursor up one line; #: Move the cursor down one line; #: Move the cursor left one char; #: Move the cursor right one char; # Deletion:; #: Clear the screen and move to home position; #: Clear to the end of the line.; #: Clear to the beginning of the line.; #: Clear to the end of the screen; # Output modes:; #: Turn on bold mode; #: Turn on blink mode; #: Turn on half-bright mode; #: Turn on reverse-video mode; #: Turn off all modes; # Cursor display:; #: Make the cursor invisible; #: Make the cursor visible; # Terminal size:; #: Width of the terminal (None for unknown); #: Height of ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/ProgressBar.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/ProgressBar.py
Usability,simpl,simply,"#!/usr/bin/env python; # Source: http://code.activestate.com/recipes/475116/, with; # modifications by Daniel Dunbar.; # Encode to UTF-8 to get binary data.; """"""; A class that can be used to portably generate formatted output to; a terminal. `TerminalController` defines a set of instance variables whose; values are initialized to the control sequence necessary to; perform a given action. These can be simply included in normal; output to the terminal:. >>> term = TerminalController(); >>> print('This is '+term.GREEN+'green'+term.NORMAL). Alternatively, the `render()` method can used, which replaces; '${action}' with the string required to perform 'action':. >>> term = TerminalController(); >>> print(term.render('This is ${GREEN}green${NORMAL}')). If the terminal doesn't support a given action, then the value of; the corresponding instance variable will be set to ''. As a; result, the above code will still work on terminals that do not; support color, except that their output will not be colored.; Also, this means that you can test whether the terminal supports a; given action by simply testing the truth value of the; corresponding instance variable:. >>> term = TerminalController(); >>> if term.CLEAR_SCREEN:; ... print('This terminal supports clearning the screen.'). Finally, if the width and height of the terminal are known, then; they will be stored in the `COLS` and `LINES` attributes.; """"""; # Cursor movement:; #: Move the cursor to the beginning of the line; #: Move the cursor up one line; #: Move the cursor down one line; #: Move the cursor left one char; #: Move the cursor right one char; # Deletion:; #: Clear the screen and move to home position; #: Clear to the end of the line.; #: Clear to the beginning of the line.; #: Clear to the end of the screen; # Output modes:; #: Turn on bold mode; #: Turn on blink mode; #: Turn on half-bright mode; #: Turn on reverse-video mode; #: Turn off all modes; # Cursor display:; #: Make the cursor invisible; #: Make the curso",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/ProgressBar.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/ProgressBar.py
Deployability,configurat,configuration,"# Suite names are not necessarily unique. Include object identity in sort; # key to avoid mixing tests of different suites.; # Construct the data we will write.; # Encode the current lit version as a schema version.; # FIXME: Record some information on the lit configuration used?; # FIXME: Record information from the individual test suites?; # Encode the tests.; # Add test metrics, if present.; # Report micro-tests separately, if present; # Expand parent test name with micro test name; # According to the XML 1.0 spec, control characters other than; # \t,\r, and \n are not permitted anywhere in the document; # (https://www.w3.org/TR/xml/#charsets) and therefore this function; # removes them to produce a valid XML document.; #; # Note: In XML 1.1 only \0 is illegal (https://www.w3.org/TR/xml11/#charsets); # but lit currently produces XML 1.0 output.; # In the unlikely case that the output contains the CDATA; # terminator we wrap it by creating a new CDATA block.; # Failing test output sometimes contains control characters like; # \x1b (e.g. if there was some -fcolor-diagnostics output) which are; # not allowed inside XML files.; # This causes problems with CI systems: for example, the Jenkins; # JUnit XML will throw an exception when ecountering those; # characters and similar problems also occur with GitLab CI.; # Encode the tests.; # Expand parent test name with micro test name; # Find when first test started so we can make start times relative.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/reports.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/reports.py
Integrability,wrap,wrap,"# Suite names are not necessarily unique. Include object identity in sort; # key to avoid mixing tests of different suites.; # Construct the data we will write.; # Encode the current lit version as a schema version.; # FIXME: Record some information on the lit configuration used?; # FIXME: Record information from the individual test suites?; # Encode the tests.; # Add test metrics, if present.; # Report micro-tests separately, if present; # Expand parent test name with micro test name; # According to the XML 1.0 spec, control characters other than; # \t,\r, and \n are not permitted anywhere in the document; # (https://www.w3.org/TR/xml/#charsets) and therefore this function; # removes them to produce a valid XML document.; #; # Note: In XML 1.1 only \0 is illegal (https://www.w3.org/TR/xml11/#charsets); # but lit currently produces XML 1.0 output.; # In the unlikely case that the output contains the CDATA; # terminator we wrap it by creating a new CDATA block.; # Failing test output sometimes contains control characters like; # \x1b (e.g. if there was some -fcolor-diagnostics output) which are; # not allowed inside XML files.; # This causes problems with CI systems: for example, the Jenkins; # JUnit XML will throw an exception when ecountering those; # characters and similar problems also occur with GitLab CI.; # Encode the tests.; # Expand parent test name with micro test name; # Find when first test started so we can make start times relative.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/reports.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/reports.py
Modifiability,config,configuration,"# Suite names are not necessarily unique. Include object identity in sort; # key to avoid mixing tests of different suites.; # Construct the data we will write.; # Encode the current lit version as a schema version.; # FIXME: Record some information on the lit configuration used?; # FIXME: Record information from the individual test suites?; # Encode the tests.; # Add test metrics, if present.; # Report micro-tests separately, if present; # Expand parent test name with micro test name; # According to the XML 1.0 spec, control characters other than; # \t,\r, and \n are not permitted anywhere in the document; # (https://www.w3.org/TR/xml/#charsets) and therefore this function; # removes them to produce a valid XML document.; #; # Note: In XML 1.1 only \0 is illegal (https://www.w3.org/TR/xml11/#charsets); # but lit currently produces XML 1.0 output.; # In the unlikely case that the output contains the CDATA; # terminator we wrap it by creating a new CDATA block.; # Failing test output sometimes contains control characters like; # \x1b (e.g. if there was some -fcolor-diagnostics output) which are; # not allowed inside XML files.; # This causes problems with CI systems: for example, the Jenkins; # JUnit XML will throw an exception when ecountering those; # characters and similar problems also occur with GitLab CI.; # Encode the tests.; # Expand parent test name with micro test name; # Find when first test started so we can make start times relative.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/reports.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/reports.py
Safety,avoid,avoid,"# Suite names are not necessarily unique. Include object identity in sort; # key to avoid mixing tests of different suites.; # Construct the data we will write.; # Encode the current lit version as a schema version.; # FIXME: Record some information on the lit configuration used?; # FIXME: Record information from the individual test suites?; # Encode the tests.; # Add test metrics, if present.; # Report micro-tests separately, if present; # Expand parent test name with micro test name; # According to the XML 1.0 spec, control characters other than; # \t,\r, and \n are not permitted anywhere in the document; # (https://www.w3.org/TR/xml/#charsets) and therefore this function; # removes them to produce a valid XML document.; #; # Note: In XML 1.1 only \0 is illegal (https://www.w3.org/TR/xml11/#charsets); # but lit currently produces XML 1.0 output.; # In the unlikely case that the output contains the CDATA; # terminator we wrap it by creating a new CDATA block.; # Failing test output sometimes contains control characters like; # \x1b (e.g. if there was some -fcolor-diagnostics output) which are; # not allowed inside XML files.; # This causes problems with CI systems: for example, the Jenkins; # JUnit XML will throw an exception when ecountering those; # characters and similar problems also occur with GitLab CI.; # Encode the tests.; # Expand parent test name with micro test name; # Find when first test started so we can make start times relative.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/reports.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/reports.py
Testability,test,tests,"# Suite names are not necessarily unique. Include object identity in sort; # key to avoid mixing tests of different suites.; # Construct the data we will write.; # Encode the current lit version as a schema version.; # FIXME: Record some information on the lit configuration used?; # FIXME: Record information from the individual test suites?; # Encode the tests.; # Add test metrics, if present.; # Report micro-tests separately, if present; # Expand parent test name with micro test name; # According to the XML 1.0 spec, control characters other than; # \t,\r, and \n are not permitted anywhere in the document; # (https://www.w3.org/TR/xml/#charsets) and therefore this function; # removes them to produce a valid XML document.; #; # Note: In XML 1.1 only \0 is illegal (https://www.w3.org/TR/xml11/#charsets); # but lit currently produces XML 1.0 output.; # In the unlikely case that the output contains the CDATA; # terminator we wrap it by creating a new CDATA block.; # Failing test output sometimes contains control characters like; # \x1b (e.g. if there was some -fcolor-diagnostics output) which are; # not allowed inside XML files.; # This causes problems with CI systems: for example, the Jenkins; # JUnit XML will throw an exception when ecountering those; # characters and similar problems also occur with GitLab CI.; # Encode the tests.; # Expand parent test name with micro test name; # Find when first test started so we can make start times relative.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/reports.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/reports.py
Availability,avail,available,"""""""A concrete, configured testing run.""""""; """"""; Execute the tests in the run using up to the specified number of; parallel tasks, and inform the caller of each individual result. The; provided tests should be a subset of the tests available in this run; object. The progress_callback will be invoked for each completed test. If timeout is non-None, it should be a time in seconds after which to; stop executing tests. Returns the elapsed testing time. Upon completion, each test in the run will have its result; computed. Tests which were not actually executed (for any reason) will; be marked SKIPPED.; """"""; # Larger timeouts (one year, positive infinity) don't work on Windows.; # days * hours * minutes * seconds; # Update local test object ""in place"" from remote test object. This; # ensures that the original test object which is used for printing test; # results reflects the changes.; # Needed for getMissingRequiredFeatures(); # TODO(yln): interferes with progress bar; # Some tests use threads internally, and at least on Linux each of these; # threads counts toward the current process limit. Try to raise the (soft); # process limit so that tests don't fail due to resource exhaustion.; # the 2 is a safety factor; # Importing the resource module will likely fail on Windows.; # Warn, unless this is Windows, in which case this is expected.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/run.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/run.py
Modifiability,config,configured,"""""""A concrete, configured testing run.""""""; """"""; Execute the tests in the run using up to the specified number of; parallel tasks, and inform the caller of each individual result. The; provided tests should be a subset of the tests available in this run; object. The progress_callback will be invoked for each completed test. If timeout is non-None, it should be a time in seconds after which to; stop executing tests. Returns the elapsed testing time. Upon completion, each test in the run will have its result; computed. Tests which were not actually executed (for any reason) will; be marked SKIPPED.; """"""; # Larger timeouts (one year, positive infinity) don't work on Windows.; # days * hours * minutes * seconds; # Update local test object ""in place"" from remote test object. This; # ensures that the original test object which is used for printing test; # results reflects the changes.; # Needed for getMissingRequiredFeatures(); # TODO(yln): interferes with progress bar; # Some tests use threads internally, and at least on Linux each of these; # threads counts toward the current process limit. Try to raise the (soft); # process limit so that tests don't fail due to resource exhaustion.; # the 2 is a safety factor; # Importing the resource module will likely fail on Windows.; # Warn, unless this is Windows, in which case this is expected.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/run.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/run.py
Safety,timeout,timeout,"""""""A concrete, configured testing run.""""""; """"""; Execute the tests in the run using up to the specified number of; parallel tasks, and inform the caller of each individual result. The; provided tests should be a subset of the tests available in this run; object. The progress_callback will be invoked for each completed test. If timeout is non-None, it should be a time in seconds after which to; stop executing tests. Returns the elapsed testing time. Upon completion, each test in the run will have its result; computed. Tests which were not actually executed (for any reason) will; be marked SKIPPED.; """"""; # Larger timeouts (one year, positive infinity) don't work on Windows.; # days * hours * minutes * seconds; # Update local test object ""in place"" from remote test object. This; # ensures that the original test object which is used for printing test; # results reflects the changes.; # Needed for getMissingRequiredFeatures(); # TODO(yln): interferes with progress bar; # Some tests use threads internally, and at least on Linux each of these; # threads counts toward the current process limit. Try to raise the (soft); # process limit so that tests don't fail due to resource exhaustion.; # the 2 is a safety factor; # Importing the resource module will likely fail on Windows.; # Warn, unless this is Windows, in which case this is expected.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/run.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/run.py
Testability,test,testing,"""""""A concrete, configured testing run.""""""; """"""; Execute the tests in the run using up to the specified number of; parallel tasks, and inform the caller of each individual result. The; provided tests should be a subset of the tests available in this run; object. The progress_callback will be invoked for each completed test. If timeout is non-None, it should be a time in seconds after which to; stop executing tests. Returns the elapsed testing time. Upon completion, each test in the run will have its result; computed. Tests which were not actually executed (for any reason) will; be marked SKIPPED.; """"""; # Larger timeouts (one year, positive infinity) don't work on Windows.; # days * hours * minutes * seconds; # Update local test object ""in place"" from remote test object. This; # ensures that the original test object which is used for printing test; # results reflects the changes.; # Needed for getMissingRequiredFeatures(); # TODO(yln): interferes with progress bar; # Some tests use threads internally, and at least on Linux each of these; # threads counts toward the current process limit. Try to raise the (soft); # process limit so that tests don't fail due to resource exhaustion.; # the 2 is a safety factor; # Importing the resource module will likely fail on Windows.; # Warn, unless this is Windows, in which case this is expected.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/run.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/run.py
Usability,progress bar,progress bar,"""""""A concrete, configured testing run.""""""; """"""; Execute the tests in the run using up to the specified number of; parallel tasks, and inform the caller of each individual result. The; provided tests should be a subset of the tests available in this run; object. The progress_callback will be invoked for each completed test. If timeout is non-None, it should be a time in seconds after which to; stop executing tests. Returns the elapsed testing time. Upon completion, each test in the run will have its result; computed. Tests which were not actually executed (for any reason) will; be marked SKIPPED.; """"""; # Larger timeouts (one year, positive infinity) don't work on Windows.; # days * hours * minutes * seconds; # Update local test object ""in place"" from remote test object. This; # ensures that the original test object which is used for printing test; # results reflects the changes.; # Needed for getMissingRequiredFeatures(); # TODO(yln): interferes with progress bar; # Some tests use threads internally, and at least on Linux each of these; # threads counts toward the current process limit. Try to raise the (soft); # process limit so that tests don't fail due to resource exhaustion.; # the 2 is a safety factor; # Importing the resource module will likely fail on Windows.; # Warn, unless this is Windows, in which case this is expected.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/run.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/run.py
Security,validat,validation,# For debugging / validation.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/ShCommands.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/ShCommands.py
Modifiability,portab,portable,"""""""; maybe_eat(c) - Consume the character c if it is the next character,; returning True if a character was consumed.""""""; # Get the leading whitespace free section.; # If it has special characters, the fast path failed.; # This is an annoying case; we treat '2>' as a single token so; # we don't have to track whitespace tokens.; # If the parse string isn't an integer, do the usual thing.; # Otherwise, lex the operator and convert to a redirection; # token.; # Outside of a string, '\\' escapes everything.; # If a quote character is present, lex_arg_quoted will remove the quotes; # and append the argument directly. This causes a problem when the; # quoted portion contains a glob character, as the character will no; # longer be treated literally. If glob characters occur *only* inside; # of quotes, then we can handle this by not globbing at all, and if; # glob characters occur *only* outside of quotes, we can still glob just; # fine. But if a glob character occurs both inside and outside of; # quotes this presents a problem. In practice this is such an obscure; # edge case that it doesn't seem worth the added complexity to support.; # By adding an assertion, it means some bot somewhere will catch this; # and flag the user of a non-portable test (which could almost certainly; # be re-written to work correctly without triggering this).; # Inside a '""' quoted string, '\\' only escapes the quote; # character and backslash, otherwise it is preserved.; #; """"""; lex_one_token - Lex a single 'sh' token.""""""; ###; # EOF?; # If this is an argument, just add it to the current command.; # Otherwise see if it is a terminator.; # Otherwise it must be a redirection.; # FIXME: Operator precedence!!",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/ShUtil.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/ShUtil.py
Testability,assert,assertion,"""""""; maybe_eat(c) - Consume the character c if it is the next character,; returning True if a character was consumed.""""""; # Get the leading whitespace free section.; # If it has special characters, the fast path failed.; # This is an annoying case; we treat '2>' as a single token so; # we don't have to track whitespace tokens.; # If the parse string isn't an integer, do the usual thing.; # Otherwise, lex the operator and convert to a redirection; # token.; # Outside of a string, '\\' escapes everything.; # If a quote character is present, lex_arg_quoted will remove the quotes; # and append the argument directly. This causes a problem when the; # quoted portion contains a glob character, as the character will no; # longer be treated literally. If glob characters occur *only* inside; # of quotes, then we can handle this by not globbing at all, and if; # glob characters occur *only* outside of quotes, we can still glob just; # fine. But if a glob character occurs both inside and outside of; # quotes this presents a problem. In practice this is such an obscure; # edge case that it doesn't seem worth the added complexity to support.; # By adding an assertion, it means some bot somewhere will catch this; # and flag the user of a non-portable test (which could almost certainly; # be re-written to work correctly without triggering this).; # Inside a '""' quoted string, '\\' only escapes the quote; # character and backslash, otherwise it is preserved.; #; """"""; lex_one_token - Lex a single 'sh' token.""""""; ###; # EOF?; # If this is an argument, just add it to the current command.; # Otherwise see if it is a terminator.; # Otherwise it must be a redirection.; # FIXME: Operator precedence!!",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/ShUtil.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/ShUtil.py
Availability,error,error,"part of the; console output.; """"""; """"""; todata() -> json-serializable data. Convert this metric to content suitable for serializing in the JSON test; output.; """"""; """"""; JSONMetricValue is used for types that are representable in the output; but that are otherwise uninterpreted.; """"""; # Ensure the value is a serializable by trying to encode it.; # WARNING: The value may change before it is encoded again, and may; # not be encodable after the change.; # 'long' is only present in python2; # Try to create a JSONMetricValue and let the constructor throw; # if value is not a valid type.; # Test results.; """"""Wrapper for the results of executing an individual test.""""""; # The result code.; # The test output.; # The wall timing to execute the test, if timing.; # The metrics reported by this test.; # The micro-test results reported by this test.; """"""; addMetric(name, value). Attach a test metric to the test result, with the given name and list of; values. It is an error to attempt to attach the metrics with the same; name multiple times. Each value must be an instance of a MetricValue subclass.; """"""; """"""; addMicroResult(microResult). Attach a micro-test result to the test result, with the given name and; result. It is an error to attempt to attach a micro-test with the; same name multiple times. Each micro-test result must be an instance of the Result class.; """"""; # Test classes.; """"""TestSuite - Information on a group of tests. A test suite groups together a set of logically related tests.; """"""; # The test suite configuration.; """"""Test - Information on a single test instance.""""""; # A list of conditions under which this test is expected to fail.; # Each condition is a boolean expression of features, or '*'.; # These can optionally be provided by test format handlers,; # and will be honored when the test result is supplied.; # If true, ignore all items in self.xfails.; # A list of conditions that must be satisfied before running the test.; # Each condition is a boolean expressio",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/Test.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/Test.py
Deployability,configurat,configuration,"ue and let the constructor throw; # if value is not a valid type.; # Test results.; """"""Wrapper for the results of executing an individual test.""""""; # The result code.; # The test output.; # The wall timing to execute the test, if timing.; # The metrics reported by this test.; # The micro-test results reported by this test.; """"""; addMetric(name, value). Attach a test metric to the test result, with the given name and list of; values. It is an error to attempt to attach the metrics with the same; name multiple times. Each value must be an instance of a MetricValue subclass.; """"""; """"""; addMicroResult(microResult). Attach a micro-test result to the test result, with the given name and; result. It is an error to attempt to attach a micro-test with the; same name multiple times. Each micro-test result must be an instance of the Result class.; """"""; # Test classes.; """"""TestSuite - Information on a group of tests. A test suite groups together a set of logically related tests.; """"""; # The test suite configuration.; """"""Test - Information on a single test instance.""""""; # A list of conditions under which this test is expected to fail.; # Each condition is a boolean expression of features, or '*'.; # These can optionally be provided by test format handlers,; # and will be honored when the test result is supplied.; # If true, ignore all items in self.xfails.; # A list of conditions that must be satisfied before running the test.; # Each condition is a boolean expression of features. All of them; # must be True for the test to run.; # A list of conditions that prevent execution of the test.; # Each condition is a boolean expression of features. All of them; # must be False for the test to run.; # An optional number of retries allowed before the test finally succeeds.; # The test is run at most once plus the number of retries specified here.; # The test result, once complete.; # The previous test failure state, if applicable.; # The previous test elapsed time, if applicable.; # Synt",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/Test.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/Test.py
Modifiability,config,configuration,"ue and let the constructor throw; # if value is not a valid type.; # Test results.; """"""Wrapper for the results of executing an individual test.""""""; # The result code.; # The test output.; # The wall timing to execute the test, if timing.; # The metrics reported by this test.; # The micro-test results reported by this test.; """"""; addMetric(name, value). Attach a test metric to the test result, with the given name and list of; values. It is an error to attempt to attach the metrics with the same; name multiple times. Each value must be an instance of a MetricValue subclass.; """"""; """"""; addMicroResult(microResult). Attach a micro-test result to the test result, with the given name and; result. It is an error to attempt to attach a micro-test with the; same name multiple times. Each micro-test result must be an instance of the Result class.; """"""; # Test classes.; """"""TestSuite - Information on a group of tests. A test suite groups together a set of logically related tests.; """"""; # The test suite configuration.; """"""Test - Information on a single test instance.""""""; # A list of conditions under which this test is expected to fail.; # Each condition is a boolean expression of features, or '*'.; # These can optionally be provided by test format handlers,; # and will be honored when the test result is supplied.; # If true, ignore all items in self.xfails.; # A list of conditions that must be satisfied before running the test.; # Each condition is a boolean expression of features. All of them; # must be True for the test to run.; # A list of conditions that prevent execution of the test.; # Each condition is a boolean expression of features. All of them; # must be False for the test to run.; # An optional number of retries allowed before the test finally succeeds.; # The test is run at most once plus the number of retries specified here.; # The test result, once complete.; # The previous test failure state, if applicable.; # The previous test elapsed time, if applicable.; # Synt",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/Test.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/Test.py
Testability,test,test,"# Test result codes.; """"""Test result codes.""""""; # All result codes (including user-defined ones) in declaration order; # We override __new__ and __getnewargs__ to ensure that pickling still; # provides unique ResultCode objects in any particular instance.; # Successes; # Failures; # Test metric values.; """"""; format() -> str. Convert this metric to a string suitable for displaying as part of the; console output.; """"""; """"""; todata() -> json-serializable data. Convert this metric to content suitable for serializing in the JSON test; output.; """"""; """"""; JSONMetricValue is used for types that are representable in the output; but that are otherwise uninterpreted.; """"""; # Ensure the value is a serializable by trying to encode it.; # WARNING: The value may change before it is encoded again, and may; # not be encodable after the change.; # 'long' is only present in python2; # Try to create a JSONMetricValue and let the constructor throw; # if value is not a valid type.; # Test results.; """"""Wrapper for the results of executing an individual test.""""""; # The result code.; # The test output.; # The wall timing to execute the test, if timing.; # The metrics reported by this test.; # The micro-test results reported by this test.; """"""; addMetric(name, value). Attach a test metric to the test result, with the given name and list of; values. It is an error to attempt to attach the metrics with the same; name multiple times. Each value must be an instance of a MetricValue subclass.; """"""; """"""; addMicroResult(microResult). Attach a micro-test result to the test result, with the given name and; result. It is an error to attempt to attach a micro-test with the; same name multiple times. Each micro-test result must be an instance of the Result class.; """"""; # Test classes.; """"""TestSuite - Information on a group of tests. A test suite groups together a set of logically related tests.; """"""; # The test suite configuration.; """"""Test - Information on a single test instance.""""""; # A list of conditi",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/Test.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/Test.py
Availability,avail,available,"""""""; TestingConfig - Information on the tests inside a suite.; """"""; """"""; fromdefaults(litConfig) -> TestingConfig. Create a TestingConfig object with default values.; """"""; # Set the environment based on the command line arguments.; # Avoid Windows heuristics which try to detect potential installer; # programs (which may need to run with elevated privileges) and ask; # if the user wants to run them in that way. This heuristic may; # match for executables containing the substrings ""patch"" (which is; # a substring of ""dispatch""), ""update"", ""setup"", etc. Set this; # environment variable indicating that we want to execute them with; # the current user.; # Check for empty string as some variables such as LD_PRELOAD cannot be empty; # ('') for OS's such as OpenBSD.; # Set the default available features based on the LitConfig.; """"""; load_from_path(path, litConfig). Load the configuration module at the provided path into the given config; object.; """"""; # Load the config script data.; # Execute the config script to initialize the object.; # We allow normal system exit inside a config file to just; # return control without error.; # This list is used by TestRunner.py to restrict running only tests that; # require one of the features in this list if this list is non-empty.; # Configurations can set this list to restrict the set of tests to run.; """"""finish() - Finish this config object, after loading is complete.""""""; # FIXME: This should really only be suite in test suite config; # files. Should we distinguish them?; # FIXME: This should really only be suite in test suite config; # files. Should we distinguish them?; """"""root attribute - The root configuration for the test suite.""""""; """"""; Helper class to indicate that the substitutions contains backreferences. This can be used as the following in lit.cfg to mark subsitutions as having; back-references::. config.substutions.append(('\b[^ ]*.cpp', SubstituteCaptures('\0.txt'))). """"""",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/TestingConfig.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/TestingConfig.py
Deployability,install,installer,"""""""; TestingConfig - Information on the tests inside a suite.; """"""; """"""; fromdefaults(litConfig) -> TestingConfig. Create a TestingConfig object with default values.; """"""; # Set the environment based on the command line arguments.; # Avoid Windows heuristics which try to detect potential installer; # programs (which may need to run with elevated privileges) and ask; # if the user wants to run them in that way. This heuristic may; # match for executables containing the substrings ""patch"" (which is; # a substring of ""dispatch""), ""update"", ""setup"", etc. Set this; # environment variable indicating that we want to execute them with; # the current user.; # Check for empty string as some variables such as LD_PRELOAD cannot be empty; # ('') for OS's such as OpenBSD.; # Set the default available features based on the LitConfig.; """"""; load_from_path(path, litConfig). Load the configuration module at the provided path into the given config; object.; """"""; # Load the config script data.; # Execute the config script to initialize the object.; # We allow normal system exit inside a config file to just; # return control without error.; # This list is used by TestRunner.py to restrict running only tests that; # require one of the features in this list if this list is non-empty.; # Configurations can set this list to restrict the set of tests to run.; """"""finish() - Finish this config object, after loading is complete.""""""; # FIXME: This should really only be suite in test suite config; # files. Should we distinguish them?; # FIXME: This should really only be suite in test suite config; # files. Should we distinguish them?; """"""root attribute - The root configuration for the test suite.""""""; """"""; Helper class to indicate that the substitutions contains backreferences. This can be used as the following in lit.cfg to mark subsitutions as having; back-references::. config.substutions.append(('\b[^ ]*.cpp', SubstituteCaptures('\0.txt'))). """"""",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/TestingConfig.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/TestingConfig.py
Modifiability,variab,variable,"""""""; TestingConfig - Information on the tests inside a suite.; """"""; """"""; fromdefaults(litConfig) -> TestingConfig. Create a TestingConfig object with default values.; """"""; # Set the environment based on the command line arguments.; # Avoid Windows heuristics which try to detect potential installer; # programs (which may need to run with elevated privileges) and ask; # if the user wants to run them in that way. This heuristic may; # match for executables containing the substrings ""patch"" (which is; # a substring of ""dispatch""), ""update"", ""setup"", etc. Set this; # environment variable indicating that we want to execute them with; # the current user.; # Check for empty string as some variables such as LD_PRELOAD cannot be empty; # ('') for OS's such as OpenBSD.; # Set the default available features based on the LitConfig.; """"""; load_from_path(path, litConfig). Load the configuration module at the provided path into the given config; object.; """"""; # Load the config script data.; # Execute the config script to initialize the object.; # We allow normal system exit inside a config file to just; # return control without error.; # This list is used by TestRunner.py to restrict running only tests that; # require one of the features in this list if this list is non-empty.; # Configurations can set this list to restrict the set of tests to run.; """"""finish() - Finish this config object, after loading is complete.""""""; # FIXME: This should really only be suite in test suite config; # files. Should we distinguish them?; # FIXME: This should really only be suite in test suite config; # files. Should we distinguish them?; """"""root attribute - The root configuration for the test suite.""""""; """"""; Helper class to indicate that the substitutions contains backreferences. This can be used as the following in lit.cfg to mark subsitutions as having; back-references::. config.substutions.append(('\b[^ ]*.cpp', SubstituteCaptures('\0.txt'))). """"""",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/TestingConfig.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/TestingConfig.py
Performance,load,loading,"""""""; TestingConfig - Information on the tests inside a suite.; """"""; """"""; fromdefaults(litConfig) -> TestingConfig. Create a TestingConfig object with default values.; """"""; # Set the environment based on the command line arguments.; # Avoid Windows heuristics which try to detect potential installer; # programs (which may need to run with elevated privileges) and ask; # if the user wants to run them in that way. This heuristic may; # match for executables containing the substrings ""patch"" (which is; # a substring of ""dispatch""), ""update"", ""setup"", etc. Set this; # environment variable indicating that we want to execute them with; # the current user.; # Check for empty string as some variables such as LD_PRELOAD cannot be empty; # ('') for OS's such as OpenBSD.; # Set the default available features based on the LitConfig.; """"""; load_from_path(path, litConfig). Load the configuration module at the provided path into the given config; object.; """"""; # Load the config script data.; # Execute the config script to initialize the object.; # We allow normal system exit inside a config file to just; # return control without error.; # This list is used by TestRunner.py to restrict running only tests that; # require one of the features in this list if this list is non-empty.; # Configurations can set this list to restrict the set of tests to run.; """"""finish() - Finish this config object, after loading is complete.""""""; # FIXME: This should really only be suite in test suite config; # files. Should we distinguish them?; # FIXME: This should really only be suite in test suite config; # files. Should we distinguish them?; """"""root attribute - The root configuration for the test suite.""""""; """"""; Helper class to indicate that the substitutions contains backreferences. This can be used as the following in lit.cfg to mark subsitutions as having; back-references::. config.substutions.append(('\b[^ ]*.cpp', SubstituteCaptures('\0.txt'))). """"""",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/TestingConfig.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/TestingConfig.py
Safety,detect,detect,"""""""; TestingConfig - Information on the tests inside a suite.; """"""; """"""; fromdefaults(litConfig) -> TestingConfig. Create a TestingConfig object with default values.; """"""; # Set the environment based on the command line arguments.; # Avoid Windows heuristics which try to detect potential installer; # programs (which may need to run with elevated privileges) and ask; # if the user wants to run them in that way. This heuristic may; # match for executables containing the substrings ""patch"" (which is; # a substring of ""dispatch""), ""update"", ""setup"", etc. Set this; # environment variable indicating that we want to execute them with; # the current user.; # Check for empty string as some variables such as LD_PRELOAD cannot be empty; # ('') for OS's such as OpenBSD.; # Set the default available features based on the LitConfig.; """"""; load_from_path(path, litConfig). Load the configuration module at the provided path into the given config; object.; """"""; # Load the config script data.; # Execute the config script to initialize the object.; # We allow normal system exit inside a config file to just; # return control without error.; # This list is used by TestRunner.py to restrict running only tests that; # require one of the features in this list if this list is non-empty.; # Configurations can set this list to restrict the set of tests to run.; """"""finish() - Finish this config object, after loading is complete.""""""; # FIXME: This should really only be suite in test suite config; # files. Should we distinguish them?; # FIXME: This should really only be suite in test suite config; # files. Should we distinguish them?; """"""root attribute - The root configuration for the test suite.""""""; """"""; Helper class to indicate that the substitutions contains backreferences. This can be used as the following in lit.cfg to mark subsitutions as having; back-references::. config.substutions.append(('\b[^ ]*.cpp', SubstituteCaptures('\0.txt'))). """"""",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/TestingConfig.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/TestingConfig.py
Testability,test,tests,"""""""; TestingConfig - Information on the tests inside a suite.; """"""; """"""; fromdefaults(litConfig) -> TestingConfig. Create a TestingConfig object with default values.; """"""; # Set the environment based on the command line arguments.; # Avoid Windows heuristics which try to detect potential installer; # programs (which may need to run with elevated privileges) and ask; # if the user wants to run them in that way. This heuristic may; # match for executables containing the substrings ""patch"" (which is; # a substring of ""dispatch""), ""update"", ""setup"", etc. Set this; # environment variable indicating that we want to execute them with; # the current user.; # Check for empty string as some variables such as LD_PRELOAD cannot be empty; # ('') for OS's such as OpenBSD.; # Set the default available features based on the LitConfig.; """"""; load_from_path(path, litConfig). Load the configuration module at the provided path into the given config; object.; """"""; # Load the config script data.; # Execute the config script to initialize the object.; # We allow normal system exit inside a config file to just; # return control without error.; # This list is used by TestRunner.py to restrict running only tests that; # require one of the features in this list if this list is non-empty.; # Configurations can set this list to restrict the set of tests to run.; """"""finish() - Finish this config object, after loading is complete.""""""; # FIXME: This should really only be suite in test suite config; # files. Should we distinguish them?; # FIXME: This should really only be suite in test suite config; # files. Should we distinguish them?; """"""root attribute - The root configuration for the test suite.""""""; """"""; Helper class to indicate that the substitutions contains backreferences. This can be used as the following in lit.cfg to mark subsitutions as having; back-references::. config.substutions.append(('\b[^ ]*.cpp', SubstituteCaptures('\0.txt'))). """"""",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/TestingConfig.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/TestingConfig.py
Availability,error,error,"""""""; A script had a fatal error such that there's no point in retrying. The; message has not been emitted on stdout or stderr but is instead included in; this exception.; """"""; # Don't use close_fds on Windows.; # Use temporary files to replace /dev/null on Windows.; # A regex that matches %dbg(ARG), which lit inserts at the beginning of each; # run command pipeline such that ARG specifies the pipeline's source line; # number. lit later expands each %dbg(ARG) to a command that behaves as a null; # command in the target shell so that the line number is seen in lit's verbose; # mode.; #; # This regex captures ARG. ARG must not contain a right parenthesis, which; # terminates %dbg. ARG must not contain quotes, in which ARG might be enclosed; # during expansion.; #; # COMMAND that follows %dbg(ARG) is also captured. COMMAND can be; # empty as a result of conditinal substitution.; """"""Mutable shell environment containing things like CWD and env vars. Environment variables are not implemented, but cwd tracking is. In addition,; we maintain a dir stack for pushd/popd.; """"""; """"""; Object used to helper manage enforcing a timeout in; _executeShCmd(). It is passed through recursive calls; to collect processes that have been executed so that when; the timeout happens they can be killed.; """"""; # This lock will be used to protect concurrent access; # to _procs and _doneKillPass; # Avoid re-entering the lock by finding out if kill needs to be run; # again here but call it if necessary once we have left the lock.; # We could use a reentrant lock here instead but this code seems; # clearer to me.; # The initial call to _kill() from the timer thread already happened so; # we need to call it again from this thread, otherwise this process; # will be left to run even though the timeout was already hit; # Do some late initialisation that's only needed; # if there is a timeout set; """"""; This method may be called multiple times as we might get unlucky; and be in the middle of creating a new p",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/TestRunner.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/TestRunner.py
Deployability,pipeline,pipeline,"""""""; A script had a fatal error such that there's no point in retrying. The; message has not been emitted on stdout or stderr but is instead included in; this exception.; """"""; # Don't use close_fds on Windows.; # Use temporary files to replace /dev/null on Windows.; # A regex that matches %dbg(ARG), which lit inserts at the beginning of each; # run command pipeline such that ARG specifies the pipeline's source line; # number. lit later expands each %dbg(ARG) to a command that behaves as a null; # command in the target shell so that the line number is seen in lit's verbose; # mode.; #; # This regex captures ARG. ARG must not contain a right parenthesis, which; # terminates %dbg. ARG must not contain quotes, in which ARG might be enclosed; # during expansion.; #; # COMMAND that follows %dbg(ARG) is also captured. COMMAND can be; # empty as a result of conditinal substitution.; """"""Mutable shell environment containing things like CWD and env vars. Environment variables are not implemented, but cwd tracking is. In addition,; we maintain a dir stack for pushd/popd.; """"""; """"""; Object used to helper manage enforcing a timeout in; _executeShCmd(). It is passed through recursive calls; to collect processes that have been executed so that when; the timeout happens they can be killed.; """"""; # This lock will be used to protect concurrent access; # to _procs and _doneKillPass; # Avoid re-entering the lock by finding out if kill needs to be run; # again here but call it if necessary once we have left the lock.; # We could use a reentrant lock here instead but this code seems; # clearer to me.; # The initial call to _kill() from the timer thread already happened so; # we need to call it again from this thread, otherwise this process; # will be left to run even though the timeout was already hit; # Do some late initialisation that's only needed; # if there is a timeout set; """"""; This method may be called multiple times as we might get unlucky; and be in the middle of creating a new p",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/TestRunner.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/TestRunner.py
Integrability,message,message,"""""""; A script had a fatal error such that there's no point in retrying. The; message has not been emitted on stdout or stderr but is instead included in; this exception.; """"""; # Don't use close_fds on Windows.; # Use temporary files to replace /dev/null on Windows.; # A regex that matches %dbg(ARG), which lit inserts at the beginning of each; # run command pipeline such that ARG specifies the pipeline's source line; # number. lit later expands each %dbg(ARG) to a command that behaves as a null; # command in the target shell so that the line number is seen in lit's verbose; # mode.; #; # This regex captures ARG. ARG must not contain a right parenthesis, which; # terminates %dbg. ARG must not contain quotes, in which ARG might be enclosed; # during expansion.; #; # COMMAND that follows %dbg(ARG) is also captured. COMMAND can be; # empty as a result of conditinal substitution.; """"""Mutable shell environment containing things like CWD and env vars. Environment variables are not implemented, but cwd tracking is. In addition,; we maintain a dir stack for pushd/popd.; """"""; """"""; Object used to helper manage enforcing a timeout in; _executeShCmd(). It is passed through recursive calls; to collect processes that have been executed so that when; the timeout happens they can be killed.; """"""; # This lock will be used to protect concurrent access; # to _procs and _doneKillPass; # Avoid re-entering the lock by finding out if kill needs to be run; # again here but call it if necessary once we have left the lock.; # We could use a reentrant lock here instead but this code seems; # clearer to me.; # The initial call to _kill() from the timer thread already happened so; # we need to call it again from this thread, otherwise this process; # will be left to run even though the timeout was already hit; # Do some late initialisation that's only needed; # if there is a timeout set; """"""; This method may be called multiple times as we might get unlucky; and be in the middle of creating a new p",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/TestRunner.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/TestRunner.py
Modifiability,variab,variables,"""""""; A script had a fatal error such that there's no point in retrying. The; message has not been emitted on stdout or stderr but is instead included in; this exception.; """"""; # Don't use close_fds on Windows.; # Use temporary files to replace /dev/null on Windows.; # A regex that matches %dbg(ARG), which lit inserts at the beginning of each; # run command pipeline such that ARG specifies the pipeline's source line; # number. lit later expands each %dbg(ARG) to a command that behaves as a null; # command in the target shell so that the line number is seen in lit's verbose; # mode.; #; # This regex captures ARG. ARG must not contain a right parenthesis, which; # terminates %dbg. ARG must not contain quotes, in which ARG might be enclosed; # during expansion.; #; # COMMAND that follows %dbg(ARG) is also captured. COMMAND can be; # empty as a result of conditinal substitution.; """"""Mutable shell environment containing things like CWD and env vars. Environment variables are not implemented, but cwd tracking is. In addition,; we maintain a dir stack for pushd/popd.; """"""; """"""; Object used to helper manage enforcing a timeout in; _executeShCmd(). It is passed through recursive calls; to collect processes that have been executed so that when; the timeout happens they can be killed.; """"""; # This lock will be used to protect concurrent access; # to _procs and _doneKillPass; # Avoid re-entering the lock by finding out if kill needs to be run; # again here but call it if necessary once we have left the lock.; # We could use a reentrant lock here instead but this code seems; # clearer to me.; # The initial call to _kill() from the timer thread already happened so; # we need to call it again from this thread, otherwise this process; # will be left to run even though the timeout was already hit; # Do some late initialisation that's only needed; # if there is a timeout set; """"""; This method may be called multiple times as we might get unlucky; and be in the middle of creating a new p",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/TestRunner.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/TestRunner.py
Performance,concurren,concurrent," source line; # number. lit later expands each %dbg(ARG) to a command that behaves as a null; # command in the target shell so that the line number is seen in lit's verbose; # mode.; #; # This regex captures ARG. ARG must not contain a right parenthesis, which; # terminates %dbg. ARG must not contain quotes, in which ARG might be enclosed; # during expansion.; #; # COMMAND that follows %dbg(ARG) is also captured. COMMAND can be; # empty as a result of conditinal substitution.; """"""Mutable shell environment containing things like CWD and env vars. Environment variables are not implemented, but cwd tracking is. In addition,; we maintain a dir stack for pushd/popd.; """"""; """"""; Object used to helper manage enforcing a timeout in; _executeShCmd(). It is passed through recursive calls; to collect processes that have been executed so that when; the timeout happens they can be killed.; """"""; # This lock will be used to protect concurrent access; # to _procs and _doneKillPass; # Avoid re-entering the lock by finding out if kill needs to be run; # again here but call it if necessary once we have left the lock.; # We could use a reentrant lock here instead but this code seems; # clearer to me.; # The initial call to _kill() from the timer thread already happened so; # we need to call it again from this thread, otherwise this process; # will be left to run even though the timeout was already hit; # Do some late initialisation that's only needed; # if there is a timeout set; """"""; This method may be called multiple times as we might get unlucky; and be in the middle of creating a new process in _executeShCmd(); which won't yet be in ``self._procs``. By locking here and in; addProcess() we should be able to kill processes launched after; the initial call to _kill(); """"""; # Empty the list and note that we've done a pass over the list; # Python2 doesn't have list.clear(); """"""Captures the result of an individual command.""""""; """"""; Wrapper around _executeShCmd that handles; timeout; """"""; #",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/TestRunner.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/TestRunner.py
Safety,timeout,timeout," or stderr but is instead included in; this exception.; """"""; # Don't use close_fds on Windows.; # Use temporary files to replace /dev/null on Windows.; # A regex that matches %dbg(ARG), which lit inserts at the beginning of each; # run command pipeline such that ARG specifies the pipeline's source line; # number. lit later expands each %dbg(ARG) to a command that behaves as a null; # command in the target shell so that the line number is seen in lit's verbose; # mode.; #; # This regex captures ARG. ARG must not contain a right parenthesis, which; # terminates %dbg. ARG must not contain quotes, in which ARG might be enclosed; # during expansion.; #; # COMMAND that follows %dbg(ARG) is also captured. COMMAND can be; # empty as a result of conditinal substitution.; """"""Mutable shell environment containing things like CWD and env vars. Environment variables are not implemented, but cwd tracking is. In addition,; we maintain a dir stack for pushd/popd.; """"""; """"""; Object used to helper manage enforcing a timeout in; _executeShCmd(). It is passed through recursive calls; to collect processes that have been executed so that when; the timeout happens they can be killed.; """"""; # This lock will be used to protect concurrent access; # to _procs and _doneKillPass; # Avoid re-entering the lock by finding out if kill needs to be run; # again here but call it if necessary once we have left the lock.; # We could use a reentrant lock here instead but this code seems; # clearer to me.; # The initial call to _kill() from the timer thread already happened so; # we need to call it again from this thread, otherwise this process; # will be left to run even though the timeout was already hit; # Do some late initialisation that's only needed; # if there is a timeout set; """"""; This method may be called multiple times as we might get unlucky; and be in the middle of creating a new process in _executeShCmd(); which won't yet be in ``self._procs``. By locking here and in; addProcess() we should be",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/TestRunner.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/TestRunner.py
Security,access,access," source line; # number. lit later expands each %dbg(ARG) to a command that behaves as a null; # command in the target shell so that the line number is seen in lit's verbose; # mode.; #; # This regex captures ARG. ARG must not contain a right parenthesis, which; # terminates %dbg. ARG must not contain quotes, in which ARG might be enclosed; # during expansion.; #; # COMMAND that follows %dbg(ARG) is also captured. COMMAND can be; # empty as a result of conditinal substitution.; """"""Mutable shell environment containing things like CWD and env vars. Environment variables are not implemented, but cwd tracking is. In addition,; we maintain a dir stack for pushd/popd.; """"""; """"""; Object used to helper manage enforcing a timeout in; _executeShCmd(). It is passed through recursive calls; to collect processes that have been executed so that when; the timeout happens they can be killed.; """"""; # This lock will be used to protect concurrent access; # to _procs and _doneKillPass; # Avoid re-entering the lock by finding out if kill needs to be run; # again here but call it if necessary once we have left the lock.; # We could use a reentrant lock here instead but this code seems; # clearer to me.; # The initial call to _kill() from the timer thread already happened so; # we need to call it again from this thread, otherwise this process; # will be left to run even though the timeout was already hit; # Do some late initialisation that's only needed; # if there is a timeout set; """"""; This method may be called multiple times as we might get unlucky; and be in the middle of creating a new process in _executeShCmd(); which won't yet be in ``self._procs``. By locking here and in; addProcess() we should be able to kill processes launched after; the initial call to _kill(); """"""; # Empty the list and note that we've done a pass over the list; # Python2 doesn't have list.clear(); """"""Captures the result of an individual command.""""""; """"""; Wrapper around _executeShCmd that handles; timeout; """"""; #",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/TestRunner.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/TestRunner.py
Testability,log,logic,"tead but this code seems; # clearer to me.; # The initial call to _kill() from the timer thread already happened so; # we need to call it again from this thread, otherwise this process; # will be left to run even though the timeout was already hit; # Do some late initialisation that's only needed; # if there is a timeout set; """"""; This method may be called multiple times as we might get unlucky; and be in the middle of creating a new process in _executeShCmd(); which won't yet be in ``self._procs``. By locking here and in; addProcess() we should be able to kill processes launched after; the initial call to _kill(); """"""; # Empty the list and note that we've done a pass over the list; # Python2 doesn't have list.clear(); """"""Captures the result of an individual command.""""""; """"""; Wrapper around _executeShCmd that handles; timeout; """"""; # Use the helper even when no timeout is required to make; # other code simpler (i.e. avoid bunch of ``!= None`` checks); # Add a space to separate this argument from the others; # This logic differs from upstream list2cmdline.; # Don't know if we need to double yet.; # Double backslashes.; # Normal char; # Add remaining backslashes, if any.; # args are from 'export' or 'env' command.; # Skips the command, and parses its arguments.; # Modifies env accordingly.; # Returns copy of args without the command or its arguments.; # Support for the -u flag (unsetting) for env command; # e.g., env -u FOO -u BAR will remove both FOO and BAR; # from the environment.; # Partition the string into KEY=VALUE.; # Stop if there was no equals.; """"""executeBuiltinCd - Change the current directory.""""""; # Update the cwd in the parent environment.; # The cd builtin always succeeds. If the directory does not exist, the; # following Popen calls will fail instead.; """"""executeBuiltinPushd - Change the current dir and save the old.""""""; """"""executeBuiltinPopd - Restore a previously saved working directory.""""""; """"""executeBuiltinExport - Set an environment variable.""""""; ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/TestRunner.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/TestRunner.py
Usability,clear,clearer,"in lit's verbose; # mode.; #; # This regex captures ARG. ARG must not contain a right parenthesis, which; # terminates %dbg. ARG must not contain quotes, in which ARG might be enclosed; # during expansion.; #; # COMMAND that follows %dbg(ARG) is also captured. COMMAND can be; # empty as a result of conditinal substitution.; """"""Mutable shell environment containing things like CWD and env vars. Environment variables are not implemented, but cwd tracking is. In addition,; we maintain a dir stack for pushd/popd.; """"""; """"""; Object used to helper manage enforcing a timeout in; _executeShCmd(). It is passed through recursive calls; to collect processes that have been executed so that when; the timeout happens they can be killed.; """"""; # This lock will be used to protect concurrent access; # to _procs and _doneKillPass; # Avoid re-entering the lock by finding out if kill needs to be run; # again here but call it if necessary once we have left the lock.; # We could use a reentrant lock here instead but this code seems; # clearer to me.; # The initial call to _kill() from the timer thread already happened so; # we need to call it again from this thread, otherwise this process; # will be left to run even though the timeout was already hit; # Do some late initialisation that's only needed; # if there is a timeout set; """"""; This method may be called multiple times as we might get unlucky; and be in the middle of creating a new process in _executeShCmd(); which won't yet be in ``self._procs``. By locking here and in; addProcess() we should be able to kill processes launched after; the initial call to _kill(); """"""; # Empty the list and note that we've done a pass over the list; # Python2 doesn't have list.clear(); """"""Captures the result of an individual command.""""""; """"""; Wrapper around _executeShCmd that handles; timeout; """"""; # Use the helper even when no timeout is required to make; # other code simpler (i.e. avoid bunch of ``!= None`` checks); # Add a space to separate this arg",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/TestRunner.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/TestRunner.py
Availability,error,error,"ault; # repr-line encoding.; # By this point, here's what we *don't* have:; #; # - In Python2:; # - 'str' or 'bytes' (1st branch above); # - In Python3:; # - 'str' (1st branch above); # - 'bytes' (2nd branch above); #; # The last type we might expect is the Python2 'unicode' type. There is no; # 'unicode' type in Python3 (all the Python3 cases were already handled). In; # order to get a 'str' object, we need to encode the 'unicode' object.; """"""Return the parameter as type which supports unicode, possibly decoding; it. In Python2, this is the unicode type. In Python3 it's the str type. """"""; # In Python2, this branch is taken for both 'str' and 'bytes'.; # In Python3, this branch is taken only for 'bytes'.; """"""Return the number of cores the current process can use, if supported.; Otherwise, return the total number of cores (like `os.cpu_count()`).; Default to 1 if undetermined. """"""; # On Windows with more than 60 processes, multiprocessing's call to; # _winapi.WaitForMultipleObjects() prints an error and lit hangs.; """"""Return the absolute path without resolving drive mappings on Windows. """"""; # Windows has limitations on path length (MAX_PATH) that; # can be worked around using substitute drives, which map; # a drive letter to a longer path on another drive.; # Since Python 3.8, os.path.realpath resolves sustitute drives,; # so we should not use it. In Python 3.7, os.path.realpath; # was implemented as os.path.abspath.; # On UNIX, the current directory always has symbolic links resolved,; # so any program accepting relative paths cannot preserve symbolic; # links in paths and we should always use os.path.realpath.; # Make sure that the path uses backslashes here, in case; # python would have happened to use forward slashes, as the; # NT path format only supports backslashes.; # ignore EEXIST, which may occur during a race condition; """"""mkdir_p(path) - Make the ""path"" directory, if it does not exist; this; will also make directories for any missing parent directories.""""",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/util.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/util.py
Integrability,message,message,"ut`` (str), use string to pass; no input.; * Max execution time ``timeout`` (int) seconds. Use 0 for no timeout.; * ``redirect_stderr`` (bool), use True if redirect stderr to stdout. Returns a tuple (out, err, exitCode) where; * ``out`` (str) is the standard output of running the command; * ``err`` (str) is the standard error of running the command; * ``exitCode`` (int) is the exitCode of running the command. If the timeout is hit an ``ExecuteCommandTimeoutException``; is raised. """"""; # FIXME: Because of the way nested function scopes work in Python 2.x we; # need to use a reference to a mutable object rather than a plain; # bool. In Python 3 we could use the ""nonlocal"" keyword but we need; # to support Python 2 as well.; # We may be invoking a shell so we need to kill the; # process and all its children.; # Ensure the resulting output is always of string type.; # Detect Ctrl-C in subprocess.; """"""Whether the given target triple is for AIX,; e.g. powerpc64-ibm-aix; """"""; """"""Add the AIX version to the given target triple,; e.g. powerpc64-ibm-aix7.2.5.6; """"""; """"""Whether the given target triple is for macOS,; e.g. x86_64-apple-darwin, arm64-apple-macos; """"""; # On Darwin, support relocatable SDKs by providing Clang with a; # default system root path.; """"""; Returns a tuple (<supported> , <error message>); where; `<supported>` is True if `killProcessAndChildren()` is supported on; the current host, returns False otherwise.; `<error message>` is an empty string if `<supported>` is True,; otherwise is contains a string describing why the function is; not supported.; """"""; # noqa: F401; """"""This function kills a process with ``pid`` and all its running children; (recursively). It is currently implemented using the psutil module on some; platforms which provides a simple platform neutral implementation. TODO: Reimplement this without using psutil on all platforms so we can; remove our dependency on it. """"""; # Handle the different psutil API versions; # psutil >= 2.x; # psutil 1.x",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/util.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/util.py
Modifiability,variab,variable,"s for any missing parent directories.""""""; """"""Yields files in a directory. Filenames that are not excluded by rules below are yielded one at a time, as; basenames (i.e., without dirname). Files starting with '.' are always skipped. If 'suffixes' is not None, then only filenames ending with one of its; members will be yielded. These can be extensions, like '.exe', or strings,; like 'Test'. (It is a lexicographic check; so an empty sequence will yield; nothing, but a single empty string will yield all filenames.). If 'exclude_filenames' is not None, then none of the file basenames in it; will be yielded. If specified, the containers for 'suffixes' and 'exclude_filenames' must; support membership checking for strs. Args:; dirname: a directory path.; suffixes: (optional) a sequence of strings (set, list, etc.).; exclude_filenames: (optional) a sequence of strings. Yields:; Filenames as returned by os.listdir (generally, str). """"""; """"""which(command, [paths]) - Look up the given command in the paths string; (or the PATH environment variable, if unspecified).""""""; # Check for absolute match first.; # Would be nice if Python had a lib function for this.; # Get suffixes to search.; # On Cygwin, 'PATHEXT' may exist but it should not be used.; # Search the paths...; # Select first ""nice"" bar height that produces more than 10 bars.; # Close extra file handles on UNIX (on Windows this cannot be done while; # also redirecting input).; """"""Execute command ``command`` (list of arguments or string) with. * working directory ``cwd`` (str), use None to use the current; working directory; * environment ``env`` (dict), use None for none; * Input to the command ``input`` (str), use string to pass; no input.; * Max execution time ``timeout`` (int) seconds. Use 0 for no timeout.; * ``redirect_stderr`` (bool), use True if redirect stderr to stdout. Returns a tuple (out, err, exitCode) where; * ``out`` (str) is the standard output of running the command; * ``err`` (str) is the standard error of ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/util.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/util.py
Performance,race condition,race condition,"ndows with more than 60 processes, multiprocessing's call to; # _winapi.WaitForMultipleObjects() prints an error and lit hangs.; """"""Return the absolute path without resolving drive mappings on Windows. """"""; # Windows has limitations on path length (MAX_PATH) that; # can be worked around using substitute drives, which map; # a drive letter to a longer path on another drive.; # Since Python 3.8, os.path.realpath resolves sustitute drives,; # so we should not use it. In Python 3.7, os.path.realpath; # was implemented as os.path.abspath.; # On UNIX, the current directory always has symbolic links resolved,; # so any program accepting relative paths cannot preserve symbolic; # links in paths and we should always use os.path.realpath.; # Make sure that the path uses backslashes here, in case; # python would have happened to use forward slashes, as the; # NT path format only supports backslashes.; # ignore EEXIST, which may occur during a race condition; """"""mkdir_p(path) - Make the ""path"" directory, if it does not exist; this; will also make directories for any missing parent directories.""""""; """"""Yields files in a directory. Filenames that are not excluded by rules below are yielded one at a time, as; basenames (i.e., without dirname). Files starting with '.' are always skipped. If 'suffixes' is not None, then only filenames ending with one of its; members will be yielded. These can be extensions, like '.exe', or strings,; like 'Test'. (It is a lexicographic check; so an empty sequence will yield; nothing, but a single empty string will yield all filenames.). If 'exclude_filenames' is not None, then none of the file basenames in it; will be yielded. If specified, the containers for 'suffixes' and 'exclude_filenames' must; support membership checking for strs. Args:; dirname: a directory path.; suffixes: (optional) a sequence of strings (set, list, etc.).; exclude_filenames: (optional) a sequence of strings. Yields:; Filenames as returned by os.listdir (generally, str). """""";",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/util.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/util.py
Safety,timeout,timeout,": a directory path.; suffixes: (optional) a sequence of strings (set, list, etc.).; exclude_filenames: (optional) a sequence of strings. Yields:; Filenames as returned by os.listdir (generally, str). """"""; """"""which(command, [paths]) - Look up the given command in the paths string; (or the PATH environment variable, if unspecified).""""""; # Check for absolute match first.; # Would be nice if Python had a lib function for this.; # Get suffixes to search.; # On Cygwin, 'PATHEXT' may exist but it should not be used.; # Search the paths...; # Select first ""nice"" bar height that produces more than 10 bars.; # Close extra file handles on UNIX (on Windows this cannot be done while; # also redirecting input).; """"""Execute command ``command`` (list of arguments or string) with. * working directory ``cwd`` (str), use None to use the current; working directory; * environment ``env`` (dict), use None for none; * Input to the command ``input`` (str), use string to pass; no input.; * Max execution time ``timeout`` (int) seconds. Use 0 for no timeout.; * ``redirect_stderr`` (bool), use True if redirect stderr to stdout. Returns a tuple (out, err, exitCode) where; * ``out`` (str) is the standard output of running the command; * ``err`` (str) is the standard error of running the command; * ``exitCode`` (int) is the exitCode of running the command. If the timeout is hit an ``ExecuteCommandTimeoutException``; is raised. """"""; # FIXME: Because of the way nested function scopes work in Python 2.x we; # need to use a reference to a mutable object rather than a plain; # bool. In Python 3 we could use the ""nonlocal"" keyword but we need; # to support Python 2 as well.; # We may be invoking a shell so we need to kill the; # process and all its children.; # Ensure the resulting output is always of string type.; # Detect Ctrl-C in subprocess.; """"""Whether the given target triple is for AIX,; e.g. powerpc64-ibm-aix; """"""; """"""Add the AIX version to the given target triple,; e.g. powerpc64-ibm-aix7.2.5.6;",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/util.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/util.py
Usability,simpl,simple,"ut`` (str), use string to pass; no input.; * Max execution time ``timeout`` (int) seconds. Use 0 for no timeout.; * ``redirect_stderr`` (bool), use True if redirect stderr to stdout. Returns a tuple (out, err, exitCode) where; * ``out`` (str) is the standard output of running the command; * ``err`` (str) is the standard error of running the command; * ``exitCode`` (int) is the exitCode of running the command. If the timeout is hit an ``ExecuteCommandTimeoutException``; is raised. """"""; # FIXME: Because of the way nested function scopes work in Python 2.x we; # need to use a reference to a mutable object rather than a plain; # bool. In Python 3 we could use the ""nonlocal"" keyword but we need; # to support Python 2 as well.; # We may be invoking a shell so we need to kill the; # process and all its children.; # Ensure the resulting output is always of string type.; # Detect Ctrl-C in subprocess.; """"""Whether the given target triple is for AIX,; e.g. powerpc64-ibm-aix; """"""; """"""Add the AIX version to the given target triple,; e.g. powerpc64-ibm-aix7.2.5.6; """"""; """"""Whether the given target triple is for macOS,; e.g. x86_64-apple-darwin, arm64-apple-macos; """"""; # On Darwin, support relocatable SDKs by providing Clang with a; # default system root path.; """"""; Returns a tuple (<supported> , <error message>); where; `<supported>` is True if `killProcessAndChildren()` is supported on; the current host, returns False otherwise.; `<error message>` is an empty string if `<supported>` is True,; otherwise is contains a string describing why the function is; not supported.; """"""; # noqa: F401; """"""This function kills a process with ``pid`` and all its running children; (recursively). It is currently implemented using the psutil module on some; platforms which provides a simple platform neutral implementation. TODO: Reimplement this without using psutil on all platforms so we can; remove our dependency on it. """"""; # Handle the different psutil API versions; # psutil >= 2.x; # psutil 1.x",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/util.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/util.py
Energy Efficiency,reduce,reduces,"""""""; The functions in this module are meant to run on a separate worker process.; Exception: in single process mode _execute is called directly. For efficiency, we copy all data needed to execute all tests into each worker; and store it in global variables. This reduces the cost of each task.; """"""; """"""Copy data shared by all test executions into worker processes""""""; # We use the following strategy for dealing with Ctrl+C/KeyboardInterrupt in; # subprocesses created by the multiprocessing.Pool.; # https://noswap.com/blog/python-multiprocessing-keyboardinterrupt; """"""Run one test in a multiprocessing.Pool. Side effects in this function and functions it calls are not visible in the; main lit process. Arguments and results of this function are pickled, so they should be cheap; to copy.; """"""; # TODO(python3): replace with contextlib.nullcontext; # Do not inline! Directly used by LitTestCase.py; # Support deprecated result from execute() which returned the result; # code and additional output as a tuple.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/worker.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/worker.py
Modifiability,variab,variables,"""""""; The functions in this module are meant to run on a separate worker process.; Exception: in single process mode _execute is called directly. For efficiency, we copy all data needed to execute all tests into each worker; and store it in global variables. This reduces the cost of each task.; """"""; """"""Copy data shared by all test executions into worker processes""""""; # We use the following strategy for dealing with Ctrl+C/KeyboardInterrupt in; # subprocesses created by the multiprocessing.Pool.; # https://noswap.com/blog/python-multiprocessing-keyboardinterrupt; """"""Run one test in a multiprocessing.Pool. Side effects in this function and functions it calls are not visible in the; main lit process. Arguments and results of this function are pickled, so they should be cheap; to copy.; """"""; # TODO(python3): replace with contextlib.nullcontext; # Do not inline! Directly used by LitTestCase.py; # Support deprecated result from execute() which returned the result; # code and additional output as a tuple.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/worker.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/worker.py
Testability,test,tests,"""""""; The functions in this module are meant to run on a separate worker process.; Exception: in single process mode _execute is called directly. For efficiency, we copy all data needed to execute all tests into each worker; and store it in global variables. This reduces the cost of each task.; """"""; """"""Copy data shared by all test executions into worker processes""""""; # We use the following strategy for dealing with Ctrl+C/KeyboardInterrupt in; # subprocesses created by the multiprocessing.Pool.; # https://noswap.com/blog/python-multiprocessing-keyboardinterrupt; """"""Run one test in a multiprocessing.Pool. Side effects in this function and functions it calls are not visible in the; main lit process. Arguments and results of this function are pickled, so they should be cheap; to copy.; """"""; # TODO(python3): replace with contextlib.nullcontext; # Do not inline! Directly used by LitTestCase.py; # Support deprecated result from execute() which returned the result; # code and additional output as a tuple.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/worker.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/worker.py
Deployability,configurat,configuration,"""""""; Given the path to a test in the test suite, generates the Lit tests associated; to that path. There can be zero, one or more tests. For example, some testing; formats allow expanding a single path in the test suite into multiple Lit tests; (e.g. they are generated on the fly). Note that this method is only used when Lit needs to actually perform test; discovery, which is not the case for configs with standalone tests.; """"""; ###; """"""; Expand each path in a test suite to a Lit test using that path and assuming; it is a file containing the test. File extensions excluded by the configuration; or not contained in the allowed extensions are ignored.; """"""; # Ignore dot files and excluded tests.; ###; # Check exit code of a simple executable with no input",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/formats/base.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/formats/base.py
Modifiability,config,configs,"""""""; Given the path to a test in the test suite, generates the Lit tests associated; to that path. There can be zero, one or more tests. For example, some testing; formats allow expanding a single path in the test suite into multiple Lit tests; (e.g. they are generated on the fly). Note that this method is only used when Lit needs to actually perform test; discovery, which is not the case for configs with standalone tests.; """"""; ###; """"""; Expand each path in a test suite to a Lit test using that path and assuming; it is a file containing the test. File extensions excluded by the configuration; or not contained in the allowed extensions are ignored.; """"""; # Ignore dot files and excluded tests.; ###; # Check exit code of a simple executable with no input",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/formats/base.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/formats/base.py
Performance,perform,perform,"""""""; Given the path to a test in the test suite, generates the Lit tests associated; to that path. There can be zero, one or more tests. For example, some testing; formats allow expanding a single path in the test suite into multiple Lit tests; (e.g. they are generated on the fly). Note that this method is only used when Lit needs to actually perform test; discovery, which is not the case for configs with standalone tests.; """"""; ###; """"""; Expand each path in a test suite to a Lit test using that path and assuming; it is a file containing the test. File extensions excluded by the configuration; or not contained in the allowed extensions are ignored.; """"""; # Ignore dot files and excluded tests.; ###; # Check exit code of a simple executable with no input",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/formats/base.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/formats/base.py
Testability,test,test,"""""""; Given the path to a test in the test suite, generates the Lit tests associated; to that path. There can be zero, one or more tests. For example, some testing; formats allow expanding a single path in the test suite into multiple Lit tests; (e.g. they are generated on the fly). Note that this method is only used when Lit needs to actually perform test; discovery, which is not the case for configs with standalone tests.; """"""; ###; """"""; Expand each path in a test suite to a Lit test using that path and assuming; it is a file containing the test. File extensions excluded by the configuration; or not contained in the allowed extensions are ignored.; """"""; # Ignore dot files and excluded tests.; ###; # Check exit code of a simple executable with no input",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/formats/base.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/formats/base.py
Usability,simpl,simple,"""""""; Given the path to a test in the test suite, generates the Lit tests associated; to that path. There can be zero, one or more tests. For example, some testing; formats allow expanding a single path in the test suite into multiple Lit tests; (e.g. they are generated on the fly). Note that this method is only used when Lit needs to actually perform test; discovery, which is not the case for configs with standalone tests.; """"""; ###; """"""; Expand each path in a test suite to a Lit test using that path and assuming; it is a file containing the test. File extensions excluded by the configuration; or not contained in the allowed extensions are ignored.; """"""; # Ignore dot files and excluded tests.; ###; # Check exit code of a simple executable with no input",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/formats/base.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/formats/base.py
Availability,failure,failures,"# On Windows, assume tests will also end in '.exe'.; # Also check for .py files for testing purposes.; # number of tests in a shard; # Discover the tests in this executable.; # Compute the number of shards.; # Create one lit test for each shard.; # This doesn't look like a valid gtest file. This can; # have a number of causes, none of them good. For; # instance, we could have created a broken executable.; # Alternatively, someone has cruft in their test; # directory. If we don't return a test here, then no; # failures will get reported, so return a dummy test name; # so that the failure is reported later.; # Handle GTest parameterized and typed tests, whose name includes; # some '/'s.; # In some situations, like running tests with sanitizers, all test passes but; # the shard could still fail due to memory issues.; """"""Insert interpreter if needed. It inserts the python exe into the command if cmd[0] ends in .py or caller; specified run_under.; We cannot rely on the system to interpret shebang lines for us on; Windows, so add the python executable to the command if this is a .py; script.; """"""; # In case gtest has bugs such that no JSON file was emitted.; # Load json file to retrieve results.; # Ignore disabled tests.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/formats/googletest.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/formats/googletest.py
Modifiability,parameteriz,parameterized,"# On Windows, assume tests will also end in '.exe'.; # Also check for .py files for testing purposes.; # number of tests in a shard; # Discover the tests in this executable.; # Compute the number of shards.; # Create one lit test for each shard.; # This doesn't look like a valid gtest file. This can; # have a number of causes, none of them good. For; # instance, we could have created a broken executable.; # Alternatively, someone has cruft in their test; # directory. If we don't return a test here, then no; # failures will get reported, so return a dummy test name; # so that the failure is reported later.; # Handle GTest parameterized and typed tests, whose name includes; # some '/'s.; # In some situations, like running tests with sanitizers, all test passes but; # the shard could still fail due to memory issues.; """"""Insert interpreter if needed. It inserts the python exe into the command if cmd[0] ends in .py or caller; specified run_under.; We cannot rely on the system to interpret shebang lines for us on; Windows, so add the python executable to the command if this is a .py; script.; """"""; # In case gtest has bugs such that no JSON file was emitted.; # Load json file to retrieve results.; # Ignore disabled tests.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/formats/googletest.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/formats/googletest.py
Security,sanitiz,sanitizers,"# On Windows, assume tests will also end in '.exe'.; # Also check for .py files for testing purposes.; # number of tests in a shard; # Discover the tests in this executable.; # Compute the number of shards.; # Create one lit test for each shard.; # This doesn't look like a valid gtest file. This can; # have a number of causes, none of them good. For; # instance, we could have created a broken executable.; # Alternatively, someone has cruft in their test; # directory. If we don't return a test here, then no; # failures will get reported, so return a dummy test name; # so that the failure is reported later.; # Handle GTest parameterized and typed tests, whose name includes; # some '/'s.; # In some situations, like running tests with sanitizers, all test passes but; # the shard could still fail due to memory issues.; """"""Insert interpreter if needed. It inserts the python exe into the command if cmd[0] ends in .py or caller; specified run_under.; We cannot rely on the system to interpret shebang lines for us on; Windows, so add the python executable to the command if this is a .py; script.; """"""; # In case gtest has bugs such that no JSON file was emitted.; # Load json file to retrieve results.; # Ignore disabled tests.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/formats/googletest.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/formats/googletest.py
Testability,test,tests,"# On Windows, assume tests will also end in '.exe'.; # Also check for .py files for testing purposes.; # number of tests in a shard; # Discover the tests in this executable.; # Compute the number of shards.; # Create one lit test for each shard.; # This doesn't look like a valid gtest file. This can; # have a number of causes, none of them good. For; # instance, we could have created a broken executable.; # Alternatively, someone has cruft in their test; # directory. If we don't return a test here, then no; # failures will get reported, so return a dummy test name; # so that the failure is reported later.; # Handle GTest parameterized and typed tests, whose name includes; # some '/'s.; # In some situations, like running tests with sanitizers, all test passes but; # the shard could still fail due to memory issues.; """"""Insert interpreter if needed. It inserts the python exe into the command if cmd[0] ends in .py or caller; specified run_under.; We cannot rely on the system to interpret shebang lines for us on; Windows, so add the python executable to the command if this is a .py; script.; """"""; # In case gtest has bugs such that no JSON file was emitted.; # Load json file to retrieve results.; # Ignore disabled tests.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/formats/googletest.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/formats/googletest.py
Deployability,pipeline,pipelines,"""""""ShTest is a format with one file per test. This is the primary format for regression tests as described in the LLVM; testing guide:. http://llvm.org/docs/TestingGuide.html. The ShTest files contain some number of shell-like command pipelines, along; with assertions about what should be in the output.; """"""",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/formats/shtest.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/formats/shtest.py
Testability,test,test,"""""""ShTest is a format with one file per test. This is the primary format for regression tests as described in the LLVM; testing guide:. http://llvm.org/docs/TestingGuide.html. The ShTest files contain some number of shell-like command pipelines, along; with assertions about what should be in the output.; """"""",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/formats/shtest.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/formats/shtest.py
Usability,guid,guide,"""""""ShTest is a format with one file per test. This is the primary format for regression tests as described in the LLVM; testing guide:. http://llvm.org/docs/TestingGuide.html. The ShTest files contain some number of shell-like command pipelines, along; with assertions about what should be in the output.; """"""",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/formats/shtest.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/formats/shtest.py
Availability,avail,available,"# os.getuid() is not available on all platforms; # Tweak PATH for Win32 to decide to use bash.exe or not.; # Seek necessary tools in directories and set to $PATH.; # Many tools behave strangely if these environment variables aren't; # set.; # Choose between lit's internal shell pipeline runner and a real shell.; # If LIT_USE_INTERNAL_SHELL is in the environment, we use that as an; # override.; # Running on Darwin OS; # FIXME: lld uses the first, other projects use the second.; # We should standardize on the former.; # For tests that require Windows to run.; # Native compilation: host arch == default triple arch; # Both of these values should probably be in every site config (e.g. as; # part of the standard header. But currently they aren't); # Sanitizers.; # Check if we should run long running tests.; # Allow use of an explicit path for gmalloc library.; # Will default to '/usr/lib/libgmalloc.dylib' if not set.; # Search both the 64 and 32-bit hives, as well as HKLM + HKCU; # We found it, stop enumerating.; # For paths, we should be able to take a list of them and process; # all of them.; # If we are passed a list [a b c], then iterating this list forwards; # and adding each to the beginning would result in c b a. So we; # need to iterate in reverse to end up with the original ordering.; # Move it to the front if it already exists, otherwise insert; # it at the beginning.; # Ask llvm-config about the specified feature.; # We should have either a callable or a dictionary. If it's a; # dictionary, grep each key against the output and use the value if; # it matches. If it's a callable, it does the entire translation.; # Note that when substituting %clang_cc1 also fill in the include directory; # of the builtin headers. Those are part of even a freestanding; # environment, but Clang relies on the driver to locate them.; # FIXME: Rather than just getting the version, we should have clang; # print out its resource dir here in an easy to scrape form.; # Don't pass dosish pa",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/llvm/config.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/llvm/config.py
Deployability,pipeline,pipeline,"# os.getuid() is not available on all platforms; # Tweak PATH for Win32 to decide to use bash.exe or not.; # Seek necessary tools in directories and set to $PATH.; # Many tools behave strangely if these environment variables aren't; # set.; # Choose between lit's internal shell pipeline runner and a real shell.; # If LIT_USE_INTERNAL_SHELL is in the environment, we use that as an; # override.; # Running on Darwin OS; # FIXME: lld uses the first, other projects use the second.; # We should standardize on the former.; # For tests that require Windows to run.; # Native compilation: host arch == default triple arch; # Both of these values should probably be in every site config (e.g. as; # part of the standard header. But currently they aren't); # Sanitizers.; # Check if we should run long running tests.; # Allow use of an explicit path for gmalloc library.; # Will default to '/usr/lib/libgmalloc.dylib' if not set.; # Search both the 64 and 32-bit hives, as well as HKLM + HKCU; # We found it, stop enumerating.; # For paths, we should be able to take a list of them and process; # all of them.; # If we are passed a list [a b c], then iterating this list forwards; # and adding each to the beginning would result in c b a. So we; # need to iterate in reverse to end up with the original ordering.; # Move it to the front if it already exists, otherwise insert; # it at the beginning.; # Ask llvm-config about the specified feature.; # We should have either a callable or a dictionary. If it's a; # dictionary, grep each key against the output and use the value if; # it matches. If it's a callable, it does the entire translation.; # Note that when substituting %clang_cc1 also fill in the include directory; # of the builtin headers. Those are part of even a freestanding; # environment, but Clang relies on the driver to locate them.; # FIXME: Rather than just getting the version, we should have clang; # print out its resource dir here in an easy to scrape form.; # Don't pass dosish pa",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/llvm/config.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/llvm/config.py
Integrability,message,message," substituting %clang_cc1 also fill in the include directory; # of the builtin headers. Those are part of even a freestanding; # environment, but Clang relies on the driver to locate them.; # FIXME: Rather than just getting the version, we should have clang; # print out its resource dir here in an easy to scrape form.; # Don't pass dosish path separator to msys bash.exe.; # Ensure the result is an ascii string, across Python2.5+ - Python3.; # On macOS, LSan is only supported on clang versions 5 and higher; # Apple clang doesn't yet support LSan; # All non-windows triples use the Itanium ABI.; # If the OS is windows and environment is msvc, we're done.; # For x86 ISAs, adjust the OS.; # -msvc is not supported for non-x86 targets; use a default.; # Either no match occurred, or there was an unresolved match that; # is ignored.; # An unresolved match occurred that can't be ignored. Fail without; # adding any of the previously-discovered substitutions.; # Python's strerror may not supply the same message; # as C++ std::error_code. One example of such a platform is; # Visual Studio. errc_messages may be supplied which contains the error; # messages for ENOENT, EISDIR, EINVAL and EACCES as a semi colon; # separated string. LLVM testsuites can use get_errc_messages in cmake; # to automatically get the messages and pass them into lit.; # Handle these specially as they are strings searched for during; # testing.; """"""Find the executable program 'name', optionally using the specified; environment variable as an override before searching the build directory; and then optionally the configuration's PATH.""""""; # If the override is specified in the environment, use it without; # validation.; # Use the specified search paths.; # Otherwise look in the path, if enabled.; """"""Configure the test suite to be able to invoke clang. Sets up some environment variables important to clang, locates a; just-built or optionally an installed clang, and add a set of standard; substitutions useful to a",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/llvm/config.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/llvm/config.py
Modifiability,variab,variables,"# os.getuid() is not available on all platforms; # Tweak PATH for Win32 to decide to use bash.exe or not.; # Seek necessary tools in directories and set to $PATH.; # Many tools behave strangely if these environment variables aren't; # set.; # Choose between lit's internal shell pipeline runner and a real shell.; # If LIT_USE_INTERNAL_SHELL is in the environment, we use that as an; # override.; # Running on Darwin OS; # FIXME: lld uses the first, other projects use the second.; # We should standardize on the former.; # For tests that require Windows to run.; # Native compilation: host arch == default triple arch; # Both of these values should probably be in every site config (e.g. as; # part of the standard header. But currently they aren't); # Sanitizers.; # Check if we should run long running tests.; # Allow use of an explicit path for gmalloc library.; # Will default to '/usr/lib/libgmalloc.dylib' if not set.; # Search both the 64 and 32-bit hives, as well as HKLM + HKCU; # We found it, stop enumerating.; # For paths, we should be able to take a list of them and process; # all of them.; # If we are passed a list [a b c], then iterating this list forwards; # and adding each to the beginning would result in c b a. So we; # need to iterate in reverse to end up with the original ordering.; # Move it to the front if it already exists, otherwise insert; # it at the beginning.; # Ask llvm-config about the specified feature.; # We should have either a callable or a dictionary. If it's a; # dictionary, grep each key against the output and use the value if; # it matches. If it's a callable, it does the entire translation.; # Note that when substituting %clang_cc1 also fill in the include directory; # of the builtin headers. Those are part of even a freestanding; # environment, but Clang relies on the driver to locate them.; # FIXME: Rather than just getting the version, we should have clang; # print out its resource dir here in an easy to scrape form.; # Don't pass dosish pa",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/llvm/config.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/llvm/config.py
Safety,avoid,avoid,"e specified search paths.; # Otherwise look in the path, if enabled.; """"""Configure the test suite to be able to invoke clang. Sets up some environment variables important to clang, locates a; just-built or optionally an installed clang, and add a set of standard; substitutions useful to any test suite that makes use of clang. """"""; # Clear some environment variables that might affect Clang.; #; # This first set of vars are read by Clang, but shouldn't affect tests; # that aren't specifically looking for these features, or are required; # simply to run the tests at all.; #; # FIXME: Should we have a tool that enforces this?; # safe_env_vars = (; # 'TMPDIR', 'TEMP', 'TMP', 'USERPROFILE', 'PWD',; # 'MACOSX_DEPLOYMENT_TARGET', 'IPHONEOS_DEPLOYMENT_TARGET',; # 'VCINSTALLDIR', 'VC100COMNTOOLS', 'VC90COMNTOOLS',; # 'VC80COMNTOOLS'); # Clang/Win32 may refer to %INCLUDE%. vsvarsall.bat sets it.; # Tweak the PATH to include the tools dir and the scripts dir.; # Put Clang first to avoid LLVM from overriding out-of-tree clang; # builds.; # Discover the 'clang' and 'clangcc' to use.; # There will be no default target triple if one was not specifically; # set, and the host's architecture is not an enabled target.; # The host triple might not be set, at least if we're compiling clang; # from an already installed llvm.; # TODO: Many tests work across many language standards. Before; # https://discourse.llvm.org/t/lit-run-a-run-line-multiple-times-with-different-replacements/64932; # has a solution, provide substitutions to conveniently try every standard with LIT_CLANG_STD_GROUP.; # Let LIT_CLANG_STD_GROUP=0 pick the highest value (likely the most relevant; # standard).; # FIXME: Find nicer way to prohibit this.; '''\""*** Do not use '%s' in tests, use '%s'. ***\""'''; '''\""*** invalid substitution, use '%clang_cc1'. ***\""'''; '''\""*** invalid substitution, use '%clang_cpp'. ***\""'''; '''\""*** invalid substitution, use '%clang_cl'. ***\""'''; """"""Configure the test suite to be able to i",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/llvm/config.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/llvm/config.py
Security,validat,validation,"; # For x86 ISAs, adjust the OS.; # -msvc is not supported for non-x86 targets; use a default.; # Either no match occurred, or there was an unresolved match that; # is ignored.; # An unresolved match occurred that can't be ignored. Fail without; # adding any of the previously-discovered substitutions.; # Python's strerror may not supply the same message; # as C++ std::error_code. One example of such a platform is; # Visual Studio. errc_messages may be supplied which contains the error; # messages for ENOENT, EISDIR, EINVAL and EACCES as a semi colon; # separated string. LLVM testsuites can use get_errc_messages in cmake; # to automatically get the messages and pass them into lit.; # Handle these specially as they are strings searched for during; # testing.; """"""Find the executable program 'name', optionally using the specified; environment variable as an override before searching the build directory; and then optionally the configuration's PATH.""""""; # If the override is specified in the environment, use it without; # validation.; # Use the specified search paths.; # Otherwise look in the path, if enabled.; """"""Configure the test suite to be able to invoke clang. Sets up some environment variables important to clang, locates a; just-built or optionally an installed clang, and add a set of standard; substitutions useful to any test suite that makes use of clang. """"""; # Clear some environment variables that might affect Clang.; #; # This first set of vars are read by Clang, but shouldn't affect tests; # that aren't specifically looking for these features, or are required; # simply to run the tests at all.; #; # FIXME: Should we have a tool that enforces this?; # safe_env_vars = (; # 'TMPDIR', 'TEMP', 'TMP', 'USERPROFILE', 'PWD',; # 'MACOSX_DEPLOYMENT_TARGET', 'IPHONEOS_DEPLOYMENT_TARGET',; # 'VCINSTALLDIR', 'VC100COMNTOOLS', 'VC90COMNTOOLS',; # 'VC80COMNTOOLS'); # Clang/Win32 may refer to %INCLUDE%. vsvarsall.bat sets it.; # Tweak the PATH to include the tools dir and the",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/llvm/config.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/llvm/config.py
Testability,test,tests,"# os.getuid() is not available on all platforms; # Tweak PATH for Win32 to decide to use bash.exe or not.; # Seek necessary tools in directories and set to $PATH.; # Many tools behave strangely if these environment variables aren't; # set.; # Choose between lit's internal shell pipeline runner and a real shell.; # If LIT_USE_INTERNAL_SHELL is in the environment, we use that as an; # override.; # Running on Darwin OS; # FIXME: lld uses the first, other projects use the second.; # We should standardize on the former.; # For tests that require Windows to run.; # Native compilation: host arch == default triple arch; # Both of these values should probably be in every site config (e.g. as; # part of the standard header. But currently they aren't); # Sanitizers.; # Check if we should run long running tests.; # Allow use of an explicit path for gmalloc library.; # Will default to '/usr/lib/libgmalloc.dylib' if not set.; # Search both the 64 and 32-bit hives, as well as HKLM + HKCU; # We found it, stop enumerating.; # For paths, we should be able to take a list of them and process; # all of them.; # If we are passed a list [a b c], then iterating this list forwards; # and adding each to the beginning would result in c b a. So we; # need to iterate in reverse to end up with the original ordering.; # Move it to the front if it already exists, otherwise insert; # it at the beginning.; # Ask llvm-config about the specified feature.; # We should have either a callable or a dictionary. If it's a; # dictionary, grep each key against the output and use the value if; # it matches. If it's a callable, it does the entire translation.; # Note that when substituting %clang_cc1 also fill in the include directory; # of the builtin headers. Those are part of even a freestanding; # environment, but Clang relies on the driver to locate them.; # FIXME: Rather than just getting the version, we should have clang; # print out its resource dir here in an easy to scrape form.; # Don't pass dosish pa",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/llvm/config.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/llvm/config.py
Usability,simpl,simply,"CES as a semi colon; # separated string. LLVM testsuites can use get_errc_messages in cmake; # to automatically get the messages and pass them into lit.; # Handle these specially as they are strings searched for during; # testing.; """"""Find the executable program 'name', optionally using the specified; environment variable as an override before searching the build directory; and then optionally the configuration's PATH.""""""; # If the override is specified in the environment, use it without; # validation.; # Use the specified search paths.; # Otherwise look in the path, if enabled.; """"""Configure the test suite to be able to invoke clang. Sets up some environment variables important to clang, locates a; just-built or optionally an installed clang, and add a set of standard; substitutions useful to any test suite that makes use of clang. """"""; # Clear some environment variables that might affect Clang.; #; # This first set of vars are read by Clang, but shouldn't affect tests; # that aren't specifically looking for these features, or are required; # simply to run the tests at all.; #; # FIXME: Should we have a tool that enforces this?; # safe_env_vars = (; # 'TMPDIR', 'TEMP', 'TMP', 'USERPROFILE', 'PWD',; # 'MACOSX_DEPLOYMENT_TARGET', 'IPHONEOS_DEPLOYMENT_TARGET',; # 'VCINSTALLDIR', 'VC100COMNTOOLS', 'VC90COMNTOOLS',; # 'VC80COMNTOOLS'); # Clang/Win32 may refer to %INCLUDE%. vsvarsall.bat sets it.; # Tweak the PATH to include the tools dir and the scripts dir.; # Put Clang first to avoid LLVM from overriding out-of-tree clang; # builds.; # Discover the 'clang' and 'clangcc' to use.; # There will be no default target triple if one was not specifically; # set, and the host's architecture is not an enabled target.; # The host triple might not be set, at least if we're compiling clang; # from an already installed llvm.; # TODO: Many tests work across many language standards. Before; # https://discourse.llvm.org/t/lit-run-a-run-line-multiple-times-with-different-replacements/6",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/llvm/config.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/llvm/config.py
Availability,error,error,"h paths.; """"""String-like class used to build regex substitution patterns for llvm; tools. Handles things like adding word-boundary patterns, and filtering; characters from the beginning an end of a tool name. """"""; """"""Construct a ToolSubst. key: The text which is to be substituted. command: The command to substitute when the key is matched. By default,; this will treat `key` as a tool name and search for it. If it is a; string, it is interpreted as an exact path. If it is an instance of; FindTool, the specified tool name is searched for on disk. pre: If specified, the substitution will not find matches where; the character immediately preceding the word-boundary that begins; `key` is any of the characters in the string `pre`. post: If specified, the substitution will not find matches where; the character immediately after the word-boundary that ends `key`; is any of the characters specified in the string `post`. verbatim: If True, `key` is an exact regex that is passed to the; underlying substitution. unresolved: Action to take if the tool substitution cannot be; resolved. Valid values:; 'warn' - log a warning but add the substitution anyway.; 'fatal' - Exit the test suite and log a fatal error.; 'break' - Don't add any of the substitutions from the current; group, and return a value indicating a failure.; 'ignore' - Don't add the substitution, and don't log an error. extra_args: If specified, represents a list of arguments that will be; appended to the tool's substitution. """"""; # Extract the tool name from the pattern. This relies on the tool name; # being surrounded by \b word match operators. If the pattern starts; # with ""| "", include it in the string to be substituted.; # Warn, but still provide a substitution.; # The function won't even return in this case, this leads to; # sys.exit; # By returning a valid result with an empty command, the; # caller treats this as a failure.; # By returning None, the caller just assumes there was no; # match in the first place.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/llvm/subst.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/llvm/subst.py
Testability,log,log,"h paths.; """"""String-like class used to build regex substitution patterns for llvm; tools. Handles things like adding word-boundary patterns, and filtering; characters from the beginning an end of a tool name. """"""; """"""Construct a ToolSubst. key: The text which is to be substituted. command: The command to substitute when the key is matched. By default,; this will treat `key` as a tool name and search for it. If it is a; string, it is interpreted as an exact path. If it is an instance of; FindTool, the specified tool name is searched for on disk. pre: If specified, the substitution will not find matches where; the character immediately preceding the word-boundary that begins; `key` is any of the characters in the string `pre`. post: If specified, the substitution will not find matches where; the character immediately after the word-boundary that ends `key`; is any of the characters specified in the string `post`. verbatim: If True, `key` is an exact regex that is passed to the; underlying substitution. unresolved: Action to take if the tool substitution cannot be; resolved. Valid values:; 'warn' - log a warning but add the substitution anyway.; 'fatal' - Exit the test suite and log a fatal error.; 'break' - Don't add any of the substitutions from the current; group, and return a value indicating a failure.; 'ignore' - Don't add the substitution, and don't log an error. extra_args: If specified, represents a list of arguments that will be; appended to the tool's substitution. """"""; # Extract the tool name from the pattern. This relies on the tool name; # being surrounded by \b word match operators. If the pattern starts; # with ""| "", include it in the string to be substituted.; # Warn, but still provide a substitution.; # The function won't even return in this case, this leads to; # sys.exit; # By returning a valid result with an empty command, the; # caller treats this as a failure.; # By returning None, the caller just assumes there was no; # match in the first place.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/lit/lit/llvm/subst.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/lit/llvm/subst.py
Availability,avail,availability,"#!/usr/bin/env python; #; # This is a tool that works like debug location coverage calculator.; # It parses the llvm-dwarfdump --statistics output by reporting it; # in a more human readable way.; #; # This special value has been used to mark statistics that overflowed.; # Initialize the plot.; # Finalize the plot.; # Holds the debug location statistics.; # Get the PC ranges coverage.; # Pretty print the debug location buckets.; # Only if we are processing all the variables output the total; # availability.; # Draw a plot representing the location buckets.; # Place the text box with the coverage info.; # Compare the two LocationStats objects and draw a plot showing; # the difference.; # Define the location buckets.; # Parse the JSON representing the debug statistics, and create a; # LocationStats object.; # These will be different due to different options enabled.; # Get the directory of the LLVM tools.; # The statistics llvm-dwarfdump option.; # Generate the stats with the llvm-dwarfdump.; # TODO: Handle errors that are coming from llvm-dwarfdump.; # Get the JSON and parse it.; # TODO: Parse the statistics Version from JSON.; # Read the JSON only for local variables.; # Read the JSON only for formal parameters.; # Read the JSON for both local variables and formal parameters.; # Parse the program arguments.; # Verify that the program inputs meet the requirements.; # Draw a histogram representing the location buckets.; # Pretty print collected info on the standard output.; # Draw a plot showing the difference in debug location coverage between; # two files.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/llvm-locstats/llvm-locstats.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/llvm-locstats/llvm-locstats.py
Modifiability,variab,variables,"#!/usr/bin/env python; #; # This is a tool that works like debug location coverage calculator.; # It parses the llvm-dwarfdump --statistics output by reporting it; # in a more human readable way.; #; # This special value has been used to mark statistics that overflowed.; # Initialize the plot.; # Finalize the plot.; # Holds the debug location statistics.; # Get the PC ranges coverage.; # Pretty print the debug location buckets.; # Only if we are processing all the variables output the total; # availability.; # Draw a plot representing the location buckets.; # Place the text box with the coverage info.; # Compare the two LocationStats objects and draw a plot showing; # the difference.; # Define the location buckets.; # Parse the JSON representing the debug statistics, and create a; # LocationStats object.; # These will be different due to different options enabled.; # Get the directory of the LLVM tools.; # The statistics llvm-dwarfdump option.; # Generate the stats with the llvm-dwarfdump.; # TODO: Handle errors that are coming from llvm-dwarfdump.; # Get the JSON and parse it.; # TODO: Parse the statistics Version from JSON.; # Read the JSON only for local variables.; # Read the JSON only for formal parameters.; # Read the JSON for both local variables and formal parameters.; # Parse the program arguments.; # Verify that the program inputs meet the requirements.; # Draw a histogram representing the location buckets.; # Pretty print collected info on the standard output.; # Draw a plot showing the difference in debug location coverage between; # two files.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/llvm-locstats/llvm-locstats.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/llvm-locstats/llvm-locstats.py
Performance,perform,performed,"# Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; """"""Extract IR for training. Extract IR for training, either from a compile_commands.json file produced by; cmake, or a linker parameter list file. Only run with; 'python compiler_opt/tools/extract_ir.py ...'. The compilation is assumed to have been performed with clang, using; -fembed-bitcode=all passed to cc1 (i.e. pass clang -Xclang=-fembed-bitcode=all). In a distributed ThinLTO case, the compilation is assumed to have been performed; specifying -mllvm -lto-embed-bitcode=post-merge-pre-opt. In a local ThinLTO case, the compilation is assumedto have been performed; specifying -Wl,--save-temps=import -Wl,--thinlto-emit-index-files. To change the logging verbosity, pass an integer representing the desired; verbosity to the --verbosity flag. Use 0 for all logs, status information,; and detailed debug information, -1 for solely warnings, and -2 to not produce; any output.; """"""",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/mlgo-utils/mlgo/corpus/extract_ir.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/mlgo-utils/mlgo/corpus/extract_ir.py
Testability,log,logging,"# Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; """"""Extract IR for training. Extract IR for training, either from a compile_commands.json file produced by; cmake, or a linker parameter list file. Only run with; 'python compiler_opt/tools/extract_ir.py ...'. The compilation is assumed to have been performed with clang, using; -fembed-bitcode=all passed to cc1 (i.e. pass clang -Xclang=-fembed-bitcode=all). In a distributed ThinLTO case, the compilation is assumed to have been performed; specifying -mllvm -lto-embed-bitcode=post-merge-pre-opt. In a local ThinLTO case, the compilation is assumedto have been performed; specifying -Wl,--save-temps=import -Wl,--thinlto-emit-index-files. To change the logging verbosity, pass an integer representing the desired; verbosity to the --verbosity flag. Use 0 for all logs, status information,; and detailed debug information, -1 for solely warnings, and -2 to not produce; any output.; """"""",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/mlgo-utils/mlgo/corpus/extract_ir.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/mlgo-utils/mlgo/corpus/extract_ir.py
Performance,perform,performed,"ing not starting with '-', and; # ending in a '.o', is an object file.; """"""Create an object file array by globbing an entire drectory. Args:; obj_base_dir: The base build directory that all object files will be; written out as being relative to.; output_dir: The output directory where extracted .bc and .cmd files should; be placed.; """"""; # .3.import.bc is the suffix attached to post-merge-pre-opt ('postimport'); # IR bitcode saved by lld. It is hardcoded into lld. ThinLTO index files; # are also emitted next to the postimport bitcode, with the suffix; # .thinlto.bc instead; # Cut away .3.import.bc; """"""Extracts all specified object files into the corpus directory. Args:; objs: A list of TrainingIRExtractor Objects that represent the object files; to extract bitcode/commands from.; num_workers: The number of parallel processes to spawn to run the; extraction.; llvm_objcopy_path: The path to the llvm-objcopy to use for dumping sections.; cmd_filter: A regular expression that is used to select for compilations; performed with specific flags. If you want to include all compilations,; set this to None.; thinlto_build: Whether or not this is a ThinLTO build, and if so, the type.; Set this to None if the build was not done with ThinLTO.; cmd_section_name: The name of the command line section created by the; bitcode embedding.; bitcode_section_name: The name of the bitcode section created by the; bitcode embedding.; """"""; """"""Writes a corpus_manifest.json containing all necessary information about; the corpus. Args:; thinlto_build: Whether or not the build was done with ThinLTO and if so,; what kind of ThinLTO. Set this to none if the build was not performed with; ThinLTO.; relative_output_paths: The relative (to the corpus directory) output paths; of all the bitcode files that should be placed in the corpus manifest; output_dir: The corpus directory where the corpus manifest should be; placed.; """"""; # This comes first rather than later so global_command_override is at the top",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/mlgo-utils/mlgo/corpus/extract_ir_lib.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/mlgo-utils/mlgo/corpus/extract_ir_lib.py
Availability,down,downstream,"# Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; """"""Library functions for making a corpus from arbitrary bitcode.""""""; """"""Finds bitcode files to extract from a given directory. Args:; bitcode_base_dir: The base directory where the bitcode to be copied; is from.; output_dir: The directory to place the bitcode in. Returns an array of paths representing the relative path to the bitcode; file from the base direcotry.; """"""; """"""Copies bitcode files from the base directory to the output directory. Args:; relative_paths: An array of relative paths to bitcode files that are copied; over to the output directory, preserving relative location.; bitcode_base_dir: The base directory where the bitcode is located.; output_dir: The output directory to place the bitcode in.; """"""; """"""Creates a corpus manifest describing the bitcode that has been found. Args:; relative_output_paths: A list of paths to each bitcode file relative to the; output directory.; outout_dir: The output directory where the corpus is being created.; default_args: An array of compiler flags that should be used to compile; the bitcode when using further downstream tooling.""""""",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/mlgo-utils/mlgo/corpus/make_corpus_lib.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/mlgo-utils/mlgo/corpus/make_corpus_lib.py
Deployability,pipeline,pipeline,#!/usr/bin/env python3; # Automatically formatted with yapf (https://github.com/google/yapf); # Fake 'opt' program that can be made to crash on request. For testing; # the 'reduce_pipeline.py' automatic 'opt' NPM pipeline reducer.; # Expand pipeline if '-print-pipeline-passes'.; # Parse '-crash-seq'.; # Parse '-passes' and see if we need to crash.; # Copy input to output.; # Crash if all 'crash_seq' passes occurred in right order.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/reduce_pipeline_test/fake_opt.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/reduce_pipeline_test/fake_opt.py
Energy Efficiency,reduce,reducer,#!/usr/bin/env python3; # Automatically formatted with yapf (https://github.com/google/yapf); # Fake 'opt' program that can be made to crash on request. For testing; # the 'reduce_pipeline.py' automatic 'opt' NPM pipeline reducer.; # Expand pipeline if '-print-pipeline-passes'.; # Parse '-crash-seq'.; # Parse '-passes' and see if we need to crash.; # Copy input to output.; # Crash if all 'crash_seq' passes occurred in right order.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/reduce_pipeline_test/fake_opt.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/reduce_pipeline_test/fake_opt.py
Testability,test,testing,#!/usr/bin/env python3; # Automatically formatted with yapf (https://github.com/google/yapf); # Fake 'opt' program that can be made to crash on request. For testing; # the 'reduce_pipeline.py' automatic 'opt' NPM pipeline reducer.; # Expand pipeline if '-print-pipeline-passes'.; # Parse '-crash-seq'.; # Parse '-passes' and see if we need to crash.; # Copy input to output.; # Crash if all 'crash_seq' passes occurred in right order.,MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/reduce_pipeline_test/fake_opt.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/reduce_pipeline_test/fake_opt.py
Deployability,configurat,configuration,"#!/usr/bin/env python3; # This script bumps the version of LLVM in *all* the different places where; # it needs to be defined. Which is quite a few.; # Print the failing line just to inform the user.; # Return a string from the version class; # optionally include the suffix (-rcX); # llvm/CMakeLists.txt; # LLVM_VERSION_SUFFIX should be set to -rcX or be blank if we are; # building a final version.; # Check the rest of the LLVM_VERSION_ lines.; # GN build system; # LIT python file, a simple tuple; # Handle libc++ config header; # match #define _LIBCPP_VERSION 160000 in a relaxed way; # parse the version string with distutils.; # note that -rc will end up as version.pre here; # since it's a prerelease; # Find llvm-project root; # Main CMakeLists.; # Lit configuration; # GN build system",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/release/bump-version.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/release/bump-version.py
Modifiability,config,config,"#!/usr/bin/env python3; # This script bumps the version of LLVM in *all* the different places where; # it needs to be defined. Which is quite a few.; # Print the failing line just to inform the user.; # Return a string from the version class; # optionally include the suffix (-rcX); # llvm/CMakeLists.txt; # LLVM_VERSION_SUFFIX should be set to -rcX or be blank if we are; # building a final version.; # Check the rest of the LLVM_VERSION_ lines.; # GN build system; # LIT python file, a simple tuple; # Handle libc++ config header; # match #define _LIBCPP_VERSION 160000 in a relaxed way; # parse the version string with distutils.; # note that -rc will end up as version.pre here; # since it's a prerelease; # Find llvm-project root; # Main CMakeLists.; # Lit configuration; # GN build system",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/release/bump-version.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/release/bump-version.py
Usability,simpl,simple,"#!/usr/bin/env python3; # This script bumps the version of LLVM in *all* the different places where; # it needs to be defined. Which is quite a few.; # Print the failing line just to inform the user.; # Return a string from the version class; # optionally include the suffix (-rcX); # llvm/CMakeLists.txt; # LLVM_VERSION_SUFFIX should be set to -rcX or be blank if we are; # building a final version.; # Check the rest of the LLVM_VERSION_ lines.; # GN build system; # LIT python file, a simple tuple; # Handle libc++ config header; # match #define _LIBCPP_VERSION 160000 in a relaxed way; # parse the version string with distutils.; # note that -rc will end up as version.pre here; # since it's a prerelease; # Find llvm-project root; # Main CMakeLists.; # Lit configuration; # GN build system",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/release/bump-version.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/release/bump-version.py
Testability,test,test,"#!/usr/bin/env python; # Cleanup weird stuff; # Diff results and look for regressions.; # Check if the test passed or failed.; # For execution time, if there is no result, its a fail.; # Main",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/release/findRegressions-nightly.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/release/findRegressions-nightly.py
Testability,test,test,"#!/usr/bin/env python; # Cleanup weird stuff; # print ""ERROR!""; # Diff results and look for regressions.; # Check if the test passed or failed.; # For execution time, if there is no result it's a fail.; # Main",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/release/findRegressions-simple.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/release/findRegressions-simple.py
Availability,avail,available,"#!/usr/bin/env python3; # ===-- github-upload-release.py ------------------------------------------===#; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ===------------------------------------------------------------------------===#; #; # Create and manage releases in the llvm github project.; #; # This script requires python3 and the PyGithub module.; #; # Example Usage:; #; # You will need to obtain a personal access token for your github account in; # order to use this script. Instructions for doing this can be found here:; # https://help.github.com/en/articles/creating-a-personal-access-token-for-the-command-line; #; # Create a new release from an existing tag:; # ./github-upload-release.py --token $github_token --release 8.0.1-rc4 create; #; # Upload files for a release; # ./github-upload-release.py --token $github_token --release 8.0.1-rc4 upload --files llvm-8.0.1rc4.src.tar.xz; #; # You can upload as many files as you want at a time and use wildcards e.g.; # ./github-upload-release.py --token $github_token --release 8.0.1-rc4 upload --files *.src.*; # ===------------------------------------------------------------------------===#; """"""\; LLVM {} Release. # A note on binaries. Volunteers make binaries for the LLVM project, which will be uploaded; when they have had time to test and build these binaries. They might; not be available directly or not at all for each release. We suggest; you use the binaries from your distribution or build your own if you; rely on a specific platform or configuration.""""""; # All args; # Upload args; # Validate that this user is allowed to modify releases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/release/github-upload-release.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/release/github-upload-release.py
Deployability,release,release,"#!/usr/bin/env python3; # ===-- github-upload-release.py ------------------------------------------===#; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ===------------------------------------------------------------------------===#; #; # Create and manage releases in the llvm github project.; #; # This script requires python3 and the PyGithub module.; #; # Example Usage:; #; # You will need to obtain a personal access token for your github account in; # order to use this script. Instructions for doing this can be found here:; # https://help.github.com/en/articles/creating-a-personal-access-token-for-the-command-line; #; # Create a new release from an existing tag:; # ./github-upload-release.py --token $github_token --release 8.0.1-rc4 create; #; # Upload files for a release; # ./github-upload-release.py --token $github_token --release 8.0.1-rc4 upload --files llvm-8.0.1rc4.src.tar.xz; #; # You can upload as many files as you want at a time and use wildcards e.g.; # ./github-upload-release.py --token $github_token --release 8.0.1-rc4 upload --files *.src.*; # ===------------------------------------------------------------------------===#; """"""\; LLVM {} Release. # A note on binaries. Volunteers make binaries for the LLVM project, which will be uploaded; when they have had time to test and build these binaries. They might; not be available directly or not at all for each release. We suggest; you use the binaries from your distribution or build your own if you; rely on a specific platform or configuration.""""""; # All args; # Upload args; # Validate that this user is allowed to modify releases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/release/github-upload-release.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/release/github-upload-release.py
Modifiability,config,configuration,"#!/usr/bin/env python3; # ===-- github-upload-release.py ------------------------------------------===#; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ===------------------------------------------------------------------------===#; #; # Create and manage releases in the llvm github project.; #; # This script requires python3 and the PyGithub module.; #; # Example Usage:; #; # You will need to obtain a personal access token for your github account in; # order to use this script. Instructions for doing this can be found here:; # https://help.github.com/en/articles/creating-a-personal-access-token-for-the-command-line; #; # Create a new release from an existing tag:; # ./github-upload-release.py --token $github_token --release 8.0.1-rc4 create; #; # Upload files for a release; # ./github-upload-release.py --token $github_token --release 8.0.1-rc4 upload --files llvm-8.0.1rc4.src.tar.xz; #; # You can upload as many files as you want at a time and use wildcards e.g.; # ./github-upload-release.py --token $github_token --release 8.0.1-rc4 upload --files *.src.*; # ===------------------------------------------------------------------------===#; """"""\; LLVM {} Release. # A note on binaries. Volunteers make binaries for the LLVM project, which will be uploaded; when they have had time to test and build these binaries. They might; not be available directly or not at all for each release. We suggest; you use the binaries from your distribution or build your own if you; rely on a specific platform or configuration.""""""; # All args; # Upload args; # Validate that this user is allowed to modify releases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/release/github-upload-release.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/release/github-upload-release.py
Security,access,access,"#!/usr/bin/env python3; # ===-- github-upload-release.py ------------------------------------------===#; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ===------------------------------------------------------------------------===#; #; # Create and manage releases in the llvm github project.; #; # This script requires python3 and the PyGithub module.; #; # Example Usage:; #; # You will need to obtain a personal access token for your github account in; # order to use this script. Instructions for doing this can be found here:; # https://help.github.com/en/articles/creating-a-personal-access-token-for-the-command-line; #; # Create a new release from an existing tag:; # ./github-upload-release.py --token $github_token --release 8.0.1-rc4 create; #; # Upload files for a release; # ./github-upload-release.py --token $github_token --release 8.0.1-rc4 upload --files llvm-8.0.1rc4.src.tar.xz; #; # You can upload as many files as you want at a time and use wildcards e.g.; # ./github-upload-release.py --token $github_token --release 8.0.1-rc4 upload --files *.src.*; # ===------------------------------------------------------------------------===#; """"""\; LLVM {} Release. # A note on binaries. Volunteers make binaries for the LLVM project, which will be uploaded; when they have had time to test and build these binaries. They might; not be available directly or not at all for each release. We suggest; you use the binaries from your distribution or build your own if you; rely on a specific platform or configuration.""""""; # All args; # Upload args; # Validate that this user is allowed to modify releases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/release/github-upload-release.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/release/github-upload-release.py
Testability,test,test,"#!/usr/bin/env python3; # ===-- github-upload-release.py ------------------------------------------===#; #; # Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; #; # ===------------------------------------------------------------------------===#; #; # Create and manage releases in the llvm github project.; #; # This script requires python3 and the PyGithub module.; #; # Example Usage:; #; # You will need to obtain a personal access token for your github account in; # order to use this script. Instructions for doing this can be found here:; # https://help.github.com/en/articles/creating-a-personal-access-token-for-the-command-line; #; # Create a new release from an existing tag:; # ./github-upload-release.py --token $github_token --release 8.0.1-rc4 create; #; # Upload files for a release; # ./github-upload-release.py --token $github_token --release 8.0.1-rc4 upload --files llvm-8.0.1rc4.src.tar.xz; #; # You can upload as many files as you want at a time and use wildcards e.g.; # ./github-upload-release.py --token $github_token --release 8.0.1-rc4 upload --files *.src.*; # ===------------------------------------------------------------------------===#; """"""\; LLVM {} Release. # A note on binaries. Volunteers make binaries for the LLVM project, which will be uploaded; when they have had time to test and build these binaries. They might; not be available directly or not at all for each release. We suggest; you use the binaries from your distribution or build your own if you; rely on a specific platform or configuration.""""""; # All args; # Upload args; # Validate that this user is allowed to modify releases.",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/release/github-upload-release.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/release/github-upload-release.py
Deployability,install,install,"#!/usr/bin/env python; # Setting up a virtualenv to run this script can be done by running the; # following commands:; # $ virtualenv venv; # $ . ./venv/bin/activate; # $ pip install Phabricator; # The below PhabXXX classes represent objects as modelled by Phabricator.; # The classes can be serialized to disk, to try and make sure that we don't; # needlessly have to re-fetch lots of data from Phabricator, as that would; # make this script unusably slow.; """"""; FIXME: consider if serializing to JSON would bring interoperability; advantages over serializing to pickle.; """"""; # self.actual_lines_changed_offset will contain the offsets of the; # lines that were changed in this hunk.; # line is a new line that got introduced in this patch.; # Do not record it as a changed line.; # line was changed or removed from the older version of the; # code. Record it as a changed line.; # line is a context line.; # The above algorithm could result in adjacent or overlapping ranges; # being recorded into self.actual_lines_changed_offset.; # Merge the adjacent and overlapping ranges in there:; # We did fetch all records. Mark the cache to contain all info since; # the start of time.; # phid = reviewInfo[""phid""]; # dateCreated = int(info[""fields""][""dateCreated""]); # All of the above code is about fetching data from Phabricator and caching it; # on local disk. The below code contains the actual ""business logic"" for this; # script.; # Print out a summary per reviewer.; # FIXME: the blame cache could probably be made more effective still if; # instead of storing the requested base_revision in the cache, the last; # revision before the base revision this file/path got changed in gets; # stored. That way multiple project revisions for which this specific; # file/patch hasn't changed would get cache hits (instead of misses in; # the current implementation).; # Heuristic 1: assume good reviewers are the ones that touched the same; # lines before as this patch is touching.; # Heuristic 2: assume",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/Reviewing/find_interesting_reviews.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/Reviewing/find_interesting_reviews.py
Integrability,interoperab,interoperability,"#!/usr/bin/env python; # Setting up a virtualenv to run this script can be done by running the; # following commands:; # $ virtualenv venv; # $ . ./venv/bin/activate; # $ pip install Phabricator; # The below PhabXXX classes represent objects as modelled by Phabricator.; # The classes can be serialized to disk, to try and make sure that we don't; # needlessly have to re-fetch lots of data from Phabricator, as that would; # make this script unusably slow.; """"""; FIXME: consider if serializing to JSON would bring interoperability; advantages over serializing to pickle.; """"""; # self.actual_lines_changed_offset will contain the offsets of the; # lines that were changed in this hunk.; # line is a new line that got introduced in this patch.; # Do not record it as a changed line.; # line was changed or removed from the older version of the; # code. Record it as a changed line.; # line is a context line.; # The above algorithm could result in adjacent or overlapping ranges; # being recorded into self.actual_lines_changed_offset.; # Merge the adjacent and overlapping ranges in there:; # We did fetch all records. Mark the cache to contain all info since; # the start of time.; # phid = reviewInfo[""phid""]; # dateCreated = int(info[""fields""][""dateCreated""]); # All of the above code is about fetching data from Phabricator and caching it; # on local disk. The below code contains the actual ""business logic"" for this; # script.; # Print out a summary per reviewer.; # FIXME: the blame cache could probably be made more effective still if; # instead of storing the requested base_revision in the cache, the last; # revision before the base revision this file/path got changed in gets; # stored. That way multiple project revisions for which this specific; # file/patch hasn't changed would get cache hits (instead of misses in; # the current implementation).; # Heuristic 1: assume good reviewers are the ones that touched the same; # lines before as this patch is touching.; # Heuristic 2: assume",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/Reviewing/find_interesting_reviews.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/Reviewing/find_interesting_reviews.py
Performance,cache,cache,"env/bin/activate; # $ pip install Phabricator; # The below PhabXXX classes represent objects as modelled by Phabricator.; # The classes can be serialized to disk, to try and make sure that we don't; # needlessly have to re-fetch lots of data from Phabricator, as that would; # make this script unusably slow.; """"""; FIXME: consider if serializing to JSON would bring interoperability; advantages over serializing to pickle.; """"""; # self.actual_lines_changed_offset will contain the offsets of the; # lines that were changed in this hunk.; # line is a new line that got introduced in this patch.; # Do not record it as a changed line.; # line was changed or removed from the older version of the; # code. Record it as a changed line.; # line is a context line.; # The above algorithm could result in adjacent or overlapping ranges; # being recorded into self.actual_lines_changed_offset.; # Merge the adjacent and overlapping ranges in there:; # We did fetch all records. Mark the cache to contain all info since; # the start of time.; # phid = reviewInfo[""phid""]; # dateCreated = int(info[""fields""][""dateCreated""]); # All of the above code is about fetching data from Phabricator and caching it; # on local disk. The below code contains the actual ""business logic"" for this; # script.; # Print out a summary per reviewer.; # FIXME: the blame cache could probably be made more effective still if; # instead of storing the requested base_revision in the cache, the last; # revision before the base revision this file/path got changed in gets; # stored. That way multiple project revisions for which this specific; # file/patch hasn't changed would get cache hits (instead of misses in; # the current implementation).; # Heuristic 1: assume good reviewers are the ones that touched the same; # lines before as this patch is touching.; # Heuristic 2: assume good reviewers are the ones that touched the same; # files before as this patch is touching.; # Assume last revision before diff was modified is the",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/Reviewing/find_interesting_reviews.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/Reviewing/find_interesting_reviews.py
Testability,log,logic,"Phabricator, as that would; # make this script unusably slow.; """"""; FIXME: consider if serializing to JSON would bring interoperability; advantages over serializing to pickle.; """"""; # self.actual_lines_changed_offset will contain the offsets of the; # lines that were changed in this hunk.; # line is a new line that got introduced in this patch.; # Do not record it as a changed line.; # line was changed or removed from the older version of the; # code. Record it as a changed line.; # line is a context line.; # The above algorithm could result in adjacent or overlapping ranges; # being recorded into self.actual_lines_changed_offset.; # Merge the adjacent and overlapping ranges in there:; # We did fetch all records. Mark the cache to contain all info since; # the start of time.; # phid = reviewInfo[""phid""]; # dateCreated = int(info[""fields""][""dateCreated""]); # All of the above code is about fetching data from Phabricator and caching it; # on local disk. The below code contains the actual ""business logic"" for this; # script.; # Print out a summary per reviewer.; # FIXME: the blame cache could probably be made more effective still if; # instead of storing the requested base_revision in the cache, the last; # revision before the base revision this file/path got changed in gets; # stored. That way multiple project revisions for which this specific; # file/patch hasn't changed would get cache hits (instead of misses in; # the current implementation).; # Heuristic 1: assume good reviewers are the ones that touched the same; # lines before as this patch is touching.; # Heuristic 2: assume good reviewers are the ones that touched the same; # files before as this patch is touching.; # Assume last revision before diff was modified is the revision the diff; # applies to.; # Compute heuristic 1: look at context of patch lines.; # Collect git blame results for authors in those ranges.; # Compute heuristic 2: don't look at context, just at files touched.; # Collect git blame results",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/Reviewing/find_interesting_reviews.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/Reviewing/find_interesting_reviews.py
Availability,error,error,":; * %config - to change the behaviour of the kernel overall, including; changing defaults for things like resets. Global magic must be written in the same way as cell magic. ```tablgen; %args; %reset; %args --print-records --print-detailed-records; class Stuff {; string Name;; }. def a_thing : Stuff {}; ```. """"""; # A list of (code, magic) tuples.; # All the previous cell's code we have run since the last reset.; # This emulates a persistent state like a Python interpreter would have.; # The most recent set of magic since the last reset.; # The default cache reset behaviour. True means do not cache anything; # between cells.; """"""If this is the first run, search for llvm-tblgen.; Otherwise return the cached path to it.""""""; """"""Config should be a list of parameters given to the %config command.; We allow only one setting per %config line and that setting can only; have one value. Assuming the parameters are valid, update the kernel's setting with; the new value. If there is an error, raise a TableGenKernelException. >>> k.parse_config_magic([]); Traceback (most recent call last):; ...; TableGenKernelException: Incorrect number of parameters to %config. Expected %config <setting> <value>.; >>> k._cell_reset; False; >>> k.parse_config_magic([""a"", ""b"", ""c""]); Traceback (most recent call last):; ...; TableGenKernelException: Incorrect number of parameters to %config. Expected %config <setting> <value>.; >>> k.parse_config_magic([""notasetting"", ""...""]); Traceback (most recent call last):; ...; TableGenKernelException: Unknown kernel setting ""notasetting"". Possible settings are: ""cellreset"".; >>> k.parse_config_magic([""cellreset"", ""food""]); Traceback (most recent call last):; ...; TableGenKernelException: Invalid value for setting ""cellreset"", expected ""on"" or ""off"".; >>> k.parse_config_magic([""cellreset"", ""on""]); >>> k._cell_reset; True; >>> k.parse_config_magic([""cellreset"", ""off""]); >>> k._cell_reset; False; """"""; """"""Given a block of code remove the magic lines from it.; Re",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/TableGen/jupyter/tablegen_kernel/kernel.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/jupyter/tablegen_kernel/kernel.py
Deployability,update,update,"following are global magic (that applies to all cells going; forward):; * %config - to change the behaviour of the kernel overall, including; changing defaults for things like resets. Global magic must be written in the same way as cell magic. ```tablgen; %args; %reset; %args --print-records --print-detailed-records; class Stuff {; string Name;; }. def a_thing : Stuff {}; ```. """"""; # A list of (code, magic) tuples.; # All the previous cell's code we have run since the last reset.; # This emulates a persistent state like a Python interpreter would have.; # The most recent set of magic since the last reset.; # The default cache reset behaviour. True means do not cache anything; # between cells.; """"""If this is the first run, search for llvm-tblgen.; Otherwise return the cached path to it.""""""; """"""Config should be a list of parameters given to the %config command.; We allow only one setting per %config line and that setting can only; have one value. Assuming the parameters are valid, update the kernel's setting with; the new value. If there is an error, raise a TableGenKernelException. >>> k.parse_config_magic([]); Traceback (most recent call last):; ...; TableGenKernelException: Incorrect number of parameters to %config. Expected %config <setting> <value>.; >>> k._cell_reset; False; >>> k.parse_config_magic([""a"", ""b"", ""c""]); Traceback (most recent call last):; ...; TableGenKernelException: Incorrect number of parameters to %config. Expected %config <setting> <value>.; >>> k.parse_config_magic([""notasetting"", ""...""]); Traceback (most recent call last):; ...; TableGenKernelException: Unknown kernel setting ""notasetting"". Possible settings are: ""cellreset"".; >>> k.parse_config_magic([""cellreset"", ""food""]); Traceback (most recent call last):; ...; TableGenKernelException: Invalid value for setting ""cellreset"", expected ""on"" or ""off"".; >>> k.parse_config_magic([""cellreset"", ""on""]); >>> k._cell_reset; True; >>> k.parse_config_magic([""cellreset"", ""off""]); >>> k._cell_reset; Fa",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/TableGen/jupyter/tablegen_kernel/kernel.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/jupyter/tablegen_kernel/kernel.py
Modifiability,config,config,"# Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; """"""Kernel using llvm-tblgen inside jupyter. All input is treated as TableGen unless the first non whitespace character; is ""%"" in which case it is a ""magic"" line. The supported cell magic is:; * %args - to set the arguments passed to llvm-tblgen.; * %reset - to reset the cached code and magic state.; * %noreset - to not reset the cached code and magic state; (useful when you have changed the default to always; reset the cache). These are ""cell magic"" meaning it applies to the whole cell. Therefore; it must be the first line, or part of a run of magic lines starting; from the first line. The following are global magic (that applies to all cells going; forward):; * %config - to change the behaviour of the kernel overall, including; changing defaults for things like resets. Global magic must be written in the same way as cell magic. ```tablgen; %args; %reset; %args --print-records --print-detailed-records; class Stuff {; string Name;; }. def a_thing : Stuff {}; ```. """"""; # A list of (code, magic) tuples.; # All the previous cell's code we have run since the last reset.; # This emulates a persistent state like a Python interpreter would have.; # The most recent set of magic since the last reset.; # The default cache reset behaviour. True means do not cache anything; # between cells.; """"""If this is the first run, search for llvm-tblgen.; Otherwise return the cached path to it.""""""; """"""Config should be a list of parameters given to the %config command.; We allow only one setting per %config line and that setting can only; have one value. Assuming the parameters are valid, update the kernel's setting with; the new value. If there is an error, raise a TableGenKernelException. >>> k.parse_config_magic([]); Traceback (most recent call last):; ...; TableGenKernelException: Incorrec",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/TableGen/jupyter/tablegen_kernel/kernel.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/jupyter/tablegen_kernel/kernel.py
Performance,cache,cached,"# Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; # See https://llvm.org/LICENSE.txt for license information.; # SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; """"""Kernel using llvm-tblgen inside jupyter. All input is treated as TableGen unless the first non whitespace character; is ""%"" in which case it is a ""magic"" line. The supported cell magic is:; * %args - to set the arguments passed to llvm-tblgen.; * %reset - to reset the cached code and magic state.; * %noreset - to not reset the cached code and magic state; (useful when you have changed the default to always; reset the cache). These are ""cell magic"" meaning it applies to the whole cell. Therefore; it must be the first line, or part of a run of magic lines starting; from the first line. The following are global magic (that applies to all cells going; forward):; * %config - to change the behaviour of the kernel overall, including; changing defaults for things like resets. Global magic must be written in the same way as cell magic. ```tablgen; %args; %reset; %args --print-records --print-detailed-records; class Stuff {; string Name;; }. def a_thing : Stuff {}; ```. """"""; # A list of (code, magic) tuples.; # All the previous cell's code we have run since the last reset.; # This emulates a persistent state like a Python interpreter would have.; # The most recent set of magic since the last reset.; # The default cache reset behaviour. True means do not cache anything; # between cells.; """"""If this is the first run, search for llvm-tblgen.; Otherwise return the cached path to it.""""""; """"""Config should be a list of parameters given to the %config command.; We allow only one setting per %config line and that setting can only; have one value. Assuming the parameters are valid, update the kernel's setting with; the new value. If there is an error, raise a TableGenKernelException. >>> k.parse_config_magic([]); Traceback (most recent call last):; ...; TableGenKernelException: Incorrec",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/TableGen/jupyter/tablegen_kernel/kernel.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/jupyter/tablegen_kernel/kernel.py
Testability,test,test,"#!/usr/bin/env python; # Auto-generates an exhaustive and repetitive test for correct bundle-locked; # alignment on x86.; # For every possible offset in an aligned bundle, a bundle-locked group of every; # size in the inclusive range [1, bundle_size] is inserted. An appropriate CHECK; # is added to verify that NOP padding occurred (or did not occur) as expected.; # Run with --align-to-end to generate a similar test with align_to_end for each; # .bundle_lock directive.; # This script runs with Python 2.7 and 3.2+; """"""; # RUN: llvm-mc -filetype=obj -triple i386-pc-linux-gnu %s -o - \\; # RUN: | llvm-objdump -triple i386 -disassemble -no-show-raw-insn - | FileCheck %s. # !!! This test is auto-generated from utils/testgen/mc-bundling-x86-gen.py !!!; # It tests that bundle-aligned grouping works correctly in MC. Read the; # source of the script for more details. .text; .bundle_align_mode {0}; """"""; # Spread out all the instructions to not worry about cross-bundle; # interference.; # Now generate an appropriate CHECK line; # had it not been padded...; # No padding needed; # Pad to end at nearest bundle boundary; # offset + instlen > BUNDLE_SIZE; # Pad to end at next bundle boundary, splitting the nop sequence; # at the nearest bundle boundary; # Padding needed; # No padding needed",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/testgen/mc-bundling-x86-gen.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/testgen/mc-bundling-x86-gen.py
Modifiability,variab,variable,"ough some backends require some additional groups like: ""directives""; # and ""func_name_separator""; # drop .L<func>$local:; # drop .type .L<func>$local; # drop optional cfi; # f: (name of function); # drop .L<func>$local:; # drop .type .L<func>$local; # .fnstart; # (body of the function); # .Lfunc_end0: or # -- End function; # drop optional cfi noise; # This list is incomplete; # (body of the function); # This list is incomplete; # drop optional cfi; # (body of the function); # This list is incomplete; # drop .L<func>$local:; # drop .type .L<func>$local,@function; # (body of the function); # f: (name of func); # optional .Ltmp<N> for EH; # Mips+LLVM standard asm prologue; # (body of the function); # Mips+LLVM standard asm epilogue; # $func_end0: (mips32 - O32) or; # .Lfunc_end0: (mips64 - NewABI); # $func_end0:; # This list is incomplete; # optional .L<func>$local: due to -fno-semantic-interposition; # optional .type .L<func>$local; # drop optional cfi noise; # We parse the function name from OpName, and grab the variable name 'var'; # for this function. Then we match that when the variable is assigned with; # OpFunction and match its body.; # optional .L<func>$local: due to -fno-semantic-interposition; # optional .type .L<func>$local; # function attributes and retval; # .visible .func (.param .align 16 .b8 func_retval0[32]); # r'^(\.visible\s+)?\.func\s+(\([^\)]*\)\s*)?'; # function name; # function name separator (opening brace); # function parameters; # (; # .param .align 16 .b8 callee_St8x4_param_0[32]; # ) // -- Begin function callee_St8x4; # function body; # function body end marker; # Scrub runs of whitespace out of the assembly, but leave the leading; # whitespace in place.; # Expand the tabs used for indentation.; # Detect shuffle asm comments and hide the operands in favor of the comments.; # Detect stack spills and reloads and hide their exact offset and whether; # they used the stack pointer or frame pointer.; # Generically match the stack offset of a mem",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/UpdateTestChecks/asm.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/UpdateTestChecks/asm.py
Availability,error,error,".; # TODO: This should not be handled differently from the other options; # TODO: This should not be handled differently from the other options; # Discard any previous script advertising.; # On Windows we must expand the patterns ourselves.; # If we're generating a new test, set the default version to the latest.; # Skip any blank comment lines in the IR.; # Skip a special double comment line we use as a separator.; # Skip any blank lines in the IR.; # if input_line.strip() == '':; # return False; # And skip any CHECK lines. We're building our own.; # Perform lit-like substitutions; # Invoke the tool that is being tested.; # TODO Remove the str form which is used by update_test_checks.py and; # update_llc_test_checks.py; # The safer list form is used by update_cc_test_checks.py; # Allow pre-processing the IR file (e.g. using sed):; # TODO: use a list instead of using shell; # Python 2.7 doesn't have subprocess.DEVNULL:; # FYI, if you crashed here with a decode error, your run line probably; # results in bitcode or other binary format being written to the pipe.; # For an opt test, you probably want to add -S or -disable-output.; # Fix line endings to unix CR style.; ##### LLVM IR parser; # Python2 does not allow def debug(*args, file=sys.stderr, **kwargs):; # If we only used filter-out, keep the line, otherwise discard it since no; # filter matched.; # Scrub runs of whitespace out of the assembly, but leave the leading; # whitespace in place.; # Expand the tabs used for indentation.; # Strip trailing whitespace.; # Build up a dictionary of all the function bodies.; # Check without replacements, the replacements are not applied to the; # body for backend checks.; # Strip double-quotes if input was read by UTC_ARGS; # Strip double-quotes if input was read by UTC_ARGS; # func_name_separator is the string that is placed right after function name at the; # beginning of assembly function definition. In most assemblies, that is just a; # colon: `foo:`. But, for example, in ",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/UpdateTestChecks/common.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/UpdateTestChecks/common.py
Integrability,wrap,wraps,"##### Common utilities for update_*test_checks.py; """"""; Version changelog:. 1: Initial version, used by tests that don't specify --version explicitly.; 2: --function-signature is now enabled by default and also checks return; type/attributes.; 3: Opening parenthesis of function args is kept on the first LABEL line; in case arguments are split to a separate SAME line.; 4: --check-globals now has a third option ('smart'). The others are now called; 'none' and 'all'. 'smart' is the default.; """"""; """"""Wrap a compiled regular expression object to allow deep copy of a regexp.; This is required for the deep copy done in do_scrub. """"""; """"""Augment a Regex object with a flag indicating whether a match should be; added (!is_filter_out) or removed (is_filter_out) from the generated checks. """"""; """"""Add a regular expression option value to a list of regular expressions.; This compiles the expression, wraps it in a Regex and adds it to the option; value list.""""""; """"""Add a filter to a list of filter option values.""""""; """"""Filters are applied to each output line according to the order given. The; first matching filter terminates filter processing for that current line.""""""; # FIXME: in 3.9, we can use argparse.BooleanOptionalAction. At that point,; # we need to rename the flag to just -generate-body-for-unused-prefixes.; # This is the default when regenerating existing tests. The default when; # generating new tests is determined by DEFAULT_VERSION.; # TODO: This should not be handled differently from the other options; # TODO: This should not be handled differently from the other options; # Discard any previous script advertising.; # On Windows we must expand the patterns ourselves.; # If we're generating a new test, set the default version to the latest.; # Skip any blank comment lines in the IR.; # Skip a special double comment line we use as a separator.; # Skip any blank lines in the IR.; # if input_line.strip() == '':; # return False; # And skip any CHECK lines. We're building our",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/UpdateTestChecks/common.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/UpdateTestChecks/common.py
Modifiability,variab,variable,"atch the regex with the same; # capture groups set.; # This means a previous RUN line produced a body for this function; # that is different from the one produced by this current RUN line,; # so the body can't be common across RUN lines. We use None to; # indicate that.; # An earlier RUN line used this check prefixes but didn't produce; # a body for this function. This happens in Clang tests that use; # preprocesser directives to exclude individual functions from some; # RUN lines.; """"""; Mark a set of prefixes as having had at least one applicable RUN line fully; processed. This is used to filter out function bodies that don't have; outputs for all RUN lines.; """"""; # This returns the list of those prefixes that failed to match any function,; # because there were conflicting bodies produced by different RUN lines, in; # all instances of the prefix.; ##### Generator of LLVM IR CHECK lines; # TODO: We should also derive check lines for global, debug, loop declarations, etc..; # Some variable numbers (e.g. MCINST1234) will change based on unrelated; # modifications to LLVM, replace those with an incrementing counter.; # Return true if this kind of IR value is ""local"", basically if it matches '%{{.*}}'.; # Return true if this kind of IR value is ""global"", basically if it matches '#{{.*}}'.; # Return the IR prefix and check prefix we use for this kind or IR value,; # e.g., (%, TMP) for locals. If the IR prefix is a regex, return the prefix; # used in the IR output; # Return the IR regexp we use for this kind or IR value, e.g., [\w.-]+? for locals; # for backwards compatibility we check locals with '.*'; # Create a FileCheck variable name based on an IR name.; # Replace variable with an incrementing counter; # This is a nameless value, prepend check_prefix.; # This is a named value that clashes with the check_prefix, prepend with; # _prefix_filecheck_ir_name, if it has been defined.; # Create a FileCheck variable from regex.; # for backwards compatibility we check locals wi",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/UpdateTestChecks/common.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/UpdateTestChecks/common.py
Safety,safe,safer,"e.BooleanOptionalAction. At that point,; # we need to rename the flag to just -generate-body-for-unused-prefixes.; # This is the default when regenerating existing tests. The default when; # generating new tests is determined by DEFAULT_VERSION.; # TODO: This should not be handled differently from the other options; # TODO: This should not be handled differently from the other options; # Discard any previous script advertising.; # On Windows we must expand the patterns ourselves.; # If we're generating a new test, set the default version to the latest.; # Skip any blank comment lines in the IR.; # Skip a special double comment line we use as a separator.; # Skip any blank lines in the IR.; # if input_line.strip() == '':; # return False; # And skip any CHECK lines. We're building our own.; # Perform lit-like substitutions; # Invoke the tool that is being tested.; # TODO Remove the str form which is used by update_test_checks.py and; # update_llc_test_checks.py; # The safer list form is used by update_cc_test_checks.py; # Allow pre-processing the IR file (e.g. using sed):; # TODO: use a list instead of using shell; # Python 2.7 doesn't have subprocess.DEVNULL:; # FYI, if you crashed here with a decode error, your run line probably; # results in bitcode or other binary format being written to the pipe.; # For an opt test, you probably want to add -S or -disable-output.; # Fix line endings to unix CR style.; ##### LLVM IR parser; # Python2 does not allow def debug(*args, file=sys.stderr, **kwargs):; # If we only used filter-out, keep the line, otherwise discard it since no; # filter matched.; # Scrub runs of whitespace out of the assembly, but leave the leading; # whitespace in place.; # Expand the tabs used for indentation.; # Strip trailing whitespace.; # Build up a dictionary of all the function bodies.; # Check without replacements, the replacements are not applied to the; # body for backend checks.; # Strip double-quotes if input was read by UTC_ARGS; # Strip double",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/UpdateTestChecks/common.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/UpdateTestChecks/common.py
Testability,test,tests,"##### Common utilities for update_*test_checks.py; """"""; Version changelog:. 1: Initial version, used by tests that don't specify --version explicitly.; 2: --function-signature is now enabled by default and also checks return; type/attributes.; 3: Opening parenthesis of function args is kept on the first LABEL line; in case arguments are split to a separate SAME line.; 4: --check-globals now has a third option ('smart'). The others are now called; 'none' and 'all'. 'smart' is the default.; """"""; """"""Wrap a compiled regular expression object to allow deep copy of a regexp.; This is required for the deep copy done in do_scrub. """"""; """"""Augment a Regex object with a flag indicating whether a match should be; added (!is_filter_out) or removed (is_filter_out) from the generated checks. """"""; """"""Add a regular expression option value to a list of regular expressions.; This compiles the expression, wraps it in a Regex and adds it to the option; value list.""""""; """"""Add a filter to a list of filter option values.""""""; """"""Filters are applied to each output line according to the order given. The; first matching filter terminates filter processing for that current line.""""""; # FIXME: in 3.9, we can use argparse.BooleanOptionalAction. At that point,; # we need to rename the flag to just -generate-body-for-unused-prefixes.; # This is the default when regenerating existing tests. The default when; # generating new tests is determined by DEFAULT_VERSION.; # TODO: This should not be handled differently from the other options; # TODO: This should not be handled differently from the other options; # Discard any previous script advertising.; # On Windows we must expand the patterns ourselves.; # If we're generating a new test, set the default version to the latest.; # Skip any blank comment lines in the IR.; # Skip a special double comment line we use as a separator.; # Skip any blank lines in the IR.; # if input_line.strip() == '':; # return False; # And skip any CHECK lines. We're building our",MatchSource.CODE_COMMENT,interpreter/llvm-project/llvm/utils/UpdateTestChecks/common.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/UpdateTestChecks/common.py
Energy Efficiency,adapt,adaptation,"th one list element per file; the first tuple entry being the root file,; the second a list of subdirectories,; each being represented as a list itself with a string per level; e.g.; rootls tutorial/tmva/TMVA.root:Method_BDT/BDT turns into; [('tutorials/tmva/TMVA.root', [['Method_BDT','BDT']])]; vars(args): a dictionary of matched options, e.g.; {'longListing': False,; 'oneColumn': False,; 'treeListing': False,; 'recursiveListing'; False,; 'FILE': ['tutorials/tmva/TMVA.root:Method_BDT/BDT']; }; """"""; """"""; Get the list of tuples of sources, create destination name, destination pathSplit; and the dictionary with options; """"""; # The end of the set of functions to put the arguments in shape; ##########; ##########; # Several functions shared by rootcp, rootmv and rootrm; """"""; Initialize the recursive function 'copyRootObjectRecursive', written to be as unix-like as possible; """"""; # Multiple input and un-existing or non-directory destination; # TARGET_ERROR; # Entire ROOT file or directory in input omitting ""-r"" option; # OMITTING_FILE_ERROR or OMITTING_DIRECTORY_ERROR; # Run copyRootObjectRecursive function with the wish; # to follow the unix copy behaviour; """"""; Delete the object 'pathSplit[-1]' from (rootFile,pathSplit[:-1]); """"""; """"""; Copy objects from a file or directory (sourceFile,sourcePathSplit); to an other file or directory (destFile,destPathSplit); - Has the will to be unix-like; - that's a recursive function; - Python adaptation of a root input/output tutorial :; $ROOTSYS/tutorials/io/copyFiles.C; """"""; # write keys only if the cycle is higher than before; # probably the object was written with kSingleKey; """"""; Remove the object (rootFile,pathSplit); -interactive : prompt before every removal; -recursive : allow directory, and ROOT file, removal; """"""; # End of functions shared by rootcp, rootmv and rootrm; ##########; ##########; # Help strings for ROOT command line tools; # Arguments; # Options; """"""change the compression settings of the; destination file (if n",MatchSource.CODE_COMMENT,main/python/cmdLineUtils.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/main/python/cmdLineUtils.py
Integrability,rout,routine,"riley !!; """"""getTerminalSize(); - get width and height of console; - works on linux,os x,windows,cygwin(windows); originally retrieved from:; http://stackoverflow.com/questions/566746/how-to-get-console-window-width-in-python""""""; # needed for window's python in cygwin's xterm!; # print ""default""; # _get_terminal_size_windows() or _get_terminal_size_tput don't work; # default value; # stdin handle is -10; # stdout handle is -11; # stderr handle is -12; # get terminal width; # src: http://stackoverflow.com/questions/263890/how-do-i-find-the-width-height-of-a-terminal-window; # End of getTerminalSize code; ##; """"""Print list of strings in columns; - blue for directories; - green for trees""""""; # This code is adapted from the pprint_list function here :; # http://stackoverflow.com/questions/25026556/output-list-like-ls; # Thanks hawkjo !!; # Start with max possible number of columns and reduce until it fits; # indentation; # Don't add spaces after the last element of the line or of the list; # No spaces after the last element of the line or of the list; """"""Print informations given by keyList with a rootLs; style chosen with the options""""""; """"""rootls main routine for one file looping over paths in the file. sorts out directories and key, and loops over all paths, then forwards to; (_rootLsPrintLongLs or _rootLsPrintSimpleLs) - split in _rootLsPrint. args:; oneColumn (bool):; longListing (bool):; treeListing (bool):; recursiveListing(bool):; indent (int): how many columns the printout should be indented globally; manySources (bool): if more than one file is printed; fileName (str): the root file name; pathSplitList: a list of subdirectories,; each being represented as a list itself with a string per level; e.g.; [['Method_BDT','BDT']]; Returns:; retcode (int): 0 in case of success, 1 if the file could not be opened; """"""; # keyList lists the TKey objects from pathSplitList; # dirList is 'just the pathSplitList' for what aren't TKeys; # Loop on the directories; """"""rootls main ",MatchSource.CODE_COMMENT,main/python/cmdLineUtils.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/main/python/cmdLineUtils.py
Modifiability,inherit,inheritable,"#!/usr/bin/env @python@; # ROOT command line tools module: cmdLineUtils; # Author: Julien Ripoche; # Mail: julien.ripoche@u-psud.fr; # Date: 20/08/15; """"""Contain utils for ROOT command line tools""""""; ##########; # Stream redirect functions; # The original code of the these functions can be found here :; # http://stackoverflow.com/questions/4675728/redirect-stdout-to-a-file-in-python/22434262#22434262; # Thanks J.F. Sebastian !!; """"""; Look for 'fileno' attribute.; """"""; """"""; Redirect the output from source to destination.; """"""; # copy stdout_fd before it is overwritten; # NOTE: `copied` is inheritable on Windows when duplicating a standard stream; # flush library buffers that dup2 knows nothing about; # $ exec >&destination; # filename; # $ exec > destination; # allow code to be run with the redirected stream; # restore source to its previous value; # NOTE: dup2 makes stdout_fd inheritable unconditionally; # $ exec >&copied; """"""; Redirect the output from sys.stdout to os.devnull.; """"""; """"""; Redirect the output from sys.stderr to os.devnull.; """"""; # The end of streamRedirected functions; ##########; ##########; # Imports; ##; # redirect output (escape characters during ROOT importation...); # Silence Davix warning (see ROOT-7577); # The end of imports; ##########; ##########; # Different functions to get a parser of arguments and options; """"""; Get a commandline parser with the defaults of the commandline utils.; """"""; """"""; Get a commandline parser with the defaults of the commandline utils and a; source file or not.; """"""; """"""; Get a commandline parser with the defaults of the commandline utils and a; list of source files.; """"""; """"""; Get a commandline parser with the defaults of the commandline utils,; a list of source files and a destination file.; """"""; # The end of get parser functions; ##########; ##########; # Several utils; """"""; Change the current directory (ROOT.gDirectory) by the corresponding (rootFile,pathSplit); """"""; """"""; Add a directory named 'pathSplit[-1]' in",MatchSource.CODE_COMMENT,main/python/cmdLineUtils.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/main/python/cmdLineUtils.py
Safety,avoid,avoiding,"t a directory then get the key in a list; """"""; """"""; Sort list of keys by their names ignoring the case; """"""; """"""; Sort list of tuples by their first elements ignoring the case; """"""; """"""; Sort list of directories by their names ignoring the case; """"""; """"""; Return a list of directories and a list of keys corresponding; to the other objects, for rootls and rootprint use; """"""; """"""; Open the ROOT file corresponding to fileName in the corresponding mode,; redirecting the output not to see missing dictionnaries. Returns:; theFile (TFile); """"""; # with stderrRedirected():; """"""; Open a ROOT file (like openROOTFile) with the possibility; to change compression settings; """"""; """"""; Join the pathSplit with '/'; """"""; """"""; Search for double occurence of the same pathSplit and remove them; """"""; """"""; Get the list of pathSplit of objects in the ROOT file; corresponding to fileName that match with the pattern; """"""; # Open ROOT file; # Split pattern avoiding multiple slash problem; # Main loop; # No match; # Same match (remove double occurrences from the list); """"""; Get the list of fileName that match with objPattern; """"""; """"""; Get the list of pathSplit that match with objPattern; """"""; """"""; Get the list of tuple containing both :; - ROOT file name; - list of splited path (in the corresponding file) of objects that matche; Use unix wildcards by default; """"""; # End of utils; ##########; ##########; # Set of functions to put the arguments in shape; """"""; Get arguments corresponding to parser.; """"""; """"""; Create a list of tuples that contain source ROOT file names; and lists of path in these files as well as the original arguments; """"""; """"""; Get the list of tuples and the dictionary with options. returns:; sourceList: a list of tuples with one list element per file; the first tuple entry being the root file,; the second a list of subdirectories,; each being represented as a list itself with a string per level; e.g.; rootls tutorial/tmva/TMVA.root:Method_BDT/BDT turns into; [('tutorials/tmva/TMV",MatchSource.CODE_COMMENT,main/python/cmdLineUtils.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/main/python/cmdLineUtils.py
Testability,log,logic,"an indentation; equal to indent and specifying the end character""""""; """"""Print recursively tree informations""""""; # Width informations; # Print loop; """"""Get time in the proper shape; ex : 174512 for 17h 45m 12s; ex : 094023 for 09h 40m 23s""""""; # here we list the inclusive ranges, therefore we have a -1; """"""Prints a list of `TKey`s and some information. The information of each key is printed with the following pattern:. TKeyClassName {date time pattern} TKeyName;TKeyCycle TKeyTitle {optional: [current/backup cycle]}. An example:. ```; $ rootls -l https://root.cern/files/tutorials/hsimple.root; TProfile Jun 30 23:59 2018 hprof;1 ""Profile of pz versus px""; TH1F Jun 30 23:59 2018 hpx;1 ""This is the px distribution""; TH2F Jun 30 23:59 2018 hpxpy;1 ""py vs px""; TNtuple Jun 30 23:59 2018 ntuple;1 ""Demo ntuple""; ```; """"""; # Early return if the keyList is empty; # Input keyList is a THashList. Convert it to a Python list to make it work; # with zip_longest later; # Mimic the logic used in TDirectoryFile::ls(Option_t *option); # For any key in the list, we need to grab the previous one and the next one.; # To do this, we use the iterator returned by zip_longest. The three input; # lists to zip_longest can be visualized as follows:; #; # a = [""key_1"",""key_2"",""key_3""]; # a_lagright = [None] + a[:-1]; # a_lagleft = a[1:]; # list(zip_longest(a_lagright, a, a_lagleft)); # [(None, 'key_1', 'key_2'), ('key_1', 'key_2', 'key_3'), ('key_2', 'key_3', None)]; #; # So that for any key, we can have a correct reference to the previous and; # following keys of `keyList`. The first key has no previous key and the last; # key has no following key, so the respective elements of the zip_longest; # iterator are `None`.; # If this key is the first one in the list, or if it has a different; # name than the previous one, it means that it's the first object of; # that kind in the list.; # Then we check the following key. If the current key is not; # the last key in the list and if the following key has ",MatchSource.CODE_COMMENT,main/python/cmdLineUtils.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/main/python/cmdLineUtils.py
Deployability,configurat,configuration,"#!/usr/bin/env @python@; # ROOT command line tools: rootdrawtree; # Author: Luca Giommi; # Mail: luca.giommi2@studio.unibo.it; # Date: 15/09/16; '''\; Python script that loops over a chain to create histograms.; There are two ways to do it: one is parsing the name of the configuration file as argument,; that must have a proper syntax as shown in the class documentation of TSimpleAnalysis. Example:; user@users-desktop:~$ rootdrawtree input_file.txt # input_file.txt is the configuration file. The other way is to pass as arguments the name of the output file, the name of the .root input; files, the expressions (that will be shown in the histograms) and the name of the tree (that; is optional if there is only one tree inside the first .root input file). Examples:; user@users-desktop:~$ rootdrawtree --output output.root --input hsimple.root --tree ntuple --histo 'hpxpy=px:py if px>2'; user@users-desktop:~$ rootdrawtree --output output.root --input hsimple.root hsimple2.root --histo 'hpx=px' 'hpxpy=px:py if px>2'. '''",MatchSource.CODE_COMMENT,main/python/rootdrawtree.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/main/python/rootdrawtree.py
Modifiability,config,configuration,"#!/usr/bin/env @python@; # ROOT command line tools: rootdrawtree; # Author: Luca Giommi; # Mail: luca.giommi2@studio.unibo.it; # Date: 15/09/16; '''\; Python script that loops over a chain to create histograms.; There are two ways to do it: one is parsing the name of the configuration file as argument,; that must have a proper syntax as shown in the class documentation of TSimpleAnalysis. Example:; user@users-desktop:~$ rootdrawtree input_file.txt # input_file.txt is the configuration file. The other way is to pass as arguments the name of the output file, the name of the .root input; files, the expressions (that will be shown in the histograms) and the name of the tree (that; is optional if there is only one tree inside the first .root input file). Examples:; user@users-desktop:~$ rootdrawtree --output output.root --input hsimple.root --tree ntuple --histo 'hpxpy=px:py if px>2'; user@users-desktop:~$ rootdrawtree --output output.root --input hsimple.root hsimple2.root --histo 'hpx=px' 'hpxpy=px:py if px>2'. '''",MatchSource.CODE_COMMENT,main/python/rootdrawtree.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/main/python/rootdrawtree.py
Availability,avail,available,"#!/usr/bin/env @python@; # ROOT command line tools: rooteventselector; # Author: Julien Ripoche; # Mail: julien.ripoche@u-psud.fr; # Date: 20/08/15; # Additions; # Author: Lawrence Lee; # Mail: lawrence.lee.jr@cern.ch; # Date: 1/4/16; """"""Command line to copy subsets of trees from source ROOT files to new trees on a destination ROOT file""""""; # Help strings; """"""Examples:; - rooteventselector source.root:tree dest.root; Copy the tree 'tree' from 'source.root' to 'dest.root'. - rooteventselector -f 101 source.root:tree dest.root; Copy a subset of the tree 'tree' from 'source.root' to 'dest.root'. The new tree contains events from the old tree except the first hundred. - rooteventselector -l 100 source.root:tree dest.root; Copy a subset of the tree 'tree' from 'source.root' to 'dest.root'. The new tree contains the first hundred events from the old tree. - rooteventselector --recreate source.root:tree dest.root; Recreate the destination file 'dest.root' and copy the tree 'tree' from 'source.root' to 'dest.root'. - rooteventselector -c 1 source.root:tree dest.root; Change the compression factor of the destination file 'dest.root' and copy the tree 'tree' from 'source.root' to 'dest.root'. For more information about compression settings of ROOT file, please look at the reference guide available on the ROOT site. - rooteventselector -s ""(branch1Value > 100)&&( branch2Value )"" source.root:tree dest.root; Copy the tree 'tree' from 'source.root' to 'dest.root' and apply a selection to the output tree. - rooteventselector -e ""muon_*"" source.root:tree dest.root; Copy the tree 'tree' from 'source.root' to 'dest.root' and remove branches matching ""muon_*"". - rooteventselector -e ""*"" -i ""muon_*"" source.root:tree dest.root; Copy the tree 'tree' from 'source.root' to 'dest.root' and only write branches matching ""muon_*""; """"""; # Collect arguments with the module argparse; # Put arguments in shape; # Process rootEventselector",MatchSource.CODE_COMMENT,main/python/rooteventselector.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/main/python/rooteventselector.py
Usability,guid,guide,"#!/usr/bin/env @python@; # ROOT command line tools: rooteventselector; # Author: Julien Ripoche; # Mail: julien.ripoche@u-psud.fr; # Date: 20/08/15; # Additions; # Author: Lawrence Lee; # Mail: lawrence.lee.jr@cern.ch; # Date: 1/4/16; """"""Command line to copy subsets of trees from source ROOT files to new trees on a destination ROOT file""""""; # Help strings; """"""Examples:; - rooteventselector source.root:tree dest.root; Copy the tree 'tree' from 'source.root' to 'dest.root'. - rooteventselector -f 101 source.root:tree dest.root; Copy a subset of the tree 'tree' from 'source.root' to 'dest.root'. The new tree contains events from the old tree except the first hundred. - rooteventselector -l 100 source.root:tree dest.root; Copy a subset of the tree 'tree' from 'source.root' to 'dest.root'. The new tree contains the first hundred events from the old tree. - rooteventselector --recreate source.root:tree dest.root; Recreate the destination file 'dest.root' and copy the tree 'tree' from 'source.root' to 'dest.root'. - rooteventselector -c 1 source.root:tree dest.root; Change the compression factor of the destination file 'dest.root' and copy the tree 'tree' from 'source.root' to 'dest.root'. For more information about compression settings of ROOT file, please look at the reference guide available on the ROOT site. - rooteventselector -s ""(branch1Value > 100)&&( branch2Value )"" source.root:tree dest.root; Copy the tree 'tree' from 'source.root' to 'dest.root' and apply a selection to the output tree. - rooteventselector -e ""muon_*"" source.root:tree dest.root; Copy the tree 'tree' from 'source.root' to 'dest.root' and remove branches matching ""muon_*"". - rooteventselector -e ""*"" -i ""muon_*"" source.root:tree dest.root; Copy the tree 'tree' from 'source.root' to 'dest.root' and only write branches matching ""muon_*""; """"""; # Collect arguments with the module argparse; # Put arguments in shape; # Process rootEventselector",MatchSource.CODE_COMMENT,main/python/rooteventselector.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/main/python/rooteventselector.py
Availability,error,error,"#!/usr/bin/env @python@; # ROOT command line tools: rootmkdir; # Author: Julien Ripoche; # Mail: julien.ripoche@u-psud.fr; # Date: 20/08/15; """"""Command line to add directories in ROOT files""""""; # Help strings; """"""Examples:; - rootmkdir example.root:dir; Add the directory 'dir' to the ROOT file 'example.root'. - rootmkdir example.root:dir1/dir2; Add the directory 'dir2' in 'dir1' which is into the ROOT file 'example.root'. - rootmkdir -p example.root:dir1/dir2/dir3; Make parent directories of 'dir3' as needed, no error if existing. - rootmkdir example.root; Create an empty ROOT file named 'example.root'; """"""; # Collect arguments with the module argparse; # Put arguments in shape; # Process rootMkdir",MatchSource.CODE_COMMENT,main/python/rootmkdir.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/main/python/rootmkdir.py
Availability,avail,available,"#!/usr/bin/env @python@; # ROOT command line tools: rootslimtree; # Author: Lawrence Lee via Julien Ripoche's rooteventselector; # Mail: lawrence.lee.jr@cern.ch; # Date: 4/4/16; """"""Command line to copy trees with subset of branches from source ROOT files to new trees on a destination ROOT file""""""; # Help strings; """"""Examples:; - rootslimtree source.root:tree dest.root; Copy the tree 'tree' from 'source.root' to 'dest.root'. - rootslimtree --recreate source.root:tree dest.root; Recreate the destination file 'dest.root' and copy the tree 'tree' from 'source.root' to 'dest.root'. - rootslimtree -c 1 source.root:tree dest.root; Change the compression factor of the destination file 'dest.root' and copy the tree 'tree' from 'source.root' to 'dest.root'. For more information about compression settings of ROOT file, please look at the reference guide available on the ROOT site. - rootslimtree -e ""muon_*"" source.root:tree dest.root; Copy the tree 'tree' from 'source.root' to 'dest.root' and remove branches matching ""muon_*"". - rootslimtree -e ""*"" -i ""muon_*"" source.root:tree dest.root; Copy the tree 'tree' from 'source.root' to 'dest.root' and only write branches matching ""muon_*""; """"""; # Collect arguments with the module argparse; # Put arguments in shape; # Process rootEventselector in simplified slimtree mode",MatchSource.CODE_COMMENT,main/python/rootslimtree.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/main/python/rootslimtree.py
Usability,guid,guide,"#!/usr/bin/env @python@; # ROOT command line tools: rootslimtree; # Author: Lawrence Lee via Julien Ripoche's rooteventselector; # Mail: lawrence.lee.jr@cern.ch; # Date: 4/4/16; """"""Command line to copy trees with subset of branches from source ROOT files to new trees on a destination ROOT file""""""; # Help strings; """"""Examples:; - rootslimtree source.root:tree dest.root; Copy the tree 'tree' from 'source.root' to 'dest.root'. - rootslimtree --recreate source.root:tree dest.root; Recreate the destination file 'dest.root' and copy the tree 'tree' from 'source.root' to 'dest.root'. - rootslimtree -c 1 source.root:tree dest.root; Change the compression factor of the destination file 'dest.root' and copy the tree 'tree' from 'source.root' to 'dest.root'. For more information about compression settings of ROOT file, please look at the reference guide available on the ROOT site. - rootslimtree -e ""muon_*"" source.root:tree dest.root; Copy the tree 'tree' from 'source.root' to 'dest.root' and remove branches matching ""muon_*"". - rootslimtree -e ""*"" -i ""muon_*"" source.root:tree dest.root; Copy the tree 'tree' from 'source.root' to 'dest.root' and only write branches matching ""muon_*""; """"""; # Collect arguments with the module argparse; # Put arguments in shape; # Process rootEventselector in simplified slimtree mode",MatchSource.CODE_COMMENT,main/python/rootslimtree.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/main/python/rootslimtree.py
Deployability,install,installed,"# Pretty printers for gdb; # \author: Stephan Hageboeck, CERN; # These pretty printers will make ROOT objects more readable when printed in gdb.; # If the pretty-printed output is not sufficient, one can always use ""print /r <object>""; # for raw printing.; #; # When a debug build is used, they will be installed next to the ROOT libraries.; # gdb will load them automatically if the auto-load-safe-path is set to ROOT's library directory.; # For this, one has to add `add-auto-load-safe-path <ROOT lib dir>` to .gdbinit; #; # If loaded successfully, typing `info pretty-printer` at the gdb prompt should list the; # printers registered at the end of this file.",MatchSource.CODE_COMMENT,misc/gdbPrinters/libCore.so-gdb.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/misc/gdbPrinters/libCore.so-gdb.py
Performance,load,load,"# Pretty printers for gdb; # \author: Stephan Hageboeck, CERN; # These pretty printers will make ROOT objects more readable when printed in gdb.; # If the pretty-printed output is not sufficient, one can always use ""print /r <object>""; # for raw printing.; #; # When a debug build is used, they will be installed next to the ROOT libraries.; # gdb will load them automatically if the auto-load-safe-path is set to ROOT's library directory.; # For this, one has to add `add-auto-load-safe-path <ROOT lib dir>` to .gdbinit; #; # If loaded successfully, typing `info pretty-printer` at the gdb prompt should list the; # printers registered at the end of this file.",MatchSource.CODE_COMMENT,misc/gdbPrinters/libCore.so-gdb.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/misc/gdbPrinters/libCore.so-gdb.py
Safety,safe,safe-path,"# Pretty printers for gdb; # \author: Stephan Hageboeck, CERN; # These pretty printers will make ROOT objects more readable when printed in gdb.; # If the pretty-printed output is not sufficient, one can always use ""print /r <object>""; # for raw printing.; #; # When a debug build is used, they will be installed next to the ROOT libraries.; # gdb will load them automatically if the auto-load-safe-path is set to ROOT's library directory.; # For this, one has to add `add-auto-load-safe-path <ROOT lib dir>` to .gdbinit; #; # If loaded successfully, typing `info pretty-printer` at the gdb prompt should list the; # printers registered at the end of this file.",MatchSource.CODE_COMMENT,misc/gdbPrinters/libCore.so-gdb.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/misc/gdbPrinters/libCore.so-gdb.py
Deployability,install,installed,"# Pretty printers for gdb; # \author: Stephan Hageboeck, CERN; # These pretty printers will make ROOT objects more readable when printed in gdb.; # If the pretty-printed output is not sufficient, one can always use ""print /r <object>""; # for raw printing.; #; # When a debug build is used, they will be installed next to the ROOT libraries.; # gdb will load them automatically if the auto-load-safe-path is set to ROOT's library directory.; # For this, one has to add `add-auto-load-safe-path <ROOT lib dir>` to .gdbinit; #; # If loaded successfully, typing `info pretty-printer` at the gdb prompt should list the; # printers registered at the end of this file.; # print(""<exception "" + str(sys.exc_info()[0]) + "">,"")",MatchSource.CODE_COMMENT,misc/gdbPrinters/libRooFitCore.so-gdb.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/misc/gdbPrinters/libRooFitCore.so-gdb.py
Performance,load,load,"# Pretty printers for gdb; # \author: Stephan Hageboeck, CERN; # These pretty printers will make ROOT objects more readable when printed in gdb.; # If the pretty-printed output is not sufficient, one can always use ""print /r <object>""; # for raw printing.; #; # When a debug build is used, they will be installed next to the ROOT libraries.; # gdb will load them automatically if the auto-load-safe-path is set to ROOT's library directory.; # For this, one has to add `add-auto-load-safe-path <ROOT lib dir>` to .gdbinit; #; # If loaded successfully, typing `info pretty-printer` at the gdb prompt should list the; # printers registered at the end of this file.; # print(""<exception "" + str(sys.exc_info()[0]) + "">,"")",MatchSource.CODE_COMMENT,misc/gdbPrinters/libRooFitCore.so-gdb.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/misc/gdbPrinters/libRooFitCore.so-gdb.py
Safety,safe,safe-path,"# Pretty printers for gdb; # \author: Stephan Hageboeck, CERN; # These pretty printers will make ROOT objects more readable when printed in gdb.; # If the pretty-printed output is not sufficient, one can always use ""print /r <object>""; # for raw printing.; #; # When a debug build is used, they will be installed next to the ROOT libraries.; # gdb will load them automatically if the auto-load-safe-path is set to ROOT's library directory.; # For this, one has to add `add-auto-load-safe-path <ROOT lib dir>` to .gdbinit; #; # If loaded successfully, typing `info pretty-printer` at the gdb prompt should list the; # printers registered at the end of this file.; # print(""<exception "" + str(sys.exc_info()[0]) + "">,"")",MatchSource.CODE_COMMENT,misc/gdbPrinters/libRooFitCore.so-gdb.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/misc/gdbPrinters/libRooFitCore.so-gdb.py
Performance,optimiz,optimizer,"#Define model; # Defining the model; #Construct loss function and optimizer; #Constructing random test dataset; #Training the model; #Saving the trained model; #Define model; #Construct loss function and optimizer; #Constructing random test dataset; #Training the model; #Saving the trained model; # Defining the model; #Construct loss function and optimizer; #Constructing random test dataset; #Training the model; #Saving the trained model",MatchSource.CODE_COMMENT,tmva/pymva/test/generatePyTorchModels.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/pymva/test/generatePyTorchModels.py
Testability,test,test,"#Define model; # Defining the model; #Construct loss function and optimizer; #Constructing random test dataset; #Training the model; #Saving the trained model; #Define model; #Construct loss function and optimizer; #Constructing random test dataset; #Training the model; #Saving the trained model; # Defining the model; #Construct loss function and optimizer; #Constructing random test dataset; #Training the model; #Saving the trained model",MatchSource.CODE_COMMENT,tmva/pymva/test/generatePyTorchModels.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/pymva/test/generatePyTorchModels.py
Testability,test,test,"#!/usr/bin/python3; ### generate COnv2d model using Pytorch; # output is 4x4 with optionally using group convolution; #output is same 4x4; #print(arguments); # parser.add_argument('--oneD',action='store_true', default=False, help='For 1D convolution'); #args.params = (4,2,4,1,4); # use_1d = args.oneD; #sample = torch.zeros([2,1,5,5]); # concatenate tensors; #concatenate tensors ; # evaluate model in test mode; # for i in range(0,outSize):; # print(float(yvec[i]))",MatchSource.CODE_COMMENT,tmva/sofie/test/Conv1dModelGenerator.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/sofie/test/Conv1dModelGenerator.py
Testability,test,test,"#!/usr/bin/python3; ### generate COnv2d model using Pytorch; # output is 4x4 with optionally using group convolution; #output is same 4x4; #use stride last layer ; #print(x); #print(arguments); #args.params = (4,2,4,1,4); #sample = torch.zeros([2,1,5,5]); # concatenate tensors; #concatenate tensors ; # evaluate model in test mode; # for i in range(0,outSize):; # print(float(yvec[i]))",MatchSource.CODE_COMMENT,tmva/sofie/test/Conv2dModelGenerator.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/sofie/test/Conv2dModelGenerator.py
Testability,test,test,"#!/usr/bin/python3; ### generate COnv2d model using Pytorch; # output is 4x4 with optionally using group convolution; #output is same 4x4; #use stride last layer ; #print(x); #print(arguments); #args.params = (4,2,4,1,4); # ngroups = args.params[3]; #sample = torch.zeros([2,1,5,5]); # concatenate tensors; #concatenate tensors ; # evaluate model in test mode; # for i in range(0,outSize):; # print(float(yvec[i]))",MatchSource.CODE_COMMENT,tmva/sofie/test/Conv3dModelGenerator.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/sofie/test/Conv3dModelGenerator.py
Testability,test,test,"#!/usr/bin/python3; ### generate COnv2d model using Pytorch; #if (self.use_bn): self.bn1 = nn.BatchNorm2d(4); #if (self.use_maxpool): self.pool1 = nn.MaxPool2d(2); #if (self.use_avgpool): self.pool1 = nn.AvgPool2d(2); # output is 4x4 with optionally using group convolution; #output is same 4x4; #use stride last layer; # if (self.use_maxpool or self.use_avgpool):; # x = self.pool1(x); #print(x); #print(arguments); #args.params = (4,2,4,1,4); #sample = torch.zeros([2,1,5,5]); #fill input tensor with an increasing inputs; #fill with all ones; #xa = torch.ones([1, 1, d, d]) * (ib+1); # concatenate tensors; #concatenate tensors; # evaluate model in test mode; # for i in range(0,outSize):; # print(float(yvec[i]))",MatchSource.CODE_COMMENT,tmva/sofie/test/ConvTrans2dModelGenerator.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/sofie/test/ConvTrans2dModelGenerator.py
Testability,test,test,"#!/usr/bin/python3; ### generate COnv2d model using Pytorch; # FC layer ; #Initializing hidden state for first input using method defined below; #hidden = self.init_hidden(batch_size); # Passing in the input and hidden state into the model and obtaining outputs; # Reshaping the outputs such that it can be fit into the fully connected layer; #out = lstm_out.view(self.hidden_dim,-1); # This method generates the first hidden state of zeros which we'll use in the forward pass; #print(arguments); # parser.add_argument('--avgpool', action='store_true', default=False,; # help='For using average pool layer'); ##args.params = (1,3,10,4,1); #concatenate tensors ; # evaluate model in test mode",MatchSource.CODE_COMMENT,tmva/sofie/test/RecurrentModelGenerator.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/sofie/test/RecurrentModelGenerator.py
Integrability,interface,interface,"# XGBoost has to be imported before ROOT to avoid crashes because of clashing; # std::regexp symbols that are exported by cppyy.; # See also: https://github.com/wlav/cppyy/issues/227; """"""; Compare response of XGB classifier and TMVA tree inference system.; """"""; """"""; Compare response of XGB regressor and TMVA tree inference system.; """"""; """"""; Compare response of XGB multiclass and TMVA tree inference system.; """"""; """"""; Test RBDT interface; """"""; """"""; Test model trained with binary XGBClassifier.; """"""; """"""; Test model trained with multiclass XGBClassifier.; """"""; """"""; Test model trained with XGBRegressor.; """"""",MatchSource.CODE_COMMENT,tmva/tmva/test/rbdt_xgboost.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/rbdt_xgboost.py
Safety,avoid,avoid,"# XGBoost has to be imported before ROOT to avoid crashes because of clashing; # std::regexp symbols that are exported by cppyy.; # See also: https://github.com/wlav/cppyy/issues/227; """"""; Compare response of XGB classifier and TMVA tree inference system.; """"""; """"""; Compare response of XGB regressor and TMVA tree inference system.; """"""; """"""; Compare response of XGB multiclass and TMVA tree inference system.; """"""; """"""; Test RBDT interface; """"""; """"""; Test model trained with binary XGBClassifier.; """"""; """"""; Test model trained with multiclass XGBClassifier.; """"""; """"""; Test model trained with XGBRegressor.; """"""",MatchSource.CODE_COMMENT,tmva/tmva/test/rbdt_xgboost.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/rbdt_xgboost.py
Testability,test,test,"# Necessary because in the new Cppyy if we want to instantiate; # a templated method with a type written as a string (e.g. 'float'); # we need to pass it in square brackets, otherwise it can be; # (mis)interpreted as string parameter and the method itself is; # called with 'float' as a parameter.; # For example the Take() method mentioned multiple times in this test; # has to be called e.g. with:; # Take['float'](); # instead of:; # Take('float')(); ''' {; const char* treeName = ""t"";; const char* fileName = ""fileName.root"";; TFile f(fileName, ""RECREATE"");; TTree t(treeName, treeName);; float arr[4];; t.Branch(""arr"", arr, ""arr[4]/F"");; for (auto i : ROOT::TSeqU(4)) {; for (auto j : ROOT::TSeqU(4)) {; arr[j] = i + j;; }; t.Fill();; }; t.Write();; }'''; # Workaround until we do not understand why we cannot directly use the __getitem__ operator; # commented out until we do not understand iteration; #l = rdf.Take(ColType_t+"", std::list(""+ColType_t+"")"")(""arr""); # Workaround for iteration on osx; # Workaround for iteration on osx; # Workaround for iteration on osx",MatchSource.CODE_COMMENT,tree/dataframe/test/dataframe_cache.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/dataframe/test/dataframe_cache.py
Availability,error,error,"# Necessary because in the new Cppyy if we want to instantiate; # a templated method with a type written as a string (e.g. 'float'); # we need to pass it in square brackets, otherwise it can be; # (mis)interpreted as string parameter and the method itself is; # called with 'float' as a parameter.; # For example the Take() method mentioned multiple times in this test; # has to be called e.g. with:; # Take['float'](); # instead of:; # Take('float')(); # reuse the code from the C++ unit tests to create some files; """""" {; auto dfWriter0 = ROOT::RDataFrame(5).Define(""z"", [](ULong64_t e) { return e + 100; }, {""rdfentry_""});; dfWriter0.Range(0, 2).Snapshot<ULong64_t>(""subTree"", ""PYspecTestFile2.root"", {""z""});; dfWriter0.Range(2, 4).Snapshot<ULong64_t>(""subTree"", ""PYspecTestFile3.root"", {""z""});; dfWriter0.Range(4, 5).Snapshot<ULong64_t>(""subTree"", ""PYspecTestFile4.root"", {""z""});; dfWriter0.Range(0, 2).Snapshot<ULong64_t>(""subTree1"", ""PYspecTestFile5.root"", {""z""});; dfWriter0.Range(2, 4).Snapshot<ULong64_t>(""subTree2"", ""PYspecTestFile6.root"", {""z""});; dfWriter0.Snapshot<ULong64_t>(""anotherTree"", ""PYspecTestFile7.root"", {""z""});; }""""""; # TODO: this emits a warning: Maybe you need to load the corresponding shared library?; # cling JIT session error: Failed to materialize symbols: ... emplace_back; # Despite the warning, the result is correct; # spec.WithGlobalFriends([(""subTree1"", ""PYspecTestFile5.root""),; # (""subTree2"", ""PYspecTestFile6.root""),; # (""subTree"", ""PYspecTestFile4.root"")], ""friendChainN""); # fr3P = rdf.Take(""ULong64_t"")(""friendChainN.z""); # fr3P = rdf.Take[""ULong64_t""](""friendChainN.z""); # fr3 = fr3P.GetValue(); # self.assertEqual(fr3[j], expectedRess[i][j])",MatchSource.CODE_COMMENT,tree/dataframe/test/dataframe_datasetspec.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/dataframe/test/dataframe_datasetspec.py
Performance,load,load,"# Necessary because in the new Cppyy if we want to instantiate; # a templated method with a type written as a string (e.g. 'float'); # we need to pass it in square brackets, otherwise it can be; # (mis)interpreted as string parameter and the method itself is; # called with 'float' as a parameter.; # For example the Take() method mentioned multiple times in this test; # has to be called e.g. with:; # Take['float'](); # instead of:; # Take('float')(); # reuse the code from the C++ unit tests to create some files; """""" {; auto dfWriter0 = ROOT::RDataFrame(5).Define(""z"", [](ULong64_t e) { return e + 100; }, {""rdfentry_""});; dfWriter0.Range(0, 2).Snapshot<ULong64_t>(""subTree"", ""PYspecTestFile2.root"", {""z""});; dfWriter0.Range(2, 4).Snapshot<ULong64_t>(""subTree"", ""PYspecTestFile3.root"", {""z""});; dfWriter0.Range(4, 5).Snapshot<ULong64_t>(""subTree"", ""PYspecTestFile4.root"", {""z""});; dfWriter0.Range(0, 2).Snapshot<ULong64_t>(""subTree1"", ""PYspecTestFile5.root"", {""z""});; dfWriter0.Range(2, 4).Snapshot<ULong64_t>(""subTree2"", ""PYspecTestFile6.root"", {""z""});; dfWriter0.Snapshot<ULong64_t>(""anotherTree"", ""PYspecTestFile7.root"", {""z""});; }""""""; # TODO: this emits a warning: Maybe you need to load the corresponding shared library?; # cling JIT session error: Failed to materialize symbols: ... emplace_back; # Despite the warning, the result is correct; # spec.WithGlobalFriends([(""subTree1"", ""PYspecTestFile5.root""),; # (""subTree2"", ""PYspecTestFile6.root""),; # (""subTree"", ""PYspecTestFile4.root"")], ""friendChainN""); # fr3P = rdf.Take(""ULong64_t"")(""friendChainN.z""); # fr3P = rdf.Take[""ULong64_t""](""friendChainN.z""); # fr3 = fr3P.GetValue(); # self.assertEqual(fr3[j], expectedRess[i][j])",MatchSource.CODE_COMMENT,tree/dataframe/test/dataframe_datasetspec.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/dataframe/test/dataframe_datasetspec.py
Testability,test,test,"# Necessary because in the new Cppyy if we want to instantiate; # a templated method with a type written as a string (e.g. 'float'); # we need to pass it in square brackets, otherwise it can be; # (mis)interpreted as string parameter and the method itself is; # called with 'float' as a parameter.; # For example the Take() method mentioned multiple times in this test; # has to be called e.g. with:; # Take['float'](); # instead of:; # Take('float')(); # reuse the code from the C++ unit tests to create some files; """""" {; auto dfWriter0 = ROOT::RDataFrame(5).Define(""z"", [](ULong64_t e) { return e + 100; }, {""rdfentry_""});; dfWriter0.Range(0, 2).Snapshot<ULong64_t>(""subTree"", ""PYspecTestFile2.root"", {""z""});; dfWriter0.Range(2, 4).Snapshot<ULong64_t>(""subTree"", ""PYspecTestFile3.root"", {""z""});; dfWriter0.Range(4, 5).Snapshot<ULong64_t>(""subTree"", ""PYspecTestFile4.root"", {""z""});; dfWriter0.Range(0, 2).Snapshot<ULong64_t>(""subTree1"", ""PYspecTestFile5.root"", {""z""});; dfWriter0.Range(2, 4).Snapshot<ULong64_t>(""subTree2"", ""PYspecTestFile6.root"", {""z""});; dfWriter0.Snapshot<ULong64_t>(""anotherTree"", ""PYspecTestFile7.root"", {""z""});; }""""""; # TODO: this emits a warning: Maybe you need to load the corresponding shared library?; # cling JIT session error: Failed to materialize symbols: ... emplace_back; # Despite the warning, the result is correct; # spec.WithGlobalFriends([(""subTree1"", ""PYspecTestFile5.root""),; # (""subTree2"", ""PYspecTestFile6.root""),; # (""subTree"", ""PYspecTestFile4.root"")], ""friendChainN""); # fr3P = rdf.Take(""ULong64_t"")(""friendChainN.z""); # fr3P = rdf.Take[""ULong64_t""](""friendChainN.z""); # fr3 = fr3P.GetValue(); # self.assertEqual(fr3[j], expectedRess[i][j])",MatchSource.CODE_COMMENT,tree/dataframe/test/dataframe_datasetspec.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/dataframe/test/dataframe_datasetspec.py
Testability,test,tests,"""""""Various tests for the RMergeableValue family of classes.""""""; """"""Merging results of different operations raises `TypeError`""""""; """"""Merge two averages, pickle and merge again.""""""; # float to help in the division; # float to help in the division; """"""Merge two histograms, pickle and merge again.""""""; """"""Merge 3 varied histograms, pickle and evaluate values.""""""",MatchSource.CODE_COMMENT,tree/dataframe/test/dataframe_merge_results.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/dataframe/test/dataframe_merge_results.py
Performance,load,load,"# load all libs and autoparse; # this is to check if the define is triggered!",MatchSource.CODE_COMMENT,tree/dataframe/test/dataframe_misc.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/dataframe/test/dataframe_misc.py
Testability,test,tests,"""""""Various tests for the RNTupleModel class""""""; """"""A model can be created.""""""; """"""A bare model can be created.""""""; """"""Can estimate the memory usage of a model.""""""",MatchSource.CODE_COMMENT,tree/ntuple/v7/test/ntuple_model.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/test/ntuple_model.py
Testability,test,tests,"# this is meant to be used only to run tutorials as tests",MatchSource.CODE_COMMENT,tutorials/.enableImplicitMTWrapper.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/.enableImplicitMTWrapper.py
Deployability,pipeline,pipelines,"return their result ; # wrapped in a RResultPtr or in a RDataFrame.; # But first of all, let us we define now our cut-flow with two strings.; # Filters can be expressed as strings. The content must be C++ code. The; # name of the variables must be the name of the branches. The code is; # just-in-time compiled.; # `Count` action; # The `Count` allows to retrieve the number of the entries that passed the; # filters. Here we show how the automatic selection of the column kicks; # in in case the user specifies none.; # `Min`, `Max` and `Mean` actions; # These actions allow to retrieve statistical information about the entries; # passing the cuts, if any.; # `Histo1D` action; # The `Histo1D` action allows to fill an histogram. It returns a TH1F filled; # with values of the column that passed the filters. For the most common; # types, the type of the values stored in the column is automatically; # guessed.; # Express your chain of operations with clarity!; # We are discussing an example here but it is not hard to imagine much more; # complex pipelines of actions acting on data. Those might require code; # which is well organised, for example allowing to conditionally add filters; # or again to clearly separate filters and actions without the need of; # writing the entire pipeline on one line. This can be easily achieved.; # We'll show this re-working the `Count` example:; # Now we want to count:; # Calculating quantities starting from existing columns; # Often, operations need to be carried out on quantities calculated starting; # from the ones present in the columns. We'll create in this example a third; # column, the values of which are the sum of the *b1* and *b2* ones, entry by; # entry. The way in which the new quantity is defined is via a callable.; # It is important to note two aspects at this point:; # - The value is created on the fly only if the entry passed the existing; # filters.; # - The newly created column behaves as the one present on the file on disk.; # ",MatchSource.CODE_COMMENT,tutorials/dataframe/df001_introduction.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df001_introduction.py
Integrability,depend,depending,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -nodraw; ## Basic usage of RDataFrame from python.; ##; ## This tutorial illustrates the basic features of the RDataFrame class,; ## a utility which allows to interact with data stored in TTrees following; ## a functional-chain like approach.; ##; ## \macro_code; ## \macro_output; ##; ## \date May 2017; ## \author Danilo Piparo (CERN); """"""A simple helper function to fill a test tree: this makes the example stand-alone.""""""; # We prepare an input tree to run on; # We read the tree from the file and create a RDataFrame, a class that; # allows us to interact with the data contained in the tree.; # Operations on the dataframe; # We now review some *actions* which can be performed on the data frame.; # Actions can be divided into instant actions (e. g. Foreach()) and lazy; # actions (e. g. Count()), depending on whether they trigger the event ; # loop immediately or only when one of the results is accessed for the ; # first time. Actions that return ""something"" either return their result ; # wrapped in a RResultPtr or in a RDataFrame.; # But first of all, let us we define now our cut-flow with two strings.; # Filters can be expressed as strings. The content must be C++ code. The; # name of the variables must be the name of the branches. The code is; # just-in-time compiled.; # `Count` action; # The `Count` allows to retrieve the number of the entries that passed the; # filters. Here we show how the automatic selection of the column kicks; # in in case the user specifies none.; # `Min`, `Max` and `Mean` actions; # These actions allow to retrieve statistical information about the entries; # passing the cuts, if any.; # `Histo1D` action; # The `Histo1D` action allows to fill an histogram. It returns a TH1F filled; # with values of the column that passed the filters. For the most common; # types, the type of the values stored in the column is automatically; # guessed.; # Express your chain of operations with clarity!; # We",MatchSource.CODE_COMMENT,tutorials/dataframe/df001_introduction.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df001_introduction.py
Modifiability,variab,variables,"nal-chain like approach.; ##; ## \macro_code; ## \macro_output; ##; ## \date May 2017; ## \author Danilo Piparo (CERN); """"""A simple helper function to fill a test tree: this makes the example stand-alone.""""""; # We prepare an input tree to run on; # We read the tree from the file and create a RDataFrame, a class that; # allows us to interact with the data contained in the tree.; # Operations on the dataframe; # We now review some *actions* which can be performed on the data frame.; # Actions can be divided into instant actions (e. g. Foreach()) and lazy; # actions (e. g. Count()), depending on whether they trigger the event ; # loop immediately or only when one of the results is accessed for the ; # first time. Actions that return ""something"" either return their result ; # wrapped in a RResultPtr or in a RDataFrame.; # But first of all, let us we define now our cut-flow with two strings.; # Filters can be expressed as strings. The content must be C++ code. The; # name of the variables must be the name of the branches. The code is; # just-in-time compiled.; # `Count` action; # The `Count` allows to retrieve the number of the entries that passed the; # filters. Here we show how the automatic selection of the column kicks; # in in case the user specifies none.; # `Min`, `Max` and `Mean` actions; # These actions allow to retrieve statistical information about the entries; # passing the cuts, if any.; # `Histo1D` action; # The `Histo1D` action allows to fill an histogram. It returns a TH1F filled; # with values of the column that passed the filters. For the most common; # types, the type of the values stored in the column is automatically; # guessed.; # Express your chain of operations with clarity!; # We are discussing an example here but it is not hard to imagine much more; # complex pipelines of actions acting on data. Those might require code; # which is well organised, for example allowing to conditionally add filters; # or again to clearly separate filters and action",MatchSource.CODE_COMMENT,tutorials/dataframe/df001_introduction.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df001_introduction.py
Performance,perform,performed,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -nodraw; ## Basic usage of RDataFrame from python.; ##; ## This tutorial illustrates the basic features of the RDataFrame class,; ## a utility which allows to interact with data stored in TTrees following; ## a functional-chain like approach.; ##; ## \macro_code; ## \macro_output; ##; ## \date May 2017; ## \author Danilo Piparo (CERN); """"""A simple helper function to fill a test tree: this makes the example stand-alone.""""""; # We prepare an input tree to run on; # We read the tree from the file and create a RDataFrame, a class that; # allows us to interact with the data contained in the tree.; # Operations on the dataframe; # We now review some *actions* which can be performed on the data frame.; # Actions can be divided into instant actions (e. g. Foreach()) and lazy; # actions (e. g. Count()), depending on whether they trigger the event ; # loop immediately or only when one of the results is accessed for the ; # first time. Actions that return ""something"" either return their result ; # wrapped in a RResultPtr or in a RDataFrame.; # But first of all, let us we define now our cut-flow with two strings.; # Filters can be expressed as strings. The content must be C++ code. The; # name of the variables must be the name of the branches. The code is; # just-in-time compiled.; # `Count` action; # The `Count` allows to retrieve the number of the entries that passed the; # filters. Here we show how the automatic selection of the column kicks; # in in case the user specifies none.; # `Min`, `Max` and `Mean` actions; # These actions allow to retrieve statistical information about the entries; # passing the cuts, if any.; # `Histo1D` action; # The `Histo1D` action allows to fill an histogram. It returns a TH1F filled; # with values of the column that passed the filters. For the most common; # types, the type of the values stored in the column is automatically; # guessed.; # Express your chain of operations with clarity!; # We",MatchSource.CODE_COMMENT,tutorials/dataframe/df001_introduction.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df001_introduction.py
Security,access,accessed,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -nodraw; ## Basic usage of RDataFrame from python.; ##; ## This tutorial illustrates the basic features of the RDataFrame class,; ## a utility which allows to interact with data stored in TTrees following; ## a functional-chain like approach.; ##; ## \macro_code; ## \macro_output; ##; ## \date May 2017; ## \author Danilo Piparo (CERN); """"""A simple helper function to fill a test tree: this makes the example stand-alone.""""""; # We prepare an input tree to run on; # We read the tree from the file and create a RDataFrame, a class that; # allows us to interact with the data contained in the tree.; # Operations on the dataframe; # We now review some *actions* which can be performed on the data frame.; # Actions can be divided into instant actions (e. g. Foreach()) and lazy; # actions (e. g. Count()), depending on whether they trigger the event ; # loop immediately or only when one of the results is accessed for the ; # first time. Actions that return ""something"" either return their result ; # wrapped in a RResultPtr or in a RDataFrame.; # But first of all, let us we define now our cut-flow with two strings.; # Filters can be expressed as strings. The content must be C++ code. The; # name of the variables must be the name of the branches. The code is; # just-in-time compiled.; # `Count` action; # The `Count` allows to retrieve the number of the entries that passed the; # filters. Here we show how the automatic selection of the column kicks; # in in case the user specifies none.; # `Min`, `Max` and `Mean` actions; # These actions allow to retrieve statistical information about the entries; # passing the cuts, if any.; # `Histo1D` action; # The `Histo1D` action allows to fill an histogram. It returns a TH1F filled; # with values of the column that passed the filters. For the most common; # types, the type of the values stored in the column is automatically; # guessed.; # Express your chain of operations with clarity!; # We",MatchSource.CODE_COMMENT,tutorials/dataframe/df001_introduction.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df001_introduction.py
Testability,test,test,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -nodraw; ## Basic usage of RDataFrame from python.; ##; ## This tutorial illustrates the basic features of the RDataFrame class,; ## a utility which allows to interact with data stored in TTrees following; ## a functional-chain like approach.; ##; ## \macro_code; ## \macro_output; ##; ## \date May 2017; ## \author Danilo Piparo (CERN); """"""A simple helper function to fill a test tree: this makes the example stand-alone.""""""; # We prepare an input tree to run on; # We read the tree from the file and create a RDataFrame, a class that; # allows us to interact with the data contained in the tree.; # Operations on the dataframe; # We now review some *actions* which can be performed on the data frame.; # Actions can be divided into instant actions (e. g. Foreach()) and lazy; # actions (e. g. Count()), depending on whether they trigger the event ; # loop immediately or only when one of the results is accessed for the ; # first time. Actions that return ""something"" either return their result ; # wrapped in a RResultPtr or in a RDataFrame.; # But first of all, let us we define now our cut-flow with two strings.; # Filters can be expressed as strings. The content must be C++ code. The; # name of the variables must be the name of the branches. The code is; # just-in-time compiled.; # `Count` action; # The `Count` allows to retrieve the number of the entries that passed the; # filters. Here we show how the automatic selection of the column kicks; # in in case the user specifies none.; # `Min`, `Max` and `Mean` actions; # These actions allow to retrieve statistical information about the entries; # passing the cuts, if any.; # `Histo1D` action; # The `Histo1D` action allows to fill an histogram. It returns a TH1F filled; # with values of the column that passed the filters. For the most common; # types, the type of the values stored in the column is automatically; # guessed.; # Express your chain of operations with clarity!; # We",MatchSource.CODE_COMMENT,tutorials/dataframe/df001_introduction.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df001_introduction.py
Usability,simpl,simple,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -nodraw; ## Basic usage of RDataFrame from python.; ##; ## This tutorial illustrates the basic features of the RDataFrame class,; ## a utility which allows to interact with data stored in TTrees following; ## a functional-chain like approach.; ##; ## \macro_code; ## \macro_output; ##; ## \date May 2017; ## \author Danilo Piparo (CERN); """"""A simple helper function to fill a test tree: this makes the example stand-alone.""""""; # We prepare an input tree to run on; # We read the tree from the file and create a RDataFrame, a class that; # allows us to interact with the data contained in the tree.; # Operations on the dataframe; # We now review some *actions* which can be performed on the data frame.; # Actions can be divided into instant actions (e. g. Foreach()) and lazy; # actions (e. g. Count()), depending on whether they trigger the event ; # loop immediately or only when one of the results is accessed for the ; # first time. Actions that return ""something"" either return their result ; # wrapped in a RResultPtr or in a RDataFrame.; # But first of all, let us we define now our cut-flow with two strings.; # Filters can be expressed as strings. The content must be C++ code. The; # name of the variables must be the name of the branches. The code is; # just-in-time compiled.; # `Count` action; # The `Count` allows to retrieve the number of the entries that passed the; # filters. Here we show how the automatic selection of the column kicks; # in in case the user specifies none.; # `Min`, `Max` and `Mean` actions; # These actions allow to retrieve statistical information about the entries; # passing the cuts, if any.; # `Histo1D` action; # The `Histo1D` action allows to fill an histogram. It returns a TH1F filled; # with values of the column that passed the filters. For the most common; # types, the type of the values stored in the column is automatically; # guessed.; # Express your chain of operations with clarity!; # We",MatchSource.CODE_COMMENT,tutorials/dataframe/df001_introduction.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df001_introduction.py
Energy Efficiency,energy,energy,"hows the possibility to use data models which are more; ## complex than flat ntuples with RDataFrame.; ##; ## \macro_code; ## \macro_image; ##; ## \date May 2017; ## \author Danilo Piparo (CERN); # A simple helper function to fill a test tree: this makes the example stand-alone.; '''; using FourVector = ROOT::Math::XYZTVector;; using FourVectorVec = std::vector<FourVector>;; using CylFourVector = ROOT::Math::RhoEtaPhiVector;. // A simple helper function to fill a test tree: this makes the example; // stand-alone.; void fill_tree(const char *filename, const char *treeName); {; const double M = 0.13957; // set pi+ mass; TRandom3 R(1);. auto genTracks = [&](){; FourVectorVec tracks;; const auto nPart = R.Poisson(15);; tracks.reserve(nPart);; for (int j = 0; j < nPart; ++j) {; const auto px = R.Gaus(0, 10);; const auto py = R.Gaus(0, 10);; const auto pt = sqrt(px * px + py * py);; const auto eta = R.Uniform(-3, 3);; const auto phi = R.Uniform(0.0, 2 * TMath::Pi());; CylFourVector vcyl(pt, eta, phi);; // set energy; auto E = sqrt(vcyl.R() * vcyl.R() + M * M);; // fill track vector; tracks.emplace_back(vcyl.X(), vcyl.Y(), vcyl.Z(), E);; }; return tracks;; };. ROOT::RDataFrame d(64);; d.Define(""tracks"", genTracks).Snapshot<FourVectorVec>(treeName, filename, {""tracks""});; }; '''; # We prepare an input tree to run on; # We read the tree from the file and create a RDataFrame, a class that; # allows us to interact with the data contained in the tree.; # Operating on branches which are collections of objects; # Here we deal with the simplest of the cuts: we decide to accept the event; # only if the number of tracks is greater than 8.; # Another possibility consists in creating a new column containing the; # quantity we are interested in.; # In this example, we will cut on the number of tracks and plot their; # transverse momentum.; '''; using namespace ROOT::VecOps;; ROOT::RVecD getPt(const RVec<FourVector> &tracks); {; auto pt = [](const FourVector &v) { return v.pt(); };; retu",MatchSource.CODE_COMMENT,tutorials/dataframe/df002_dataModel.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df002_dataModel.py
Testability,test,test,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## Show how to work with non-flat data models, e.g. vectors of tracks.; ##; ## This tutorial shows the possibility to use data models which are more; ## complex than flat ntuples with RDataFrame.; ##; ## \macro_code; ## \macro_image; ##; ## \date May 2017; ## \author Danilo Piparo (CERN); # A simple helper function to fill a test tree: this makes the example stand-alone.; '''; using FourVector = ROOT::Math::XYZTVector;; using FourVectorVec = std::vector<FourVector>;; using CylFourVector = ROOT::Math::RhoEtaPhiVector;. // A simple helper function to fill a test tree: this makes the example; // stand-alone.; void fill_tree(const char *filename, const char *treeName); {; const double M = 0.13957; // set pi+ mass; TRandom3 R(1);. auto genTracks = [&](){; FourVectorVec tracks;; const auto nPart = R.Poisson(15);; tracks.reserve(nPart);; for (int j = 0; j < nPart; ++j) {; const auto px = R.Gaus(0, 10);; const auto py = R.Gaus(0, 10);; const auto pt = sqrt(px * px + py * py);; const auto eta = R.Uniform(-3, 3);; const auto phi = R.Uniform(0.0, 2 * TMath::Pi());; CylFourVector vcyl(pt, eta, phi);; // set energy; auto E = sqrt(vcyl.R() * vcyl.R() + M * M);; // fill track vector; tracks.emplace_back(vcyl.X(), vcyl.Y(), vcyl.Z(), E);; }; return tracks;; };. ROOT::RDataFrame d(64);; d.Define(""tracks"", genTracks).Snapshot<FourVectorVec>(treeName, filename, {""tracks""});; }; '''; # We prepare an input tree to run on; # We read the tree from the file and create a RDataFrame, a class that; # allows us to interact with the data contained in the tree.; # Operating on branches which are collections of objects; # Here we deal with the simplest of the cuts: we decide to accept the event; # only if the number of tracks is greater than 8.; # Another possibility consists in creating a new column containing the; # quantity we are interested in.; # In this example, we will cut on the number of tracks and plot their; # transverse mome",MatchSource.CODE_COMMENT,tutorials/dataframe/df002_dataModel.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df002_dataModel.py
Usability,simpl,simple,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## Show how to work with non-flat data models, e.g. vectors of tracks.; ##; ## This tutorial shows the possibility to use data models which are more; ## complex than flat ntuples with RDataFrame.; ##; ## \macro_code; ## \macro_image; ##; ## \date May 2017; ## \author Danilo Piparo (CERN); # A simple helper function to fill a test tree: this makes the example stand-alone.; '''; using FourVector = ROOT::Math::XYZTVector;; using FourVectorVec = std::vector<FourVector>;; using CylFourVector = ROOT::Math::RhoEtaPhiVector;. // A simple helper function to fill a test tree: this makes the example; // stand-alone.; void fill_tree(const char *filename, const char *treeName); {; const double M = 0.13957; // set pi+ mass; TRandom3 R(1);. auto genTracks = [&](){; FourVectorVec tracks;; const auto nPart = R.Poisson(15);; tracks.reserve(nPart);; for (int j = 0; j < nPart; ++j) {; const auto px = R.Gaus(0, 10);; const auto py = R.Gaus(0, 10);; const auto pt = sqrt(px * px + py * py);; const auto eta = R.Uniform(-3, 3);; const auto phi = R.Uniform(0.0, 2 * TMath::Pi());; CylFourVector vcyl(pt, eta, phi);; // set energy; auto E = sqrt(vcyl.R() * vcyl.R() + M * M);; // fill track vector; tracks.emplace_back(vcyl.X(), vcyl.Y(), vcyl.Z(), E);; }; return tracks;; };. ROOT::RDataFrame d(64);; d.Define(""tracks"", genTracks).Snapshot<FourVectorVec>(treeName, filename, {""tracks""});; }; '''; # We prepare an input tree to run on; # We read the tree from the file and create a RDataFrame, a class that; # allows us to interact with the data contained in the tree.; # Operating on branches which are collections of objects; # Here we deal with the simplest of the cuts: we decide to accept the event; # only if the number of tracks is greater than 8.; # Another possibility consists in creating a new column containing the; # quantity we are interested in.; # In this example, we will cut on the number of tracks and plot their; # transverse mome",MatchSource.CODE_COMMENT,tutorials/dataframe/df002_dataModel.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df002_dataModel.py
Testability,test,test,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## Use TProfiles with RDataFrame.; ##; ## This tutorial illustrates how to use TProfiles in combination with the; ## RDataFrame. See the documentation of TProfile and TProfile2D to better; ## understand the analogy of this code with the example one.; ##; ## \macro_code; ## \macro_image; ##; ## \date February 2017; ## \author Danilo Piparo (CERN); # A simple helper function to fill a test tree: this makes the example; # stand-alone.; # We prepare an input tree to run on; # We read the tree from the file and create a RDataFrame.; # Create the profiles; # And Draw",MatchSource.CODE_COMMENT,tutorials/dataframe/df003_profiles.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df003_profiles.py
Usability,simpl,simple,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## Use TProfiles with RDataFrame.; ##; ## This tutorial illustrates how to use TProfiles in combination with the; ## RDataFrame. See the documentation of TProfile and TProfile2D to better; ## understand the analogy of this code with the example one.; ##; ## \macro_code; ## \macro_image; ##; ## \date February 2017; ## \author Danilo Piparo (CERN); # A simple helper function to fill a test tree: this makes the example; # stand-alone.; # We prepare an input tree to run on; # We read the tree from the file and create a RDataFrame.; # Create the profiles; # And Draw",MatchSource.CODE_COMMENT,tutorials/dataframe/df003_profiles.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df003_profiles.py
Integrability,depend,depends,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -nodraw; ## Use Range to limit the amount of data processed.; ##; ## This tutorial shows how to express the concept of ranges when working with the RDataFrame.; ##; ## \macro_code; ## \macro_output; ##; ## \date March 2017; ## \author Danilo Piparo (CERN); # We prepare an input tree to run on; # We read the tree from the file and create a RDataFrame.; # ## Usage of ranges; # Now we'll count some entries using ranges; # This is how you can express a range of the first 30 entries; # This is how you pick all entries from 15 onwards; # We can use a stride too, in this case we pick an event every 3 entries; # The Range here acts first on the (whole) RDataFrame graph:; # Not only actions (like Count) but also filters and new columns can be added to it.; # An important thing to notice is that the counts of a filter are relative to the; # number of entries a filter ""sees"". Therefore, if a Range depends on a filter,; # the Range will act on the entries passing the filter only.; # Ok, time to wrap up: let's print all counts!",MatchSource.CODE_COMMENT,tutorials/dataframe/df006_ranges.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df006_ranges.py
Modifiability,variab,variables,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## Write ROOT data with RDataFrame.; ##; ## This tutorial shows how to write out datasets in ROOT format using RDataFrame.; ##; ## \macro_image; ## \macro_code; ##; ## \date April 2017; ## \author Danilo Piparo (CERN); # A simple helper function to fill a test tree: this makes the example stand-alone.; # We prepare an input tree to run on; # We read the tree from the file and create a RDataFrame; # ## Select entries; # We now select some entries in the dataset; # ## Enrich the dataset; # Build some temporary columns: we'll write them out; '''; std::vector<float> getVector (float b2); {; std::vector<float> v;; for (int i = 0; i < 3; i++) v.push_back(b2*i);; return v;; }; '''; # ## Write it to disk in ROOT format; # We now write to disk a new dataset with one of the variables originally; # present in the tree and the new variables.; # The user can explicitly specify the types of the columns as template; # arguments of the Snapshot method, otherwise they will be automatically; # inferred.; # Open the new file and list the columns of the tree; # We are not forced to write the full set of column names. We can also; # specify a regular expression for that. In case nothing is specified, all; # columns are persistified.; # Open the new file and list the columns of the tree; # We can also get a fresh RDataFrame out of the snapshot and restart the; # analysis chain from it.",MatchSource.CODE_COMMENT,tutorials/dataframe/df007_snapshot.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df007_snapshot.py
Testability,test,test,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## Write ROOT data with RDataFrame.; ##; ## This tutorial shows how to write out datasets in ROOT format using RDataFrame.; ##; ## \macro_image; ## \macro_code; ##; ## \date April 2017; ## \author Danilo Piparo (CERN); # A simple helper function to fill a test tree: this makes the example stand-alone.; # We prepare an input tree to run on; # We read the tree from the file and create a RDataFrame; # ## Select entries; # We now select some entries in the dataset; # ## Enrich the dataset; # Build some temporary columns: we'll write them out; '''; std::vector<float> getVector (float b2); {; std::vector<float> v;; for (int i = 0; i < 3; i++) v.push_back(b2*i);; return v;; }; '''; # ## Write it to disk in ROOT format; # We now write to disk a new dataset with one of the variables originally; # present in the tree and the new variables.; # The user can explicitly specify the types of the columns as template; # arguments of the Snapshot method, otherwise they will be automatically; # inferred.; # Open the new file and list the columns of the tree; # We are not forced to write the full set of column names. We can also; # specify a regular expression for that. In case nothing is specified, all; # columns are persistified.; # Open the new file and list the columns of the tree; # We can also get a fresh RDataFrame out of the snapshot and restart the; # analysis chain from it.",MatchSource.CODE_COMMENT,tutorials/dataframe/df007_snapshot.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df007_snapshot.py
Usability,simpl,simple,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## Write ROOT data with RDataFrame.; ##; ## This tutorial shows how to write out datasets in ROOT format using RDataFrame.; ##; ## \macro_image; ## \macro_code; ##; ## \date April 2017; ## \author Danilo Piparo (CERN); # A simple helper function to fill a test tree: this makes the example stand-alone.; # We prepare an input tree to run on; # We read the tree from the file and create a RDataFrame; # ## Select entries; # We now select some entries in the dataset; # ## Enrich the dataset; # Build some temporary columns: we'll write them out; '''; std::vector<float> getVector (float b2); {; std::vector<float> v;; for (int i = 0; i < 3; i++) v.push_back(b2*i);; return v;; }; '''; # ## Write it to disk in ROOT format; # We now write to disk a new dataset with one of the variables originally; # present in the tree and the new variables.; # The user can explicitly specify the types of the columns as template; # arguments of the Snapshot method, otherwise they will be automatically; # inferred.; # Open the new file and list the columns of the tree; # We are not forced to write the full set of column names. We can also; # specify a regular expression for that. In case nothing is specified, all; # columns are persistified.; # Open the new file and list the columns of the tree; # We can also get a fresh RDataFrame out of the snapshot and restart the; # analysis chain from it.",MatchSource.CODE_COMMENT,tutorials/dataframe/df007_snapshot.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df007_snapshot.py
Integrability,interface,interface,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## Use the ""trivial data source"", an example data source implementation.; ##; ## This tutorial illustrates how use the RDataFrame in combination with a; ## RDataSource. In this case we use a RTrivialDS, which is nothing more; ## than a simple generator: it does not interface to any existing dataset.; ## The RTrivialDS has a single column, col0, which has value n for entry n.; ##; ## Note that RTrivialDS is only a demo data source implementation and superior alternatives; ## typically exist for production use (e.g. constructing an empty RDataFrame as `RDataFrame(nEntries)`).; ##; ## \macro_code; ##; ## \date September 2017; ## \author Danilo Piparo (CERN); # Create the data frame; # Now we have a regular RDataFrame: the ingestion of data is delegated to; # the RDataSource. At this point everything works as before.",MatchSource.CODE_COMMENT,tutorials/dataframe/df010_trivialDataSource.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df010_trivialDataSource.py
Usability,simpl,simple,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## Use the ""trivial data source"", an example data source implementation.; ##; ## This tutorial illustrates how use the RDataFrame in combination with a; ## RDataSource. In this case we use a RTrivialDS, which is nothing more; ## than a simple generator: it does not interface to any existing dataset.; ## The RTrivialDS has a single column, col0, which has value n for entry n.; ##; ## Note that RTrivialDS is only a demo data source implementation and superior alternatives; ## typically exist for production use (e.g. constructing an empty RDataFrame as `RDataFrame(nEntries)`).; ##; ## \macro_code; ##; ## \date September 2017; ## \author Danilo Piparo (CERN); # Create the data frame; # Now we have a regular RDataFrame: the ingestion of data is delegated to; # the RDataSource. At this point everything works as before.",MatchSource.CODE_COMMENT,tutorials/dataframe/df010_trivialDataSource.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df010_trivialDataSource.py
Usability,simpl,simple,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -nodraw; ## Use just-in-time-compiled Filters and Defines for quick prototyping.; ##; ## This tutorial illustrates how to use jit-compiling features of RDataFrame; ## to define data using C++ code in a Python script.; ##; ## \macro_code; ## \macro_output; ##; ## \date October 2017; ## \author Guilherme Amadio (CERN); ## We will inefficiently calculate an approximation of pi by generating; ## some data and doing very simple filtering and analysis on it.; ## We start by creating an empty dataframe where we will insert 10 million; ## random points in a square of side 2.0 (that is, with an inscribed unit; ## circle).; ## Define what data we want inside the dataframe. We do not need to define p; ## as an array, but we do it here to demonstrate how to use jitting with RDataFrame.; ## Now we have a dataframe with columns x, y, p (which is a point based on x; ## and y), and the radius r = sqrt(x*x + y*y). In order to approximate pi, we; ## need to know how many of our data points fall inside the circle of radius; ## one compared with the total number of points. The ratio of the areas is; ##; ## A_circle / A_square = pi r*r / l * l, where r = 1.0, and l = 2.0; ##; ## Therefore, we can approximate pi with four times the number of points inside; ## the unit circle over the total number of points:",MatchSource.CODE_COMMENT,tutorials/dataframe/df012_DefinesAndFiltersAsStrings.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df012_DefinesAndFiltersAsStrings.py
Performance,perform,performed,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## Process collections in RDataFrame with the help of RVec.; ##; ## This tutorial shows the potential of the VecOps approach for treating collections; ## stored in datasets, a situation very common in HEP data analysis.; ##; ## \macro_image; ## \macro_code; ##; ## \date February 2018; ## \author Danilo Piparo (CERN); '''ROOT::RVecD {0}(len);; std::transform({0}.begin(), {0}.end(), {0}.begin(), [](double){{return gRandom->Uniform(-1.0, 1.0);}});; return {0};'''; # Now we have in our hands d, a RDataFrame with two columns, x and y, which; # hold collections of coordinates. The sizes of these collections vary.; # Let's now define radii radii from the x and y coordinates. We'll do it treating ; # the collections stored in the columns without looping on the individual elements.; # Now we want to plot 2 quarters of a ring with radii .5 and 1.; # Note how the cuts are performed on RVecs, comparing them with integers and; # among themselves.",MatchSource.CODE_COMMENT,tutorials/dataframe/df016_vecOps.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df016_vecOps.py
Availability,down,down,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## Use RVecs to plot the transverse momentum of selected particles.; ##; ## This tutorial shows how VecOps can be used to slim down the programming; ## model typically adopted in HEP for analysis.; ## In this case we have a dataset containing the kinematic properties of; ## particles stored in individual arrays.; ## We want to plot the transverse momentum of these particles if the energy is; ## greater than 100 MeV.; ## \macro_code; ## \macro_image; ##; ## \date March 2018; ## \authors Danilo Piparo (CERN), Andre Vieira Silva; ## We plot twice the same quantity, the key is to look into the implementation; ## of the functions above.",MatchSource.CODE_COMMENT,tutorials/dataframe/df017_vecOpsHEP.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df017_vecOpsHEP.py
Energy Efficiency,energy,energy,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## Use RVecs to plot the transverse momentum of selected particles.; ##; ## This tutorial shows how VecOps can be used to slim down the programming; ## model typically adopted in HEP for analysis.; ## In this case we have a dataset containing the kinematic properties of; ## particles stored in individual arrays.; ## We want to plot the transverse momentum of these particles if the energy is; ## greater than 100 MeV.; ## \macro_code; ## \macro_image; ##; ## \date March 2018; ## \authors Danilo Piparo (CERN), Andre Vieira Silva; ## We plot twice the same quantity, the key is to look into the implementation; ## of the functions above.",MatchSource.CODE_COMMENT,tutorials/dataframe/df017_vecOpsHEP.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df017_vecOpsHEP.py
Availability,checkpoint,checkpointing,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## Cache a processed RDataFrame in memory for further usage.; ##; ## This tutorial shows how the content of a data frame can be cached in memory; ## in form of a dataframe. The content of the columns is stored in memory in; ## contiguous slabs of memory and is ""ready to use"", i.e. no ROOT IO operation; ## is performed.; ##; ## Creating a cached data frame storing all of its content deserialised and uncompressed; ## in memory is particularly useful when dealing with datasets of a moderate size; ## (small enough to fit the RAM) over which several explorative loops need to be; ## performed as fast as possible. In addition, caching can be useful when no file; ## on disk needs to be created as a side effect of checkpointing part of the analysis.; ##; ## All steps in the caching are lazy, i.e. the cached data frame is actually filled; ## only when the event loop is triggered on it.; ##; ## \macro_code; ## \macro_image; ##; ## \date June 2018; ## \author Danilo Piparo (CERN); # We create a data frame on top of the hsimple example.; # We apply a simple cut and define a new column.; # We cache the content of the dataset. Nothing has happened yet: the work to accomplish; # has been described.; # Now the event loop on the cached dataset is triggered by accessing the histogram.; # This event triggers the loop on the `df` data frame lazily.",MatchSource.CODE_COMMENT,tutorials/dataframe/df019_Cache.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df019_Cache.py
Performance,cache,cached,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## Cache a processed RDataFrame in memory for further usage.; ##; ## This tutorial shows how the content of a data frame can be cached in memory; ## in form of a dataframe. The content of the columns is stored in memory in; ## contiguous slabs of memory and is ""ready to use"", i.e. no ROOT IO operation; ## is performed.; ##; ## Creating a cached data frame storing all of its content deserialised and uncompressed; ## in memory is particularly useful when dealing with datasets of a moderate size; ## (small enough to fit the RAM) over which several explorative loops need to be; ## performed as fast as possible. In addition, caching can be useful when no file; ## on disk needs to be created as a side effect of checkpointing part of the analysis.; ##; ## All steps in the caching are lazy, i.e. the cached data frame is actually filled; ## only when the event loop is triggered on it.; ##; ## \macro_code; ## \macro_image; ##; ## \date June 2018; ## \author Danilo Piparo (CERN); # We create a data frame on top of the hsimple example.; # We apply a simple cut and define a new column.; # We cache the content of the dataset. Nothing has happened yet: the work to accomplish; # has been described.; # Now the event loop on the cached dataset is triggered by accessing the histogram.; # This event triggers the loop on the `df` data frame lazily.",MatchSource.CODE_COMMENT,tutorials/dataframe/df019_Cache.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df019_Cache.py
Security,access,accessing,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## Cache a processed RDataFrame in memory for further usage.; ##; ## This tutorial shows how the content of a data frame can be cached in memory; ## in form of a dataframe. The content of the columns is stored in memory in; ## contiguous slabs of memory and is ""ready to use"", i.e. no ROOT IO operation; ## is performed.; ##; ## Creating a cached data frame storing all of its content deserialised and uncompressed; ## in memory is particularly useful when dealing with datasets of a moderate size; ## (small enough to fit the RAM) over which several explorative loops need to be; ## performed as fast as possible. In addition, caching can be useful when no file; ## on disk needs to be created as a side effect of checkpointing part of the analysis.; ##; ## All steps in the caching are lazy, i.e. the cached data frame is actually filled; ## only when the event loop is triggered on it.; ##; ## \macro_code; ## \macro_image; ##; ## \date June 2018; ## \author Danilo Piparo (CERN); # We create a data frame on top of the hsimple example.; # We apply a simple cut and define a new column.; # We cache the content of the dataset. Nothing has happened yet: the work to accomplish; # has been described.; # Now the event loop on the cached dataset is triggered by accessing the histogram.; # This event triggers the loop on the `df` data frame lazily.",MatchSource.CODE_COMMENT,tutorials/dataframe/df019_Cache.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df019_Cache.py
Usability,simpl,simple,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## Cache a processed RDataFrame in memory for further usage.; ##; ## This tutorial shows how the content of a data frame can be cached in memory; ## in form of a dataframe. The content of the columns is stored in memory in; ## contiguous slabs of memory and is ""ready to use"", i.e. no ROOT IO operation; ## is performed.; ##; ## Creating a cached data frame storing all of its content deserialised and uncompressed; ## in memory is particularly useful when dealing with datasets of a moderate size; ## (small enough to fit the RAM) over which several explorative loops need to be; ## performed as fast as possible. In addition, caching can be useful when no file; ## on disk needs to be created as a side effect of checkpointing part of the analysis.; ##; ## All steps in the caching are lazy, i.e. the cached data frame is actually filled; ## only when the event loop is triggered on it.; ##; ## \macro_code; ## \macro_image; ##; ## \date June 2018; ## \author Danilo Piparo (CERN); # We create a data frame on top of the hsimple example.; # We apply a simple cut and define a new column.; # We cache the content of the dataset. Nothing has happened yet: the work to accomplish; # has been described.; # Now the event loop on the cached dataset is triggered by accessing the histogram.; # This event triggers the loop on the `df` data frame lazily.",MatchSource.CODE_COMMENT,tutorials/dataframe/df019_Cache.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df019_Cache.py
Energy Efficiency,reduce,reduce,"## \file; ## \ingroup tutorial_dataframe; ## \notebook; ## Read data from RDataFrame into Numpy arrays.; ##; ## \macro_code; ## \macro_output; ##; ## \date December 2018; ## \author Stefan Wunsch (KIT, CERN); # Let's create a simple dataframe with ten rows and two columns; # Next, we want to access the data from Python as Numpy arrays. To do so, the; # content of the dataframe is converted using the AsNumpy method. The returned; # object is a dictionary with the column names as keys and 1D numpy arrays with; # the content as values.; # Since reading out data to memory is expensive, always try to read-out only what; # is needed for your analysis. You can use all RDataFrame features to reduce your; # dataset, e.g., the Filter transformation. Furthermore, you can can pass to the; # AsNumpy method a whitelist of column names with the option `columns` or a blacklist; # with column names with the option `exclude`.; # You can read-out all objects from ROOT files since these are wrapped by PyROOT; # in the Python world. However, be aware that objects other than fundamental types,; # such as complex C++ objects and not int or float, are costly to read-out.; """"""; // Inject the C++ class CustomObject in the C++ runtime.; class CustomObject {; public:; int x = 42;; };; // Create a function that returns such an object. This is called to fill the dataframe.; CustomObject fill_object() { return CustomObject(); }; """"""; # Note that you can pass the object returned by AsNumpy directly to pandas.DataFrame; # including any complex C++ object that may be read-out.",MatchSource.CODE_COMMENT,tutorials/dataframe/df026_AsNumpyArrays.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df026_AsNumpyArrays.py
Integrability,wrap,wrapped,"## \file; ## \ingroup tutorial_dataframe; ## \notebook; ## Read data from RDataFrame into Numpy arrays.; ##; ## \macro_code; ## \macro_output; ##; ## \date December 2018; ## \author Stefan Wunsch (KIT, CERN); # Let's create a simple dataframe with ten rows and two columns; # Next, we want to access the data from Python as Numpy arrays. To do so, the; # content of the dataframe is converted using the AsNumpy method. The returned; # object is a dictionary with the column names as keys and 1D numpy arrays with; # the content as values.; # Since reading out data to memory is expensive, always try to read-out only what; # is needed for your analysis. You can use all RDataFrame features to reduce your; # dataset, e.g., the Filter transformation. Furthermore, you can can pass to the; # AsNumpy method a whitelist of column names with the option `columns` or a blacklist; # with column names with the option `exclude`.; # You can read-out all objects from ROOT files since these are wrapped by PyROOT; # in the Python world. However, be aware that objects other than fundamental types,; # such as complex C++ objects and not int or float, are costly to read-out.; """"""; // Inject the C++ class CustomObject in the C++ runtime.; class CustomObject {; public:; int x = 42;; };; // Create a function that returns such an object. This is called to fill the dataframe.; CustomObject fill_object() { return CustomObject(); }; """"""; # Note that you can pass the object returned by AsNumpy directly to pandas.DataFrame; # including any complex C++ object that may be read-out.",MatchSource.CODE_COMMENT,tutorials/dataframe/df026_AsNumpyArrays.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df026_AsNumpyArrays.py
Security,access,access,"## \file; ## \ingroup tutorial_dataframe; ## \notebook; ## Read data from RDataFrame into Numpy arrays.; ##; ## \macro_code; ## \macro_output; ##; ## \date December 2018; ## \author Stefan Wunsch (KIT, CERN); # Let's create a simple dataframe with ten rows and two columns; # Next, we want to access the data from Python as Numpy arrays. To do so, the; # content of the dataframe is converted using the AsNumpy method. The returned; # object is a dictionary with the column names as keys and 1D numpy arrays with; # the content as values.; # Since reading out data to memory is expensive, always try to read-out only what; # is needed for your analysis. You can use all RDataFrame features to reduce your; # dataset, e.g., the Filter transformation. Furthermore, you can can pass to the; # AsNumpy method a whitelist of column names with the option `columns` or a blacklist; # with column names with the option `exclude`.; # You can read-out all objects from ROOT files since these are wrapped by PyROOT; # in the Python world. However, be aware that objects other than fundamental types,; # such as complex C++ objects and not int or float, are costly to read-out.; """"""; // Inject the C++ class CustomObject in the C++ runtime.; class CustomObject {; public:; int x = 42;; };; // Create a function that returns such an object. This is called to fill the dataframe.; CustomObject fill_object() { return CustomObject(); }; """"""; # Note that you can pass the object returned by AsNumpy directly to pandas.DataFrame; # including any complex C++ object that may be read-out.",MatchSource.CODE_COMMENT,tutorials/dataframe/df026_AsNumpyArrays.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df026_AsNumpyArrays.py
Usability,simpl,simple,"## \file; ## \ingroup tutorial_dataframe; ## \notebook; ## Read data from RDataFrame into Numpy arrays.; ##; ## \macro_code; ## \macro_output; ##; ## \date December 2018; ## \author Stefan Wunsch (KIT, CERN); # Let's create a simple dataframe with ten rows and two columns; # Next, we want to access the data from Python as Numpy arrays. To do so, the; # content of the dataframe is converted using the AsNumpy method. The returned; # object is a dictionary with the column names as keys and 1D numpy arrays with; # the content as values.; # Since reading out data to memory is expensive, always try to read-out only what; # is needed for your analysis. You can use all RDataFrame features to reduce your; # dataset, e.g., the Filter transformation. Furthermore, you can can pass to the; # AsNumpy method a whitelist of column names with the option `columns` or a blacklist; # with column names with the option `exclude`.; # You can read-out all objects from ROOT files since these are wrapped by PyROOT; # in the Python world. However, be aware that objects other than fundamental types,; # such as complex C++ objects and not int or float, are costly to read-out.; """"""; // Inject the C++ class CustomObject in the C++ runtime.; class CustomObject {; public:; int x = 42;; };; // Create a function that returns such an object. This is called to fill the dataframe.; CustomObject fill_object() { return CustomObject(); }; """"""; # Note that you can pass the object returned by AsNumpy directly to pandas.DataFrame; # including any complex C++ object that may be read-out.",MatchSource.CODE_COMMENT,tutorials/dataframe/df026_AsNumpyArrays.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df026_AsNumpyArrays.py
Energy Efficiency,charge,charge,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -js; ## Show how NanoAOD files can be processed with RDataFrame.; ##; ## This tutorial illustrates how NanoAOD files can be processed with ROOT; ## dataframes. The NanoAOD-like input files are filled with 66 mio. events; ## from CMS OpenData containing muon candidates part of 2012 dataset; ## ([DOI: 10.7483/OPENDATA.CMS.YLIC.86ZZ](http://opendata.cern.ch/record/6004); ## and [DOI: 10.7483/OPENDATA.CMS.M5AD.Y3V3](http://opendata.cern.ch/record/6030)).; ## The macro matches muon pairs and produces an histogram of the dimuon mass; ## spectrum showing resonances up to the Z mass.; ## Note that the bump at 30 GeV is not a resonance but a trigger effect.; ##; ## More details about the dataset can be found on [the CERN Open Data portal](http://opendata.web.cern.ch/record/12341).; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date April 2019; ## \author Stefan Wunsch (KIT, CERN); # Enable multi-threading; # Create dataframe from NanoAOD files; # For simplicity, select only events with exactly two muons and require opposite charge; # Compute invariant mass of the dimuon system; # Make histogram of dimuon mass spectrum. Note how we can set titles and axis labels in one go.; # Request cut-flow report; # Produce plot; # Print cut-flow report",MatchSource.CODE_COMMENT,tutorials/dataframe/df102_NanoAODDimuonAnalysis.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df102_NanoAODDimuonAnalysis.py
Performance,multi-thread,multi-threading,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -js; ## Show how NanoAOD files can be processed with RDataFrame.; ##; ## This tutorial illustrates how NanoAOD files can be processed with ROOT; ## dataframes. The NanoAOD-like input files are filled with 66 mio. events; ## from CMS OpenData containing muon candidates part of 2012 dataset; ## ([DOI: 10.7483/OPENDATA.CMS.YLIC.86ZZ](http://opendata.cern.ch/record/6004); ## and [DOI: 10.7483/OPENDATA.CMS.M5AD.Y3V3](http://opendata.cern.ch/record/6030)).; ## The macro matches muon pairs and produces an histogram of the dimuon mass; ## spectrum showing resonances up to the Z mass.; ## Note that the bump at 30 GeV is not a resonance but a trigger effect.; ##; ## More details about the dataset can be found on [the CERN Open Data portal](http://opendata.web.cern.ch/record/12341).; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date April 2019; ## \author Stefan Wunsch (KIT, CERN); # Enable multi-threading; # Create dataframe from NanoAOD files; # For simplicity, select only events with exactly two muons and require opposite charge; # Compute invariant mass of the dimuon system; # Make histogram of dimuon mass spectrum. Note how we can set titles and axis labels in one go.; # Request cut-flow report; # Produce plot; # Print cut-flow report",MatchSource.CODE_COMMENT,tutorials/dataframe/df102_NanoAODDimuonAnalysis.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df102_NanoAODDimuonAnalysis.py
Usability,simpl,simplicity,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -js; ## Show how NanoAOD files can be processed with RDataFrame.; ##; ## This tutorial illustrates how NanoAOD files can be processed with ROOT; ## dataframes. The NanoAOD-like input files are filled with 66 mio. events; ## from CMS OpenData containing muon candidates part of 2012 dataset; ## ([DOI: 10.7483/OPENDATA.CMS.YLIC.86ZZ](http://opendata.cern.ch/record/6004); ## and [DOI: 10.7483/OPENDATA.CMS.M5AD.Y3V3](http://opendata.cern.ch/record/6030)).; ## The macro matches muon pairs and produces an histogram of the dimuon mass; ## spectrum showing resonances up to the Z mass.; ## Note that the bump at 30 GeV is not a resonance but a trigger effect.; ##; ## More details about the dataset can be found on [the CERN Open Data portal](http://opendata.web.cern.ch/record/12341).; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date April 2019; ## \author Stefan Wunsch (KIT, CERN); # Enable multi-threading; # Create dataframe from NanoAOD files; # For simplicity, select only events with exactly two muons and require opposite charge; # Compute invariant mass of the dimuon system; # Make histogram of dimuon mass spectrum. Note how we can set titles and axis labels in one go.; # Request cut-flow report; # Produce plot; # Print cut-flow report",MatchSource.CODE_COMMENT,tutorials/dataframe/df102_NanoAODDimuonAnalysis.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df102_NanoAODDimuonAnalysis.py
Availability,avail,available,"ow Higgs signal; # on top of the background process; # Get histogram of data points; # Draw histograms; # Add legend; # Add header; # Save plot; # Make sure canvas and objects remains existing after the macro execution; # In fast mode, take samples from */cms_opendata_2012_nanoaod_skimmed/*, which has; # the preselections from the selection_* functions already applied.; # Run on skimmed data, set to False to run on full dataset; # Create dataframes for signal, background and data samples; # Signal: Higgs -> 4 leptons; # Background: ZZ -> 4 leptons; # Note that additional background processes from the original paper; # with minor contribution were left out for this; # tutorial.; # CMS data taken in 2012 (11.6 fb^-1 integrated luminosity); # Number of bins for all histograms; # Weights; # Integrated luminosity of the data samples; # ZZ->4mu: Standard Model cross-section; # ZZ->4mu: Number of simulated events; # ZZ->4el: Standard Model cross-section; # ZZ->4el: Number of simulated events; # ZZ->2el2mu: Standard Model cross-section; # ZZ->2el2mu: Number of simulated events; # H->4l: Standard Model cross-section; # H->4l: Number of simulated events; # ZZ->4l: Scale factor for ZZ to four leptons; # Reconstruct Higgs to 4 muons; # Reconstruct Higgs to 4 electrons; # Reconstruct Higgs to 2 electrons and 2 muons; # RunGraphs allows to run the event loops of the separate RDataFrame graphs; # concurrently. This results in an improved usage of the available resources; # if each separate RDataFrame can not utilize all available resources, e.g.,; # because not enough data is available.; # Get histograms (does not rerun the event loop); # Make plots; # Combined plots; # If this was done before plotting the others, calling the `Add` function; # on the `signal_4mu` histogram would modify the underlying `TH1D` object.; # Thus, the histogram with the 4 muons reconstruction would be lost,; # instead resulting in the same plot as the aggregated histograms.; # Plot aggregated histograms",MatchSource.CODE_COMMENT,tutorials/dataframe/df103_NanoAODHiggsAnalysis.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df103_NanoAODHiggsAnalysis.py
Deployability,integrat,integrated,"ow Higgs signal; # on top of the background process; # Get histogram of data points; # Draw histograms; # Add legend; # Add header; # Save plot; # Make sure canvas and objects remains existing after the macro execution; # In fast mode, take samples from */cms_opendata_2012_nanoaod_skimmed/*, which has; # the preselections from the selection_* functions already applied.; # Run on skimmed data, set to False to run on full dataset; # Create dataframes for signal, background and data samples; # Signal: Higgs -> 4 leptons; # Background: ZZ -> 4 leptons; # Note that additional background processes from the original paper; # with minor contribution were left out for this; # tutorial.; # CMS data taken in 2012 (11.6 fb^-1 integrated luminosity); # Number of bins for all histograms; # Weights; # Integrated luminosity of the data samples; # ZZ->4mu: Standard Model cross-section; # ZZ->4mu: Number of simulated events; # ZZ->4el: Standard Model cross-section; # ZZ->4el: Number of simulated events; # ZZ->2el2mu: Standard Model cross-section; # ZZ->2el2mu: Number of simulated events; # H->4l: Standard Model cross-section; # H->4l: Number of simulated events; # ZZ->4l: Scale factor for ZZ to four leptons; # Reconstruct Higgs to 4 muons; # Reconstruct Higgs to 4 electrons; # Reconstruct Higgs to 2 electrons and 2 muons; # RunGraphs allows to run the event loops of the separate RDataFrame graphs; # concurrently. This results in an improved usage of the available resources; # if each separate RDataFrame can not utilize all available resources, e.g.,; # because not enough data is available.; # Get histograms (does not rerun the event loop); # Make plots; # Combined plots; # If this was done before plotting the others, calling the `Add` function; # on the `signal_4mu` histogram would modify the underlying `TH1D` object.; # Thus, the histogram with the 4 muons reconstruction would be lost,; # instead resulting in the same plot as the aggregated histograms.; # Plot aggregated histograms",MatchSource.CODE_COMMENT,tutorials/dataframe/df103_NanoAODHiggsAnalysis.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df103_NanoAODHiggsAnalysis.py
Integrability,integrat,integrated,"ow Higgs signal; # on top of the background process; # Get histogram of data points; # Draw histograms; # Add legend; # Add header; # Save plot; # Make sure canvas and objects remains existing after the macro execution; # In fast mode, take samples from */cms_opendata_2012_nanoaod_skimmed/*, which has; # the preselections from the selection_* functions already applied.; # Run on skimmed data, set to False to run on full dataset; # Create dataframes for signal, background and data samples; # Signal: Higgs -> 4 leptons; # Background: ZZ -> 4 leptons; # Note that additional background processes from the original paper; # with minor contribution were left out for this; # tutorial.; # CMS data taken in 2012 (11.6 fb^-1 integrated luminosity); # Number of bins for all histograms; # Weights; # Integrated luminosity of the data samples; # ZZ->4mu: Standard Model cross-section; # ZZ->4mu: Number of simulated events; # ZZ->4el: Standard Model cross-section; # ZZ->4el: Number of simulated events; # ZZ->2el2mu: Standard Model cross-section; # ZZ->2el2mu: Number of simulated events; # H->4l: Standard Model cross-section; # H->4l: Number of simulated events; # ZZ->4l: Scale factor for ZZ to four leptons; # Reconstruct Higgs to 4 muons; # Reconstruct Higgs to 4 electrons; # Reconstruct Higgs to 2 electrons and 2 muons; # RunGraphs allows to run the event loops of the separate RDataFrame graphs; # concurrently. This results in an improved usage of the available resources; # if each separate RDataFrame can not utilize all available resources, e.g.,; # because not enough data is available.; # Get histograms (does not rerun the event loop); # Make plots; # Combined plots; # If this was done before plotting the others, calling the `Add` function; # on the `signal_4mu` histogram would modify the underlying `TH1D` object.; # Thus, the histogram with the 4 muons reconstruction would be lost,; # instead resulting in the same plot as the aggregated histograms.; # Plot aggregated histograms",MatchSource.CODE_COMMENT,tutorials/dataframe/df103_NanoAODHiggsAnalysis.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df103_NanoAODHiggsAnalysis.py
Performance,perform,performed,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## An example of complex analysis with RDataFrame: reconstructing the Higgs boson.; ##; ## This tutorial is a simplified but yet complex example of an analysis reconstructing the Higgs boson decaying to two Z; ## bosons from events with four leptons. The data and simulated events are taken from CERN OpenData representing a; ## subset of the data recorded in 2012 with the CMS detector at the LHC. The tutorials follows the Higgs to four leptons; ## analysis published on CERN Open Data portal ([10.7483/OPENDATA.CMS.JKB8.RR42](http://opendata.cern.ch/record/5500)).; ## The resulting plots show the invariant mass of the selected four lepton systems in different decay modes (four muons,; ## four electrons and two of each kind) and in a combined plot indicating the decay of the Higgs boson with a mass of; ## about 125 GeV.; ##; ## The following steps are performed for each sample with data and simulated events in order to reconstruct the Higgs; ## boson from the selected muons and electrons:; ## 1. Select interesting events with multiple cuts on event properties, e.g., number of leptons, kinematics of the; ## leptons and quality of the tracks.; ## 2. Reconstruct two Z bosons of which only one on the mass shell from the selected events and apply additional cuts on; ## the reconstructed objects.; ## 3. Reconstruct the Higgs boson from the remaining Z boson candidates and calculate its invariant mass.; ##; ## Another aim of this version of the tutorial is to show a way to blend C++ and Python code. All the functions that; ## make computations on data to define new columns or filter existing ones in a precise way, better suited to be written; ## in C++, have been moved to a header that is then declared to the ROOT C++ interpreter. The functions that instead; ## create nodes of the computational graph (e.g. Filter, Define) remain inside the main Python script.; ##; ## The tutorial has the fast mode enabled by default,",MatchSource.CODE_COMMENT,tutorials/dataframe/df103_NanoAODHiggsAnalysis.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df103_NanoAODHiggsAnalysis.py
Safety,detect,detector,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## An example of complex analysis with RDataFrame: reconstructing the Higgs boson.; ##; ## This tutorial is a simplified but yet complex example of an analysis reconstructing the Higgs boson decaying to two Z; ## bosons from events with four leptons. The data and simulated events are taken from CERN OpenData representing a; ## subset of the data recorded in 2012 with the CMS detector at the LHC. The tutorials follows the Higgs to four leptons; ## analysis published on CERN Open Data portal ([10.7483/OPENDATA.CMS.JKB8.RR42](http://opendata.cern.ch/record/5500)).; ## The resulting plots show the invariant mass of the selected four lepton systems in different decay modes (four muons,; ## four electrons and two of each kind) and in a combined plot indicating the decay of the Higgs boson with a mass of; ## about 125 GeV.; ##; ## The following steps are performed for each sample with data and simulated events in order to reconstruct the Higgs; ## boson from the selected muons and electrons:; ## 1. Select interesting events with multiple cuts on event properties, e.g., number of leptons, kinematics of the; ## leptons and quality of the tracks.; ## 2. Reconstruct two Z bosons of which only one on the mass shell from the selected events and apply additional cuts on; ## the reconstructed objects.; ## 3. Reconstruct the Higgs boson from the remaining Z boson candidates and calculate its invariant mass.; ##; ## Another aim of this version of the tutorial is to show a way to blend C++ and Python code. All the functions that; ## make computations on data to define new columns or filter existing ones in a precise way, better suited to be written; ## in C++, have been moved to a header that is then declared to the ROOT C++ interpreter. The functions that instead; ## create nodes of the computational graph (e.g. Filter, Define) remain inside the main Python script.; ##; ## The tutorial has the fast mode enabled by default,",MatchSource.CODE_COMMENT,tutorials/dataframe/df103_NanoAODHiggsAnalysis.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df103_NanoAODHiggsAnalysis.py
Usability,simpl,simplified,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## An example of complex analysis with RDataFrame: reconstructing the Higgs boson.; ##; ## This tutorial is a simplified but yet complex example of an analysis reconstructing the Higgs boson decaying to two Z; ## bosons from events with four leptons. The data and simulated events are taken from CERN OpenData representing a; ## subset of the data recorded in 2012 with the CMS detector at the LHC. The tutorials follows the Higgs to four leptons; ## analysis published on CERN Open Data portal ([10.7483/OPENDATA.CMS.JKB8.RR42](http://opendata.cern.ch/record/5500)).; ## The resulting plots show the invariant mass of the selected four lepton systems in different decay modes (four muons,; ## four electrons and two of each kind) and in a combined plot indicating the decay of the Higgs boson with a mass of; ## about 125 GeV.; ##; ## The following steps are performed for each sample with data and simulated events in order to reconstruct the Higgs; ## boson from the selected muons and electrons:; ## 1. Select interesting events with multiple cuts on event properties, e.g., number of leptons, kinematics of the; ## leptons and quality of the tracks.; ## 2. Reconstruct two Z bosons of which only one on the mass shell from the selected events and apply additional cuts on; ## the reconstructed objects.; ## 3. Reconstruct the Higgs boson from the remaining Z boson candidates and calculate its invariant mass.; ##; ## Another aim of this version of the tutorial is to show a way to blend C++ and Python code. All the functions that; ## make computations on data to define new columns or filter existing ones in a precise way, better suited to be written; ## in C++, have been moved to a header that is then declared to the ROOT C++ interpreter. The functions that instead; ## create nodes of the computational graph (e.g. Filter, Define) remain inside the main Python script.; ##; ## The tutorial has the fast mode enabled by default,",MatchSource.CODE_COMMENT,tutorials/dataframe/df103_NanoAODHiggsAnalysis.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df103_NanoAODHiggsAnalysis.py
Availability,avail,available,"gs can be seen as a narrow peak around 125 GeV because of the excellent; ## reconstruction and identification efficiency of photons at the ATLAS experiment.; ##; ## The analysis is translated to a RDataFrame workflow processing 1.7 GB of simulated events and data.; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2020; ## \author Stefan Wunsch (KIT, CERN); # Enable multi-threading; # Create a ROOT dataframe for each dataset; # Apply scale factors and MC weight for simulated events and a weight of 1 for the data; # Select the events for the analysis; # Apply preselection cut on photon trigger; # Find two good muons with tight ID, pt > 25 GeV and not in the transition region between barrel and encap; # Take only isolated photons; # Compile a function to compute the invariant mass of the diphoton system; """"""; using namespace ROOT;; float ComputeInvariantMass(RVecF pt, RVecF eta, RVecF phi, RVecF e) {; ROOT::Math::PtEtaPhiEVector p1(pt[0], eta[0], phi[0], e[0]);; ROOT::Math::PtEtaPhiEVector p2(pt[1], eta[1], phi[1], e[1]);; return (p1 + p2).mass() / 1000.0;; }; """"""; # Define a new column with the invariant mass and perform final event selection; # Make four vectors and compute invariant mass; # Make additional kinematic cuts and select mass window; # Book histogram of the invariant mass with this selection; # Run the event loop; # RunGraphs allows to run the event loops of the separate RDataFrame graphs; # concurrently. This results in an improved usage of the available resources; # if each separate RDataFrame can not utilize all available resources, e.g.,; # because not enough data is available.; # Create the plot; # Set styles; # Create canvas with pads for main plot and data/MC ratio; # Fit signal + background model to data; # Draw data; # Draw fit; # Draw background; # Scale simulated events with luminosity * cross-section / sum of weights; # and merge to single Higgs signal; # Draw ratio; # Add legend; # Add ATLAS label; # Save the plot",MatchSource.CODE_COMMENT,tutorials/dataframe/df104_HiggsToTwoPhotons.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df104_HiggsToTwoPhotons.py
Deployability,release,release,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## The Higgs to two photons analysis from the ATLAS Open Data 2020 release, with RDataFrame.; ##; ## This tutorial is the Higgs to two photons analysis from the ATLAS Open Data release in 2020; ## (http://opendata.atlas.cern/release/2020/documentation/). The data was taken with the ATLAS detector; ## during 2016 at a center-of-mass energy of 13 TeV. Although the Higgs to two photons decay is very rare,; ## the contribution of the Higgs can be seen as a narrow peak around 125 GeV because of the excellent; ## reconstruction and identification efficiency of photons at the ATLAS experiment.; ##; ## The analysis is translated to a RDataFrame workflow processing 1.7 GB of simulated events and data.; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2020; ## \author Stefan Wunsch (KIT, CERN); # Enable multi-threading; # Create a ROOT dataframe for each dataset; # Apply scale factors and MC weight for simulated events and a weight of 1 for the data; # Select the events for the analysis; # Apply preselection cut on photon trigger; # Find two good muons with tight ID, pt > 25 GeV and not in the transition region between barrel and encap; # Take only isolated photons; # Compile a function to compute the invariant mass of the diphoton system; """"""; using namespace ROOT;; float ComputeInvariantMass(RVecF pt, RVecF eta, RVecF phi, RVecF e) {; ROOT::Math::PtEtaPhiEVector p1(pt[0], eta[0], phi[0], e[0]);; ROOT::Math::PtEtaPhiEVector p2(pt[1], eta[1], phi[1], e[1]);; return (p1 + p2).mass() / 1000.0;; }; """"""; # Define a new column with the invariant mass and perform final event selection; # Make four vectors and compute invariant mass; # Make additional kinematic cuts and select mass window; # Book histogram of the invariant mass with this selection; # Run the event loop; # RunGraphs allows to run the event loops of the separate RDataFrame graphs; # concurrently. This results in an improved usage ",MatchSource.CODE_COMMENT,tutorials/dataframe/df104_HiggsToTwoPhotons.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df104_HiggsToTwoPhotons.py
Energy Efficiency,energy,energy,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## The Higgs to two photons analysis from the ATLAS Open Data 2020 release, with RDataFrame.; ##; ## This tutorial is the Higgs to two photons analysis from the ATLAS Open Data release in 2020; ## (http://opendata.atlas.cern/release/2020/documentation/). The data was taken with the ATLAS detector; ## during 2016 at a center-of-mass energy of 13 TeV. Although the Higgs to two photons decay is very rare,; ## the contribution of the Higgs can be seen as a narrow peak around 125 GeV because of the excellent; ## reconstruction and identification efficiency of photons at the ATLAS experiment.; ##; ## The analysis is translated to a RDataFrame workflow processing 1.7 GB of simulated events and data.; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2020; ## \author Stefan Wunsch (KIT, CERN); # Enable multi-threading; # Create a ROOT dataframe for each dataset; # Apply scale factors and MC weight for simulated events and a weight of 1 for the data; # Select the events for the analysis; # Apply preselection cut on photon trigger; # Find two good muons with tight ID, pt > 25 GeV and not in the transition region between barrel and encap; # Take only isolated photons; # Compile a function to compute the invariant mass of the diphoton system; """"""; using namespace ROOT;; float ComputeInvariantMass(RVecF pt, RVecF eta, RVecF phi, RVecF e) {; ROOT::Math::PtEtaPhiEVector p1(pt[0], eta[0], phi[0], e[0]);; ROOT::Math::PtEtaPhiEVector p2(pt[1], eta[1], phi[1], e[1]);; return (p1 + p2).mass() / 1000.0;; }; """"""; # Define a new column with the invariant mass and perform final event selection; # Make four vectors and compute invariant mass; # Make additional kinematic cuts and select mass window; # Book histogram of the invariant mass with this selection; # Run the event loop; # RunGraphs allows to run the event loops of the separate RDataFrame graphs; # concurrently. This results in an improved usage ",MatchSource.CODE_COMMENT,tutorials/dataframe/df104_HiggsToTwoPhotons.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df104_HiggsToTwoPhotons.py
Performance,multi-thread,multi-threading,"orial is the Higgs to two photons analysis from the ATLAS Open Data release in 2020; ## (http://opendata.atlas.cern/release/2020/documentation/). The data was taken with the ATLAS detector; ## during 2016 at a center-of-mass energy of 13 TeV. Although the Higgs to two photons decay is very rare,; ## the contribution of the Higgs can be seen as a narrow peak around 125 GeV because of the excellent; ## reconstruction and identification efficiency of photons at the ATLAS experiment.; ##; ## The analysis is translated to a RDataFrame workflow processing 1.7 GB of simulated events and data.; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2020; ## \author Stefan Wunsch (KIT, CERN); # Enable multi-threading; # Create a ROOT dataframe for each dataset; # Apply scale factors and MC weight for simulated events and a weight of 1 for the data; # Select the events for the analysis; # Apply preselection cut on photon trigger; # Find two good muons with tight ID, pt > 25 GeV and not in the transition region between barrel and encap; # Take only isolated photons; # Compile a function to compute the invariant mass of the diphoton system; """"""; using namespace ROOT;; float ComputeInvariantMass(RVecF pt, RVecF eta, RVecF phi, RVecF e) {; ROOT::Math::PtEtaPhiEVector p1(pt[0], eta[0], phi[0], e[0]);; ROOT::Math::PtEtaPhiEVector p2(pt[1], eta[1], phi[1], e[1]);; return (p1 + p2).mass() / 1000.0;; }; """"""; # Define a new column with the invariant mass and perform final event selection; # Make four vectors and compute invariant mass; # Make additional kinematic cuts and select mass window; # Book histogram of the invariant mass with this selection; # Run the event loop; # RunGraphs allows to run the event loops of the separate RDataFrame graphs; # concurrently. This results in an improved usage of the available resources; # if each separate RDataFrame can not utilize all available resources, e.g.,; # because not enough data is available.; # Create the plot; # Set",MatchSource.CODE_COMMENT,tutorials/dataframe/df104_HiggsToTwoPhotons.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df104_HiggsToTwoPhotons.py
Safety,detect,detector,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## The Higgs to two photons analysis from the ATLAS Open Data 2020 release, with RDataFrame.; ##; ## This tutorial is the Higgs to two photons analysis from the ATLAS Open Data release in 2020; ## (http://opendata.atlas.cern/release/2020/documentation/). The data was taken with the ATLAS detector; ## during 2016 at a center-of-mass energy of 13 TeV. Although the Higgs to two photons decay is very rare,; ## the contribution of the Higgs can be seen as a narrow peak around 125 GeV because of the excellent; ## reconstruction and identification efficiency of photons at the ATLAS experiment.; ##; ## The analysis is translated to a RDataFrame workflow processing 1.7 GB of simulated events and data.; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2020; ## \author Stefan Wunsch (KIT, CERN); # Enable multi-threading; # Create a ROOT dataframe for each dataset; # Apply scale factors and MC weight for simulated events and a weight of 1 for the data; # Select the events for the analysis; # Apply preselection cut on photon trigger; # Find two good muons with tight ID, pt > 25 GeV and not in the transition region between barrel and encap; # Take only isolated photons; # Compile a function to compute the invariant mass of the diphoton system; """"""; using namespace ROOT;; float ComputeInvariantMass(RVecF pt, RVecF eta, RVecF phi, RVecF e) {; ROOT::Math::PtEtaPhiEVector p1(pt[0], eta[0], phi[0], e[0]);; ROOT::Math::PtEtaPhiEVector p2(pt[1], eta[1], phi[1], e[1]);; return (p1 + p2).mass() / 1000.0;; }; """"""; # Define a new column with the invariant mass and perform final event selection; # Make four vectors and compute invariant mass; # Make additional kinematic cuts and select mass window; # Book histogram of the invariant mass with this selection; # Run the event loop; # RunGraphs allows to run the event loops of the separate RDataFrame graphs; # concurrently. This results in an improved usage ",MatchSource.CODE_COMMENT,tutorials/dataframe/df104_HiggsToTwoPhotons.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df104_HiggsToTwoPhotons.py
Availability,down,down,"GB of simulated events and data.; ## By default the analysis runs on a preskimmed dataset to reduce the runtime. The full dataset can be used with; ## the --full-dataset argument and you can also run only on a fraction of the original dataset using the argument --lumi-scale.; ##; ## See the [corresponding spec json file](https://github.com/root-project/root/blob/master/tutorials/dataframe/df105_WBosonAnalysis.json).; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date March 2020; ## \author Stefan Wunsch (KIT, CERN); # Argument parsing; # Script; # Notebook; # The preskimmed dataset contains only 0.01 fb^-1; # Create a ROOT dataframe for each dataset; # Note that we load the filenames from the external json file placed in the same folder than this script.; # Construct the dataframes; # Folder name; # Sample name; # Cross-section; # Sum of weights; # Number of events; # Scale down the datasets if requested; # Select events for the analysis; # Just-in-time compile custom helper function performing complex computations; """"""; bool GoodElectronOrMuon(int type, float pt, float eta, float phi, float e, float trackd0pv, float tracksigd0pv, float z0); {; ROOT::Math::PtEtaPhiEVector p(pt / 1000.0, eta, phi, e / 1000.0);; if (abs(z0 * sin(p.theta())) > 0.5) return false;; if (type == 11 && abs(eta) < 2.46 && (abs(eta) < 1.37 || abs(eta) > 1.52)) {; if (abs(trackd0pv / tracksigd0pv) > 5) return false;; return true;; }; if (type == 13 && abs(eta) < 2.5) {; if (abs(trackd0pv / tracksigd0pv) > 3) return false;; return true;; }; return false;; }; """"""; # Select events with a muon or electron trigger and with a missing transverse energy larger than 30 GeV; # Find events with exactly one good lepton; # Apply additional cuts in case the lepton is an electron or muon; # Apply luminosity, scale factors and MC weights for simulated events; # Compute transverse mass of the W boson using the lepton and the missing transverse energy and make a histogram; """"""; float ComputeTran",MatchSource.CODE_COMMENT,tutorials/dataframe/df105_WBosonAnalysis.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df105_WBosonAnalysis.py
Deployability,release,release,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## The W boson mass analysis from the ATLAS Open Data release of 2020, with RDataFrame.; ##; ## This tutorial is the analysis of the W boson mass taken from the ATLAS Open Data release in 2020; ## (http://opendata.atlas.cern/release/2020/documentation/). The data was recorded with the ATLAS detector; ## during 2016 at a center-of-mass energy of 13 TeV. W bosons are produced frequently at the LHC and; ## are an important background to studies of Standard Model processes, for example the Higgs boson analyses.; ##; ## The analysis is translated to a RDataFrame workflow processing up to 60 GB of simulated events and data.; ## By default the analysis runs on a preskimmed dataset to reduce the runtime. The full dataset can be used with; ## the --full-dataset argument and you can also run only on a fraction of the original dataset using the argument --lumi-scale.; ##; ## See the [corresponding spec json file](https://github.com/root-project/root/blob/master/tutorials/dataframe/df105_WBosonAnalysis.json).; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date March 2020; ## \author Stefan Wunsch (KIT, CERN); # Argument parsing; # Script; # Notebook; # The preskimmed dataset contains only 0.01 fb^-1; # Create a ROOT dataframe for each dataset; # Note that we load the filenames from the external json file placed in the same folder than this script.; # Construct the dataframes; # Folder name; # Sample name; # Cross-section; # Sum of weights; # Number of events; # Scale down the datasets if requested; # Select events for the analysis; # Just-in-time compile custom helper function performing complex computations; """"""; bool GoodElectronOrMuon(int type, float pt, float eta, float phi, float e, float trackd0pv, float tracksigd0pv, float z0); {; ROOT::Math::PtEtaPhiEVector p(pt / 1000.0, eta, phi, e / 1000.0);; if (abs(z0 * sin(p.theta())) > 0.5) return false;; if (type == 11 && abs(eta) < 2.46 && (abs(eta) <",MatchSource.CODE_COMMENT,tutorials/dataframe/df105_WBosonAnalysis.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df105_WBosonAnalysis.py
Energy Efficiency,energy,energy,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## The W boson mass analysis from the ATLAS Open Data release of 2020, with RDataFrame.; ##; ## This tutorial is the analysis of the W boson mass taken from the ATLAS Open Data release in 2020; ## (http://opendata.atlas.cern/release/2020/documentation/). The data was recorded with the ATLAS detector; ## during 2016 at a center-of-mass energy of 13 TeV. W bosons are produced frequently at the LHC and; ## are an important background to studies of Standard Model processes, for example the Higgs boson analyses.; ##; ## The analysis is translated to a RDataFrame workflow processing up to 60 GB of simulated events and data.; ## By default the analysis runs on a preskimmed dataset to reduce the runtime. The full dataset can be used with; ## the --full-dataset argument and you can also run only on a fraction of the original dataset using the argument --lumi-scale.; ##; ## See the [corresponding spec json file](https://github.com/root-project/root/blob/master/tutorials/dataframe/df105_WBosonAnalysis.json).; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date March 2020; ## \author Stefan Wunsch (KIT, CERN); # Argument parsing; # Script; # Notebook; # The preskimmed dataset contains only 0.01 fb^-1; # Create a ROOT dataframe for each dataset; # Note that we load the filenames from the external json file placed in the same folder than this script.; # Construct the dataframes; # Folder name; # Sample name; # Cross-section; # Sum of weights; # Number of events; # Scale down the datasets if requested; # Select events for the analysis; # Just-in-time compile custom helper function performing complex computations; """"""; bool GoodElectronOrMuon(int type, float pt, float eta, float phi, float e, float trackd0pv, float tracksigd0pv, float z0); {; ROOT::Math::PtEtaPhiEVector p(pt / 1000.0, eta, phi, e / 1000.0);; if (abs(z0 * sin(p.theta())) > 0.5) return false;; if (type == 11 && abs(eta) < 2.46 && (abs(eta) <",MatchSource.CODE_COMMENT,tutorials/dataframe/df105_WBosonAnalysis.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df105_WBosonAnalysis.py
Performance,load,load,"tector; ## during 2016 at a center-of-mass energy of 13 TeV. W bosons are produced frequently at the LHC and; ## are an important background to studies of Standard Model processes, for example the Higgs boson analyses.; ##; ## The analysis is translated to a RDataFrame workflow processing up to 60 GB of simulated events and data.; ## By default the analysis runs on a preskimmed dataset to reduce the runtime. The full dataset can be used with; ## the --full-dataset argument and you can also run only on a fraction of the original dataset using the argument --lumi-scale.; ##; ## See the [corresponding spec json file](https://github.com/root-project/root/blob/master/tutorials/dataframe/df105_WBosonAnalysis.json).; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date March 2020; ## \author Stefan Wunsch (KIT, CERN); # Argument parsing; # Script; # Notebook; # The preskimmed dataset contains only 0.01 fb^-1; # Create a ROOT dataframe for each dataset; # Note that we load the filenames from the external json file placed in the same folder than this script.; # Construct the dataframes; # Folder name; # Sample name; # Cross-section; # Sum of weights; # Number of events; # Scale down the datasets if requested; # Select events for the analysis; # Just-in-time compile custom helper function performing complex computations; """"""; bool GoodElectronOrMuon(int type, float pt, float eta, float phi, float e, float trackd0pv, float tracksigd0pv, float z0); {; ROOT::Math::PtEtaPhiEVector p(pt / 1000.0, eta, phi, e / 1000.0);; if (abs(z0 * sin(p.theta())) > 0.5) return false;; if (type == 11 && abs(eta) < 2.46 && (abs(eta) < 1.37 || abs(eta) > 1.52)) {; if (abs(trackd0pv / tracksigd0pv) > 5) return false;; return true;; }; if (type == 13 && abs(eta) < 2.5) {; if (abs(trackd0pv / tracksigd0pv) > 3) return false;; return true;; }; return false;; }; """"""; # Select events with a muon or electron trigger and with a missing transverse energy larger than 30 GeV; # Find events with ",MatchSource.CODE_COMMENT,tutorials/dataframe/df105_WBosonAnalysis.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df105_WBosonAnalysis.py
Safety,detect,detector,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## The W boson mass analysis from the ATLAS Open Data release of 2020, with RDataFrame.; ##; ## This tutorial is the analysis of the W boson mass taken from the ATLAS Open Data release in 2020; ## (http://opendata.atlas.cern/release/2020/documentation/). The data was recorded with the ATLAS detector; ## during 2016 at a center-of-mass energy of 13 TeV. W bosons are produced frequently at the LHC and; ## are an important background to studies of Standard Model processes, for example the Higgs boson analyses.; ##; ## The analysis is translated to a RDataFrame workflow processing up to 60 GB of simulated events and data.; ## By default the analysis runs on a preskimmed dataset to reduce the runtime. The full dataset can be used with; ## the --full-dataset argument and you can also run only on a fraction of the original dataset using the argument --lumi-scale.; ##; ## See the [corresponding spec json file](https://github.com/root-project/root/blob/master/tutorials/dataframe/df105_WBosonAnalysis.json).; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date March 2020; ## \author Stefan Wunsch (KIT, CERN); # Argument parsing; # Script; # Notebook; # The preskimmed dataset contains only 0.01 fb^-1; # Create a ROOT dataframe for each dataset; # Note that we load the filenames from the external json file placed in the same folder than this script.; # Construct the dataframes; # Folder name; # Sample name; # Cross-section; # Sum of weights; # Number of events; # Scale down the datasets if requested; # Select events for the analysis; # Just-in-time compile custom helper function performing complex computations; """"""; bool GoodElectronOrMuon(int type, float pt, float eta, float phi, float e, float trackd0pv, float tracksigd0pv, float z0); {; ROOT::Math::PtEtaPhiEVector p(pt / 1000.0, eta, phi, e / 1000.0);; if (abs(z0 * sin(p.theta())) > 0.5) return false;; if (type == 11 && abs(eta) < 2.46 && (abs(eta) <",MatchSource.CODE_COMMENT,tutorials/dataframe/df105_WBosonAnalysis.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df105_WBosonAnalysis.py
Availability,error,errors,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## The Higgs to four lepton analysis from the ATLAS Open Data release of 2020, with RDataFrame.; ##; ## This tutorial is the Higgs to four lepton analysis from the ATLAS Open Data release in 2020; ## (http://opendata.atlas.cern/release/2020/documentation/). The data was taken with the ATLAS detector; ## during 2016 at a center-of-mass energy of 13 TeV. The decay of the Standard Model Higgs boson; ## to two Z bosons and subsequently to four leptons is called the ""golden channel"". The selection leads; ## to a narrow invariant mass peak on top a relatively smooth and small background, revealing; ## the Higgs at 125 GeV.; ## Systematic errors for the MC scale factors are computed and the Vary function of RDataFrame is used for plotting.; ## The analysis is translated to an RDataFrame workflow processing about 300 MB of simulated events and data.; ##; ## See the [corresponding spec json file](https://github.com/root-project/root/blob/master/tutorials/dataframe/df106_HiggsToFourLeptons_spec.json).; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date March 2020, August 2022, August 2023; ## \authors Stefan Wunsch (KIT, CERN), Julia Mathe (CERN), Marta Czurylo (CERN); # Enable Multi-threaded mode; # Create the RDataFrame from the spec json file. The df106_HiggsToFourLeptons_spec.json is provided in the same folder as this tutorial; # Creates a single dataframe for all the samples; # Add the ProgressBar feature; # Access metadata information that is stored in the JSON config file of the RDataFrame.; # The metadata contained in the JSON file is accessible within a `DefinePerSample` call, through the `RSampleInfo` class.; # We must further apply an MC correction for the ZZ decay due to missing gg->ZZ processes.; """"""; float scale(unsigned int slot, const ROOT::RDF::RSampleInfo &id){; return id.Contains(""mc_363490.llll.4lep.root"") ? 1.3f : 1.0f;; }; """"""; # Select events for the analysis; """"""; using ROOT",MatchSource.CODE_COMMENT,tutorials/dataframe/df106_HiggsToFourLeptons.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df106_HiggsToFourLeptons.py
Deployability,release,release,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## The Higgs to four lepton analysis from the ATLAS Open Data release of 2020, with RDataFrame.; ##; ## This tutorial is the Higgs to four lepton analysis from the ATLAS Open Data release in 2020; ## (http://opendata.atlas.cern/release/2020/documentation/). The data was taken with the ATLAS detector; ## during 2016 at a center-of-mass energy of 13 TeV. The decay of the Standard Model Higgs boson; ## to two Z bosons and subsequently to four leptons is called the ""golden channel"". The selection leads; ## to a narrow invariant mass peak on top a relatively smooth and small background, revealing; ## the Higgs at 125 GeV.; ## Systematic errors for the MC scale factors are computed and the Vary function of RDataFrame is used for plotting.; ## The analysis is translated to an RDataFrame workflow processing about 300 MB of simulated events and data.; ##; ## See the [corresponding spec json file](https://github.com/root-project/root/blob/master/tutorials/dataframe/df106_HiggsToFourLeptons_spec.json).; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date March 2020, August 2022, August 2023; ## \authors Stefan Wunsch (KIT, CERN), Julia Mathe (CERN), Marta Czurylo (CERN); # Enable Multi-threaded mode; # Create the RDataFrame from the spec json file. The df106_HiggsToFourLeptons_spec.json is provided in the same folder as this tutorial; # Creates a single dataframe for all the samples; # Add the ProgressBar feature; # Access metadata information that is stored in the JSON config file of the RDataFrame.; # The metadata contained in the JSON file is accessible within a `DefinePerSample` call, through the `RSampleInfo` class.; # We must further apply an MC correction for the ZZ decay due to missing gg->ZZ processes.; """"""; float scale(unsigned int slot, const ROOT::RDF::RSampleInfo &id){; return id.Contains(""mc_363490.llll.4lep.root"") ? 1.3f : 1.0f;; }; """"""; # Select events for the analysis; """"""; using ROOT",MatchSource.CODE_COMMENT,tutorials/dataframe/df106_HiggsToFourLeptons.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df106_HiggsToFourLeptons.py
Energy Efficiency,energy,energy,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## The Higgs to four lepton analysis from the ATLAS Open Data release of 2020, with RDataFrame.; ##; ## This tutorial is the Higgs to four lepton analysis from the ATLAS Open Data release in 2020; ## (http://opendata.atlas.cern/release/2020/documentation/). The data was taken with the ATLAS detector; ## during 2016 at a center-of-mass energy of 13 TeV. The decay of the Standard Model Higgs boson; ## to two Z bosons and subsequently to four leptons is called the ""golden channel"". The selection leads; ## to a narrow invariant mass peak on top a relatively smooth and small background, revealing; ## the Higgs at 125 GeV.; ## Systematic errors for the MC scale factors are computed and the Vary function of RDataFrame is used for plotting.; ## The analysis is translated to an RDataFrame workflow processing about 300 MB of simulated events and data.; ##; ## See the [corresponding spec json file](https://github.com/root-project/root/blob/master/tutorials/dataframe/df106_HiggsToFourLeptons_spec.json).; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date March 2020, August 2022, August 2023; ## \authors Stefan Wunsch (KIT, CERN), Julia Mathe (CERN), Marta Czurylo (CERN); # Enable Multi-threaded mode; # Create the RDataFrame from the spec json file. The df106_HiggsToFourLeptons_spec.json is provided in the same folder as this tutorial; # Creates a single dataframe for all the samples; # Add the ProgressBar feature; # Access metadata information that is stored in the JSON config file of the RDataFrame.; # The metadata contained in the JSON file is accessible within a `DefinePerSample` call, through the `RSampleInfo` class.; # We must further apply an MC correction for the ZZ decay due to missing gg->ZZ processes.; """"""; float scale(unsigned int slot, const ROOT::RDF::RSampleInfo &id){; return id.Contains(""mc_363490.llll.4lep.root"") ? 1.3f : 1.0f;; }; """"""; # Select events for the analysis; """"""; using ROOT",MatchSource.CODE_COMMENT,tutorials/dataframe/df106_HiggsToFourLeptons.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df106_HiggsToFourLeptons.py
Integrability,depend,depending,"uons(const RVecI &type, const RVecF &pt, const RVecF &eta, const RVecF &phi, const RVecF &e, const RVecF &trackd0pv, const RVecF &tracksigd0pv, const RVecF &z0); {; for (size_t i = 0; i < type.size(); i++) {; ROOT::Math::PtEtaPhiEVector p(0.001*pt[i], eta[i], phi[i], 0.001*e[i]);; if (type[i] == 11) {; if (pt[i] < 7000 || abs(eta[i]) > 2.47 || abs(trackd0pv[i] / tracksigd0pv[i]) > 5 || abs(z0[i] * sin(p.Theta())) > 0.5) return false;; } else {; if (abs(trackd0pv[i] / tracksigd0pv[i]) > 5 || abs(z0[i] * sin(p.Theta())) > 0.5) return false;; }; }; return true;; }; """"""; # Select electron or muon trigger; # Select events with exactly four good leptons conserving charge and lepton numbers; # Note that all collections are RVecs and good_lep is the mask for the good leptons.; # The lepton types are PDG numbers and set to 11 or 13 for an electron or muon; # irrespective of the charge.; # Apply additional cuts depending on lepton flavour; # Create new columns with the kinematics of good leptons; # Select leptons with high transverse momentum; # Reweighting of the samples is different for ""data"" and ""MC"". This is the function to add reweighting for MC samples; """"""; double weights(float scaleFactor_1, float scaleFactor_2, float scaleFactor_3, float scaleFactor_4, float scale, float mcWeight, double xsecs, double sumws, double lumi); {; return scaleFactor_1 * scaleFactor_2 * scaleFactor_3 * scaleFactor_4 * scale * mcWeight * xsecs / sumws * lumi;; }; """"""; # Use DefinePerSample to define which samples are MC and hence need reweighting; # Compute invariant mass of the four lepton system; """"""; float ComputeInvariantMass(RVecF pt, RVecF eta, RVecF phi, RVecF e); {; ROOT::Math::PtEtaPhiEVector p1{pt[0], eta[0], phi[0], e[0]};; ROOT::Math::PtEtaPhiEVector p2{pt[1], eta[1], phi[1], e[1]};; ROOT::Math::PtEtaPhiEVector p3{pt[2], eta[2], phi[2], e[2]};; ROOT::Math::PtEtaPhiEVector p4{pt[3], eta[3], phi[3], e[3]};; return 0.001 * (p1 + p2 + p3 + p4).M();; }; """"""; # Save data for statistica",MatchSource.CODE_COMMENT,tutorials/dataframe/df106_HiggsToFourLeptons.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df106_HiggsToFourLeptons.py
Modifiability,config,config,"and subsequently to four leptons is called the ""golden channel"". The selection leads; ## to a narrow invariant mass peak on top a relatively smooth and small background, revealing; ## the Higgs at 125 GeV.; ## Systematic errors for the MC scale factors are computed and the Vary function of RDataFrame is used for plotting.; ## The analysis is translated to an RDataFrame workflow processing about 300 MB of simulated events and data.; ##; ## See the [corresponding spec json file](https://github.com/root-project/root/blob/master/tutorials/dataframe/df106_HiggsToFourLeptons_spec.json).; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date March 2020, August 2022, August 2023; ## \authors Stefan Wunsch (KIT, CERN), Julia Mathe (CERN), Marta Czurylo (CERN); # Enable Multi-threaded mode; # Create the RDataFrame from the spec json file. The df106_HiggsToFourLeptons_spec.json is provided in the same folder as this tutorial; # Creates a single dataframe for all the samples; # Add the ProgressBar feature; # Access metadata information that is stored in the JSON config file of the RDataFrame.; # The metadata contained in the JSON file is accessible within a `DefinePerSample` call, through the `RSampleInfo` class.; # We must further apply an MC correction for the ZZ decay due to missing gg->ZZ processes.; """"""; float scale(unsigned int slot, const ROOT::RDF::RSampleInfo &id){; return id.Contains(""mc_363490.llll.4lep.root"") ? 1.3f : 1.0f;; }; """"""; # Select events for the analysis; """"""; using ROOT::RVecF;; using ROOT::RVecI;; bool GoodElectronsAndMuons(const RVecI &type, const RVecF &pt, const RVecF &eta, const RVecF &phi, const RVecF &e, const RVecF &trackd0pv, const RVecF &tracksigd0pv, const RVecF &z0); {; for (size_t i = 0; i < type.size(); i++) {; ROOT::Math::PtEtaPhiEVector p(0.001*pt[i], eta[i], phi[i], 0.001*e[i]);; if (type[i] == 11) {; if (pt[i] < 7000 || abs(eta[i]) > 2.47 || abs(trackd0pv[i] / tracksigd0pv[i]) > 5 || abs(z0[i] * sin(p.Theta())) > 0.5) retu",MatchSource.CODE_COMMENT,tutorials/dataframe/df106_HiggsToFourLeptons.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df106_HiggsToFourLeptons.py
Performance,perform,performed,"ated in https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/MUON-2018-03/.; # Electrons uncertainties are evaluated based on the plots available in https://doi.org/10.48550/arXiv.1908.00005.; # The uncertainties are linearly interpolated, using the `TGraph::Eval()` method, to cover a range of pT values covered by the analysis.; # Create a VaryHelper to interpolate the available data.; """"""; using namespace ROOT::VecOps;. class VaryHelper; {; const std::vector<double> x{5.50e3, 5.52e3, 12.54e3, 17.43e3, 22.40e3, 27.48e3, 30e3, 10000e3};; const std::vector<double> y{0.06628, 0.06395, 0.06396, 0.03372, 0.02441, 0.01403, 0, 0};; TGraph graph;. public:; VaryHelper() : graph(x.size(), x.data(), y.data()) {}; RVec<double> operator()(const double &w, const RVecF &pt, const RVec<unsigned int> &type); {; const auto v = Mean(Map(pt[type == 11], [this](auto p); {return this->graph.Eval(p); }); );; return RVec<double>{(1 + v) * w, (1 - v) * w};; }; };. VaryHelper variationsFactory;; """"""; # Use the Vary method to add the systematic variations to the total MC scale factor (""weight"") of the analysis.; # We reached the end of the analysis part. We now evaluate the total MC uncertainty based on the variations.; # No computation graph was triggered yet, we trigger the computation graph for all histograms at once now,; # by calling 'histos_mc[""nominal""].GetXaxis()'.; # Note, in this case the uncertainties are symmetric.; # Make the plot of the data, individual MC contributions and the total MC scale factor systematic variations.; # Set styles; # Create canvas with pad; # Draw stack with MC contributions; # Retrieve values of the data and MC histograms in order to plot them.; # Draw cloned histograms to preserve graphics when original objects goes out of scope; # Note: GetValue() action operation is performed after all lazy actions of the RDF were defined first.; # Draw MC scale factor and variations; # Draw data histogram; # Draw raw data with errorbars; # Add legend; # Save the plot",MatchSource.CODE_COMMENT,tutorials/dataframe/df106_HiggsToFourLeptons.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df106_HiggsToFourLeptons.py
Safety,detect,detector,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## The Higgs to four lepton analysis from the ATLAS Open Data release of 2020, with RDataFrame.; ##; ## This tutorial is the Higgs to four lepton analysis from the ATLAS Open Data release in 2020; ## (http://opendata.atlas.cern/release/2020/documentation/). The data was taken with the ATLAS detector; ## during 2016 at a center-of-mass energy of 13 TeV. The decay of the Standard Model Higgs boson; ## to two Z bosons and subsequently to four leptons is called the ""golden channel"". The selection leads; ## to a narrow invariant mass peak on top a relatively smooth and small background, revealing; ## the Higgs at 125 GeV.; ## Systematic errors for the MC scale factors are computed and the Vary function of RDataFrame is used for plotting.; ## The analysis is translated to an RDataFrame workflow processing about 300 MB of simulated events and data.; ##; ## See the [corresponding spec json file](https://github.com/root-project/root/blob/master/tutorials/dataframe/df106_HiggsToFourLeptons_spec.json).; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date March 2020, August 2022, August 2023; ## \authors Stefan Wunsch (KIT, CERN), Julia Mathe (CERN), Marta Czurylo (CERN); # Enable Multi-threaded mode; # Create the RDataFrame from the spec json file. The df106_HiggsToFourLeptons_spec.json is provided in the same folder as this tutorial; # Creates a single dataframe for all the samples; # Add the ProgressBar feature; # Access metadata information that is stored in the JSON config file of the RDataFrame.; # The metadata contained in the JSON file is accessible within a `DefinePerSample` call, through the `RSampleInfo` class.; # We must further apply an MC correction for the ZZ decay due to missing gg->ZZ processes.; """"""; float scale(unsigned int slot, const ROOT::RDF::RSampleInfo &id){; return id.Contains(""mc_363490.llll.4lep.root"") ? 1.3f : 1.0f;; }; """"""; # Select events for the analysis; """"""; using ROOT",MatchSource.CODE_COMMENT,tutorials/dataframe/df106_HiggsToFourLeptons.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df106_HiggsToFourLeptons.py
Security,access,accessible,"vealing; ## the Higgs at 125 GeV.; ## Systematic errors for the MC scale factors are computed and the Vary function of RDataFrame is used for plotting.; ## The analysis is translated to an RDataFrame workflow processing about 300 MB of simulated events and data.; ##; ## See the [corresponding spec json file](https://github.com/root-project/root/blob/master/tutorials/dataframe/df106_HiggsToFourLeptons_spec.json).; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date March 2020, August 2022, August 2023; ## \authors Stefan Wunsch (KIT, CERN), Julia Mathe (CERN), Marta Czurylo (CERN); # Enable Multi-threaded mode; # Create the RDataFrame from the spec json file. The df106_HiggsToFourLeptons_spec.json is provided in the same folder as this tutorial; # Creates a single dataframe for all the samples; # Add the ProgressBar feature; # Access metadata information that is stored in the JSON config file of the RDataFrame.; # The metadata contained in the JSON file is accessible within a `DefinePerSample` call, through the `RSampleInfo` class.; # We must further apply an MC correction for the ZZ decay due to missing gg->ZZ processes.; """"""; float scale(unsigned int slot, const ROOT::RDF::RSampleInfo &id){; return id.Contains(""mc_363490.llll.4lep.root"") ? 1.3f : 1.0f;; }; """"""; # Select events for the analysis; """"""; using ROOT::RVecF;; using ROOT::RVecI;; bool GoodElectronsAndMuons(const RVecI &type, const RVecF &pt, const RVecF &eta, const RVecF &phi, const RVecF &e, const RVecF &trackd0pv, const RVecF &tracksigd0pv, const RVecF &z0); {; for (size_t i = 0; i < type.size(); i++) {; ROOT::Math::PtEtaPhiEVector p(0.001*pt[i], eta[i], phi[i], 0.001*e[i]);; if (type[i] == 11) {; if (pt[i] < 7000 || abs(eta[i]) > 2.47 || abs(trackd0pv[i] / tracksigd0pv[i]) > 5 || abs(z0[i] * sin(p.Theta())) > 0.5) return false;; } else {; if (abs(trackd0pv[i] / tracksigd0pv[i]) > 5 || abs(z0[i] * sin(p.Theta())) > 0.5) return false;; }; }; return true;; }; """"""; # Select electron or muon t",MatchSource.CODE_COMMENT,tutorials/dataframe/df106_HiggsToFourLeptons.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df106_HiggsToFourLeptons.py
Availability,down,down,"dataset to reduce the runtime. The full dataset can be used with; ## the --full-dataset argument and you can also run only on a fraction of the original dataset using the argument --lumi-scale.; ##; ## See the [corresponding spec json file](https://github.com/root-project/root/blob/master/tutorials/dataframe/df107_SingleTopAnalysis.json).; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date July 2020; ## \author Stefan Wunsch (KIT, CERN); # Argument parsing; # Script; # Notebook; # The preskimmed dataset contains only 0.5 fb^-1; # Create a ROOT dataframe for each dataset; # Note that we load the filenames from the external json file placed in the same folder than this script.; # Construct the dataframes; # Folder name; # Sample name; # Cross-section; # Sum of weights; # Number of events; # Scale down the datasets if requested; # Select events for the analysis and make histograms of the top mass; # Just-in-time compile custom helper function performing complex computations; """"""; using cRVecF = const ROOT::RVecF &;; using cRVecI = const ROOT::RVecI &;; int FindGoodLepton(cRVecI goodlep, cRVecI type, cRVecF lep_pt, cRVecF lep_eta, cRVecF lep_phi, cRVecF lep_e, cRVecF trackd0pv, cRVecF tracksigd0pv, cRVecF z0); {; int idx = -1; // Return -1 if no good lepton is found.; for(auto i = 0; i < type.size(); i++) {; if(!goodlep[i]) continue;; if (type[i] == 11 && abs(lep_eta[i]) < 2.47 && (abs(lep_eta[i]) < 1.37 || abs(lep_eta[i]) > 1.52) && abs(trackd0pv[i] / tracksigd0pv[i]) < 5) {; const ROOT::Math::PtEtaPhiEVector p(lep_pt[i], lep_eta[i], lep_phi[i], lep_e[i]);; if (abs(z0[i] * sin(p.Theta())) < 0.5) {; if (idx == -1) idx = i;; else return -1; // Accept only events with exactly one good lepton; }; }; if (type[i] == 13 && abs(lep_eta[i]) < 2.5 && abs(trackd0pv[i] / tracksigd0pv[i]) < 3) {; const ROOT::Math::PtEtaPhiEVector p(lep_pt[i], lep_eta[i], lep_phi[i], lep_e[i]);; if (abs(z0[i] * sin(p.Theta())) < 0.5) {; if (idx == -1) idx = i;; else return -1; // Ac",MatchSource.CODE_COMMENT,tutorials/dataframe/df107_SingleTopAnalysis.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df107_SingleTopAnalysis.py
Deployability,release,release,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## A single top analysis using the ATLAS Open Data release of 2020, with RDataFrame.; ##; ## This tutorial is the analysis of single top production adapted from the ATLAS Open Data release in 2020; ## (http://opendata.atlas.cern/release/2020/documentation/). The data was recorded with the ATLAS detector; ## during 2016 at a center-of-mass energy of 13 TeV. Top quarks with a mass of about 172 GeV are mostly; ## produced in pairs but also appear alone, dominantly from the decays of a W boson in association with a light jet.; ##; ## The analysis is translated to a RDataFrame workflow processing up to 60 GB of simulated events and data.; ## By default the analysis runs on a preskimmed dataset to reduce the runtime. The full dataset can be used with; ## the --full-dataset argument and you can also run only on a fraction of the original dataset using the argument --lumi-scale.; ##; ## See the [corresponding spec json file](https://github.com/root-project/root/blob/master/tutorials/dataframe/df107_SingleTopAnalysis.json).; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date July 2020; ## \author Stefan Wunsch (KIT, CERN); # Argument parsing; # Script; # Notebook; # The preskimmed dataset contains only 0.5 fb^-1; # Create a ROOT dataframe for each dataset; # Note that we load the filenames from the external json file placed in the same folder than this script.; # Construct the dataframes; # Folder name; # Sample name; # Cross-section; # Sum of weights; # Number of events; # Scale down the datasets if requested; # Select events for the analysis and make histograms of the top mass; # Just-in-time compile custom helper function performing complex computations; """"""; using cRVecF = const ROOT::RVecF &;; using cRVecI = const ROOT::RVecI &;; int FindGoodLepton(cRVecI goodlep, cRVecI type, cRVecF lep_pt, cRVecF lep_eta, cRVecF lep_phi, cRVecF lep_e, cRVecF trackd0pv, cRVecF tracksigd0pv, cRVecF z0); {; int",MatchSource.CODE_COMMENT,tutorials/dataframe/df107_SingleTopAnalysis.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df107_SingleTopAnalysis.py
Energy Efficiency,adapt,adapted,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## A single top analysis using the ATLAS Open Data release of 2020, with RDataFrame.; ##; ## This tutorial is the analysis of single top production adapted from the ATLAS Open Data release in 2020; ## (http://opendata.atlas.cern/release/2020/documentation/). The data was recorded with the ATLAS detector; ## during 2016 at a center-of-mass energy of 13 TeV. Top quarks with a mass of about 172 GeV are mostly; ## produced in pairs but also appear alone, dominantly from the decays of a W boson in association with a light jet.; ##; ## The analysis is translated to a RDataFrame workflow processing up to 60 GB of simulated events and data.; ## By default the analysis runs on a preskimmed dataset to reduce the runtime. The full dataset can be used with; ## the --full-dataset argument and you can also run only on a fraction of the original dataset using the argument --lumi-scale.; ##; ## See the [corresponding spec json file](https://github.com/root-project/root/blob/master/tutorials/dataframe/df107_SingleTopAnalysis.json).; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date July 2020; ## \author Stefan Wunsch (KIT, CERN); # Argument parsing; # Script; # Notebook; # The preskimmed dataset contains only 0.5 fb^-1; # Create a ROOT dataframe for each dataset; # Note that we load the filenames from the external json file placed in the same folder than this script.; # Construct the dataframes; # Folder name; # Sample name; # Cross-section; # Sum of weights; # Number of events; # Scale down the datasets if requested; # Select events for the analysis and make histograms of the top mass; # Just-in-time compile custom helper function performing complex computations; """"""; using cRVecF = const ROOT::RVecF &;; using cRVecI = const ROOT::RVecI &;; int FindGoodLepton(cRVecI goodlep, cRVecI type, cRVecF lep_pt, cRVecF lep_eta, cRVecF lep_phi, cRVecF lep_e, cRVecF trackd0pv, cRVecF tracksigd0pv, cRVecF z0); {; int",MatchSource.CODE_COMMENT,tutorials/dataframe/df107_SingleTopAnalysis.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df107_SingleTopAnalysis.py
Modifiability,adapt,adapted,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## A single top analysis using the ATLAS Open Data release of 2020, with RDataFrame.; ##; ## This tutorial is the analysis of single top production adapted from the ATLAS Open Data release in 2020; ## (http://opendata.atlas.cern/release/2020/documentation/). The data was recorded with the ATLAS detector; ## during 2016 at a center-of-mass energy of 13 TeV. Top quarks with a mass of about 172 GeV are mostly; ## produced in pairs but also appear alone, dominantly from the decays of a W boson in association with a light jet.; ##; ## The analysis is translated to a RDataFrame workflow processing up to 60 GB of simulated events and data.; ## By default the analysis runs on a preskimmed dataset to reduce the runtime. The full dataset can be used with; ## the --full-dataset argument and you can also run only on a fraction of the original dataset using the argument --lumi-scale.; ##; ## See the [corresponding spec json file](https://github.com/root-project/root/blob/master/tutorials/dataframe/df107_SingleTopAnalysis.json).; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date July 2020; ## \author Stefan Wunsch (KIT, CERN); # Argument parsing; # Script; # Notebook; # The preskimmed dataset contains only 0.5 fb^-1; # Create a ROOT dataframe for each dataset; # Note that we load the filenames from the external json file placed in the same folder than this script.; # Construct the dataframes; # Folder name; # Sample name; # Cross-section; # Sum of weights; # Number of events; # Scale down the datasets if requested; # Select events for the analysis and make histograms of the top mass; # Just-in-time compile custom helper function performing complex computations; """"""; using cRVecF = const ROOT::RVecF &;; using cRVecI = const ROOT::RVecI &;; int FindGoodLepton(cRVecI goodlep, cRVecI type, cRVecF lep_pt, cRVecF lep_eta, cRVecF lep_phi, cRVecF lep_e, cRVecF trackd0pv, cRVecF tracksigd0pv, cRVecF z0); {; int",MatchSource.CODE_COMMENT,tutorials/dataframe/df107_SingleTopAnalysis.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df107_SingleTopAnalysis.py
Performance,load,load,"ring 2016 at a center-of-mass energy of 13 TeV. Top quarks with a mass of about 172 GeV are mostly; ## produced in pairs but also appear alone, dominantly from the decays of a W boson in association with a light jet.; ##; ## The analysis is translated to a RDataFrame workflow processing up to 60 GB of simulated events and data.; ## By default the analysis runs on a preskimmed dataset to reduce the runtime. The full dataset can be used with; ## the --full-dataset argument and you can also run only on a fraction of the original dataset using the argument --lumi-scale.; ##; ## See the [corresponding spec json file](https://github.com/root-project/root/blob/master/tutorials/dataframe/df107_SingleTopAnalysis.json).; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date July 2020; ## \author Stefan Wunsch (KIT, CERN); # Argument parsing; # Script; # Notebook; # The preskimmed dataset contains only 0.5 fb^-1; # Create a ROOT dataframe for each dataset; # Note that we load the filenames from the external json file placed in the same folder than this script.; # Construct the dataframes; # Folder name; # Sample name; # Cross-section; # Sum of weights; # Number of events; # Scale down the datasets if requested; # Select events for the analysis and make histograms of the top mass; # Just-in-time compile custom helper function performing complex computations; """"""; using cRVecF = const ROOT::RVecF &;; using cRVecI = const ROOT::RVecI &;; int FindGoodLepton(cRVecI goodlep, cRVecI type, cRVecF lep_pt, cRVecF lep_eta, cRVecF lep_phi, cRVecF lep_e, cRVecF trackd0pv, cRVecF tracksigd0pv, cRVecF z0); {; int idx = -1; // Return -1 if no good lepton is found.; for(auto i = 0; i < type.size(); i++) {; if(!goodlep[i]) continue;; if (type[i] == 11 && abs(lep_eta[i]) < 2.47 && (abs(lep_eta[i]) < 1.37 || abs(lep_eta[i]) > 1.52) && abs(trackd0pv[i] / tracksigd0pv[i]) < 5) {; const ROOT::Math::PtEtaPhiEVector p(lep_pt[i], lep_eta[i], lep_phi[i], lep_e[i]);; if (abs(z0[i] * sin(p.T",MatchSource.CODE_COMMENT,tutorials/dataframe/df107_SingleTopAnalysis.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df107_SingleTopAnalysis.py
Safety,detect,detector,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## A single top analysis using the ATLAS Open Data release of 2020, with RDataFrame.; ##; ## This tutorial is the analysis of single top production adapted from the ATLAS Open Data release in 2020; ## (http://opendata.atlas.cern/release/2020/documentation/). The data was recorded with the ATLAS detector; ## during 2016 at a center-of-mass energy of 13 TeV. Top quarks with a mass of about 172 GeV are mostly; ## produced in pairs but also appear alone, dominantly from the decays of a W boson in association with a light jet.; ##; ## The analysis is translated to a RDataFrame workflow processing up to 60 GB of simulated events and data.; ## By default the analysis runs on a preskimmed dataset to reduce the runtime. The full dataset can be used with; ## the --full-dataset argument and you can also run only on a fraction of the original dataset using the argument --lumi-scale.; ##; ## See the [corresponding spec json file](https://github.com/root-project/root/blob/master/tutorials/dataframe/df107_SingleTopAnalysis.json).; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date July 2020; ## \author Stefan Wunsch (KIT, CERN); # Argument parsing; # Script; # Notebook; # The preskimmed dataset contains only 0.5 fb^-1; # Create a ROOT dataframe for each dataset; # Note that we load the filenames from the external json file placed in the same folder than this script.; # Construct the dataframes; # Folder name; # Sample name; # Cross-section; # Sum of weights; # Number of events; # Scale down the datasets if requested; # Select events for the analysis and make histograms of the top mass; # Just-in-time compile custom helper function performing complex computations; """"""; using cRVecF = const ROOT::RVecF &;; using cRVecI = const ROOT::RVecI &;; int FindGoodLepton(cRVecI goodlep, cRVecI type, cRVecF lep_pt, cRVecF lep_eta, cRVecF lep_phi, cRVecF lep_e, cRVecF trackd0pv, cRVecF tracksigd0pv, cRVecF z0); {; int",MatchSource.CODE_COMMENT,tutorials/dataframe/df107_SingleTopAnalysis.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df107_SingleTopAnalysis.py
Security,validat,validation,"ction of highly isolated leptons; # Find a single good lepton, otherwise return -1 as index; # Compute transverse mass of the W boson using the missing transverse energy and the good lepton; # Use only events with a transverse mass of the reconstructed W boson larger than 60 GeV; # Perform preselection of jets; # Select events with two good jets and one b-jet and find the indices in the collections; # Select events based on the jet kinematics and the scalar sum of the transverse momentum; # from the lepton, jets and met above 195 GeV; # Compute luminosity, scale factors and MC weights for simulated events; # The single top MC weights are either 1 or -1; # Reconstruct the top mass from the lepton, the missing transverse energy and the b-jet; # Just-in-time compile the function to compute the top mass from the constituents; """"""; float ComputeTopMass(float lep_pt, float lep_eta, float lep_phi, float lep_e, float jet_pt, float jet_eta, float jet_phi, float jet_e, float met_et, float met_phi); {; const ROOT::Math::PtEtaPhiEVector lep(lep_pt / 1000.0, lep_eta, lep_phi, lep_e / 1000.0);; const ROOT::Math::PtEtaPhiEVector met(met_et / 1000.0, 0, met_phi, met_et / 1000.0);; const ROOT::Math::PtEtaPhiEVector bjet(jet_pt / 1000.0, jet_eta, jet_phi, jet_e / 1000.0);; // Please note that we treat here the missing transverse energy as the neutrino, even though the z component is missing!; return (lep + met + bjet).M();; }; """"""; # Run the event loop and merge histograms of the respective processes; # RunGraphs allows to run the event loops of the separate RDataFrame graphs; # concurrently. This results in an improved usage of the available resources; # if each separate RDataFrame can not utilize all available resources, e.g.,; # because not enough data is available.; # Create the plot; # Set styles; # Create canvas with pad; # Draw stack with MC contributions; # Corrected normalization derived from a validation region; # Draw data; # Add legend; # Add ATLAS label; # Save the plot",MatchSource.CODE_COMMENT,tutorials/dataframe/df107_SingleTopAnalysis.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/df107_SingleTopAnalysis.py
Availability,avail,available,"d a; ## SparkContext object created with the desired options. After this initial; ## setup, an RDataFrame with distributed capabilities is created and connected; ## to the SparkContext instance. Finally, a couple of histograms are drawn from; ## the created columns in the dataset.; ##; ## \macro_code; ## \macro_image; ##; ## \date March 2021; ## \author Vincenzo Eduardo Padulano; # Point RDataFrame calls to Spark RDataFrame object; # Setup the connection to Spark; # First create a dictionary with keys representing Spark specific configuration; # parameters. In this tutorial we use the following configuration parameters:; #; # 1. spark.app.name: The name of the Spark application; # 2. spark.master: The Spark endpoint responsible for running the; # application. With the syntax ""local[2]"" we signal Spark we want to run; # locally on the same machine with 2 cores, each running a separate; # process. The default behaviour of a Spark application would run; # locally on the same machine with as many concurrent processes as; # available cores, that could be also written as ""local[*]"".; #; # If you have access to a remote cluster you should substitute the endpoint URL; # of your Spark master in the form ""spark://HOST:PORT"" in the value of; # `spark.master`. Depending on the availability of your cluster you may request; # more computing nodes or cores per node with a similar configuration:; #; # sparkconf = pyspark.SparkConf().setAll(; # {""spark.master"": ""spark://HOST:PORT"",; # ""spark.executor.instances"": <number_of_nodes>,; # ""spark.executor.cores"" <cores_per_node>,}.items()); #; # You can find all configuration options and more details in the official Spark; # documentation at https://spark.apache.org/docs/latest/configuration.html .; # Create a SparkConf object with all the desired Spark configuration parameters; # Create a SparkContext with the configuration stored in `sparkconf`; # Create an RDataFrame that will use Spark as a backend for computations; # Set the random se",MatchSource.CODE_COMMENT,tutorials/dataframe/distrdf001_spark_connection.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/distrdf001_spark_connection.py
Deployability,configurat,configuration,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## Configure a Spark connection and fill two histograms distributedly.; ##; ## This tutorial shows the ingredients needed to setup the connection to a Spark; ## cluster, namely a SparkConf object holding configuration parameters and a; ## SparkContext object created with the desired options. After this initial; ## setup, an RDataFrame with distributed capabilities is created and connected; ## to the SparkContext instance. Finally, a couple of histograms are drawn from; ## the created columns in the dataset.; ##; ## \macro_code; ## \macro_image; ##; ## \date March 2021; ## \author Vincenzo Eduardo Padulano; # Point RDataFrame calls to Spark RDataFrame object; # Setup the connection to Spark; # First create a dictionary with keys representing Spark specific configuration; # parameters. In this tutorial we use the following configuration parameters:; #; # 1. spark.app.name: The name of the Spark application; # 2. spark.master: The Spark endpoint responsible for running the; # application. With the syntax ""local[2]"" we signal Spark we want to run; # locally on the same machine with 2 cores, each running a separate; # process. The default behaviour of a Spark application would run; # locally on the same machine with as many concurrent processes as; # available cores, that could be also written as ""local[*]"".; #; # If you have access to a remote cluster you should substitute the endpoint URL; # of your Spark master in the form ""spark://HOST:PORT"" in the value of; # `spark.master`. Depending on the availability of your cluster you may request; # more computing nodes or cores per node with a similar configuration:; #; # sparkconf = pyspark.SparkConf().setAll(; # {""spark.master"": ""spark://HOST:PORT"",; # ""spark.executor.instances"": <number_of_nodes>,; # ""spark.executor.cores"" <cores_per_node>,}.items()); #; # You can find all configuration options and more details in the official Spark; # documentation at https://sp",MatchSource.CODE_COMMENT,tutorials/dataframe/distrdf001_spark_connection.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/distrdf001_spark_connection.py
Modifiability,config,configuration,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## Configure a Spark connection and fill two histograms distributedly.; ##; ## This tutorial shows the ingredients needed to setup the connection to a Spark; ## cluster, namely a SparkConf object holding configuration parameters and a; ## SparkContext object created with the desired options. After this initial; ## setup, an RDataFrame with distributed capabilities is created and connected; ## to the SparkContext instance. Finally, a couple of histograms are drawn from; ## the created columns in the dataset.; ##; ## \macro_code; ## \macro_image; ##; ## \date March 2021; ## \author Vincenzo Eduardo Padulano; # Point RDataFrame calls to Spark RDataFrame object; # Setup the connection to Spark; # First create a dictionary with keys representing Spark specific configuration; # parameters. In this tutorial we use the following configuration parameters:; #; # 1. spark.app.name: The name of the Spark application; # 2. spark.master: The Spark endpoint responsible for running the; # application. With the syntax ""local[2]"" we signal Spark we want to run; # locally on the same machine with 2 cores, each running a separate; # process. The default behaviour of a Spark application would run; # locally on the same machine with as many concurrent processes as; # available cores, that could be also written as ""local[*]"".; #; # If you have access to a remote cluster you should substitute the endpoint URL; # of your Spark master in the form ""spark://HOST:PORT"" in the value of; # `spark.master`. Depending on the availability of your cluster you may request; # more computing nodes or cores per node with a similar configuration:; #; # sparkconf = pyspark.SparkConf().setAll(; # {""spark.master"": ""spark://HOST:PORT"",; # ""spark.executor.instances"": <number_of_nodes>,; # ""spark.executor.cores"" <cores_per_node>,}.items()); #; # You can find all configuration options and more details in the official Spark; # documentation at https://sp",MatchSource.CODE_COMMENT,tutorials/dataframe/distrdf001_spark_connection.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/distrdf001_spark_connection.py
Performance,concurren,concurrent,"d a; ## SparkContext object created with the desired options. After this initial; ## setup, an RDataFrame with distributed capabilities is created and connected; ## to the SparkContext instance. Finally, a couple of histograms are drawn from; ## the created columns in the dataset.; ##; ## \macro_code; ## \macro_image; ##; ## \date March 2021; ## \author Vincenzo Eduardo Padulano; # Point RDataFrame calls to Spark RDataFrame object; # Setup the connection to Spark; # First create a dictionary with keys representing Spark specific configuration; # parameters. In this tutorial we use the following configuration parameters:; #; # 1. spark.app.name: The name of the Spark application; # 2. spark.master: The Spark endpoint responsible for running the; # application. With the syntax ""local[2]"" we signal Spark we want to run; # locally on the same machine with 2 cores, each running a separate; # process. The default behaviour of a Spark application would run; # locally on the same machine with as many concurrent processes as; # available cores, that could be also written as ""local[*]"".; #; # If you have access to a remote cluster you should substitute the endpoint URL; # of your Spark master in the form ""spark://HOST:PORT"" in the value of; # `spark.master`. Depending on the availability of your cluster you may request; # more computing nodes or cores per node with a similar configuration:; #; # sparkconf = pyspark.SparkConf().setAll(; # {""spark.master"": ""spark://HOST:PORT"",; # ""spark.executor.instances"": <number_of_nodes>,; # ""spark.executor.cores"" <cores_per_node>,}.items()); #; # You can find all configuration options and more details in the official Spark; # documentation at https://spark.apache.org/docs/latest/configuration.html .; # Create a SparkConf object with all the desired Spark configuration parameters; # Create a SparkContext with the configuration stored in `sparkconf`; # Create an RDataFrame that will use Spark as a backend for computations; # Set the random se",MatchSource.CODE_COMMENT,tutorials/dataframe/distrdf001_spark_connection.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/distrdf001_spark_connection.py
Security,access,access," the SparkContext instance. Finally, a couple of histograms are drawn from; ## the created columns in the dataset.; ##; ## \macro_code; ## \macro_image; ##; ## \date March 2021; ## \author Vincenzo Eduardo Padulano; # Point RDataFrame calls to Spark RDataFrame object; # Setup the connection to Spark; # First create a dictionary with keys representing Spark specific configuration; # parameters. In this tutorial we use the following configuration parameters:; #; # 1. spark.app.name: The name of the Spark application; # 2. spark.master: The Spark endpoint responsible for running the; # application. With the syntax ""local[2]"" we signal Spark we want to run; # locally on the same machine with 2 cores, each running a separate; # process. The default behaviour of a Spark application would run; # locally on the same machine with as many concurrent processes as; # available cores, that could be also written as ""local[*]"".; #; # If you have access to a remote cluster you should substitute the endpoint URL; # of your Spark master in the form ""spark://HOST:PORT"" in the value of; # `spark.master`. Depending on the availability of your cluster you may request; # more computing nodes or cores per node with a similar configuration:; #; # sparkconf = pyspark.SparkConf().setAll(; # {""spark.master"": ""spark://HOST:PORT"",; # ""spark.executor.instances"": <number_of_nodes>,; # ""spark.executor.cores"" <cores_per_node>,}.items()); #; # You can find all configuration options and more details in the official Spark; # documentation at https://spark.apache.org/docs/latest/configuration.html .; # Create a SparkConf object with all the desired Spark configuration parameters; # Create a SparkContext with the configuration stored in `sparkconf`; # Create an RDataFrame that will use Spark as a backend for computations; # Set the random seed and define two columns of the dataset with random numbers.; # Book an histogram for each column; # Plot the histograms side by side on a canvas; # Save the canvas",MatchSource.CODE_COMMENT,tutorials/dataframe/distrdf001_spark_connection.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/distrdf001_spark_connection.py
Availability,avail,available,"pts; directly the object previously created. In case the cluster was setup; externally, you need to provide an endpoint URL to the client, e.g.; 'https://myscheduler.domain:8786'. Through Dask, you can connect to various types of cluster resources. For; example, you can connect together a set of machines through SSH and use them; to run your computations. This is done through the `SSHCluster` class. For; example:. ```python; from dask.distributed import SSHCluster; cluster = SSHCluster(; # A list with machine host names, the first name will be used as; # scheduler, following names will become workers.; hosts=[""machine1"",""machine2"",""machine3""],; # A dictionary of options for each worker node, here we set the number; # of cores to be used on each node.; worker_options={""nprocs"":4,},; ); ```. Another common usecase is interfacing Dask to a batch system like HTCondor or; Slurm. A separate package called dask-jobqueue (https://jobqueue.dask.org); extends the available Dask cluster classes to enable running Dask computations; as batch jobs. In this case, the cluster object usually receives the parameters; that would be written in the job description file. For example:. ```python; from dask_jobqueue import HTCondorCluster; cluster = HTCondorCluster(; cores=1,; memory='2000MB',; disk='1000MB',; ); # Use the scale method to send as many jobs as needed; cluster.scale(4); ```. In this tutorial, a cluster object is created for the local machine, using; multiprocessing (processes=True) on 2 workers (n_workers=2) each using only; 1 core (threads_per_worker=1) and 2GiB of RAM (memory_limit=""2GiB"").; """"""; # This tutorial uses Python multiprocessing, so the creation of the cluster; # needs to be wrapped in the main clause as described in the Python docs; # https://docs.python.org/3/library/multiprocessing.html; # Create the connection to the mock Dask cluster on the local machine; # Create an RDataFrame that will use Dask as a backend for computations; # Set the random seed and defi",MatchSource.CODE_COMMENT,tutorials/dataframe/distrdf002_dask_connection.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/distrdf002_dask_connection.py
Energy Efficiency,schedul,scheduler,"posal. To use; only the local machine (e.g. your laptop), a `LocalCluster` object can be; used. This step can be skipped if you have access to an existing Dask; cluster; in that case, the cluster administrator should provide you with a; URL to connect to the cluster in step 2. More options for cluster creation; can be found in the Dask docs at; http://distributed.dask.org/en/stable/api.html#cluster .; 2. Creating a Dask client object that connects to the cluster. This accepts; directly the object previously created. In case the cluster was setup; externally, you need to provide an endpoint URL to the client, e.g.; 'https://myscheduler.domain:8786'. Through Dask, you can connect to various types of cluster resources. For; example, you can connect together a set of machines through SSH and use them; to run your computations. This is done through the `SSHCluster` class. For; example:. ```python; from dask.distributed import SSHCluster; cluster = SSHCluster(; # A list with machine host names, the first name will be used as; # scheduler, following names will become workers.; hosts=[""machine1"",""machine2"",""machine3""],; # A dictionary of options for each worker node, here we set the number; # of cores to be used on each node.; worker_options={""nprocs"":4,},; ); ```. Another common usecase is interfacing Dask to a batch system like HTCondor or; Slurm. A separate package called dask-jobqueue (https://jobqueue.dask.org); extends the available Dask cluster classes to enable running Dask computations; as batch jobs. In this case, the cluster object usually receives the parameters; that would be written in the job description file. For example:. ```python; from dask_jobqueue import HTCondorCluster; cluster = HTCondorCluster(; cores=1,; memory='2000MB',; disk='1000MB',; ); # Use the scale method to send as many jobs as needed; cluster.scale(4); ```. In this tutorial, a cluster object is created for the local machine, using; multiprocessing (processes=True) on 2 workers (n_workers=2)",MatchSource.CODE_COMMENT,tutorials/dataframe/distrdf002_dask_connection.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/distrdf002_dask_connection.py
Integrability,depend,depending,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## Configure a Dask connection and fill two histograms distributedly.; ##; ## This tutorial shows the ingredients needed to setup the connection to a Dask; ## cluster (e.g. a `LocalCluster` for a single machine). After this initial; ## setup, an RDataFrame with distributed capabilities is created and connected; ## to a Dask `Client` instance. Finally, a couple of histograms are drawn from; ## the created columns in the dataset. Relevant documentation can be found at; ## http://distributed.dask.org/en/stable .; ##; ## \macro_code; ## \macro_image; ##; ## \date February 2022; ## \author Vincenzo Eduardo Padulano; # Point RDataFrame calls to Dask RDataFrame object; """"""; Setup connection to a Dask cluster. Two ingredients are needed:; 1. Creating a cluster object that represents computing resources. This can be; done in various ways depending on the type of resources at disposal. To use; only the local machine (e.g. your laptop), a `LocalCluster` object can be; used. This step can be skipped if you have access to an existing Dask; cluster; in that case, the cluster administrator should provide you with a; URL to connect to the cluster in step 2. More options for cluster creation; can be found in the Dask docs at; http://distributed.dask.org/en/stable/api.html#cluster .; 2. Creating a Dask client object that connects to the cluster. This accepts; directly the object previously created. In case the cluster was setup; externally, you need to provide an endpoint URL to the client, e.g.; 'https://myscheduler.domain:8786'. Through Dask, you can connect to various types of cluster resources. For; example, you can connect together a set of machines through SSH and use them; to run your computations. This is done through the `SSHCluster` class. For; example:. ```python; from dask.distributed import SSHCluster; cluster = SSHCluster(; # A list with machine host names, the first name will be used as; # scheduler, followin",MatchSource.CODE_COMMENT,tutorials/dataframe/distrdf002_dask_connection.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/distrdf002_dask_connection.py
Modifiability,extend,extends,"pts; directly the object previously created. In case the cluster was setup; externally, you need to provide an endpoint URL to the client, e.g.; 'https://myscheduler.domain:8786'. Through Dask, you can connect to various types of cluster resources. For; example, you can connect together a set of machines through SSH and use them; to run your computations. This is done through the `SSHCluster` class. For; example:. ```python; from dask.distributed import SSHCluster; cluster = SSHCluster(; # A list with machine host names, the first name will be used as; # scheduler, following names will become workers.; hosts=[""machine1"",""machine2"",""machine3""],; # A dictionary of options for each worker node, here we set the number; # of cores to be used on each node.; worker_options={""nprocs"":4,},; ); ```. Another common usecase is interfacing Dask to a batch system like HTCondor or; Slurm. A separate package called dask-jobqueue (https://jobqueue.dask.org); extends the available Dask cluster classes to enable running Dask computations; as batch jobs. In this case, the cluster object usually receives the parameters; that would be written in the job description file. For example:. ```python; from dask_jobqueue import HTCondorCluster; cluster = HTCondorCluster(; cores=1,; memory='2000MB',; disk='1000MB',; ); # Use the scale method to send as many jobs as needed; cluster.scale(4); ```. In this tutorial, a cluster object is created for the local machine, using; multiprocessing (processes=True) on 2 workers (n_workers=2) each using only; 1 core (threads_per_worker=1) and 2GiB of RAM (memory_limit=""2GiB"").; """"""; # This tutorial uses Python multiprocessing, so the creation of the cluster; # needs to be wrapped in the main clause as described in the Python docs; # https://docs.python.org/3/library/multiprocessing.html; # Create the connection to the mock Dask cluster on the local machine; # Create an RDataFrame that will use Dask as a backend for computations; # Set the random seed and defi",MatchSource.CODE_COMMENT,tutorials/dataframe/distrdf002_dask_connection.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/distrdf002_dask_connection.py
Security,access,access,".; ##; ## This tutorial shows the ingredients needed to setup the connection to a Dask; ## cluster (e.g. a `LocalCluster` for a single machine). After this initial; ## setup, an RDataFrame with distributed capabilities is created and connected; ## to a Dask `Client` instance. Finally, a couple of histograms are drawn from; ## the created columns in the dataset. Relevant documentation can be found at; ## http://distributed.dask.org/en/stable .; ##; ## \macro_code; ## \macro_image; ##; ## \date February 2022; ## \author Vincenzo Eduardo Padulano; # Point RDataFrame calls to Dask RDataFrame object; """"""; Setup connection to a Dask cluster. Two ingredients are needed:; 1. Creating a cluster object that represents computing resources. This can be; done in various ways depending on the type of resources at disposal. To use; only the local machine (e.g. your laptop), a `LocalCluster` object can be; used. This step can be skipped if you have access to an existing Dask; cluster; in that case, the cluster administrator should provide you with a; URL to connect to the cluster in step 2. More options for cluster creation; can be found in the Dask docs at; http://distributed.dask.org/en/stable/api.html#cluster .; 2. Creating a Dask client object that connects to the cluster. This accepts; directly the object previously created. In case the cluster was setup; externally, you need to provide an endpoint URL to the client, e.g.; 'https://myscheduler.domain:8786'. Through Dask, you can connect to various types of cluster resources. For; example, you can connect together a set of machines through SSH and use them; to run your computations. This is done through the `SSHCluster` class. For; example:. ```python; from dask.distributed import SSHCluster; cluster = SSHCluster(; # A list with machine host names, the first name will be used as; # scheduler, following names will become workers.; hosts=[""machine1"",""machine2"",""machine3""],; # A dictionary of options for each worker node, here we ",MatchSource.CODE_COMMENT,tutorials/dataframe/distrdf002_dask_connection.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/distrdf002_dask_connection.py
Testability,mock,mock,"yscheduler.domain:8786'. Through Dask, you can connect to various types of cluster resources. For; example, you can connect together a set of machines through SSH and use them; to run your computations. This is done through the `SSHCluster` class. For; example:. ```python; from dask.distributed import SSHCluster; cluster = SSHCluster(; # A list with machine host names, the first name will be used as; # scheduler, following names will become workers.; hosts=[""machine1"",""machine2"",""machine3""],; # A dictionary of options for each worker node, here we set the number; # of cores to be used on each node.; worker_options={""nprocs"":4,},; ); ```. Another common usecase is interfacing Dask to a batch system like HTCondor or; Slurm. A separate package called dask-jobqueue (https://jobqueue.dask.org); extends the available Dask cluster classes to enable running Dask computations; as batch jobs. In this case, the cluster object usually receives the parameters; that would be written in the job description file. For example:. ```python; from dask_jobqueue import HTCondorCluster; cluster = HTCondorCluster(; cores=1,; memory='2000MB',; disk='1000MB',; ); # Use the scale method to send as many jobs as needed; cluster.scale(4); ```. In this tutorial, a cluster object is created for the local machine, using; multiprocessing (processes=True) on 2 workers (n_workers=2) each using only; 1 core (threads_per_worker=1) and 2GiB of RAM (memory_limit=""2GiB"").; """"""; # This tutorial uses Python multiprocessing, so the creation of the cluster; # needs to be wrapped in the main clause as described in the Python docs; # https://docs.python.org/3/library/multiprocessing.html; # Create the connection to the mock Dask cluster on the local machine; # Create an RDataFrame that will use Dask as a backend for computations; # Set the random seed and define two columns of the dataset with random numbers.; # Book an histogram for each column; # Plot the histograms side by side on a canvas; # Save the canvas",MatchSource.CODE_COMMENT,tutorials/dataframe/distrdf002_dask_connection.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/distrdf002_dask_connection.py
Modifiability,variab,variable,"## \file; ## \ingroup tutorial_dataframe; ## \notebook -draw; ## Configure a Dask connection and visualize the filling of a 1D and 2D; ## histograms distributedly.; ##; ## This tutorial showcases the process of setting up real-time data representation ; ## for distributed computations.; ## By calling the LiveVisualize function, you can observe the canvas updating; ## with the intermediate results of the histograms as the; ## distributed computation progresses. ; ##; ## \macro_code; ## \macro_image; ##; ## \date August 2023; ## \author Silia Taider; # Import the live visualization function; # Point RDataFrame calls to Dask RDataFrame object; # Function to create a Dask cluster and return the client; # Function to fit a Gaussian function to the plot; # Setup connection to a Dask cluster; # Create an RDataFrame that will use Dask as a backend for computations; # Define a gaussean distribution with a variable mean; # Create a 1D and a 2D histogram using the defined columns; # Apply LiveVisualize to the histograms. ; # The `fit_gaus` function will be applied to the accumulating partial result ; # of the 1D histogram. The 2D histogram will not be further modified, just drawn. ; # Find more details about usage of LiveVisualize in the RDataFrame docs.; # Plot the histograms side by side on a canvas",MatchSource.CODE_COMMENT,tutorials/dataframe/distrdf003_live_visualization.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/distrdf003_live_visualization.py
Availability,avail,available,"/introduction/ for details; # 3. Install the `dask_lxplus` package, which provides the `CernCluster` class; # needed to properly connect to the CERN condor pools. See; # https://batchdocs.web.cern.ch/specialpayload/dask.html for instructions; # 4. Run this tutorial; #; # The tutorial defines resources that each job will request to the condor; # scheduler, then creates a Dask client that can be used by RDataFrame to; # distribute computations.; #; # \macro_code; #; # \date September 2023; # \author Vincenzo Eduardo Padulano CERN; """"""; Creates a connection to HTCondor cluster offered by the CERN batch service.; Returns a Dask client that RDataFrame will use to distribute computations.; """"""; # The resources described in the specified arguments to this class represent; # the submission of a single job and will spawn a single Dask worker when; # the condor scheduler launches the job. Specifically, this example has Dask; # workers each with 1 core and 2 GB of memory.; # The scale method allows to launch N jobs with the description above (thus; # N Dask workers). Calling this method on the cluster object launches the; # condor jobs (i.e. it is equivalent to `condor_submit myjob.sub`). In this; # example, two jobs are requested so two Dask workers will be eventually; # launched for a total of 2 cores.; # The Dask client can be created after the condor jobs have been submitted.; # At this point, the jobs may or may not have actually started. Thus, it is; # not guaranteed that the application already has the requested resources; # available.; # It is possible to tell the Dask client to wait until the condor scheduler; # has started the requested jobs and launched the Dask workers.; # The client will wait until 'n_workers' workers have been launched. In this; # example, the client waits for all the jobs requested to start before; # continuing with the application.; """"""; Run a simple example with RDataFrame, using the previously created; connection to the HTCondor cluster.; """"""",MatchSource.CODE_COMMENT,tutorials/dataframe/distrdf004_dask_lxbatch.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/distrdf004_dask_lxbatch.py
Deployability,release,release,"# \file; # \ingroup tutorial_dataframe; #; # Configure a Dask connection to a HTCondor cluster hosted by the CERN batch; # service. To reproduce this tutorial, run the following steps:; #; # 1. Login to lxplus; # 2. Source an LCG release (minimum LCG104). See; # https://lcgdocs.web.cern.ch/lcgdocs/lcgreleases/introduction/ for details; # 3. Install the `dask_lxplus` package, which provides the `CernCluster` class; # needed to properly connect to the CERN condor pools. See; # https://batchdocs.web.cern.ch/specialpayload/dask.html for instructions; # 4. Run this tutorial; #; # The tutorial defines resources that each job will request to the condor; # scheduler, then creates a Dask client that can be used by RDataFrame to; # distribute computations.; #; # \macro_code; #; # \date September 2023; # \author Vincenzo Eduardo Padulano CERN; """"""; Creates a connection to HTCondor cluster offered by the CERN batch service.; Returns a Dask client that RDataFrame will use to distribute computations.; """"""; # The resources described in the specified arguments to this class represent; # the submission of a single job and will spawn a single Dask worker when; # the condor scheduler launches the job. Specifically, this example has Dask; # workers each with 1 core and 2 GB of memory.; # The scale method allows to launch N jobs with the description above (thus; # N Dask workers). Calling this method on the cluster object launches the; # condor jobs (i.e. it is equivalent to `condor_submit myjob.sub`). In this; # example, two jobs are requested so two Dask workers will be eventually; # launched for a total of 2 cores.; # The Dask client can be created after the condor jobs have been submitted.; # At this point, the jobs may or may not have actually started. Thus, it is; # not guaranteed that the application already has the requested resources; # available.; # It is possible to tell the Dask client to wait until the condor scheduler; # has started the requested jobs and launched the Dask ",MatchSource.CODE_COMMENT,tutorials/dataframe/distrdf004_dask_lxbatch.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/distrdf004_dask_lxbatch.py
Energy Efficiency,schedul,scheduler,"# \file; # \ingroup tutorial_dataframe; #; # Configure a Dask connection to a HTCondor cluster hosted by the CERN batch; # service. To reproduce this tutorial, run the following steps:; #; # 1. Login to lxplus; # 2. Source an LCG release (minimum LCG104). See; # https://lcgdocs.web.cern.ch/lcgdocs/lcgreleases/introduction/ for details; # 3. Install the `dask_lxplus` package, which provides the `CernCluster` class; # needed to properly connect to the CERN condor pools. See; # https://batchdocs.web.cern.ch/specialpayload/dask.html for instructions; # 4. Run this tutorial; #; # The tutorial defines resources that each job will request to the condor; # scheduler, then creates a Dask client that can be used by RDataFrame to; # distribute computations.; #; # \macro_code; #; # \date September 2023; # \author Vincenzo Eduardo Padulano CERN; """"""; Creates a connection to HTCondor cluster offered by the CERN batch service.; Returns a Dask client that RDataFrame will use to distribute computations.; """"""; # The resources described in the specified arguments to this class represent; # the submission of a single job and will spawn a single Dask worker when; # the condor scheduler launches the job. Specifically, this example has Dask; # workers each with 1 core and 2 GB of memory.; # The scale method allows to launch N jobs with the description above (thus; # N Dask workers). Calling this method on the cluster object launches the; # condor jobs (i.e. it is equivalent to `condor_submit myjob.sub`). In this; # example, two jobs are requested so two Dask workers will be eventually; # launched for a total of 2 cores.; # The Dask client can be created after the condor jobs have been submitted.; # At this point, the jobs may or may not have actually started. Thus, it is; # not guaranteed that the application already has the requested resources; # available.; # It is possible to tell the Dask client to wait until the condor scheduler; # has started the requested jobs and launched the Dask ",MatchSource.CODE_COMMENT,tutorials/dataframe/distrdf004_dask_lxbatch.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/distrdf004_dask_lxbatch.py
Usability,simpl,simple,"/introduction/ for details; # 3. Install the `dask_lxplus` package, which provides the `CernCluster` class; # needed to properly connect to the CERN condor pools. See; # https://batchdocs.web.cern.ch/specialpayload/dask.html for instructions; # 4. Run this tutorial; #; # The tutorial defines resources that each job will request to the condor; # scheduler, then creates a Dask client that can be used by RDataFrame to; # distribute computations.; #; # \macro_code; #; # \date September 2023; # \author Vincenzo Eduardo Padulano CERN; """"""; Creates a connection to HTCondor cluster offered by the CERN batch service.; Returns a Dask client that RDataFrame will use to distribute computations.; """"""; # The resources described in the specified arguments to this class represent; # the submission of a single job and will spawn a single Dask worker when; # the condor scheduler launches the job. Specifically, this example has Dask; # workers each with 1 core and 2 GB of memory.; # The scale method allows to launch N jobs with the description above (thus; # N Dask workers). Calling this method on the cluster object launches the; # condor jobs (i.e. it is equivalent to `condor_submit myjob.sub`). In this; # example, two jobs are requested so two Dask workers will be eventually; # launched for a total of 2 cores.; # The Dask client can be created after the condor jobs have been submitted.; # At this point, the jobs may or may not have actually started. Thus, it is; # not guaranteed that the application already has the requested resources; # available.; # It is possible to tell the Dask client to wait until the condor scheduler; # has started the requested jobs and launched the Dask workers.; # The client will wait until 'n_workers' workers have been launched. In this; # example, the client waits for all the jobs requested to start before; # continuing with the application.; """"""; Run a simple example with RDataFrame, using the previously created; connection to the HTCondor cluster.; """"""",MatchSource.CODE_COMMENT,tutorials/dataframe/distrdf004_dask_lxbatch.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/distrdf004_dask_lxbatch.py
Integrability,wrap,wrapped,"## \file; ## \ingroup tutorial_fit; ## \notebook; ## Combined (simultaneous) fit of two histogram with separate functions; ## and some common parameters; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \author Jonas Rembser, Lorenzo Moneta (C++ version); # definition of shared parameter background function; # exp amplitude in B histo and exp common parameter; # signal + background function; # exp amplitude in S+B histo; # exp common parameter; # Gaussian amplitude; # Gaussian mean; # Gaussian sigma; # Create the GlobalCHi2 structure; # parameter vector is first background (in common 1 and 2) and then is; # signal (only in 2); # the zero-copy way to get a numpy array from a double *; # perform now global fit; # set the data range; # create before the parameter settings in order to fix or set range on them; # fix 5-th parameter; # set limits on the third and 4-th parameter; # we can't pass the Python object globalChi2 directly to FitFCN.; # It needs to be wrapped in a ROOT::Math::Functor.; # fit FCN function; # (specify optionally data size and flag to indicate that is a chi2 fit)",MatchSource.CODE_COMMENT,tutorials/fit/combinedFit.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/fit/combinedFit.py
Performance,perform,perform,"## \file; ## \ingroup tutorial_fit; ## \notebook; ## Combined (simultaneous) fit of two histogram with separate functions; ## and some common parameters; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \author Jonas Rembser, Lorenzo Moneta (C++ version); # definition of shared parameter background function; # exp amplitude in B histo and exp common parameter; # signal + background function; # exp amplitude in S+B histo; # exp common parameter; # Gaussian amplitude; # Gaussian mean; # Gaussian sigma; # Create the GlobalCHi2 structure; # parameter vector is first background (in common 1 and 2) and then is; # signal (only in 2); # the zero-copy way to get a numpy array from a double *; # perform now global fit; # set the data range; # create before the parameter settings in order to fix or set range on them; # fix 5-th parameter; # set limits on the third and 4-th parameter; # we can't pass the Python object globalChi2 directly to FitFCN.; # It needs to be wrapped in a ROOT::Math::Functor.; # fit FCN function; # (specify optionally data size and flag to indicate that is a chi2 fit)",MatchSource.CODE_COMMENT,tutorials/fit/combinedFit.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/fit/combinedFit.py
Modifiability,variab,variables,"## \file; ## \ingroup tutorial_fit; ## \notebook; ## Tutorial for convolution of two functions; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \author Jonas Rembser, Aurelie Flandi (C++ version); # Construction of histogram to fit.; # Gives a alpha of -0.3 in the exp.; # Probability density function of the addition of two variables is the; # convolution of two density functions.; # Fit and draw result of the fit",MatchSource.CODE_COMMENT,tutorials/fit/fitConvolution.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/fit/fitConvolution.py
Availability,avail,available,"## \file; ## \ingroup tutorial_fit; ## \notebook; ## Tutorial for normalized sum of two functions; ## Here: a background exponential and a crystalball function; ## Parameters can be set:; ## 1. with the TF1 object before adding the function (for 3) and 4)); ## 2. with the TF1NormSum object (first two are the coefficients, then the non constant parameters); ## 3. with the TF1 object after adding the function; ##; ## Sum can be constructed by:; ## 1. by a string containing the names of the functions and/or the coefficient in front; ## 2. by a string containg formulas like expo, gaus...; ## 3. by the list of functions and coefficients (which are 1 by default); ## 4. by a std::vector for functions and coefficients; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \author Jonas Rembser, Lorenzo Moneta (C++ version); # I.:; # CONSTRUCTION OF THE TF1NORMSUM OBJECT ........................................; # 1) :; # 4) :; # III.:; # Note: in the C++ tutorial, the parameter value sync is done in one line with:; # ```C++; # f_sum->SetParameters(fnorm_exp_cb->GetParameters().data());; # ```; # However, TF1NormSum::GetParameters() returns an std::vector by value, which; # doesn't survive long enough in Python. That's why we have to explicitly; # assign it to a variable first and can't use a temporary.; # GENERATE HISTOGRAM TO FIT ..............................................................; # need to scale histogram with width since we are fitting a density; # fit - use Minuit2 if available; # do a least-square fit of the spectrum; # test if parameters are fine; # add parameters",MatchSource.CODE_COMMENT,tutorials/fit/fitNormSum.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/fit/fitNormSum.py
Modifiability,variab,variable,"## \file; ## \ingroup tutorial_fit; ## \notebook; ## Tutorial for normalized sum of two functions; ## Here: a background exponential and a crystalball function; ## Parameters can be set:; ## 1. with the TF1 object before adding the function (for 3) and 4)); ## 2. with the TF1NormSum object (first two are the coefficients, then the non constant parameters); ## 3. with the TF1 object after adding the function; ##; ## Sum can be constructed by:; ## 1. by a string containing the names of the functions and/or the coefficient in front; ## 2. by a string containg formulas like expo, gaus...; ## 3. by the list of functions and coefficients (which are 1 by default); ## 4. by a std::vector for functions and coefficients; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \author Jonas Rembser, Lorenzo Moneta (C++ version); # I.:; # CONSTRUCTION OF THE TF1NORMSUM OBJECT ........................................; # 1) :; # 4) :; # III.:; # Note: in the C++ tutorial, the parameter value sync is done in one line with:; # ```C++; # f_sum->SetParameters(fnorm_exp_cb->GetParameters().data());; # ```; # However, TF1NormSum::GetParameters() returns an std::vector by value, which; # doesn't survive long enough in Python. That's why we have to explicitly; # assign it to a variable first and can't use a temporary.; # GENERATE HISTOGRAM TO FIT ..............................................................; # need to scale histogram with width since we are fitting a density; # fit - use Minuit2 if available; # do a least-square fit of the spectrum; # test if parameters are fine; # add parameters",MatchSource.CODE_COMMENT,tutorials/fit/fitNormSum.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/fit/fitNormSum.py
Testability,test,test,"## \file; ## \ingroup tutorial_fit; ## \notebook; ## Tutorial for normalized sum of two functions; ## Here: a background exponential and a crystalball function; ## Parameters can be set:; ## 1. with the TF1 object before adding the function (for 3) and 4)); ## 2. with the TF1NormSum object (first two are the coefficients, then the non constant parameters); ## 3. with the TF1 object after adding the function; ##; ## Sum can be constructed by:; ## 1. by a string containing the names of the functions and/or the coefficient in front; ## 2. by a string containg formulas like expo, gaus...; ## 3. by the list of functions and coefficients (which are 1 by default); ## 4. by a std::vector for functions and coefficients; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \author Jonas Rembser, Lorenzo Moneta (C++ version); # I.:; # CONSTRUCTION OF THE TF1NORMSUM OBJECT ........................................; # 1) :; # 4) :; # III.:; # Note: in the C++ tutorial, the parameter value sync is done in one line with:; # ```C++; # f_sum->SetParameters(fnorm_exp_cb->GetParameters().data());; # ```; # However, TF1NormSum::GetParameters() returns an std::vector by value, which; # doesn't survive long enough in Python. That's why we have to explicitly; # assign it to a variable first and can't use a temporary.; # GENERATE HISTOGRAM TO FIT ..............................................................; # need to scale histogram with width since we are fitting a density; # fit - use Minuit2 if available; # do a least-square fit of the spectrum; # test if parameters are fine; # add parameters",MatchSource.CODE_COMMENT,tutorials/fit/fitNormSum.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/fit/fitNormSum.py
Availability,error,errors,"## \file; ## \ingroup tutorial_fit; ## \notebook; ## Fitting multiple functions to different ranges of a 1-D histogram; ## Example showing how to fit in a sub-range of an histogram; ## A histogram is created and filled with the bin contents and errors; ## defined in the table below.; ## Three Gaussians are fitted in sub-ranges of this histogram.; ## A new function (a sum of 3 Gaussians) is fitted on another subrange; ## Note that when fitting simple functions, such as Gaussians, the initial; ## values of parameters are automatically computed by ROOT.; ## In the more complicated case of the sum of 3 Gaussians, the initial values; ## of parameters must be given. In this particular case, the initial values; ## are taken from the result of the individual fits.; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \authors Jonas Rembser, Rene Brun (C++ version); # fmt: off; # fmt: on; # The histogram are filled with bins defined in the array x.; # Define the parameter array for the total function.; # Three TF1 objects are created, one for each subrange.; # The total is the sum of the three, each has three parameters.; # The canvas that the histograms and fit functions are drawn on.; # Fit each function and add it to the list of functions. By default, TH1::Fit(); # fits the function on the defined histogram range. You can specify the ""R""; # option in the second parameter of TH1::Fit() to restrict the fit to the range; # specified in the TF1 constructor. Alternatively, you can also specify the; # range in the call to TH1::Fit(), which we demonstrate here with the 3rd; # Gaussian. The ""+"" option needs to be added to the later fits to not replace; # existing fitted functions in the histogram.; # Get the parameters from the fit.; # Use the parameters on the sum.; # Save the plot for later inspection.",MatchSource.CODE_COMMENT,tutorials/fit/multifit.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/fit/multifit.py
Usability,simpl,simple,"## \file; ## \ingroup tutorial_fit; ## \notebook; ## Fitting multiple functions to different ranges of a 1-D histogram; ## Example showing how to fit in a sub-range of an histogram; ## A histogram is created and filled with the bin contents and errors; ## defined in the table below.; ## Three Gaussians are fitted in sub-ranges of this histogram.; ## A new function (a sum of 3 Gaussians) is fitted on another subrange; ## Note that when fitting simple functions, such as Gaussians, the initial; ## values of parameters are automatically computed by ROOT.; ## In the more complicated case of the sum of 3 Gaussians, the initial values; ## of parameters must be given. In this particular case, the initial values; ## are taken from the result of the individual fits.; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \authors Jonas Rembser, Rene Brun (C++ version); # fmt: off; # fmt: on; # The histogram are filled with bins defined in the array x.; # Define the parameter array for the total function.; # Three TF1 objects are created, one for each subrange.; # The total is the sum of the three, each has three parameters.; # The canvas that the histograms and fit functions are drawn on.; # Fit each function and add it to the list of functions. By default, TH1::Fit(); # fits the function on the defined histogram range. You can specify the ""R""; # option in the second parameter of TH1::Fit() to restrict the fit to the range; # specified in the TF1 constructor. Alternatively, you can also specify the; # range in the call to TH1::Fit(), which we demonstrate here with the 3rd; # Gaussian. The ""+"" option needs to be added to the later fits to not replace; # existing fitted functions in the histogram.; # Get the parameters from the fit.; # Use the parameters on the sum.; # Save the plot for later inspection.",MatchSource.CODE_COMMENT,tutorials/fit/multifit.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/fit/multifit.py
Availability,toler,tolerance,"# \file; # \ingroup tutorial_fit; # \notebook -nodraw; # Example on how to use the new Minimizer class in ROOT; # Show usage with all the possible minimizers.; # Minimize the Rosenbrock function (a 2D -function); #; # input : minimizer name + algorithm name; # randomSeed: = <0 : fixed value: 0 random with seed 0; >0 random with given seed; #; # \macro_code; #; # \author Lorenzo Moneta; # create minimizer giving a name and a name (optionally) for the specific algorithm; # possible choices are:; # minimizerName algoName; #; # Minuit Migrad, Simplex,Combined,Scan (default is Migrad); # Minuit2 Migrad, BFGS, Simplex,Combined,Scan (default is Migrad); # GSLMultiMin ConjugateFR, ConjugatePR, BFGS, BFGS2, SteepestDescent; # GSLSimAn; # Genetic; # Set tolerance and other minimizer parameters, one can also use default; # values; # working for Minuit/Minuit2; # for GSL minimizers - no effect in Minuit/Minuit2; # Create function wrapper for minimizer; # Evaluate function at a point; # Starting point; # Set the free variables to be minimized !; # Do the minimization; # Real minimum is f(xmin) = 0",MatchSource.CODE_COMMENT,tutorials/fit/NumericalMinimization.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/fit/NumericalMinimization.py
Integrability,wrap,wrapper,"# \file; # \ingroup tutorial_fit; # \notebook -nodraw; # Example on how to use the new Minimizer class in ROOT; # Show usage with all the possible minimizers.; # Minimize the Rosenbrock function (a 2D -function); #; # input : minimizer name + algorithm name; # randomSeed: = <0 : fixed value: 0 random with seed 0; >0 random with given seed; #; # \macro_code; #; # \author Lorenzo Moneta; # create minimizer giving a name and a name (optionally) for the specific algorithm; # possible choices are:; # minimizerName algoName; #; # Minuit Migrad, Simplex,Combined,Scan (default is Migrad); # Minuit2 Migrad, BFGS, Simplex,Combined,Scan (default is Migrad); # GSLMultiMin ConjugateFR, ConjugatePR, BFGS, BFGS2, SteepestDescent; # GSLSimAn; # Genetic; # Set tolerance and other minimizer parameters, one can also use default; # values; # working for Minuit/Minuit2; # for GSL minimizers - no effect in Minuit/Minuit2; # Create function wrapper for minimizer; # Evaluate function at a point; # Starting point; # Set the free variables to be minimized !; # Do the minimization; # Real minimum is f(xmin) = 0",MatchSource.CODE_COMMENT,tutorials/fit/NumericalMinimization.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/fit/NumericalMinimization.py
Modifiability,variab,variables,"# \file; # \ingroup tutorial_fit; # \notebook -nodraw; # Example on how to use the new Minimizer class in ROOT; # Show usage with all the possible minimizers.; # Minimize the Rosenbrock function (a 2D -function); #; # input : minimizer name + algorithm name; # randomSeed: = <0 : fixed value: 0 random with seed 0; >0 random with given seed; #; # \macro_code; #; # \author Lorenzo Moneta; # create minimizer giving a name and a name (optionally) for the specific algorithm; # possible choices are:; # minimizerName algoName; #; # Minuit Migrad, Simplex,Combined,Scan (default is Migrad); # Minuit2 Migrad, BFGS, Simplex,Combined,Scan (default is Migrad); # GSLMultiMin ConjugateFR, ConjugatePR, BFGS, BFGS2, SteepestDescent; # GSLSimAn; # Genetic; # Set tolerance and other minimizer parameters, one can also use default; # values; # working for Minuit/Minuit2; # for GSL minimizers - no effect in Minuit/Minuit2; # Create function wrapper for minimizer; # Evaluate function at a point; # Starting point; # Set the free variables to be minimized !; # Do the minimization; # Real minimum is f(xmin) = 0",MatchSource.CODE_COMMENT,tutorials/fit/NumericalMinimization.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/fit/NumericalMinimization.py
Availability,error,error,"## \file; ## \ingroup tutorial_graphs; ## \notebook -js; ## Bent error bars. Inspired from work of Olivier Couet.; ##; ## \macro_image; ## \macro_code; ##; ## \author Alberto Ferro",MatchSource.CODE_COMMENT,tutorials/graphs/bent.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/graphs/bent.py
Usability,simpl,simple,"## \file; ## \ingroup tutorial_hist; ## \notebook; ## Example creating a simple ratio plot of two histograms using the `pois` division option.; ## Two histograms are set up and filled with random numbers. The constructor of `TRatioPlot`; ## takes the to histograms, name and title for the object, drawing options for the histograms (`hist` and `E` in this case); ## and a drawing option for the output graph.; ## Inspired by the tutorial of Paul Gessinger.; ##; ## \macro_image; ## \macro_code; ##; ## \author Alberto Ferro",MatchSource.CODE_COMMENT,tutorials/hist/ratioplot1.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/hist/ratioplot1.py
Availability,error,errors,"#!/usr/bin/env python; #; # A pyROOT script that allows one to; # make quick measuremenst.; #; # This is a command-line script that; # takes in signal and background values,; # as well as potentially uncertainties on those; # values, and returns a fitted signal value; # and errors; """""" Create a simple model and run statistical tests . This script can be used to make simple statistical using histfactory.; It takes values for signal, background, and data as input, and; can optionally take uncertainties on signal or background.; The model is created and saved to an output ROOT file, and; the model can be fit if requested. """"""; # Let's parse the input; # Define the command line options of the script:; # Parse the command line options:; # Make a log; # Set the format of the log messages:; # Create the logger object:; # Set the following to DEBUG when debugging the scripts:; # So a small sanity check:; # Ensure that all necessary input has been supplied; # Okay, if all input is acceptable, we simply pass; # it to the MakeSimpleMeasurement function, which; # does the real work.; """""" Make a simple measurement using HistFactory; ; Take in simple values for signal, background data, ; and potentially uncertainty on signal and background. """"""; # Create and name a measurement; # Set the Parameter of interest, and set several; # other parameters to be constant; # We don't include Lumi here, ; # but since HistFactory gives it to ; # us for free, we set it constant; # The values are just dummies; # We set ExportOnly to false. This parameter; # defines what happens when we run MakeMeasurementAndModelFast; # If we DO run that function, we also want it to export.; # Create a channel and set; # the measured value of data ; # (no external hist necessary for cut-and-count); # Create the signal sample and set it's value; #signal.SetValue( 10 ); # Add the parameter of interest and a systematic; # Try to make intelligent choice of upper bound; # If we have a signal uncertainty, add it too; #",MatchSource.CODE_COMMENT,tutorials/histfactory/makeQuickModel.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/histfactory/makeQuickModel.py
Integrability,message,messages,"#!/usr/bin/env python; #; # A pyROOT script that allows one to; # make quick measuremenst.; #; # This is a command-line script that; # takes in signal and background values,; # as well as potentially uncertainties on those; # values, and returns a fitted signal value; # and errors; """""" Create a simple model and run statistical tests . This script can be used to make simple statistical using histfactory.; It takes values for signal, background, and data as input, and; can optionally take uncertainties on signal or background.; The model is created and saved to an output ROOT file, and; the model can be fit if requested. """"""; # Let's parse the input; # Define the command line options of the script:; # Parse the command line options:; # Make a log; # Set the format of the log messages:; # Create the logger object:; # Set the following to DEBUG when debugging the scripts:; # So a small sanity check:; # Ensure that all necessary input has been supplied; # Okay, if all input is acceptable, we simply pass; # it to the MakeSimpleMeasurement function, which; # does the real work.; """""" Make a simple measurement using HistFactory; ; Take in simple values for signal, background data, ; and potentially uncertainty on signal and background. """"""; # Create and name a measurement; # Set the Parameter of interest, and set several; # other parameters to be constant; # We don't include Lumi here, ; # but since HistFactory gives it to ; # us for free, we set it constant; # The values are just dummies; # We set ExportOnly to false. This parameter; # defines what happens when we run MakeMeasurementAndModelFast; # If we DO run that function, we also want it to export.; # Create a channel and set; # the measured value of data ; # (no external hist necessary for cut-and-count); # Create the signal sample and set it's value; #signal.SetValue( 10 ); # Add the parameter of interest and a systematic; # Try to make intelligent choice of upper bound; # If we have a signal uncertainty, add it too; #",MatchSource.CODE_COMMENT,tutorials/histfactory/makeQuickModel.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/histfactory/makeQuickModel.py
Safety,sanity check,sanity check,"#!/usr/bin/env python; #; # A pyROOT script that allows one to; # make quick measuremenst.; #; # This is a command-line script that; # takes in signal and background values,; # as well as potentially uncertainties on those; # values, and returns a fitted signal value; # and errors; """""" Create a simple model and run statistical tests . This script can be used to make simple statistical using histfactory.; It takes values for signal, background, and data as input, and; can optionally take uncertainties on signal or background.; The model is created and saved to an output ROOT file, and; the model can be fit if requested. """"""; # Let's parse the input; # Define the command line options of the script:; # Parse the command line options:; # Make a log; # Set the format of the log messages:; # Create the logger object:; # Set the following to DEBUG when debugging the scripts:; # So a small sanity check:; # Ensure that all necessary input has been supplied; # Okay, if all input is acceptable, we simply pass; # it to the MakeSimpleMeasurement function, which; # does the real work.; """""" Make a simple measurement using HistFactory; ; Take in simple values for signal, background data, ; and potentially uncertainty on signal and background. """"""; # Create and name a measurement; # Set the Parameter of interest, and set several; # other parameters to be constant; # We don't include Lumi here, ; # but since HistFactory gives it to ; # us for free, we set it constant; # The values are just dummies; # We set ExportOnly to false. This parameter; # defines what happens when we run MakeMeasurementAndModelFast; # If we DO run that function, we also want it to export.; # Create a channel and set; # the measured value of data ; # (no external hist necessary for cut-and-count); # Create the signal sample and set it's value; #signal.SetValue( 10 ); # Add the parameter of interest and a systematic; # Try to make intelligent choice of upper bound; # If we have a signal uncertainty, add it too; #",MatchSource.CODE_COMMENT,tutorials/histfactory/makeQuickModel.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/histfactory/makeQuickModel.py
Testability,test,tests,"#!/usr/bin/env python; #; # A pyROOT script that allows one to; # make quick measuremenst.; #; # This is a command-line script that; # takes in signal and background values,; # as well as potentially uncertainties on those; # values, and returns a fitted signal value; # and errors; """""" Create a simple model and run statistical tests . This script can be used to make simple statistical using histfactory.; It takes values for signal, background, and data as input, and; can optionally take uncertainties on signal or background.; The model is created and saved to an output ROOT file, and; the model can be fit if requested. """"""; # Let's parse the input; # Define the command line options of the script:; # Parse the command line options:; # Make a log; # Set the format of the log messages:; # Create the logger object:; # Set the following to DEBUG when debugging the scripts:; # So a small sanity check:; # Ensure that all necessary input has been supplied; # Okay, if all input is acceptable, we simply pass; # it to the MakeSimpleMeasurement function, which; # does the real work.; """""" Make a simple measurement using HistFactory; ; Take in simple values for signal, background data, ; and potentially uncertainty on signal and background. """"""; # Create and name a measurement; # Set the Parameter of interest, and set several; # other parameters to be constant; # We don't include Lumi here, ; # but since HistFactory gives it to ; # us for free, we set it constant; # The values are just dummies; # We set ExportOnly to false. This parameter; # defines what happens when we run MakeMeasurementAndModelFast; # If we DO run that function, we also want it to export.; # Create a channel and set; # the measured value of data ; # (no external hist necessary for cut-and-count); # Create the signal sample and set it's value; #signal.SetValue( 10 ); # Add the parameter of interest and a systematic; # Try to make intelligent choice of upper bound; # If we have a signal uncertainty, add it too; #",MatchSource.CODE_COMMENT,tutorials/histfactory/makeQuickModel.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/histfactory/makeQuickModel.py
Usability,simpl,simple,"#!/usr/bin/env python; #; # A pyROOT script that allows one to; # make quick measuremenst.; #; # This is a command-line script that; # takes in signal and background values,; # as well as potentially uncertainties on those; # values, and returns a fitted signal value; # and errors; """""" Create a simple model and run statistical tests . This script can be used to make simple statistical using histfactory.; It takes values for signal, background, and data as input, and; can optionally take uncertainties on signal or background.; The model is created and saved to an output ROOT file, and; the model can be fit if requested. """"""; # Let's parse the input; # Define the command line options of the script:; # Parse the command line options:; # Make a log; # Set the format of the log messages:; # Create the logger object:; # Set the following to DEBUG when debugging the scripts:; # So a small sanity check:; # Ensure that all necessary input has been supplied; # Okay, if all input is acceptable, we simply pass; # it to the MakeSimpleMeasurement function, which; # does the real work.; """""" Make a simple measurement using HistFactory; ; Take in simple values for signal, background data, ; and potentially uncertainty on signal and background. """"""; # Create and name a measurement; # Set the Parameter of interest, and set several; # other parameters to be constant; # We don't include Lumi here, ; # but since HistFactory gives it to ; # us for free, we set it constant; # The values are just dummies; # We set ExportOnly to false. This parameter; # defines what happens when we run MakeMeasurementAndModelFast; # If we DO run that function, we also want it to export.; # Create a channel and set; # the measured value of data ; # (no external hist necessary for cut-and-count); # Create the signal sample and set it's value; #signal.SetValue( 10 ); # Add the parameter of interest and a systematic; # Try to make intelligent choice of upper bound; # If we have a signal uncertainty, add it too; #",MatchSource.CODE_COMMENT,tutorials/histfactory/makeQuickModel.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/histfactory/makeQuickModel.py
Availability,avail,available,"## \file; ## \ingroup tutorial_math; ## \notebook; ## Show the different kinds of Bessel functions available in ROOT; ## To execute the macro type in:; ##; ## ~~~{.cpp}; ## root[0] .x Bessel.C; ## ~~~; ##; ## It will create one canvas with the representation; ## of the cylindrical and spherical Bessel functions; ## regular and modified; ##; ## Based on Bessel.C by Magdalena Slawinska; ##; ## \macro_image; ## \macro_code; ##; ## \author Juan Fernando Jaramillo Botero; # Drawing the set of Bessel J functions; #; # n is the number of functions in each pad; # Setting x axis for JBessel; # setting the title in a label style; # setting the legend; # Set canvas 2; # Drawing Bessel k; # setting title; # setting legend; # Set canvas 3; # Drawing Bessel i; # setting title; # setting legend; # Set canvas 4; # Drawing sph_bessel; # setting title; # setting legend",MatchSource.CODE_COMMENT,tutorials/math/Bessel.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/math/Bessel.py
Modifiability,variab,variables,"## \file; ## \ingroup tutorial_math; ## \notebook; ## Example macro showing some major probability density functions in ROOT.; ## The macro shows four of them with; ## respect to their two variables. In order to run the macro type:; ##; ## ~~~{.cpp}; ## root [0] .x mathcoreStatFunc.C; ## ~~~; ##; ## Original tutorial by Andras Zsenei.; ## \macro_image; ## \macro_code; ##; ## \author Alberto Ferro",MatchSource.CODE_COMMENT,tutorials/math/mathcoreStatFunc.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/math/mathcoreStatFunc.py
Deployability,configurat,configuration,"## \file; ## \ingroup tutorial_math; ## \notebook; ## Tutorial illustrating the new statistical distributions functions (pdf, cdf and quantile); ##; ## based on Anna Kreshuk's normalDist.C; ##; ## \macro_image; ## \macro_code; ##; ## \author Juan Fernando Jaramillo Botero; # Create the one dimensional functions for normal distribution.; # Set the parameters for the normal distribution with sigma to 1 and mean to; # zero. And set the color to blue and title to none.; # Set the configuration for the X and Y axis.; # Set sigma to 1 and mean to zero, for the cumulative normal distribution, and; # set the color to red and title to none.; # Set the configuration for the X and Y axis for the cumulative normal; # distribution.; # Set sigma to 1 and mean to zero, for the survival function of normal; # distribution, and set the color to green and title to none; # Set sigma to 1 and mean to zero, for the quantile of normal distribution; # To get more precision for p close to 0 or 1, set Npx to 1000; # Set the configuration of X and Y axis; # Set sigma to 1 and mean to zero of survival function of quantile of normal; # distribution, and set color to green and title to none.; # Create canvas and divide in three parts; # Draw the normal distribution; # Draw the cumulative normal distribution; # Draw the normal quantile of normal distribution",MatchSource.CODE_COMMENT,tutorials/math/normalDist.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/math/normalDist.py
Energy Efficiency,green,green,"## \file; ## \ingroup tutorial_math; ## \notebook; ## Tutorial illustrating the new statistical distributions functions (pdf, cdf and quantile); ##; ## based on Anna Kreshuk's normalDist.C; ##; ## \macro_image; ## \macro_code; ##; ## \author Juan Fernando Jaramillo Botero; # Create the one dimensional functions for normal distribution.; # Set the parameters for the normal distribution with sigma to 1 and mean to; # zero. And set the color to blue and title to none.; # Set the configuration for the X and Y axis.; # Set sigma to 1 and mean to zero, for the cumulative normal distribution, and; # set the color to red and title to none.; # Set the configuration for the X and Y axis for the cumulative normal; # distribution.; # Set sigma to 1 and mean to zero, for the survival function of normal; # distribution, and set the color to green and title to none; # Set sigma to 1 and mean to zero, for the quantile of normal distribution; # To get more precision for p close to 0 or 1, set Npx to 1000; # Set the configuration of X and Y axis; # Set sigma to 1 and mean to zero of survival function of quantile of normal; # distribution, and set color to green and title to none.; # Create canvas and divide in three parts; # Draw the normal distribution; # Draw the cumulative normal distribution; # Draw the normal quantile of normal distribution",MatchSource.CODE_COMMENT,tutorials/math/normalDist.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/math/normalDist.py
Modifiability,config,configuration,"## \file; ## \ingroup tutorial_math; ## \notebook; ## Tutorial illustrating the new statistical distributions functions (pdf, cdf and quantile); ##; ## based on Anna Kreshuk's normalDist.C; ##; ## \macro_image; ## \macro_code; ##; ## \author Juan Fernando Jaramillo Botero; # Create the one dimensional functions for normal distribution.; # Set the parameters for the normal distribution with sigma to 1 and mean to; # zero. And set the color to blue and title to none.; # Set the configuration for the X and Y axis.; # Set sigma to 1 and mean to zero, for the cumulative normal distribution, and; # set the color to red and title to none.; # Set the configuration for the X and Y axis for the cumulative normal; # distribution.; # Set sigma to 1 and mean to zero, for the survival function of normal; # distribution, and set the color to green and title to none; # Set sigma to 1 and mean to zero, for the quantile of normal distribution; # To get more precision for p close to 0 or 1, set Npx to 1000; # Set the configuration of X and Y axis; # Set sigma to 1 and mean to zero of survival function of quantile of normal; # distribution, and set color to green and title to none.; # Create canvas and divide in three parts; # Draw the normal distribution; # Draw the cumulative normal distribution; # Draw the normal quantile of normal distribution",MatchSource.CODE_COMMENT,tutorials/math/normalDist.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/math/normalDist.py
Integrability,depend,dependent,"## \file; ## \ingroup tutorial_math; ## \notebook; ## Principal Components Analysis (PCA) example; ##; ## Example of using TPrincipal as a stand alone class.; ##; ## I create n-dimensional data points, where c = trunc(n / 5) + 1; ## are correlated with the rest n - c randomly distributed variables.; ##; ## Based on principal.C by Rene Brun and Christian Holm Christensen; ##; ## \macro_output; ## \macro_code; ##; ## \authors Juan Fernando, Jaramillo Botero; """"""*************************************************; * Principal Component Analysis *; * *; * Number of variables: {0:4d} *; * Number of data points: {1:8d} *; * Number of dependent variables: {2:4d} *; * *; *************************************************""""""; # Initilase the TPrincipal object. Use the empty string for the; # final argument, if you don't wan't the covariance; # matrix. Normalising the covariance matrix is a good idea if your; # variables have different orders of magnitude.; # Use a pseudo-random number generator; # Make the m data-points; # Make a variable to hold our data; # Allocate memory for the data point; # First we create the un-correlated, random variables, according; # to one of three distributions; # Then we create the correlated variables; # Finally we're ready to add this datapoint to the PCA; # Do the actual analysis; # Print out the result on; # Test the PCA; # Make some histograms of the original, principal, residue, etc data; # Make two functions to map between feature and pattern space; # Start a browser, so that we may browse the histograms generated; # above",MatchSource.CODE_COMMENT,tutorials/math/principal.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/math/principal.py
Modifiability,variab,variables,"## \file; ## \ingroup tutorial_math; ## \notebook; ## Principal Components Analysis (PCA) example; ##; ## Example of using TPrincipal as a stand alone class.; ##; ## I create n-dimensional data points, where c = trunc(n / 5) + 1; ## are correlated with the rest n - c randomly distributed variables.; ##; ## Based on principal.C by Rene Brun and Christian Holm Christensen; ##; ## \macro_output; ## \macro_code; ##; ## \authors Juan Fernando, Jaramillo Botero; """"""*************************************************; * Principal Component Analysis *; * *; * Number of variables: {0:4d} *; * Number of data points: {1:8d} *; * Number of dependent variables: {2:4d} *; * *; *************************************************""""""; # Initilase the TPrincipal object. Use the empty string for the; # final argument, if you don't wan't the covariance; # matrix. Normalising the covariance matrix is a good idea if your; # variables have different orders of magnitude.; # Use a pseudo-random number generator; # Make the m data-points; # Make a variable to hold our data; # Allocate memory for the data point; # First we create the un-correlated, random variables, according; # to one of three distributions; # Then we create the correlated variables; # Finally we're ready to add this datapoint to the PCA; # Do the actual analysis; # Print out the result on; # Test the PCA; # Make some histograms of the original, principal, residue, etc data; # Make two functions to map between feature and pattern space; # Start a browser, so that we may browse the histograms generated; # above",MatchSource.CODE_COMMENT,tutorials/math/principal.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/math/principal.py
Performance,load,load,"## \file; ## \ingroup tutorial_math; ## \notebook; ## Example macro describing the student t distribution; ##; ## ~~~{.cpp}; ## root[0]: .x tStudent.C; ## ~~~; ##; ## It draws the pdf, the cdf and then 10 quantiles of the t Student distribution; ##; ## based on Magdalena Slawinska's tStudent.C; ##; ## \macro_image; ## \macro_code; ##; ## \author Juan Fernando Jaramillo Botero; # This is the way to force load of MathMore in Cling; # Create the pdf and the cumulative distributions; # Create the histogram and fill it with the quantiles; # For each quantile fill with the pdf; # Create the Canvas and divide in four draws, for every draw set the line width; # the title, and the line color.; # Set the colors in every quantile.",MatchSource.CODE_COMMENT,tutorials/math/tStudent.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/math/tStudent.py
Availability,alive,alive,"## \file; ## \ingroup tutorial_pyroot; ## To run, do ""python <path-to>/demo.py""; ##; ## \macro_code; ##; ## \author Wim Lavrijsen, Enric Tejedor; # To run, do ""python <path-to>/demo.py""; # enable running from another directory than the one where demo.py resides; # This macro generates a Controlbar menu.; # To execute an item, click with the left mouse button.; # To see the HELP of a button, click on the right mouse button.; # if you have a large screen, select 1.2 or 1.4; # The callbacks to python work by having CLING call the python interpreter through; # the ""TPython"" class. Note the use of ""raw strings.""; # not implemented; ## wait for input to keep the GUI (which lives on a ROOT event dispatcher) alive",MatchSource.CODE_COMMENT,tutorials/pyroot/demo.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/pyroot/demo.py
Energy Efficiency,power,powerful,"## \file; ## \ingroup tutorial_pyroot; ## \notebook; ## Example of function called when a mouse event occurs in a pad.; ## When moving the mouse in the canvas, a second canvas shows the; ## projection along X of the bin corresponding to the Y position; ## of the mouse. The resulting histogram is fitted with a gaussian.; ## A ""dynamic"" line shows the current bin position in Y.; ## This more elaborated example can be used as a starting point; ## to develop more powerful interactive applications exploiting CINT; ## as a development engine.; ##; ## Note that a class is used to hold on to the canvas that display; ## the selected slice.; ##; ## \macro_image; ## \macro_code; ##; ## \author Rene Brun, Johann Cohen-Tanugi, Wim Lavrijsen, Enric Tejedor; # erase old position and draw a line at current position; # create or set the display canvases; # draw slice corresponding to mouse position; # create a new canvas.; # create a 2-d histogram, fill and draw it; # pass ctypes doubles by reference, then retrieve their modified values with .value; # Add a TExec object to the canvas (explicit use of __main__ is for IPython)",MatchSource.CODE_COMMENT,tutorials/pyroot/DynamicSlice.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/pyroot/DynamicSlice.py
Modifiability,variab,variable,"## \file; ## \ingroup tutorial_pyroot; ## \notebook -js; ## TF1 example.; ##; ## \macro_image; ## \macro_code; ##; ## \author Wim Lavrijsen; # We create a formula object and compute the value of this formula; # for two different values of the x variable.; # Create a one dimensional function and draw it; # Before leaving this demo, we print the list of objects known to ROOT; #",MatchSource.CODE_COMMENT,tutorials/pyroot/formula1.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/pyroot/formula1.py
Availability,error,error,"## \file; ## \ingroup tutorial_pyroot; ## \notebook -js; ## A Simple Graph with error bars; ##; ## \macro_image; ## \macro_code; ##; ## \author Wim Lavrijsen",MatchSource.CODE_COMMENT,tutorials/pyroot/gerrors.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/pyroot/gerrors.py
Security,access,access,"## \file; ## \ingroup tutorial_pyroot; ## \notebook; ## A Simple Graph Example; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \author Wim Lavrijsen; # TCanvas.Update() draws the frame, after which one can change it; # If the graph does not appear, try using the ""i"" flag, e.g. ""python3 -i graph.py""; # This will access the interactive mode after executing the script, and thereby persist; # long enough for the graph to appear.",MatchSource.CODE_COMMENT,tutorials/pyroot/graph.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/pyroot/graph.py
Availability,error,errors,"## \file; ## \ingroup tutorial_pyroot; ## \notebook -js; ## A Simple histogram drawing example; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \author Wim Lavrijsen; #; # We connect the ROOT file generated in a previous tutorial; #; # Draw a global picture title; #; # Draw histogram hpx in first pad with the default option.; #; # Draw hpx as a lego. Clicking on the lego area will show; # a ""transparent cube"" to guide you rotating the lego in real time.; #; # Draw hpx with its errors and a marker.; #; # The following illustrates how to add comments using a PaveText.; # Attributes of text/lines/boxes added to a PaveText can be modified.; # The AddText function returns a pointer to the added object.",MatchSource.CODE_COMMENT,tutorials/pyroot/h1ReadAndDraw.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/pyroot/h1ReadAndDraw.py
Usability,guid,guide,"## \file; ## \ingroup tutorial_pyroot; ## \notebook -js; ## A Simple histogram drawing example; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \author Wim Lavrijsen; #; # We connect the ROOT file generated in a previous tutorial; #; # Draw a global picture title; #; # Draw histogram hpx in first pad with the default option.; #; # Draw hpx as a lego. Clicking on the lego area will show; # a ""transparent cube"" to guide you rotating the lego in real time.; #; # Draw hpx with its errors and a marker.; #; # The following illustrates how to add comments using a PaveText.; # Attributes of text/lines/boxes added to a PaveText can be modified.; # The AddText function returns a pointer to the added object.",MatchSource.CODE_COMMENT,tutorials/pyroot/h1ReadAndDraw.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/pyroot/h1ReadAndDraw.py
Performance,cache,cache,"## \file; ## \ingroup tutorial_pyroot; ## \notebook -js; ## This program creates :; ## - a one dimensional histogram; ## - a two dimensional histogram; ## - a profile histogram; ## - a memory-resident ntuple; ##; ## These objects are filled with some random numbers and saved on a file.; ##; ## \macro_image; ## \macro_code; ##; ## \author Wim Lavrijsen, Enric Tejedor; # Create a new canvas, and customize it.; # Create a new ROOT binary machine independent file.; # Note that this file may contain any kind of ROOT objects, histograms,; # pictures, graphics objects, detector geometries, tracks, events, etc..; # This file is now becoming the current directory.; # Create some histograms, a profile histogram and an ntuple; # Set canvas/frame attributes.; # Initialize random number generator.; # For speed, bind and cache the Fill member functions,; # Fill histograms randomly.; # Generate random values. Use ctypes to pass doubles by reference; # Retrieve the generated values; # Fill histograms.; # Update display every kUPDATE events.; # allow user interrupt; # Destroy member functions cache.; # Save all objects in this file.; # Note that the file is automatically closed when application terminates; # or when the file destructor is called.",MatchSource.CODE_COMMENT,tutorials/pyroot/hsimple.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/pyroot/hsimple.py
Safety,detect,detector,"## \file; ## \ingroup tutorial_pyroot; ## \notebook -js; ## This program creates :; ## - a one dimensional histogram; ## - a two dimensional histogram; ## - a profile histogram; ## - a memory-resident ntuple; ##; ## These objects are filled with some random numbers and saved on a file.; ##; ## \macro_image; ## \macro_code; ##; ## \author Wim Lavrijsen, Enric Tejedor; # Create a new canvas, and customize it.; # Create a new ROOT binary machine independent file.; # Note that this file may contain any kind of ROOT objects, histograms,; # pictures, graphics objects, detector geometries, tracks, events, etc..; # This file is now becoming the current directory.; # Create some histograms, a profile histogram and an ntuple; # Set canvas/frame attributes.; # Initialize random number generator.; # For speed, bind and cache the Fill member functions,; # Fill histograms randomly.; # Generate random values. Use ctypes to pass doubles by reference; # Retrieve the generated values; # Fill histograms.; # Update display every kUPDATE events.; # allow user interrupt; # Destroy member functions cache.; # Save all objects in this file.; # Note that the file is automatically closed when application terminates; # or when the file destructor is called.",MatchSource.CODE_COMMENT,tutorials/pyroot/hsimple.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/pyroot/hsimple.py
Deployability,update,update,"## \file; ## \ingroup tutorial_pyroot; ## \notebook -js; ## Simple example illustrating how to use the C++ interpreter; ##; ## \macro_image; ## \macro_code; ##; ## \author Wim Lavrijsen; # Create a new canvas, and customize it.; # Create some histograms.; # this makes sure that the sum of squares of weights will be stored; # Set canvas/frame attributes.; # Initialize random number generator.; # for speed, bind and cache the Fill member functions; # Fill histograms randomly; # Generate random values.; # Fill histograms.; # Update display every kUPDATE events.; # Destroy member functions cache.; # Done, finalized and trigger an update.; # to redraw axis hidden by the fill area",MatchSource.CODE_COMMENT,tutorials/pyroot/hsum.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/pyroot/hsum.py
Performance,cache,cache,"## \file; ## \ingroup tutorial_pyroot; ## \notebook -js; ## Simple example illustrating how to use the C++ interpreter; ##; ## \macro_image; ## \macro_code; ##; ## \author Wim Lavrijsen; # Create a new canvas, and customize it.; # Create some histograms.; # this makes sure that the sum of squares of weights will be stored; # Set canvas/frame attributes.; # Initialize random number generator.; # for speed, bind and cache the Fill member functions; # Fill histograms randomly; # Generate random values.; # Fill histograms.; # Update display every kUPDATE events.; # Destroy member functions cache.; # Done, finalized and trigger an update.; # to redraw axis hidden by the fill area",MatchSource.CODE_COMMENT,tutorials/pyroot/hsum.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/pyroot/hsum.py
Safety,detect,detector,"## \file; ## \ingroup tutorial_pyroot; ## This macro generates two views of the NA49 detector.; ##; ## To have a better and dynamic view of any of these pads,; ## you can click with the middle button of your mouse to select it.; ## Then select ""View with x3d"" in the VIEW menu of the Canvas.; ## Once in x3d, you are in wireframe mode by default.; ## You can switch to:; ## - Hidden Line mode by typing E; ## - Solid mode by typing R; ## - Wireframe mode by typing W; ## - Stereo mode by clicking S (and you need special glasses); ## - To leave x3d type Q; ##; ## \macro_code; ##; ## \author Wim Lavrijsen; #; # Set current geometry; # Set current pad",MatchSource.CODE_COMMENT,tutorials/pyroot/na49view.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/pyroot/na49view.py
Deployability,update,updated,"## \file; ## \ingroup tutorial_pyroot; ## Example frame with one box where the user can increase or decrease a number; ## and the shown value will be updated accordingly.; ##; ## \macro_code; ##; ## \author Wim Lavrijsen",MatchSource.CODE_COMMENT,tutorials/pyroot/numberEntry.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/pyroot/numberEntry.py
Safety,avoid,avoiding,"6 7 5.710000E-10 8.828400 24.237500 -0.008818 0 0 0 0; ## 08112010.160626 7 5.719000E-10 8.828400 24.237500 -0.008818 0 0 0 0; ## 08112010.160627 7 5.719000E-10 9.014300 24.237400 -0.028564 0 0 0 NaN; ## 08112010.160627 7 5.711000E-10 8.786000 24.237400 -0.008818 0 0 0 0; ## 08112010.160628 7 5.702000E-10 8.786000 24.237400 -0.009141 0 0 0 0; ## 08112010.160633 7 5.710000E-10 9.016200 24.237200 -0.008818 0 0 0 0; ## 7 5.710000E-10 8.903400 24.237200 -0.008818 0 0 0 0; ## ~~~; ##; ## These data require some massaging, including:; ##; ## - Date/Time has a blank ('') entry that must be handled; ## - The headers are not in the correct format; ## - Tab-separated entries with additional white space; ## - NaN entries; ##; ## \macro_code; ##; ## \author Michael Marino; # The mapping dictionary defines the proper branch names and types given a header name.; # Grab the header row of the file. In this particular example,; # the data are separated using tabs, but some of the header names; # include spaces and are not generally in the ROOT expected format, e.g.; #; # FloatData/F:StringData/C:IntData/I; #; # etc. Therefore, we grab the header_row of the file, and use; # a python dictionary to set up the appropriate branch descriptor; # line.; # Open a file, grab the first line, strip the new lines; # and split it into a list along 'tab' boundaries; # Create the branch descriptor; #print(branch_descriptor); # Handling the input and output names. Using the same; # base name for the ROOT output file.; # Clean the data entries: remove the first (header) row.; # Ensure empty strings are tagged as such since; # ROOT doesn't differentiate between different types; # of white space. Therefore, we change all of these; # entries to 'empty'. Also, avoiding any lines that begin; # with '#'; # Removing NaN, setting these entries to 0.0.; # Also joining the list of strings into one large string.; #print(file_as_string); # creating an istringstream to pass into ReadStream; # Now read the stream",MatchSource.CODE_COMMENT,tutorials/pyroot/parse_CSV_file_with_TTree_ReadStream.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/pyroot/parse_CSV_file_with_TTree_ReadStream.py
Integrability,inject,injecting,"## \file; ## \ingroup tutorial_pyroot; ## \notebook -nodraw; ## This tutorial shows how to use the `@pythonization` decorator to add extra; ## behaviour to C++ user classes that are used from Python via PyROOT.; ##; ## \macro_code; ## \macro_output; ##; ## \date November 2021; ## \author Enric Tejedor; # Let's first define a new C++ class. In this tutorial, we will see how we can; # ""pythonize"" this class, i.e. how we can add some extra behaviour to it to; # make it more pythonic or easier to use from Python.; #; # Note: In this example, the class is defined dynamically for demonstration; # purposes, but it could also be a C++ class defined in some library or header.; # For more information about loading C++ user code to be used from Python with; # PyROOT, please see:; # https://root.cern.ch/manual/python/#loading-user-libraries-and-just-in-time-compilation-jitting; '''; class MyClass {};; '''; # Next, we define a pythonizor function: the function that will be responsible; # for injecting new behaviour in our C++ class `MyClass`.; #; # To convert a given Python function into a pythonizor, we need to decorate it; # with the @pythonization decorator. Such decorator allows us to define which; # which class we want to pythonize by providing its class name and its; # namespace (if the latter is not specified, it defaults to the global; # namespace, i.e. '::').; #; # The decorated function - the pythonizor - must accept either one or two; # parameters:; # 1. The class to be pythonized (proxy object where new behaviour can be; # injected); # 2. The fully-qualified name of that class (optional).; #; # Let's see all this with a simple example. Suppose I would like to define how; # `MyClass` objects are represented as a string in Python (i.e. what would be; # shown when I print that object). For that purpose, I can define the following; # pythonizor function. There are two important things to be noted here:; # - The @pythonization decorator has one argument that specifies our ",MatchSource.CODE_COMMENT,tutorials/pyroot/pyroot002_pythonizationDecorator.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/pyroot/pyroot002_pythonizationDecorator.py
Performance,load,loading,"## \file; ## \ingroup tutorial_pyroot; ## \notebook -nodraw; ## This tutorial shows how to use the `@pythonization` decorator to add extra; ## behaviour to C++ user classes that are used from Python via PyROOT.; ##; ## \macro_code; ## \macro_output; ##; ## \date November 2021; ## \author Enric Tejedor; # Let's first define a new C++ class. In this tutorial, we will see how we can; # ""pythonize"" this class, i.e. how we can add some extra behaviour to it to; # make it more pythonic or easier to use from Python.; #; # Note: In this example, the class is defined dynamically for demonstration; # purposes, but it could also be a C++ class defined in some library or header.; # For more information about loading C++ user code to be used from Python with; # PyROOT, please see:; # https://root.cern.ch/manual/python/#loading-user-libraries-and-just-in-time-compilation-jitting; '''; class MyClass {};; '''; # Next, we define a pythonizor function: the function that will be responsible; # for injecting new behaviour in our C++ class `MyClass`.; #; # To convert a given Python function into a pythonizor, we need to decorate it; # with the @pythonization decorator. Such decorator allows us to define which; # which class we want to pythonize by providing its class name and its; # namespace (if the latter is not specified, it defaults to the global; # namespace, i.e. '::').; #; # The decorated function - the pythonizor - must accept either one or two; # parameters:; # 1. The class to be pythonized (proxy object where new behaviour can be; # injected); # 2. The fully-qualified name of that class (optional).; #; # Let's see all this with a simple example. Suppose I would like to define how; # `MyClass` objects are represented as a string in Python (i.e. what would be; # shown when I print that object). For that purpose, I can define the following; # pythonizor function. There are two important things to be noted here:; # - The @pythonization decorator has one argument that specifies our ",MatchSource.CODE_COMMENT,tutorials/pyroot/pyroot002_pythonizationDecorator.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/pyroot/pyroot002_pythonizationDecorator.py
Security,inject,injecting,"## \file; ## \ingroup tutorial_pyroot; ## \notebook -nodraw; ## This tutorial shows how to use the `@pythonization` decorator to add extra; ## behaviour to C++ user classes that are used from Python via PyROOT.; ##; ## \macro_code; ## \macro_output; ##; ## \date November 2021; ## \author Enric Tejedor; # Let's first define a new C++ class. In this tutorial, we will see how we can; # ""pythonize"" this class, i.e. how we can add some extra behaviour to it to; # make it more pythonic or easier to use from Python.; #; # Note: In this example, the class is defined dynamically for demonstration; # purposes, but it could also be a C++ class defined in some library or header.; # For more information about loading C++ user code to be used from Python with; # PyROOT, please see:; # https://root.cern.ch/manual/python/#loading-user-libraries-and-just-in-time-compilation-jitting; '''; class MyClass {};; '''; # Next, we define a pythonizor function: the function that will be responsible; # for injecting new behaviour in our C++ class `MyClass`.; #; # To convert a given Python function into a pythonizor, we need to decorate it; # with the @pythonization decorator. Such decorator allows us to define which; # which class we want to pythonize by providing its class name and its; # namespace (if the latter is not specified, it defaults to the global; # namespace, i.e. '::').; #; # The decorated function - the pythonizor - must accept either one or two; # parameters:; # 1. The class to be pythonized (proxy object where new behaviour can be; # injected); # 2. The fully-qualified name of that class (optional).; #; # Let's see all this with a simple example. Suppose I would like to define how; # `MyClass` objects are represented as a string in Python (i.e. what would be; # shown when I print that object). For that purpose, I can define the following; # pythonizor function. There are two important things to be noted here:; # - The @pythonization decorator has one argument that specifies our ",MatchSource.CODE_COMMENT,tutorials/pyroot/pyroot002_pythonizationDecorator.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/pyroot/pyroot002_pythonizationDecorator.py
Usability,simpl,simple,"ss defined in some library or header.; # For more information about loading C++ user code to be used from Python with; # PyROOT, please see:; # https://root.cern.ch/manual/python/#loading-user-libraries-and-just-in-time-compilation-jitting; '''; class MyClass {};; '''; # Next, we define a pythonizor function: the function that will be responsible; # for injecting new behaviour in our C++ class `MyClass`.; #; # To convert a given Python function into a pythonizor, we need to decorate it; # with the @pythonization decorator. Such decorator allows us to define which; # which class we want to pythonize by providing its class name and its; # namespace (if the latter is not specified, it defaults to the global; # namespace, i.e. '::').; #; # The decorated function - the pythonizor - must accept either one or two; # parameters:; # 1. The class to be pythonized (proxy object where new behaviour can be; # injected); # 2. The fully-qualified name of that class (optional).; #; # Let's see all this with a simple example. Suppose I would like to define how; # `MyClass` objects are represented as a string in Python (i.e. what would be; # shown when I print that object). For that purpose, I can define the following; # pythonizor function. There are two important things to be noted here:; # - The @pythonization decorator has one argument that specifies our target; # class is `MyClass`.; # - The pythonizor function `pythonizor_of_myclass` provides and injects a new; # implementation for `__str__`, the mechanism that Python provides to define; # how to represent objects as strings. This new implementation; # always returns the string ""This is a MyClass object"".; # Once we have defined our pythonizor function, let's see it in action.; # We will now use the `MyClass` class for the first time from Python: we will; # create a new instance of that class. At this moment, the pythonizor will; # execute and modify the class - pythonizors are always lazily run when a given; # class is used fo",MatchSource.CODE_COMMENT,tutorials/pyroot/pyroot002_pythonizationDecorator.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/pyroot/pyroot002_pythonizationDecorator.py
Energy Efficiency,power,powered,"## \file; ## \ingroup tutorial_pyroot; ## \notebook -nodraw; ## This tutorial illustrates the pretty printing feature of PyROOT, which reveals; ## the content of the object if a string representation is requested, e.g., by; ## Python's print statement. The printing behaves similar to the ROOT prompt; ## powered by the C++ interpreter cling.; ##; ## \macro_code; ## \macro_output; ##; ## \date June 2018; ## \author Stefan Wunsch, Enric Tejedor; # Create an object with PyROOT; # Print the object, which reveals the content. Note that `print` calls the special; # method `__str__` of the object internally.; # The output can be retrieved as string by any function that triggers the `__str__`; # special method of the object, e.g., `str` or `format`.; # Note that the interactive Python prompt does not call `__str__`, it calls; # `__repr__`, which implements a formal and unique string representation of; # the object.; # The print output behaves similar to the ROOT prompt, e.g., here for a ROOT histogram.; # If cling cannot produce any nice representation for the class, we fall back to a; # ""<ClassName at address>"" format, which is what `__repr__` returns",MatchSource.CODE_COMMENT,tutorials/pyroot/pyroot003_prettyPrinting.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/pyroot/pyroot003_prettyPrinting.py
Availability,avail,available,"## \file; ## \ingroup tutorial_pyroot; ## \notebook -nodraw; ## This tutorial illustrates how PyROOT supports declaring C++ callables from; ## Python callables making them, for example, usable with RDataFrame. The feature; ## uses the numba Python package for just-in-time compilation of the Python callable; ## and supports fundamental types and ROOT::RVec thereof.; ##; ## \macro_code; ## \macro_output; ##; ## \date March 2020; ## \author Stefan Wunsch; # To mark a Python callable to be used from C++, you have to use the decorator; # provided by PyROOT passing the C++ types of the input arguments and the return; # value.; # The Python callable is now available from C++ in the Numba namespace.; # For example, we can use it from the interpreter.; # Or we can use the callable as well within a RDataFrame workflow.; # ROOT uses the numba Python package to create C++ functions from python ones.; # We support as input and return types of the callable fundamental types and; # ROOT::RVec thereof. See the following callable computing the power of the; # elements in an array.; '''; ROOT::RVecF x = {0, 1, 2, 3};; cout << ""pypowarray("" << x << "", 3) = "" << Numba::pypowarray(x, 3) << endl;; '''; # and now with RDataFrame; # 1 + 4 + 9 == 14",MatchSource.CODE_COMMENT,tutorials/pyroot/pyroot004_NumbaDeclare.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/pyroot/pyroot004_NumbaDeclare.py
Energy Efficiency,power,power,"## \file; ## \ingroup tutorial_pyroot; ## \notebook -nodraw; ## This tutorial illustrates how PyROOT supports declaring C++ callables from; ## Python callables making them, for example, usable with RDataFrame. The feature; ## uses the numba Python package for just-in-time compilation of the Python callable; ## and supports fundamental types and ROOT::RVec thereof.; ##; ## \macro_code; ## \macro_output; ##; ## \date March 2020; ## \author Stefan Wunsch; # To mark a Python callable to be used from C++, you have to use the decorator; # provided by PyROOT passing the C++ types of the input arguments and the return; # value.; # The Python callable is now available from C++ in the Numba namespace.; # For example, we can use it from the interpreter.; # Or we can use the callable as well within a RDataFrame workflow.; # ROOT uses the numba Python package to create C++ functions from python ones.; # We support as input and return types of the callable fundamental types and; # ROOT::RVec thereof. See the following callable computing the power of the; # elements in an array.; '''; ROOT::RVecF x = {0, 1, 2, 3};; cout << ""pypowarray("" << x << "", 3) = "" << Numba::pypowarray(x, 3) << endl;; '''; # and now with RDataFrame; # 1 + 4 + 9 == 14",MatchSource.CODE_COMMENT,tutorials/pyroot/pyroot004_NumbaDeclare.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/pyroot/pyroot004_NumbaDeclare.py
Usability,usab,usable,"## \file; ## \ingroup tutorial_pyroot; ## \notebook -nodraw; ## This tutorial illustrates how PyROOT supports declaring C++ callables from; ## Python callables making them, for example, usable with RDataFrame. The feature; ## uses the numba Python package for just-in-time compilation of the Python callable; ## and supports fundamental types and ROOT::RVec thereof.; ##; ## \macro_code; ## \macro_output; ##; ## \date March 2020; ## \author Stefan Wunsch; # To mark a Python callable to be used from C++, you have to use the decorator; # provided by PyROOT passing the C++ types of the input arguments and the return; # value.; # The Python callable is now available from C++ in the Numba namespace.; # For example, we can use it from the interpreter.; # Or we can use the callable as well within a RDataFrame workflow.; # ROOT uses the numba Python package to create C++ functions from python ones.; # We support as input and return types of the callable fundamental types and; # ROOT::RVec thereof. See the following callable computing the power of the; # elements in an array.; '''; ROOT::RVecF x = {0, 1, 2, 3};; cout << ""pypowarray("" << x << "", 3) = "" << Numba::pypowarray(x, 3) << endl;; '''; # and now with RDataFrame; # 1 + 4 + 9 == 14",MatchSource.CODE_COMMENT,tutorials/pyroot/pyroot004_NumbaDeclare.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/pyroot/pyroot004_NumbaDeclare.py
Availability,alive,alive," as a Python context; ## manager.; ##; ## \macro_code; ## \macro_output; ##; ## \date March 2022; ## \author Vincenzo Eduardo Padulano CERN/UPV; # By default, objects of some ROOT types such as `TH1` and its derived types; # are automatically attached to a ROOT.TDirectory when they are created.; # Specifically, at any given point of a ROOT application, the ROOT.gDirectory; # object tells which is the current directory where objects will be attached to.; # The next line will print 'PyROOT' as the name of the current directory.; # That is the global directory created when using ROOT from Python, which is; # the ROOT.gROOT object.; # We can check to which directory a newly created histogram is attached.; # For quick saving and forgetting of objects into ROOT files, it is possible to; # open a TFile as a Python context manager. In the context, objects can be; # created, modified and finally written to the file. At the end of the context,; # the file will be automatically closed.; # Inside the context, the current directory is the open file; # And the created histogram is automatically attached to the file; # Before exiting the context, objects can be written to the file; # When the TFile.Close method is called, the current directory is automatically; # set again to ROOT.gROOT. Objects that were attached to the file inside the; # context are automatically deleted and made 'None' when the file is closed.; # Also reading data from a TFile can be done in a context manager. Information; # stored in the objects of the file can be queried and used inside the context.; # After the context, the objects are not usable anymore because the file is; # automatically closed. This means you should use this pattern as a quick way; # to get information or modify objects from a certain file, without needing to; # keep the histograms alive afterwards.; # Retrieve histogram using the name given to f.WriteObject in the previous; # with statement; # Cleanup the file created for this tutorial",MatchSource.CODE_COMMENT,tutorials/pyroot/pyroot005_tfile_context_manager.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/pyroot/pyroot005_tfile_context_manager.py
Usability,usab,usable," as a Python context; ## manager.; ##; ## \macro_code; ## \macro_output; ##; ## \date March 2022; ## \author Vincenzo Eduardo Padulano CERN/UPV; # By default, objects of some ROOT types such as `TH1` and its derived types; # are automatically attached to a ROOT.TDirectory when they are created.; # Specifically, at any given point of a ROOT application, the ROOT.gDirectory; # object tells which is the current directory where objects will be attached to.; # The next line will print 'PyROOT' as the name of the current directory.; # That is the global directory created when using ROOT from Python, which is; # the ROOT.gROOT object.; # We can check to which directory a newly created histogram is attached.; # For quick saving and forgetting of objects into ROOT files, it is possible to; # open a TFile as a Python context manager. In the context, objects can be; # created, modified and finally written to the file. At the end of the context,; # the file will be automatically closed.; # Inside the context, the current directory is the open file; # And the created histogram is automatically attached to the file; # Before exiting the context, objects can be written to the file; # When the TFile.Close method is called, the current directory is automatically; # set again to ROOT.gROOT. Objects that were attached to the file inside the; # context are automatically deleted and made 'None' when the file is closed.; # Also reading data from a TFile can be done in a context manager. Information; # stored in the objects of the file can be queried and used inside the context.; # After the context, the objects are not usable anymore because the file is; # automatically closed. This means you should use this pattern as a quick way; # to get information or modify objects from a certain file, without needing to; # keep the histograms alive afterwards.; # Retrieve histogram using the name given to f.WriteObject in the previous; # with statement; # Cleanup the file created for this tutorial",MatchSource.CODE_COMMENT,tutorials/pyroot/pyroot005_tfile_context_manager.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/pyroot/pyroot005_tfile_context_manager.py
Availability,avail,available,"## \file; ## \ingroup tutorial_pyroot; ## \notebook -nodraw; ## This tutorial demonstrates the usage of the TContext class as a Python context; ## manager. This functionality is related with how TFile works, so it is; ## suggested to also take a look at the pyroot005 tutorial.; ##; ## \macro_code; ## \macro_output; ##; ## \date March 2022; ## \author Vincenzo Eduardo Padulano CERN/UPV; # Sometimes it is useful to have multiple open files at once. In such cases,; # the current directory will always be the file that was open last.; # Changing directory into another file can be safely done through a TContext; # context manager.; # Inside the statement, the current directory is file_1; # After the context, the current directory is restored back to file_2. Also, the; # two files are kept open. This means that objects read, written or modified; # inside the context are still available afterwards.; # TContext and TFile context managers can also be used in conjunction, allowing; # for safely:; # - Opening a file, creating, modifying, writing and reading objects in it.; # - Closing the file, storing it on disk.; # - Restoring the previous value of gDirectory to the latest file opened before; # this context, rather than to the global ROOT.gROOT; # Remember that the TContext must be initialized before the TFile, otherwise the; # current directory would already be set to the file opened for this context.; # Cleanup the files created for this tutorial",MatchSource.CODE_COMMENT,tutorials/pyroot/pyroot006_tcontext_context_manager.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/pyroot/pyroot006_tcontext_context_manager.py
Safety,safe,safely,"## \file; ## \ingroup tutorial_pyroot; ## \notebook -nodraw; ## This tutorial demonstrates the usage of the TContext class as a Python context; ## manager. This functionality is related with how TFile works, so it is; ## suggested to also take a look at the pyroot005 tutorial.; ##; ## \macro_code; ## \macro_output; ##; ## \date March 2022; ## \author Vincenzo Eduardo Padulano CERN/UPV; # Sometimes it is useful to have multiple open files at once. In such cases,; # the current directory will always be the file that was open last.; # Changing directory into another file can be safely done through a TContext; # context manager.; # Inside the statement, the current directory is file_1; # After the context, the current directory is restored back to file_2. Also, the; # two files are kept open. This means that objects read, written or modified; # inside the context are still available afterwards.; # TContext and TFile context managers can also be used in conjunction, allowing; # for safely:; # - Opening a file, creating, modifying, writing and reading objects in it.; # - Closing the file, storing it on disk.; # - Restoring the previous value of gDirectory to the latest file opened before; # this context, rather than to the global ROOT.gROOT; # Remember that the TContext must be initialized before the TFile, otherwise the; # current directory would already be set to the file opened for this context.; # Cleanup the files created for this tutorial",MatchSource.CODE_COMMENT,tutorials/pyroot/pyroot006_tcontext_context_manager.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/pyroot/pyroot006_tcontext_context_manager.py
Availability,error,errors,"## \file; ## \ingroup tutorial_pyroot; ## \notebook; ## Display two histograms and their ratio.; ##; ## This program illustrates how to plot two histograms and their; ## ratio on the same canvas. Original macro by Olivier Couet.; ##; ## \macro_code; ##; ## \author Michael Moran; # Set up plot for markers and errors; # Adjust y-axis settings; # Adjust x-axis settings; # Upper histogram plot is pad1; # joins upper and lower plot; # Lower ratio plot is pad2; # returns to main canvas before defining pad2; # joins upper and lower plot; # create required parts; # draw everything; # to avoid clipping the bottom zero, redraw a small axis; # To hold window open when running from command line; # text = raw_input()",MatchSource.CODE_COMMENT,tutorials/pyroot/ratioplot.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/pyroot/ratioplot.py
Safety,avoid,avoid,"## \file; ## \ingroup tutorial_pyroot; ## \notebook; ## Display two histograms and their ratio.; ##; ## This program illustrates how to plot two histograms and their; ## ratio on the same canvas. Original macro by Olivier Couet.; ##; ## \macro_code; ##; ## \author Michael Moran; # Set up plot for markers and errors; # Adjust y-axis settings; # Adjust x-axis settings; # Upper histogram plot is pad1; # joins upper and lower plot; # Lower ratio plot is pad2; # returns to main canvas before defining pad2; # joins upper and lower plot; # create required parts; # draw everything; # to avoid clipping the bottom zero, redraw a small axis; # To hold window open when running from command line; # text = raw_input()",MatchSource.CODE_COMMENT,tutorials/pyroot/ratioplot.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/pyroot/ratioplot.py
Security,access,access,"## \file; ## \ingroup tutorial_pyroot; ## \notebook -nodraw; ## example of macro to read data from an ascii file and; ## create a root file with a Tree.; ##; ## NOTE: comparing the results of this macro with those of staff.C, you'll; ## notice that the resultant file is a couple of bytes smaller, because the; ## code below strips all white-spaces, whereas the .C version does not.; ##; ## \macro_code; ##; ## \author Wim Lavrijsen; ## A C/C++ structure is required, to allow memory based access; ## Function to read in data from ASCII file and fill the ROOT tree; # The input file cern.dat is a copy of the CERN staff data base; # from 1988; # note that the branches Division and Nation cannot be on the first branch; # assign as integers; # assign as strings; #### run fill function if invoked on CLI",MatchSource.CODE_COMMENT,tutorials/pyroot/staff.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/pyroot/staff.py
Performance,load,loaded,"## \file; ## \ingroup tutorial_pyroot; ## \notebook; ## This macro is an example of graphs in log scales with annotations.; ##; ## The presented results; ## are predictions of invariant cross-section of Direct Photons produced; ## at RHIC energies, based on the universality of scaling function H(z).; ##; ##; ## These Figures were published in JINR preprint E2-98-64, Dubna,; ## 1998 and submitted to CPC.; ##; ## \macro_image; ## \macro_code; ##; ## \authors Michael Tokarev, Elena Potrebenikova (JINR Dubna); #_______________________________________________________________________________; # print 'ENR= %f DENS= %f PTMIN= %f PTMAX= %f DELP= %f ' % (ENERG,DENS,PTMIN,PTMAX,DELP); #_______________________________________________________________________________; # Create a new canvas.; # prevent deteletion at end of zdemo; #; # Cross-section of direct photon production in pp collisions at 500 GeV vs Pt; #; # create a 2-d histogram to define the range; # note the label that is used!; #; # Cross-section of direct photon production in pp collisions at 200 GeV vs Pt; #; #; # Cross-section of direct photon production in pp collisions at 500 GeV vs Pt; #; # note the label that is used!; # note the label that is used!; # note the label that is used!; # run if loaded as script",MatchSource.CODE_COMMENT,tutorials/pyroot/zdemo.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/pyroot/zdemo.py
Safety,predict,predictions,"## \file; ## \ingroup tutorial_pyroot; ## \notebook; ## This macro is an example of graphs in log scales with annotations.; ##; ## The presented results; ## are predictions of invariant cross-section of Direct Photons produced; ## at RHIC energies, based on the universality of scaling function H(z).; ##; ##; ## These Figures were published in JINR preprint E2-98-64, Dubna,; ## 1998 and submitted to CPC.; ##; ## \macro_image; ## \macro_code; ##; ## \authors Michael Tokarev, Elena Potrebenikova (JINR Dubna); #_______________________________________________________________________________; # print 'ENR= %f DENS= %f PTMIN= %f PTMAX= %f DELP= %f ' % (ENERG,DENS,PTMIN,PTMAX,DELP); #_______________________________________________________________________________; # Create a new canvas.; # prevent deteletion at end of zdemo; #; # Cross-section of direct photon production in pp collisions at 500 GeV vs Pt; #; # create a 2-d histogram to define the range; # note the label that is used!; #; # Cross-section of direct photon production in pp collisions at 200 GeV vs Pt; #; #; # Cross-section of direct photon production in pp collisions at 500 GeV vs Pt; #; # note the label that is used!; # note the label that is used!; # note the label that is used!; # run if loaded as script",MatchSource.CODE_COMMENT,tutorials/pyroot/zdemo.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/pyroot/zdemo.py
Testability,log,log,"## \file; ## \ingroup tutorial_pyroot; ## \notebook; ## This macro is an example of graphs in log scales with annotations.; ##; ## The presented results; ## are predictions of invariant cross-section of Direct Photons produced; ## at RHIC energies, based on the universality of scaling function H(z).; ##; ##; ## These Figures were published in JINR preprint E2-98-64, Dubna,; ## 1998 and submitted to CPC.; ##; ## \macro_image; ## \macro_code; ##; ## \authors Michael Tokarev, Elena Potrebenikova (JINR Dubna); #_______________________________________________________________________________; # print 'ENR= %f DENS= %f PTMIN= %f PTMAX= %f DELP= %f ' % (ENERG,DENS,PTMIN,PTMAX,DELP); #_______________________________________________________________________________; # Create a new canvas.; # prevent deteletion at end of zdemo; #; # Cross-section of direct photon production in pp collisions at 500 GeV vs Pt; #; # create a 2-d histogram to define the range; # note the label that is used!; #; # Cross-section of direct photon production in pp collisions at 200 GeV vs Pt; #; #; # Cross-section of direct photon production in pp collisions at 500 GeV vs Pt; #; # note the label that is used!; # note the label that is used!; # note the label that is used!; # run if loaded as script",MatchSource.CODE_COMMENT,tutorials/pyroot/zdemo.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/pyroot/zdemo.py
Availability,avail,available,"ut with usage of ROOT7 graphics; ## Run macro with python3 -i df104.py command to get interactive canvas; ##; ## \macro_image (df104.png); ## \macro_code; ##; ## \date 2021-06-15; ## \authors Stefan Wunsch (KIT, CERN) Sergey Linev (GSI); # Enable multi-threading; # Create a ROOT dataframe for each dataset; # Apply scale factors and MC weight for simulated events and a weight of 1 for the data; # Select the events for the analysis; # Apply preselection cut on photon trigger; # Find two good muons with tight ID, pt > 25 GeV and not in the transition region between barrel and encap; # Take only isolated photons; # Compile a function to compute the invariant mass of the diphoton system; """"""; using Vec_t = const ROOT::VecOps::RVec<float>;; float ComputeInvariantMass(Vec_t& pt, Vec_t& eta, Vec_t& phi, Vec_t& e) {; ROOT::Math::PtEtaPhiEVector p1(pt[0], eta[0], phi[0], e[0]);; ROOT::Math::PtEtaPhiEVector p2(pt[1], eta[1], phi[1], e[1]);; return (p1 + p2).mass() / 1000.0;; }; """"""; # Define a new column with the invariant mass and perform final event selection; # Make four vectors and compute invariant mass; # Make additional kinematic cuts and select mass window; # Book histogram of the invariant mass with this selection; # Run the event loop; # RunGraphs allows to run the event loops of the separate RDataFrame graphs; # concurrently. This results in an improved usage of the available resources; # if each separate RDataFrame can not utilize all available resources, e.g.,; # because not enough data is available.; # Create the plot; # Set styles - not yet available for v7; # ROOT.gROOT.SetStyle(""ATLAS""); # Create canvas with pads for main plot and data/MC ratio; # Fit signal + background model to data; # do not draw fit function; # Draw data; # Draw fit; # Draw background; # Scale simulated events with luminosity * cross-section / sum of weights; # and merge to single Higgs signal; # Draw ratio; # Add RLegend; # Add ATLAS labels; # show canvas finally; # Save plot in PNG file",MatchSource.CODE_COMMENT,tutorials/rcanvas/df104.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/rcanvas/df104.py
Deployability,release,release,"## \file; ## \ingroup tutorial_rcanvas; ## The Higgs to two photons analysis from the ATLAS Open Data 2020 release, with RDataFrame.; ##; ## This tutorial is the Higgs to two photons analysis from the ATLAS Open Data release in 2020; ## (http://opendata.atlas.cern/release/2020/documentation/). The data was taken with the ATLAS detector; ## during 2016 at a center-of-mass energy of 13 TeV. Although the Higgs to two photons decay is very rare,; ## the contribution of the Higgs can be seen as a narrow peak around 125 GeV because of the excellent; ## reconstruction and identification efficiency of photons at the ATLAS experiment.; ##; ## The analysis is translated to a RDataFrame workflow processing 1.7 GB of simulated events and data.; ##; ## This macro is replica of tutorials/dataframe/df104_HiggsToTwoPhotons.py, but with usage of ROOT7 graphics; ## Run macro with python3 -i df104.py command to get interactive canvas; ##; ## \macro_image (df104.png); ## \macro_code; ##; ## \date 2021-06-15; ## \authors Stefan Wunsch (KIT, CERN) Sergey Linev (GSI); # Enable multi-threading; # Create a ROOT dataframe for each dataset; # Apply scale factors and MC weight for simulated events and a weight of 1 for the data; # Select the events for the analysis; # Apply preselection cut on photon trigger; # Find two good muons with tight ID, pt > 25 GeV and not in the transition region between barrel and encap; # Take only isolated photons; # Compile a function to compute the invariant mass of the diphoton system; """"""; using Vec_t = const ROOT::VecOps::RVec<float>;; float ComputeInvariantMass(Vec_t& pt, Vec_t& eta, Vec_t& phi, Vec_t& e) {; ROOT::Math::PtEtaPhiEVector p1(pt[0], eta[0], phi[0], e[0]);; ROOT::Math::PtEtaPhiEVector p2(pt[1], eta[1], phi[1], e[1]);; return (p1 + p2).mass() / 1000.0;; }; """"""; # Define a new column with the invariant mass and perform final event selection; # Make four vectors and compute invariant mass; # Make additional kinematic cuts and select mass window; # Bo",MatchSource.CODE_COMMENT,tutorials/rcanvas/df104.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/rcanvas/df104.py
Energy Efficiency,energy,energy,"## \file; ## \ingroup tutorial_rcanvas; ## The Higgs to two photons analysis from the ATLAS Open Data 2020 release, with RDataFrame.; ##; ## This tutorial is the Higgs to two photons analysis from the ATLAS Open Data release in 2020; ## (http://opendata.atlas.cern/release/2020/documentation/). The data was taken with the ATLAS detector; ## during 2016 at a center-of-mass energy of 13 TeV. Although the Higgs to two photons decay is very rare,; ## the contribution of the Higgs can be seen as a narrow peak around 125 GeV because of the excellent; ## reconstruction and identification efficiency of photons at the ATLAS experiment.; ##; ## The analysis is translated to a RDataFrame workflow processing 1.7 GB of simulated events and data.; ##; ## This macro is replica of tutorials/dataframe/df104_HiggsToTwoPhotons.py, but with usage of ROOT7 graphics; ## Run macro with python3 -i df104.py command to get interactive canvas; ##; ## \macro_image (df104.png); ## \macro_code; ##; ## \date 2021-06-15; ## \authors Stefan Wunsch (KIT, CERN) Sergey Linev (GSI); # Enable multi-threading; # Create a ROOT dataframe for each dataset; # Apply scale factors and MC weight for simulated events and a weight of 1 for the data; # Select the events for the analysis; # Apply preselection cut on photon trigger; # Find two good muons with tight ID, pt > 25 GeV and not in the transition region between barrel and encap; # Take only isolated photons; # Compile a function to compute the invariant mass of the diphoton system; """"""; using Vec_t = const ROOT::VecOps::RVec<float>;; float ComputeInvariantMass(Vec_t& pt, Vec_t& eta, Vec_t& phi, Vec_t& e) {; ROOT::Math::PtEtaPhiEVector p1(pt[0], eta[0], phi[0], e[0]);; ROOT::Math::PtEtaPhiEVector p2(pt[1], eta[1], phi[1], e[1]);; return (p1 + p2).mass() / 1000.0;; }; """"""; # Define a new column with the invariant mass and perform final event selection; # Make four vectors and compute invariant mass; # Make additional kinematic cuts and select mass window; # Bo",MatchSource.CODE_COMMENT,tutorials/rcanvas/df104.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/rcanvas/df104.py
Performance,multi-thread,multi-threading,"ss energy of 13 TeV. Although the Higgs to two photons decay is very rare,; ## the contribution of the Higgs can be seen as a narrow peak around 125 GeV because of the excellent; ## reconstruction and identification efficiency of photons at the ATLAS experiment.; ##; ## The analysis is translated to a RDataFrame workflow processing 1.7 GB of simulated events and data.; ##; ## This macro is replica of tutorials/dataframe/df104_HiggsToTwoPhotons.py, but with usage of ROOT7 graphics; ## Run macro with python3 -i df104.py command to get interactive canvas; ##; ## \macro_image (df104.png); ## \macro_code; ##; ## \date 2021-06-15; ## \authors Stefan Wunsch (KIT, CERN) Sergey Linev (GSI); # Enable multi-threading; # Create a ROOT dataframe for each dataset; # Apply scale factors and MC weight for simulated events and a weight of 1 for the data; # Select the events for the analysis; # Apply preselection cut on photon trigger; # Find two good muons with tight ID, pt > 25 GeV and not in the transition region between barrel and encap; # Take only isolated photons; # Compile a function to compute the invariant mass of the diphoton system; """"""; using Vec_t = const ROOT::VecOps::RVec<float>;; float ComputeInvariantMass(Vec_t& pt, Vec_t& eta, Vec_t& phi, Vec_t& e) {; ROOT::Math::PtEtaPhiEVector p1(pt[0], eta[0], phi[0], e[0]);; ROOT::Math::PtEtaPhiEVector p2(pt[1], eta[1], phi[1], e[1]);; return (p1 + p2).mass() / 1000.0;; }; """"""; # Define a new column with the invariant mass and perform final event selection; # Make four vectors and compute invariant mass; # Make additional kinematic cuts and select mass window; # Book histogram of the invariant mass with this selection; # Run the event loop; # RunGraphs allows to run the event loops of the separate RDataFrame graphs; # concurrently. This results in an improved usage of the available resources; # if each separate RDataFrame can not utilize all available resources, e.g.,; # because not enough data is available.; # Create the plot;",MatchSource.CODE_COMMENT,tutorials/rcanvas/df104.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/rcanvas/df104.py
Safety,detect,detector,"## \file; ## \ingroup tutorial_rcanvas; ## The Higgs to two photons analysis from the ATLAS Open Data 2020 release, with RDataFrame.; ##; ## This tutorial is the Higgs to two photons analysis from the ATLAS Open Data release in 2020; ## (http://opendata.atlas.cern/release/2020/documentation/). The data was taken with the ATLAS detector; ## during 2016 at a center-of-mass energy of 13 TeV. Although the Higgs to two photons decay is very rare,; ## the contribution of the Higgs can be seen as a narrow peak around 125 GeV because of the excellent; ## reconstruction and identification efficiency of photons at the ATLAS experiment.; ##; ## The analysis is translated to a RDataFrame workflow processing 1.7 GB of simulated events and data.; ##; ## This macro is replica of tutorials/dataframe/df104_HiggsToTwoPhotons.py, but with usage of ROOT7 graphics; ## Run macro with python3 -i df104.py command to get interactive canvas; ##; ## \macro_image (df104.png); ## \macro_code; ##; ## \date 2021-06-15; ## \authors Stefan Wunsch (KIT, CERN) Sergey Linev (GSI); # Enable multi-threading; # Create a ROOT dataframe for each dataset; # Apply scale factors and MC weight for simulated events and a weight of 1 for the data; # Select the events for the analysis; # Apply preselection cut on photon trigger; # Find two good muons with tight ID, pt > 25 GeV and not in the transition region between barrel and encap; # Take only isolated photons; # Compile a function to compute the invariant mass of the diphoton system; """"""; using Vec_t = const ROOT::VecOps::RVec<float>;; float ComputeInvariantMass(Vec_t& pt, Vec_t& eta, Vec_t& phi, Vec_t& e) {; ROOT::Math::PtEtaPhiEVector p1(pt[0], eta[0], phi[0], e[0]);; ROOT::Math::PtEtaPhiEVector p2(pt[1], eta[1], phi[1], e[1]);; return (p1 + p2).mass() / 1000.0;; }; """"""; # Define a new column with the invariant mass and perform final event selection; # Make four vectors and compute invariant mass; # Make additional kinematic cuts and select mass window; # Bo",MatchSource.CODE_COMMENT,tutorials/rcanvas/df104.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/rcanvas/df104.py
Availability,down,down,"lysis runs on a preskimmed dataset to reduce the runtime. The full dataset can be used with; ## the --full-dataset argument and you can also run only on a fraction of the original dataset using the argument --lumi-scale.; ##; ## This macro is replica of tutorials/dataframe/df105_WBosonAnalysis.py, but with usage of ROOT7 graphics; ## Run macro with python3 -i df105.py command to get interactive canvas; ##; ## \macro_image (rcanvas_js); ## \macro_code; ##; ## \date March 2020; ## \authors Stefan Wunsch (KIT, CERN) Sergey Linev (GSI); # Argument parsing; # Script; # Notebook; # The preskimmed dataset contains only 0.01 fb^-1; # Create a ROOT dataframe for each dataset; # Note that we load the filenames from the external json file placed in the same folder than this script.; # Construct the dataframes; # Folder name; # Sample name; # Cross-section; # Sum of weights; # Number of events; # Scale down the datasets if requested; # Select events for the analysis; # Just-in-time compile custom helper function performing complex computations; """"""; bool GoodElectronOrMuon(int type, float pt, float eta, float phi, float e, float trackd0pv, float tracksigd0pv, float z0); {; ROOT::Math::PtEtaPhiEVector p(pt / 1000.0, eta, phi, e / 1000.0);; if (abs(z0 * sin(p.theta())) > 0.5) return false;; if (type == 11 && abs(eta) < 2.46 && (abs(eta) < 1.37 || abs(eta) > 1.52)) {; if (abs(trackd0pv / tracksigd0pv) > 5) return false;; return true;; }; if (type == 13 && abs(eta) < 2.5) {; if (abs(trackd0pv / tracksigd0pv) > 3) return false;; return true;; }; return false;; }; """"""; # Select events with a muon or electron trigger and with a missing transverse energy larger than 30 GeV; # Find events with exactly one good lepton; # Apply additional cuts in case the lepton is an electron or muon; # Apply luminosity, scale factors and MC weights for simulated events; # Compute transverse mass of the W boson using the lepton and the missing transverse energy and make a histogram; """"""; float ComputeTran",MatchSource.CODE_COMMENT,tutorials/rcanvas/df105.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/rcanvas/df105.py
Deployability,release,release,"## \file; ## \ingroup tutorial_rcanvas; ## The W boson mass analysis from the ATLAS Open Data release of 2020, with RDataFrame.; ##; ## This tutorial is the analysis of the W boson mass taken from the ATLAS Open Data release in 2020; ## (http://opendata.atlas.cern/release/2020/documentation/). The data was recorded with the ATLAS detector; ## during 2016 at a center-of-mass energy of 13 TeV. W bosons are produced frequently at the LHC and; ## are an important background to studies of Standard Model processes, for example the Higgs boson analyses.; ##; ## The analysis is translated to a RDataFrame workflow processing up to 60 GB of simulated events and data.; ## By default the analysis runs on a preskimmed dataset to reduce the runtime. The full dataset can be used with; ## the --full-dataset argument and you can also run only on a fraction of the original dataset using the argument --lumi-scale.; ##; ## This macro is replica of tutorials/dataframe/df105_WBosonAnalysis.py, but with usage of ROOT7 graphics; ## Run macro with python3 -i df105.py command to get interactive canvas; ##; ## \macro_image (rcanvas_js); ## \macro_code; ##; ## \date March 2020; ## \authors Stefan Wunsch (KIT, CERN) Sergey Linev (GSI); # Argument parsing; # Script; # Notebook; # The preskimmed dataset contains only 0.01 fb^-1; # Create a ROOT dataframe for each dataset; # Note that we load the filenames from the external json file placed in the same folder than this script.; # Construct the dataframes; # Folder name; # Sample name; # Cross-section; # Sum of weights; # Number of events; # Scale down the datasets if requested; # Select events for the analysis; # Just-in-time compile custom helper function performing complex computations; """"""; bool GoodElectronOrMuon(int type, float pt, float eta, float phi, float e, float trackd0pv, float tracksigd0pv, float z0); {; ROOT::Math::PtEtaPhiEVector p(pt / 1000.0, eta, phi, e / 1000.0);; if (abs(z0 * sin(p.theta())) > 0.5) return false;; if (type == 11 ",MatchSource.CODE_COMMENT,tutorials/rcanvas/df105.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/rcanvas/df105.py
Energy Efficiency,energy,energy,"## \file; ## \ingroup tutorial_rcanvas; ## The W boson mass analysis from the ATLAS Open Data release of 2020, with RDataFrame.; ##; ## This tutorial is the analysis of the W boson mass taken from the ATLAS Open Data release in 2020; ## (http://opendata.atlas.cern/release/2020/documentation/). The data was recorded with the ATLAS detector; ## during 2016 at a center-of-mass energy of 13 TeV. W bosons are produced frequently at the LHC and; ## are an important background to studies of Standard Model processes, for example the Higgs boson analyses.; ##; ## The analysis is translated to a RDataFrame workflow processing up to 60 GB of simulated events and data.; ## By default the analysis runs on a preskimmed dataset to reduce the runtime. The full dataset can be used with; ## the --full-dataset argument and you can also run only on a fraction of the original dataset using the argument --lumi-scale.; ##; ## This macro is replica of tutorials/dataframe/df105_WBosonAnalysis.py, but with usage of ROOT7 graphics; ## Run macro with python3 -i df105.py command to get interactive canvas; ##; ## \macro_image (rcanvas_js); ## \macro_code; ##; ## \date March 2020; ## \authors Stefan Wunsch (KIT, CERN) Sergey Linev (GSI); # Argument parsing; # Script; # Notebook; # The preskimmed dataset contains only 0.01 fb^-1; # Create a ROOT dataframe for each dataset; # Note that we load the filenames from the external json file placed in the same folder than this script.; # Construct the dataframes; # Folder name; # Sample name; # Cross-section; # Sum of weights; # Number of events; # Scale down the datasets if requested; # Select events for the analysis; # Just-in-time compile custom helper function performing complex computations; """"""; bool GoodElectronOrMuon(int type, float pt, float eta, float phi, float e, float trackd0pv, float tracksigd0pv, float z0); {; ROOT::Math::PtEtaPhiEVector p(pt / 1000.0, eta, phi, e / 1000.0);; if (abs(z0 * sin(p.theta())) > 0.5) return false;; if (type == 11 ",MatchSource.CODE_COMMENT,tutorials/rcanvas/df105.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/rcanvas/df105.py
Modifiability,config,configure," Select events for the analysis; # Just-in-time compile custom helper function performing complex computations; """"""; bool GoodElectronOrMuon(int type, float pt, float eta, float phi, float e, float trackd0pv, float tracksigd0pv, float z0); {; ROOT::Math::PtEtaPhiEVector p(pt / 1000.0, eta, phi, e / 1000.0);; if (abs(z0 * sin(p.theta())) > 0.5) return false;; if (type == 11 && abs(eta) < 2.46 && (abs(eta) < 1.37 || abs(eta) > 1.52)) {; if (abs(trackd0pv / tracksigd0pv) > 5) return false;; return true;; }; if (type == 13 && abs(eta) < 2.5) {; if (abs(trackd0pv / tracksigd0pv) > 3) return false;; return true;; }; return false;; }; """"""; # Select events with a muon or electron trigger and with a missing transverse energy larger than 30 GeV; # Find events with exactly one good lepton; # Apply additional cuts in case the lepton is an electron or muon; # Apply luminosity, scale factors and MC weights for simulated events; # Compute transverse mass of the W boson using the lepton and the missing transverse energy and make a histogram; """"""; float ComputeTransverseMass(float met_et, float met_phi, float lep_pt, float lep_eta, float lep_phi, float lep_e); {; ROOT::Math::PtEtaPhiEVector met(met_et, 0, met_phi, met_et);; ROOT::Math::PtEtaPhiEVector lep(lep_pt, lep_eta, lep_phi, lep_e);; return (met + lep).Mt() / 1000.0;; }; """"""; # Run the event loop and merge histograms of the respective processes; # RunGraphs allows to run the event loops of the separate RDataFrame graphs; # concurrently. This results in an improved usage of the available resources; # if each separate RDataFrame can not utilize all available resources, e.g.,; # because not enough data is available.; # Create the plot; # Set styles; # Create canvas and configure frame with axis attributes; # c.SetTickx(0); # c.SetTicky(0); # instruct RFrame to draw axes; # Draw stack with MC contributions; # Draw data; # Add TLegend while histograms packed in the THStack; # Add ATLAS label; # show canvas finally; # Save the plot",MatchSource.CODE_COMMENT,tutorials/rcanvas/df105.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/rcanvas/df105.py
Performance,load,load," TeV. W bosons are produced frequently at the LHC and; ## are an important background to studies of Standard Model processes, for example the Higgs boson analyses.; ##; ## The analysis is translated to a RDataFrame workflow processing up to 60 GB of simulated events and data.; ## By default the analysis runs on a preskimmed dataset to reduce the runtime. The full dataset can be used with; ## the --full-dataset argument and you can also run only on a fraction of the original dataset using the argument --lumi-scale.; ##; ## This macro is replica of tutorials/dataframe/df105_WBosonAnalysis.py, but with usage of ROOT7 graphics; ## Run macro with python3 -i df105.py command to get interactive canvas; ##; ## \macro_image (rcanvas_js); ## \macro_code; ##; ## \date March 2020; ## \authors Stefan Wunsch (KIT, CERN) Sergey Linev (GSI); # Argument parsing; # Script; # Notebook; # The preskimmed dataset contains only 0.01 fb^-1; # Create a ROOT dataframe for each dataset; # Note that we load the filenames from the external json file placed in the same folder than this script.; # Construct the dataframes; # Folder name; # Sample name; # Cross-section; # Sum of weights; # Number of events; # Scale down the datasets if requested; # Select events for the analysis; # Just-in-time compile custom helper function performing complex computations; """"""; bool GoodElectronOrMuon(int type, float pt, float eta, float phi, float e, float trackd0pv, float tracksigd0pv, float z0); {; ROOT::Math::PtEtaPhiEVector p(pt / 1000.0, eta, phi, e / 1000.0);; if (abs(z0 * sin(p.theta())) > 0.5) return false;; if (type == 11 && abs(eta) < 2.46 && (abs(eta) < 1.37 || abs(eta) > 1.52)) {; if (abs(trackd0pv / tracksigd0pv) > 5) return false;; return true;; }; if (type == 13 && abs(eta) < 2.5) {; if (abs(trackd0pv / tracksigd0pv) > 3) return false;; return true;; }; return false;; }; """"""; # Select events with a muon or electron trigger and with a missing transverse energy larger than 30 GeV; # Find events with ",MatchSource.CODE_COMMENT,tutorials/rcanvas/df105.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/rcanvas/df105.py
Safety,detect,detector,"## \file; ## \ingroup tutorial_rcanvas; ## The W boson mass analysis from the ATLAS Open Data release of 2020, with RDataFrame.; ##; ## This tutorial is the analysis of the W boson mass taken from the ATLAS Open Data release in 2020; ## (http://opendata.atlas.cern/release/2020/documentation/). The data was recorded with the ATLAS detector; ## during 2016 at a center-of-mass energy of 13 TeV. W bosons are produced frequently at the LHC and; ## are an important background to studies of Standard Model processes, for example the Higgs boson analyses.; ##; ## The analysis is translated to a RDataFrame workflow processing up to 60 GB of simulated events and data.; ## By default the analysis runs on a preskimmed dataset to reduce the runtime. The full dataset can be used with; ## the --full-dataset argument and you can also run only on a fraction of the original dataset using the argument --lumi-scale.; ##; ## This macro is replica of tutorials/dataframe/df105_WBosonAnalysis.py, but with usage of ROOT7 graphics; ## Run macro with python3 -i df105.py command to get interactive canvas; ##; ## \macro_image (rcanvas_js); ## \macro_code; ##; ## \date March 2020; ## \authors Stefan Wunsch (KIT, CERN) Sergey Linev (GSI); # Argument parsing; # Script; # Notebook; # The preskimmed dataset contains only 0.01 fb^-1; # Create a ROOT dataframe for each dataset; # Note that we load the filenames from the external json file placed in the same folder than this script.; # Construct the dataframes; # Folder name; # Sample name; # Cross-section; # Sum of weights; # Number of events; # Scale down the datasets if requested; # Select events for the analysis; # Just-in-time compile custom helper function performing complex computations; """"""; bool GoodElectronOrMuon(int type, float pt, float eta, float phi, float e, float trackd0pv, float tracksigd0pv, float z0); {; ROOT::Math::PtEtaPhiEVector p(pt / 1000.0, eta, phi, e / 1000.0);; if (abs(z0 * sin(p.theta())) > 0.5) return false;; if (type == 11 ",MatchSource.CODE_COMMENT,tutorials/rcanvas/df105.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/rcanvas/df105.py
Availability,error,errors,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## This tutorial illustrates the basic features of RooFit.; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Set up model; # ---------------------; # Declare variables x,mean,sigma with associated name, title, initial; # value and allowed range; # Build gaussian pdf in terms of x,mean and sigma; # Construct plot frame in 'x'; # RooPlot; # Plot model and change parameter values; # ---------------------------------------------------------------------------; # Plot gauss in frame (i.e. in x); # Change the value of sigma to 3; # Plot gauss in frame (i.e. in x) and draw frame on canvas; # Generate events; # -----------------------------; # Generate a dataset of 1000 events in x from gauss; # ROOT.RooDataSet; # Make a second plot frame in x and draw both the; # data and the pdf in the frame; # RooPlot; # Fit model to data; # -----------------------------; # Fit pdf to data; # Print values of mean and sigma (that now reflect fitted values and; # errors); # Draw all frames on a canvas",MatchSource.CODE_COMMENT,tutorials/roofit/rf101_basics.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf101_basics.py
Modifiability,variab,variables,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## This tutorial illustrates the basic features of RooFit.; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Set up model; # ---------------------; # Declare variables x,mean,sigma with associated name, title, initial; # value and allowed range; # Build gaussian pdf in terms of x,mean and sigma; # Construct plot frame in 'x'; # RooPlot; # Plot model and change parameter values; # ---------------------------------------------------------------------------; # Plot gauss in frame (i.e. in x); # Change the value of sigma to 3; # Plot gauss in frame (i.e. in x) and draw frame on canvas; # Generate events; # -----------------------------; # Generate a dataset of 1000 events in x from gauss; # ROOT.RooDataSet; # Make a second plot frame in x and draw both the; # data and the pdf in the frame; # RooPlot; # Fit model to data; # -----------------------------; # Fit pdf to data; # Print values of mean and sigma (that now reflect fitted values and; # errors); # Draw all frames on a canvas",MatchSource.CODE_COMMENT,tutorials/roofit/rf101_basics.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf101_basics.py
Availability,error,error,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## 'BASIC FUNCTIONALITY' RooFit tutorial macro #102; ## Importing data from ROOT TTrees and THx histograms; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C version); # Create ROOT ROOT.TH1 filled with a Gaussian distribution; # Create ROOT ROOT.TTree filled with a Gaussian distribution in x and a; # uniform distribution in y; ############################; # Importing ROOT histograms; ############################; # Import ROOT TH1 into a RooDataHist; # ---------------------------------------------------------; # Create a ROOT TH1 histogram; # Declare observable x; # Create a binned dataset that imports contents of ROOT.TH1 and associates; # its contents to observable 'x'; # Plot and fit a RooDataHist; # ---------------------------------------------------; # Make plot of binned dataset showing Poisson error bars (RooFit default); # Fit a Gaussian p.d.f to the data; # Plot and fit a RooDataHist with internal errors; # ---------------------------------------------------------------------------------------------; # If histogram has custom error (i.e. its contents is does not originate from a Poisson process; # but e.g. is a sum of weighted events) you can data with symmetric 'sum-of-weights' error instead; # (same error bars as shown by ROOT); # Please note that error bars shown (Poisson or SumW2) are for visualization only, the are NOT used; # in a maximum likelihood fit; #; # A (binned) ML fit will ALWAYS assume the Poisson error interpretation of data (the mathematical definition; # of likelihood does not take any external definition of errors). Data with non-unit weights can only be correctly; # fitted with a chi^2 fit (see rf602_chi2fit.py); #; # Importing ROOT TTrees; # -----------------------------------------------------------; # Import ROOT TTree into a RooDataSet; # Define 2nd observable y; # Construct unbinned dataset importing ",MatchSource.CODE_COMMENT,tutorials/roofit/rf102_dataimport.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf102_dataimport.py
Integrability,message,message," a sum of weighted events) you can data with symmetric 'sum-of-weights' error instead; # (same error bars as shown by ROOT); # Please note that error bars shown (Poisson or SumW2) are for visualization only, the are NOT used; # in a maximum likelihood fit; #; # A (binned) ML fit will ALWAYS assume the Poisson error interpretation of data (the mathematical definition; # of likelihood does not take any external definition of errors). Data with non-unit weights can only be correctly; # fitted with a chi^2 fit (see rf602_chi2fit.py); #; # Importing ROOT TTrees; # -----------------------------------------------------------; # Import ROOT TTree into a RooDataSet; # Define 2nd observable y; # Construct unbinned dataset importing tree branches x and y matching between branches and ROOT.RooRealVars; # is done by name of the branch/RRV; #; # Note that ONLY entries for which x,y have values within their allowed ranges as defined in; # ROOT.RooRealVar x and y are imported. Since the y values in the import tree are in the range [-15,15]; # and RRV y defines a range [-10,10] this means that the ROOT.RooDataSet; # below will have less entries than the ROOT.TTree 'tree'; # Use ascii import/export for datasets; # ------------------------------------------------------------------------------------; # Write data to output stream; # Optionally, adjust the stream here (e.g. std::setprecision); # Read data from input stream. The variables of the dataset need to be supplied; # to the RooDataSet::read() function.; # variables to be read. If the file has more fields, these are ignored.; # Prints if a RooFit message stream listens for debug messages. Use Q for quiet.; # Plot data set with multiple binning choices; # ------------------------------------------------------------------------------------; # Print number of events in dataset; # Print unbinned dataset with default frame binning (100 bins); # Print unbinned dataset with custom binning choice (20 bins); # Draw all frames on a canvas",MatchSource.CODE_COMMENT,tutorials/roofit/rf102_dataimport.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf102_dataimport.py
Modifiability,variab,variables," a sum of weighted events) you can data with symmetric 'sum-of-weights' error instead; # (same error bars as shown by ROOT); # Please note that error bars shown (Poisson or SumW2) are for visualization only, the are NOT used; # in a maximum likelihood fit; #; # A (binned) ML fit will ALWAYS assume the Poisson error interpretation of data (the mathematical definition; # of likelihood does not take any external definition of errors). Data with non-unit weights can only be correctly; # fitted with a chi^2 fit (see rf602_chi2fit.py); #; # Importing ROOT TTrees; # -----------------------------------------------------------; # Import ROOT TTree into a RooDataSet; # Define 2nd observable y; # Construct unbinned dataset importing tree branches x and y matching between branches and ROOT.RooRealVars; # is done by name of the branch/RRV; #; # Note that ONLY entries for which x,y have values within their allowed ranges as defined in; # ROOT.RooRealVar x and y are imported. Since the y values in the import tree are in the range [-15,15]; # and RRV y defines a range [-10,10] this means that the ROOT.RooDataSet; # below will have less entries than the ROOT.TTree 'tree'; # Use ascii import/export for datasets; # ------------------------------------------------------------------------------------; # Write data to output stream; # Optionally, adjust the stream here (e.g. std::setprecision); # Read data from input stream. The variables of the dataset need to be supplied; # to the RooDataSet::read() function.; # variables to be read. If the file has more fields, these are ignored.; # Prints if a RooFit message stream listens for debug messages. Use Q for quiet.; # Plot data set with multiple binning choices; # ------------------------------------------------------------------------------------; # Print number of events in dataset; # Print unbinned dataset with default frame binning (100 bins); # Print unbinned dataset with custom binning choice (20 bins); # Draw all frames on a canvas",MatchSource.CODE_COMMENT,tutorials/roofit/rf102_dataimport.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf102_dataimport.py
Modifiability,variab,variable,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Basic functionality: the class factory for functions and pdfs; ##; ## NOTE: This demo uses code that is generated by the macro,; ## which can be compiled on the fly (set to MyPdfV3 below).; ## To use MyPdfV1 or MyPdfV2, adjust lines below accordingly.; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Write class skeleton code; # --------------------------------------------------; # Write skeleton pdf class with variable x,a,b; # To use this class,; # - Edit the file MyPdfV1.cxx and implement the evaluate() method in terms of x,a and b; # - Compile and link class with '.x MyPdfV1.cxx+'; #; # With added initial value expression; # ---------------------------------------------------------------------; # Write skeleton pdf class with variable x,a,b and given formula expression; # To use this class,; # - Compile and link class with '.x MyPdfV2.cxx+'; #; # With added analytical integral expression; # ---------------------------------------------------------------------------------; # Write skeleton pdf class with variable x,a,b, given formula expression _and_; # given expression for analytical integral over x; # To use this class,; # - Compile and link class with '.x MyPdfV3.cxx+'; #; # Use instance of created class; # ---------------------------------------------------------; # Compile MyPdfV3 class; # Creat instance of MyPdfV3 class; # Generate toy data from pdf and plot data and pdf on frame; # /; # C o m p i l e d v e r s i o n o f e x a m p l e r f 1 0 3 #; # /; # Declare observable x; # The ROOT.RooClassFactory.makePdfInstance() function performs code writing, compiling, linking; # and object instantiation in one go and can serve as a straight; # replacement of ROOT.RooGenericPdf; # Generate a toy dataset from the interpreted pdf; # Fit the interpreted pdf to the generated data; # Make a plot of the data and the pdf ove",MatchSource.CODE_COMMENT,tutorials/roofit/rf104_classfactory.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf104_classfactory.py
Performance,perform,performs," ## \notebook; ## Basic functionality: the class factory for functions and pdfs; ##; ## NOTE: This demo uses code that is generated by the macro,; ## which can be compiled on the fly (set to MyPdfV3 below).; ## To use MyPdfV1 or MyPdfV2, adjust lines below accordingly.; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Write class skeleton code; # --------------------------------------------------; # Write skeleton pdf class with variable x,a,b; # To use this class,; # - Edit the file MyPdfV1.cxx and implement the evaluate() method in terms of x,a and b; # - Compile and link class with '.x MyPdfV1.cxx+'; #; # With added initial value expression; # ---------------------------------------------------------------------; # Write skeleton pdf class with variable x,a,b and given formula expression; # To use this class,; # - Compile and link class with '.x MyPdfV2.cxx+'; #; # With added analytical integral expression; # ---------------------------------------------------------------------------------; # Write skeleton pdf class with variable x,a,b, given formula expression _and_; # given expression for analytical integral over x; # To use this class,; # - Compile and link class with '.x MyPdfV3.cxx+'; #; # Use instance of created class; # ---------------------------------------------------------; # Compile MyPdfV3 class; # Creat instance of MyPdfV3 class; # Generate toy data from pdf and plot data and pdf on frame; # /; # C o m p i l e d v e r s i o n o f e x a m p l e r f 1 0 3 #; # /; # Declare observable x; # The ROOT.RooClassFactory.makePdfInstance() function performs code writing, compiling, linking; # and object instantiation in one go and can serve as a straight; # replacement of ROOT.RooGenericPdf; # Generate a toy dataset from the interpreted pdf; # Fit the interpreted pdf to the generated data; # Make a plot of the data and the pdf overlaid; # Draw all frames on a canvas",MatchSource.CODE_COMMENT,tutorials/roofit/rf104_classfactory.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf104_classfactory.py
Modifiability,variab,variables,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## 'BASIC FUNCTIONALITY' RooFit tutorial macro #105; ## Demonstration of binding ROOT Math functions as RooFit functions; ## and pdfs; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C version); # Bind ROOT TMath::Erf C function; # ---------------------------------------------------; # Bind one-dimensional ROOT.TMath.Erf function as ROOT.RooAbsReal function; # Print erf definition; # Plot erf on frame; # Bind ROOT::Math::beta_pdf C function; # -----------------------------------------------------------------------; # Bind pdf ROOT.Math.Beta with three variables as ROOT.RooAbsPdf function; # Perf beta definition; # Generate some events and fit; # Plot data and pdf on frame; # Bind ROOT TF1 as RooFit function; # ---------------------------------------------------------------; # Create a ROOT TF1 function; # Create an observable; # Create binding of TF1 object to above observable; # Print rfa1 definition; # Make plot frame in observable, TF1 binding function",MatchSource.CODE_COMMENT,tutorials/roofit/rf105_funcbinding.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf105_funcbinding.py
Availability,error,error,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Basic functionality: demonstration of various plotting styles of data, functions in a RooPlot; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Set up model; # ---------------------; # Create observables; # Create Gaussian; # Generate a sample of 100 events with sigma=3; # Fit pdf to data; # Make plot frames; # -------------------------------; # Make four plot frames to demonstrate various plotting features; # Data plotting styles; # ---------------------------------------; # Use sqrt(sum(weights^2)) error instead of Poisson errors; # Remove horizontal error bars; # Blue markers and error bors; # Filled bar chart; # Function plotting styles; # -----------------------------------------------; # Change line color to red; # Change line style to dashed; # Filled shapes in green color; #",MatchSource.CODE_COMMENT,tutorials/roofit/rf107_plotstyles.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf107_plotstyles.py
Energy Efficiency,green,green,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Basic functionality: demonstration of various plotting styles of data, functions in a RooPlot; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Set up model; # ---------------------; # Create observables; # Create Gaussian; # Generate a sample of 100 events with sigma=3; # Fit pdf to data; # Make plot frames; # -------------------------------; # Make four plot frames to demonstrate various plotting features; # Data plotting styles; # ---------------------------------------; # Use sqrt(sum(weights^2)) error instead of Poisson errors; # Remove horizontal error bars; # Blue markers and error bors; # Filled bar chart; # Function plotting styles; # -----------------------------------------------; # Change line color to red; # Change line style to dashed; # Filled shapes in green color; #",MatchSource.CODE_COMMENT,tutorials/roofit/rf107_plotstyles.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf107_plotstyles.py
Modifiability,variab,variable,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Basic functionality: plotting unbinned data with alternate and variable binnings; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Set up model; # ---------------------; # Build a B decay pdf with mixing; # Build a gaussian resolution model; # Construct Bdecay (x) gauss; # Sample data from model; # --------------------------------------------; # Sample 2000 events in (dt,mixState,tagFlav) from bmix; # Show dt distribution with custom binning; # -------------------------------------------------------------------------------; # Make plot of dt distribution of data in range (-15,15) with fine binning; # for dt>0 and coarse binning for dt<0; # Create binning object with range (-15,15); # Add 60 bins with uniform spacing in range (-15,0); # Add 15 bins with uniform spacing in range (0,15); # Make plot with specified binning; # NB: Note that bin density for each bin is adjusted to that of default frame binning as shown; # in Y axis label (100 bins -. Events/0.4*Xaxis-dim) so that all bins; # represent a consistent density distribution; # Show mixstate asymmetry with custom binning; # ------------------------------------------------------------------------------------; # Make plot of dt distribution of data asymmetry in 'mixState' with; # variable binning; # Create binning object with range (-10,10); # Add boundaries at 0, (-1,1), (-2,2), (-3,3), (-4,4) and (-6,6); # Create plot frame in dt; # Plot mixState asymmetry of data with specified customg binning; # Plot corresponding property of pdf; # Adjust vertical range of plot to sensible values for an asymmetry; # NB: For asymmetry distributions no density corrects are needed (and are; # thus not applied); # Draw plots on canvas",MatchSource.CODE_COMMENT,tutorials/roofit/rf108_plotbinning.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf108_plotbinning.py
Deployability,integrat,integration,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Basic functionality: examples on normalization and integration of pdfs, construction; ## of cumulative distribution functions from monodimensional pdfs; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Set up model; # ---------------------; # Create observables x,y; # Create pdf gaussx(x,-2,3); # Retrieve raw & normalized values of RooFit pdfs; # --------------------------------------------------------------------------------------------------; # Return 'raw' unnormalized value of gx; # Return value of gx normalized over x in range [-10,10]; # Create object representing integral over gx; # which is used to calculate gx_Norm[x] == gx / gx_Int[x]; # Integrate normalized pdf over subrange; # ----------------------------------------------------------------------------; # Define a range named ""signal"" in x from -5,5; # Create an integral of gx_Norm[x] over x in range ""signal""; # ROOT.This is the fraction of of pdf gx_Norm[x] which is in the; # range named ""signal""; # Construct cumulative distribution function from pdf; # -----------------------------------------------------------------------------------------------------; # Create the cumulative distribution function of gx; # i.e. calculate Int[-10,x] gx(x') dx'; # Plot cdf of gx versus x; # Draw plot on canvas",MatchSource.CODE_COMMENT,tutorials/roofit/rf110_normintegration.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf110_normintegration.py
Integrability,integrat,integration,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Basic functionality: examples on normalization and integration of pdfs, construction; ## of cumulative distribution functions from monodimensional pdfs; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Set up model; # ---------------------; # Create observables x,y; # Create pdf gaussx(x,-2,3); # Retrieve raw & normalized values of RooFit pdfs; # --------------------------------------------------------------------------------------------------; # Return 'raw' unnormalized value of gx; # Return value of gx normalized over x in range [-10,10]; # Create object representing integral over gx; # which is used to calculate gx_Norm[x] == gx / gx_Int[x]; # Integrate normalized pdf over subrange; # ----------------------------------------------------------------------------; # Define a range named ""signal"" in x from -5,5; # Create an integral of gx_Norm[x] over x in range ""signal""; # ROOT.This is the fraction of of pdf gx_Norm[x] which is in the; # range named ""signal""; # Construct cumulative distribution function from pdf; # -----------------------------------------------------------------------------------------------------; # Create the cumulative distribution function of gx; # i.e. calculate Int[-10,x] gx(x') dx'; # Plot cdf of gx versus x; # Draw plot on canvas",MatchSource.CODE_COMMENT,tutorials/roofit/rf110_normintegration.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf110_normintegration.py
Modifiability,variab,variables,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Basic functionality: numerical 1st, and 3rd order derivatives w.r.t. observables and parameters; ##; ## ```; ## pdf = gauss(x,m,s); ## ```; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Set up model; # ---------------------; # Declare variables x,mean, with associated name, title, value and allowed; # range; # Build gaussian pdf in terms of x, and sigma; # Create and plot derivatives w.r.t. x; # ----------------------------------------------------------------------; # Derivative of normalized gauss(x) w.r.t. observable x; # Second and third derivative of normalized gauss(x) w.r.t. observable x; # Construct plot frame in 'x'; # Plot gauss in frame (i.e. in x); # Plot derivatives in same frame; # Create and plot derivatives w.r.t. sigma; # ------------------------------------------------------------------------------; # Derivative of normalized gauss(x) w.r.t. parameter sigma; # Second and third derivative of normalized gauss(x) w.r.t. parameter sigma; # Construct plot frame in 'sigma'; # Plot gauss in frame (i.e. in x); # Plot derivatives in same frame; # Draw all frames on a canvas",MatchSource.CODE_COMMENT,tutorials/roofit/rf111_derivatives.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf111_derivatives.py
Modifiability,extend,extended,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Addition and convolution: setting up an extended maximum likelihood fit; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Set up component pdfs; # ---------------------------------------; # Declare observable x; # Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and; # their parameters; # Build Chebychev polynomial pdf; # Sum the signal components into a composite signal pdf; # Method 1 - Construct extended composite model; # -------------------------------------------------------------------; # Sum the composite signal and background into an extended pdf; # nsig*sig+nbkg*bkg; # Sample, fit and plot extended model; # ---------------------------------------------------------------------; # Generate a data sample of expected number events in x from model; # = model.expectedEvents() = nsig+nbkg; # Fit model to data, ML term automatically included; # Plot data and PDF overlaid, expected number of events for pdf projection normalization; # rather than observed number of events (==data.numEntries()); # Overlay the background component of model with a dashed line; # Overlay the background+sig2 components of model with a dotted line; # Print structure of composite pdf; # Method 2 - Construct extended components first; # ---------------------------------------------------------------------; # Associated nsig/nbkg as expected number of events with sig/bkg; # Sum extended components without coefs; # -------------------------------------------------------------------------; # Construct sum of two extended pdf (no coefficients required); # Draw the frame on the canvas",MatchSource.CODE_COMMENT,tutorials/roofit/rf202_extendedmlfit.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf202_extendedmlfit.py
Modifiability,extend,extend,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Extended maximum likelihood fit in multiple ranges.; ##; ## \macro_code; ## \macro_output; ##; ## \date March 2021; ## \authors Harshal Shende, Stephan Hageboeck (C++ version); # Setup component pdfs; # ---------------------; # Declare observable x; # Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and their parameters; # Build Chebychev polynomial pdf; # Sum the signal components into a composite signal pdf; # Extend the pdfs; # -----------------------------; # Define signal range in which events counts are to be defined; # Associated nsig/nbkg as expected number of events with sig/bkg _in_the_range_ ""signalRange""; # Use AddPdf to extend the model. Giving as many coefficients as pdfs switches on extension.; # Sample data, fit model; # -------------------------------------------; # Generate 1000 events from model so that nsig,nbkg come out to numbers <<500 in fit; # Fit full range; # -------------------------------------------; # Perform unbinned ML fit to data, full range; # IMPORTANT:; # The model needs to be copied when fitting with different ranges because; # the interpretation of the coefficients is tied to the fit range; # that's used in the first fit; # Fit in two regions; # -------------------------------------------; # Fit in one region; # -------------------------------------------; # Note how restricting the region to only the left tail increases; # the fit uncertainty",MatchSource.CODE_COMMENT,tutorials/roofit/rf204a_extendedLikelihood.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf204a_extendedLikelihood.py
Modifiability,extend,extended,"## \file; ## \ingroup tutorial_roofit; ## \notebook -nodraw; ## This macro demonstrates how to set up a fit in two ranges for plain; ## likelihoods and extended likelihoods.; ##; ## ### 1. Shape fits (plain likelihood); ##; ## If you fit a non-extended pdf in two ranges, e.g. `pdf.fitTo(data,Range=""Range1,Range2"")`,; ## it will fit the shapes in the two selected ranges and also take into account the relative; ## predicted yields in those ranges.; ##; ## This is useful for example to represent a full-range fit, but with a; ## blinded signal region inside it.; ##; ##; ## ### 2. Shape+rate fits (extended likelihood); ##; ## If your pdf is extended, i.e. measuring both the distribution in the observable as well; ## as the event count in the fitted region, some intervention is needed to make fits in ranges; ## work in a way that corresponds to intuition.; ##; ## If an extended fit is performed in a sub-range, the observed yield is only that of the subrange, hence; ## the expected event count will converge to a number that is smaller than what's visible in a plot.; ## In such cases, it is often preferred to interpret the extended term with respect to the full range; ## that's plotted, i.e., apply a correction to the extended likelihood term in such a way; ## that the interpretation of the expected event count remains that of the full range. This can; ## be done by applying a correcion factor (equal to the fraction of the pdf that is contained in the; ## fitted range) in the Poisson term that represents the extended likelihood term.; ##; ## If an extended likelihood fit is performed over *two* sub-ranges, this correction is; ## even more important: without it, each component likelihood would have a different interpretation; ## of the expected event count (each corresponding to the count in its own region), and a joint; ## fit of these regions with different interpretations of the same model parameter results; ## in a number that is not easily interpreted.; ##; ## If both re",MatchSource.CODE_COMMENT,tutorials/roofit/rf204b_extendedLikelihood_rangedFit.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf204b_extendedLikelihood_rangedFit.py
Performance,perform,performed,"## \file; ## \ingroup tutorial_roofit; ## \notebook -nodraw; ## This macro demonstrates how to set up a fit in two ranges for plain; ## likelihoods and extended likelihoods.; ##; ## ### 1. Shape fits (plain likelihood); ##; ## If you fit a non-extended pdf in two ranges, e.g. `pdf.fitTo(data,Range=""Range1,Range2"")`,; ## it will fit the shapes in the two selected ranges and also take into account the relative; ## predicted yields in those ranges.; ##; ## This is useful for example to represent a full-range fit, but with a; ## blinded signal region inside it.; ##; ##; ## ### 2. Shape+rate fits (extended likelihood); ##; ## If your pdf is extended, i.e. measuring both the distribution in the observable as well; ## as the event count in the fitted region, some intervention is needed to make fits in ranges; ## work in a way that corresponds to intuition.; ##; ## If an extended fit is performed in a sub-range, the observed yield is only that of the subrange, hence; ## the expected event count will converge to a number that is smaller than what's visible in a plot.; ## In such cases, it is often preferred to interpret the extended term with respect to the full range; ## that's plotted, i.e., apply a correction to the extended likelihood term in such a way; ## that the interpretation of the expected event count remains that of the full range. This can; ## be done by applying a correcion factor (equal to the fraction of the pdf that is contained in the; ## fitted range) in the Poisson term that represents the extended likelihood term.; ##; ## If an extended likelihood fit is performed over *two* sub-ranges, this correction is; ## even more important: without it, each component likelihood would have a different interpretation; ## of the expected event count (each corresponding to the count in its own region), and a joint; ## fit of these regions with different interpretations of the same model parameter results; ## in a number that is not easily interpreted.; ##; ## If both re",MatchSource.CODE_COMMENT,tutorials/roofit/rf204b_extendedLikelihood_rangedFit.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf204b_extendedLikelihood_rangedFit.py
Safety,predict,predicted,"## \file; ## \ingroup tutorial_roofit; ## \notebook -nodraw; ## This macro demonstrates how to set up a fit in two ranges for plain; ## likelihoods and extended likelihoods.; ##; ## ### 1. Shape fits (plain likelihood); ##; ## If you fit a non-extended pdf in two ranges, e.g. `pdf.fitTo(data,Range=""Range1,Range2"")`,; ## it will fit the shapes in the two selected ranges and also take into account the relative; ## predicted yields in those ranges.; ##; ## This is useful for example to represent a full-range fit, but with a; ## blinded signal region inside it.; ##; ##; ## ### 2. Shape+rate fits (extended likelihood); ##; ## If your pdf is extended, i.e. measuring both the distribution in the observable as well; ## as the event count in the fitted region, some intervention is needed to make fits in ranges; ## work in a way that corresponds to intuition.; ##; ## If an extended fit is performed in a sub-range, the observed yield is only that of the subrange, hence; ## the expected event count will converge to a number that is smaller than what's visible in a plot.; ## In such cases, it is often preferred to interpret the extended term with respect to the full range; ## that's plotted, i.e., apply a correction to the extended likelihood term in such a way; ## that the interpretation of the expected event count remains that of the full range. This can; ## be done by applying a correcion factor (equal to the fraction of the pdf that is contained in the; ## fitted range) in the Poisson term that represents the extended likelihood term.; ##; ## If an extended likelihood fit is performed over *two* sub-ranges, this correction is; ## even more important: without it, each component likelihood would have a different interpretation; ## of the expected event count (each corresponding to the count in its own region), and a joint; ## fit of these regions with different interpretations of the same model parameter results; ## in a number that is not easily interpreted.; ##; ## If both re",MatchSource.CODE_COMMENT,tutorials/roofit/rf204b_extendedLikelihood_rangedFit.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf204b_extendedLikelihood_rangedFit.py
Usability,intuit,intuition,"## \file; ## \ingroup tutorial_roofit; ## \notebook -nodraw; ## This macro demonstrates how to set up a fit in two ranges for plain; ## likelihoods and extended likelihoods.; ##; ## ### 1. Shape fits (plain likelihood); ##; ## If you fit a non-extended pdf in two ranges, e.g. `pdf.fitTo(data,Range=""Range1,Range2"")`,; ## it will fit the shapes in the two selected ranges and also take into account the relative; ## predicted yields in those ranges.; ##; ## This is useful for example to represent a full-range fit, but with a; ## blinded signal region inside it.; ##; ##; ## ### 2. Shape+rate fits (extended likelihood); ##; ## If your pdf is extended, i.e. measuring both the distribution in the observable as well; ## as the event count in the fitted region, some intervention is needed to make fits in ranges; ## work in a way that corresponds to intuition.; ##; ## If an extended fit is performed in a sub-range, the observed yield is only that of the subrange, hence; ## the expected event count will converge to a number that is smaller than what's visible in a plot.; ## In such cases, it is often preferred to interpret the extended term with respect to the full range; ## that's plotted, i.e., apply a correction to the extended likelihood term in such a way; ## that the interpretation of the expected event count remains that of the full range. This can; ## be done by applying a correcion factor (equal to the fraction of the pdf that is contained in the; ## fitted range) in the Poisson term that represents the extended likelihood term.; ##; ## If an extended likelihood fit is performed over *two* sub-ranges, this correction is; ## even more important: without it, each component likelihood would have a different interpretation; ## of the expected event count (each corresponding to the count in its own region), and a joint; ## fit of these regions with different interpretations of the same model parameter results; ## in a number that is not easily interpreted.; ##; ## If both re",MatchSource.CODE_COMMENT,tutorials/roofit/rf204b_extendedLikelihood_rangedFit.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf204b_extendedLikelihood_rangedFit.py
Availability,avail,available,"## \file; ## \ingroup tutorial_roofit; ## \notebook -nodraw; ## Addition and convolution: tools for visualization of ROOT.RooAbsArg expression trees; ##; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Set up composite pdf; # --------------------------------------; # Declare observable x; # Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and; # their parameters; # Sum the signal components into a composite signal pdf; # Build Chebychev polynomial pdf; # Build expontential pdf; # Sum the background components into a composite background pdf; # Sum the composite signal and background; # Print composite tree in ASCII; # -----------------------------------------------------------; # Print tree to stdout; # Print tree to file; # Draw composite tree graphically; # -------------------------------------------------------------; # Print GraphViz DOT file with representation of tree; # Make graphic output file with one of the GraphViz tools; # (freely available from www.graphviz.org); #; # 'Top-to-bottom graph'; # unix> dot -Tgif -o rf207_model_dot.gif rf207_model.dot; #; # 'Spring-model graph'; # unix> fdp -Tgif -o rf207_model_fdp.gif rf207_model.dot",MatchSource.CODE_COMMENT,tutorials/roofit/rf206_treevistools.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf206_treevistools.py
Modifiability,variab,variables,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## 'ADDITION AND CONVOLUTION' RooFit tutorial macro #207; ## Tools and utilities for manipulation of composite objects; ##; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C version); # Set up composite pdf dataset; # --------------------------------------------------------; # Declare observable x; # Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and; # their parameters; # Build Chebychev polynomial p.d.f.; # Build expontential pdf; # Sum the background components into a composite background p.d.f.; # Sum the composite signal and background; # Create dummy dataset that has more observables than the above pdf; # Basic information requests; # ---------------------------------------------; # Get list of observables; # ---------------------------------------------; # Get list of observables of pdf in context of a dataset; #; # Observables are define each context as the variables; # shared between a model and a dataset. In self case; # that is the variable 'x'; # Get list of parameters; # -------------------------------------------; # Get list of parameters, list of observables; # Get list of parameters, a dataset; # (Gives identical results to operation above); # Get list of components; # -------------------------------------------; # Get list of component objects, top-level node; # Modifications to structure of composites; # -------------------------------------------; # Create a second Gaussian; # Create a sum of the original Gaussian plus the second Gaussian; # Construct a customizer utility to customize model; # Instruct the customizer to replace node 'sig' with node 'sigsum'; # Build a clone of the input pdf according to the above customization; # instructions. Each node that requires modified is clone so that the; # original pdf remained untouched. The name of each cloned node is that; # of the original node suffixed by the name of the cu",MatchSource.CODE_COMMENT,tutorials/roofit/rf207_comptools.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf207_comptools.py
Performance,perform,performed,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Convolution in cyclical angular observables theta, and; ## construction of p.d.f in terms of transformed angular; ## coordinates, e.g. cos(theta), the convolution; ## is performed in theta rather than cos(theta); ##; ## (require ROOT to be compiled with --enable-fftw3); ##; ## pdf(theta) = ROOT.T(theta) (x) gauss(theta); ## pdf(cosTheta) = ROOT.T(acos(cosTheta)) (x) gauss(acos(cosTheta)); ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C version); # Set up component pdfs; # ---------------------------------------; # Define angle psi; # Define physics p.d.f T(psi); # Define resolution R(psi); # Define cos(psi) and function psif that calculates psi from cos(psi); # Define physics p.d.f. also as function of cos(psi): T(psif(cpsi)) = T(cpsi); # Construct convolution pdf in psi; # --------------------------------------------------------------; # Define convoluted p.d.f. as function of psi: M=[T(x)R](psi) = M(psi); # Set the buffer fraction to zero to obtain a ROOT.True cyclical; # convolution; # Sample, fit and plot convoluted pdf (psi); # --------------------------------------------------------------------------------; # Generate some events in observable psi; # Fit convoluted model as function of angle psi; # Plot cos(psi) frame with Mf(cpsi); # Overlay comparison to unsmeared physics p.d.f ROOT.T(psi); # Construct convolution pdf in cos(psi); # --------------------------------------------------------------------------; # Define convoluted p.d.f. as function of cos(psi): M=[T(x)R](psif cpsi)) = M(cpsi:; #; # Need to give both observable psi here (for definition of convolution); # and function psif here (for definition of observables, in cpsi); # Set the buffer fraction to zero to obtain a ROOT.True cyclical; # convolution; # Sample, fit and plot convoluted pdf (cospsi); # ------------------------------------------------------------------",MatchSource.CODE_COMMENT,tutorials/roofit/rf210_angularconv.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf210_angularconv.py
Performance,cache,cache,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## 'ADDITION AND CONVOLUTION' RooFit tutorial macro #211; ## Working a with a p.d.f. with a convolution operator in terms; ## of a parameter; ##; ## (require ROOT to be compiled with --enable-fftw3); ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C version); # Set up component pdfs; # ---------------------------------------; # Gaussian g(x ; mean,sigma); # Block function in mean; # Convolution in mean model = g(x,mean,sigma) (x) block(mean); # Configure convolution to construct a 2-D cache in (x,mean); # rather than a 1-d cache in mean that needs to be recalculated; # for each value of x; # Integrate model over projModel = Int model dmean; # Generate 1000 toy events; # Fit p.d.f. to toy data; # Plot data and fitted p.d.f.; # Make 2d histogram of model(x;mean); # Draw frame on canvas",MatchSource.CODE_COMMENT,tutorials/roofit/rf211_paramconv.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf211_paramconv.py
Deployability,integrat,integrated,"macro_output; ##; ## \date June 2021; ## \author Harshal Shende, Stephan Hageboeck (C++ version); # Make a fit model; # Define the sidebands (e.g. background regions); # Generate toy data, and cut out the blinded region.; # Kick tau a bit, and run an unbinned fit where the blinded data are missing.; # ----------------------------------------------------------------------------------------------------------; # The fit should be done only in the unblinded regions, otherwise it would try; # to make the model adapt to the empty bins in the blinded region.; # Clear the ""fitrange"" attribute of the PDF. Otherwise, the fitrange would be; # automatically taken as the NormRange() for plotting. We want to avoid this,; # because the point of this tutorial is to show what can go wrong when the; # NormRange() is not specified.; # Here we will plot the results; # Wrong:; # ----------------------------------------------------------------------------------------------------------; # Plotting each slice on its own normalises the PDF over its plotting range. For the full curve, that means; # that the blinded region where data is missing is included in the normalisation calculation. The PDF therefore; # comes out too low, and doesn't match up with the slices in the side bands, which are normalised to ""their"" data.; # Plot only the blinded data, and then plot the PDF over the full range as well as both sidebands; # Right:; # ----------------------------------------------------------------------------------------------------------; # Make the same plot, but normalise each piece with respect to the regions ""left"" AND ""right"". This requires setting; # a ""NormRange"", which tells RooFit over which range the PDF has to be integrated to normalise.; # This means that the normalisation of the blue and green curves is slightly different from the left plot,; # because they get a common scale factor.; # Plot only the blinded data, and then plot the PDF over the full range as well as both sidebands",MatchSource.CODE_COMMENT,tutorials/roofit/rf212_plottingInRanges_blinding.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf212_plottingInRanges_blinding.py
Energy Efficiency,adapt,adapt,"sjunct ranges, and get normalisation right.; ##; ## Usually, when comparing a fit to data, one should first plot the data, and then the PDF.; ## In this case, the PDF is automatically normalised to match the number of data events in the plot.; ## However, when plotting only a sub-range, when e.g. a signal region has to be blinded,; ## one has to exclude the blinded region from the computation of the normalisation.; ##; ## In this tutorial, we show how to explicitly choose the normalisation when plotting using `NormRange()`.; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date June 2021; ## \author Harshal Shende, Stephan Hageboeck (C++ version); # Make a fit model; # Define the sidebands (e.g. background regions); # Generate toy data, and cut out the blinded region.; # Kick tau a bit, and run an unbinned fit where the blinded data are missing.; # ----------------------------------------------------------------------------------------------------------; # The fit should be done only in the unblinded regions, otherwise it would try; # to make the model adapt to the empty bins in the blinded region.; # Clear the ""fitrange"" attribute of the PDF. Otherwise, the fitrange would be; # automatically taken as the NormRange() for plotting. We want to avoid this,; # because the point of this tutorial is to show what can go wrong when the; # NormRange() is not specified.; # Here we will plot the results; # Wrong:; # ----------------------------------------------------------------------------------------------------------; # Plotting each slice on its own normalises the PDF over its plotting range. For the full curve, that means; # that the blinded region where data is missing is included in the normalisation calculation. The PDF therefore; # comes out too low, and doesn't match up with the slices in the side bands, which are normalised to ""their"" data.; # Plot only the blinded data, and then plot the PDF over the full range as well as both sidebands; # Right:; # ",MatchSource.CODE_COMMENT,tutorials/roofit/rf212_plottingInRanges_blinding.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf212_plottingInRanges_blinding.py
Integrability,integrat,integrated,"macro_output; ##; ## \date June 2021; ## \author Harshal Shende, Stephan Hageboeck (C++ version); # Make a fit model; # Define the sidebands (e.g. background regions); # Generate toy data, and cut out the blinded region.; # Kick tau a bit, and run an unbinned fit where the blinded data are missing.; # ----------------------------------------------------------------------------------------------------------; # The fit should be done only in the unblinded regions, otherwise it would try; # to make the model adapt to the empty bins in the blinded region.; # Clear the ""fitrange"" attribute of the PDF. Otherwise, the fitrange would be; # automatically taken as the NormRange() for plotting. We want to avoid this,; # because the point of this tutorial is to show what can go wrong when the; # NormRange() is not specified.; # Here we will plot the results; # Wrong:; # ----------------------------------------------------------------------------------------------------------; # Plotting each slice on its own normalises the PDF over its plotting range. For the full curve, that means; # that the blinded region where data is missing is included in the normalisation calculation. The PDF therefore; # comes out too low, and doesn't match up with the slices in the side bands, which are normalised to ""their"" data.; # Plot only the blinded data, and then plot the PDF over the full range as well as both sidebands; # Right:; # ----------------------------------------------------------------------------------------------------------; # Make the same plot, but normalise each piece with respect to the regions ""left"" AND ""right"". This requires setting; # a ""NormRange"", which tells RooFit over which range the PDF has to be integrated to normalise.; # This means that the normalisation of the blue and green curves is slightly different from the left plot,; # because they get a common scale factor.; # Plot only the blinded data, and then plot the PDF over the full range as well as both sidebands",MatchSource.CODE_COMMENT,tutorials/roofit/rf212_plottingInRanges_blinding.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf212_plottingInRanges_blinding.py
Modifiability,adapt,adapt,"sjunct ranges, and get normalisation right.; ##; ## Usually, when comparing a fit to data, one should first plot the data, and then the PDF.; ## In this case, the PDF is automatically normalised to match the number of data events in the plot.; ## However, when plotting only a sub-range, when e.g. a signal region has to be blinded,; ## one has to exclude the blinded region from the computation of the normalisation.; ##; ## In this tutorial, we show how to explicitly choose the normalisation when plotting using `NormRange()`.; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date June 2021; ## \author Harshal Shende, Stephan Hageboeck (C++ version); # Make a fit model; # Define the sidebands (e.g. background regions); # Generate toy data, and cut out the blinded region.; # Kick tau a bit, and run an unbinned fit where the blinded data are missing.; # ----------------------------------------------------------------------------------------------------------; # The fit should be done only in the unblinded regions, otherwise it would try; # to make the model adapt to the empty bins in the blinded region.; # Clear the ""fitrange"" attribute of the PDF. Otherwise, the fitrange would be; # automatically taken as the NormRange() for plotting. We want to avoid this,; # because the point of this tutorial is to show what can go wrong when the; # NormRange() is not specified.; # Here we will plot the results; # Wrong:; # ----------------------------------------------------------------------------------------------------------; # Plotting each slice on its own normalises the PDF over its plotting range. For the full curve, that means; # that the blinded region where data is missing is included in the normalisation calculation. The PDF therefore; # comes out too low, and doesn't match up with the slices in the side bands, which are normalised to ""their"" data.; # Plot only the blinded data, and then plot the PDF over the full range as well as both sidebands; # Right:; # ",MatchSource.CODE_COMMENT,tutorials/roofit/rf212_plottingInRanges_blinding.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf212_plottingInRanges_blinding.py
Safety,avoid,avoid,"d,; ## one has to exclude the blinded region from the computation of the normalisation.; ##; ## In this tutorial, we show how to explicitly choose the normalisation when plotting using `NormRange()`.; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date June 2021; ## \author Harshal Shende, Stephan Hageboeck (C++ version); # Make a fit model; # Define the sidebands (e.g. background regions); # Generate toy data, and cut out the blinded region.; # Kick tau a bit, and run an unbinned fit where the blinded data are missing.; # ----------------------------------------------------------------------------------------------------------; # The fit should be done only in the unblinded regions, otherwise it would try; # to make the model adapt to the empty bins in the blinded region.; # Clear the ""fitrange"" attribute of the PDF. Otherwise, the fitrange would be; # automatically taken as the NormRange() for plotting. We want to avoid this,; # because the point of this tutorial is to show what can go wrong when the; # NormRange() is not specified.; # Here we will plot the results; # Wrong:; # ----------------------------------------------------------------------------------------------------------; # Plotting each slice on its own normalises the PDF over its plotting range. For the full curve, that means; # that the blinded region where data is missing is included in the normalisation calculation. The PDF therefore; # comes out too low, and doesn't match up with the slices in the side bands, which are normalised to ""their"" data.; # Plot only the blinded data, and then plot the PDF over the full range as well as both sidebands; # Right:; # ----------------------------------------------------------------------------------------------------------; # Make the same plot, but normalise each piece with respect to the regions ""left"" AND ""right"". This requires setting; # a ""NormRange"", which tells RooFit over which range the PDF has to be integrated to normalise.; # This ",MatchSource.CODE_COMMENT,tutorials/roofit/rf212_plottingInRanges_blinding.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf212_plottingInRanges_blinding.py
Integrability,depend,depends,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Multidimensional models: multi-dimensional pdfs through composition, e.g. substituting; ## a pdf parameter with a function that depends on other observables; ##; ## `pdf = gauss(x,f(y),s)` with `f(y) = a0 + a1*y`; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Setup composed model gauss(x, m(y), s); # -----------------------------------------------------------------------; # Create observables; # Create function f(y) = a0 + a1*y; # Creat gauss(x,f(y),s); # Sample data, plot data and pdf on x and y; # ---------------------------------------------------------------------------------; # Generate 10000 events in x and y from model; # Plot x distribution of data and projection of model x = Int(dy); # model(x,y); # Plot x distribution of data and projection of model y = Int(dx); # model(x,y); # Make two-dimensional plot in x vs y; # Make canvas and draw ROOT.RooPlots",MatchSource.CODE_COMMENT,tutorials/roofit/rf301_composition.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf301_composition.py
Availability,avail,available,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Multidimensional models: utility functions classes available for use in tailoring of; ## composite (multidimensional) pdfs; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create observables, parameters; # -----------------------------------------------------------; # Create observables; # Create parameters; # Using RooFormulaVar to tailor pdf; # -----------------------------------------------------------------------; # Create interpreted function f(y) = a0 - a1*sqrt(10*abs(y)); # Create gauss(x,f(y),s); # Using RooPolyVar to tailor pdf; # -----------------------------------------------------------------------; # Create polynomial function f(y) = a0 + a1*y; # Create gauss(x,f(y),s); # Using RooAddition to tailor pdf; # -----------------------------------------------------------------------; # Create sum function f(y) = a0 + y; # Create gauss(x,f(y),s); # Using RooProduct to tailor pdf; # -----------------------------------------------------------------------; # Create product function f(y) = a1*y; # Create gauss(x,f(y),s); # Plot all pdfs; # ----------------------------; # Make two-dimensional plots in x vs y; # Make canvas and draw ROOT.RooPlots",MatchSource.CODE_COMMENT,tutorials/roofit/rf302_utilfuncs.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf302_utilfuncs.py
Modifiability,variab,variables,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Multidimensional models: simple uncorrelated multi-dimensional pdfs; ##; ## `pdf = gauss(x,mx,sx) * gauss(y,my,sy)`; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create component pdfs in x and y; # ----------------------------------------------------------------; # Create two pdfs gaussx(x,meanx,sigmax) gaussy(y,meany,sigmay) and its; # variables; # Construct uncorrelated product pdf; # -------------------------------------------------------------------; # Multiply gaussx and gaussy into a two-dimensional pdf gaussxy; # Sample pdf, plot projection on x and y; # ---------------------------------------------------------------------------; # Generate 10000 events in x and y from gaussxy; # Plot x distribution of data and projection of gaussxy x = Int(dy); # gaussxy(x,y); # Plot x distribution of data and projection of gaussxy y = Int(dx); # gaussxy(x,y); # Make canvas and draw ROOT.RooPlots",MatchSource.CODE_COMMENT,tutorials/roofit/rf304_uncorrprod.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf304_uncorrprod.py
Usability,simpl,simple,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Multidimensional models: simple uncorrelated multi-dimensional pdfs; ##; ## `pdf = gauss(x,mx,sx) * gauss(y,my,sy)`; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create component pdfs in x and y; # ----------------------------------------------------------------; # Create two pdfs gaussx(x,meanx,sigmax) gaussy(y,meany,sigmay) and its; # variables; # Construct uncorrelated product pdf; # -------------------------------------------------------------------; # Multiply gaussx and gaussy into a two-dimensional pdf gaussxy; # Sample pdf, plot projection on x and y; # ---------------------------------------------------------------------------; # Generate 10000 events in x and y from gaussxy; # Plot x distribution of data and projection of gaussxy x = Int(dy); # gaussxy(x,y); # Plot x distribution of data and projection of gaussxy y = Int(dx); # gaussxy(x,y); # Make canvas and draw ROOT.RooPlots",MatchSource.CODE_COMMENT,tutorials/roofit/rf304_uncorrprod.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf304_uncorrprod.py
Availability,error,errors,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Multidimensional models: complete example with use of conditional pdf with per-event errors; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # B-physics pdf with per-event Gaussian resolution; # ----------------------------------------------------------------------------------------------; # Observables; # Build a gaussian resolution model scaled by the per-error =; # gauss(dt,bias,sigma*dterr); # Construct decay(dt) (x) gauss1(dt|dterr); # Construct fake 'external' data with per-event error; # ------------------------------------------------------------------------------------------------------; # Use landau pdf to get somewhat realistic distribution with long tail; # Sample data from conditional decay_gm(dt|dterr); # ---------------------------------------------------------------------------------------------; # Specify external dataset with dterr values to use decay_dm as; # conditional pdf; # Fit conditional decay_dm(dt|dterr); # ---------------------------------------------------------------------; # Specify dterr as conditional observable; # Plot conditional decay_dm(dt|dterr); # ---------------------------------------------------------------------; # Make two-dimensional plot of conditional pdf in (dt,dterr); # Plot decay_gm(dt|dterr) at various values of dterr; # Make projection of data an dt; # Make projection of decay(dt|dterr) on dt.; #; # Instead of integrating out dterr, a weighted average of curves; # at values dterr_i as given in the external dataset.; # (The kTRUE argument bins the data before projection to speed up the process); # Draw all frames on canvas",MatchSource.CODE_COMMENT,tutorials/roofit/rf306_condpereventerrors.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf306_condpereventerrors.py
Deployability,integrat,integrating,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Multidimensional models: complete example with use of conditional pdf with per-event errors; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # B-physics pdf with per-event Gaussian resolution; # ----------------------------------------------------------------------------------------------; # Observables; # Build a gaussian resolution model scaled by the per-error =; # gauss(dt,bias,sigma*dterr); # Construct decay(dt) (x) gauss1(dt|dterr); # Construct fake 'external' data with per-event error; # ------------------------------------------------------------------------------------------------------; # Use landau pdf to get somewhat realistic distribution with long tail; # Sample data from conditional decay_gm(dt|dterr); # ---------------------------------------------------------------------------------------------; # Specify external dataset with dterr values to use decay_dm as; # conditional pdf; # Fit conditional decay_dm(dt|dterr); # ---------------------------------------------------------------------; # Specify dterr as conditional observable; # Plot conditional decay_dm(dt|dterr); # ---------------------------------------------------------------------; # Make two-dimensional plot of conditional pdf in (dt,dterr); # Plot decay_gm(dt|dterr) at various values of dterr; # Make projection of data an dt; # Make projection of decay(dt|dterr) on dt.; #; # Instead of integrating out dterr, a weighted average of curves; # at values dterr_i as given in the external dataset.; # (The kTRUE argument bins the data before projection to speed up the process); # Draw all frames on canvas",MatchSource.CODE_COMMENT,tutorials/roofit/rf306_condpereventerrors.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf306_condpereventerrors.py
Integrability,integrat,integrating,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Multidimensional models: complete example with use of conditional pdf with per-event errors; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # B-physics pdf with per-event Gaussian resolution; # ----------------------------------------------------------------------------------------------; # Observables; # Build a gaussian resolution model scaled by the per-error =; # gauss(dt,bias,sigma*dterr); # Construct decay(dt) (x) gauss1(dt|dterr); # Construct fake 'external' data with per-event error; # ------------------------------------------------------------------------------------------------------; # Use landau pdf to get somewhat realistic distribution with long tail; # Sample data from conditional decay_gm(dt|dterr); # ---------------------------------------------------------------------------------------------; # Specify external dataset with dterr values to use decay_dm as; # conditional pdf; # Fit conditional decay_dm(dt|dterr); # ---------------------------------------------------------------------; # Specify dterr as conditional observable; # Plot conditional decay_dm(dt|dterr); # ---------------------------------------------------------------------; # Make two-dimensional plot of conditional pdf in (dt,dterr); # Plot decay_gm(dt|dterr) at various values of dterr; # Make projection of data an dt; # Make projection of decay(dt|dterr) on dt.; #; # Instead of integrating out dterr, a weighted average of curves; # at values dterr_i as given in the external dataset.; # (The kTRUE argument bins the data before projection to speed up the process); # Draw all frames on canvas",MatchSource.CODE_COMMENT,tutorials/roofit/rf306_condpereventerrors.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf306_condpereventerrors.py
Availability,error,errors,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Multidimensional models: usage of full pdf with per-event errors; ##; ## \macro_code; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # B-physics pdf with per-event Gaussian resolution; # ----------------------------------------------------------------------------------------------; # Observables; # Build a gaussian resolution model scaled by the per-error =; # gauss(dt,bias,sigma*dterr); # Construct decay(dt) (x) gauss1(dt|dterr); # Construct empirical pdf for per-event error; # -----------------------------------------------------------------; # Use landau pdf to get empirical distribution with long tail; # Construct a histogram pdf to describe the shape of the dtErr distribution; # Construct conditional product decay_dm(dt|dterr)*pdf(dterr); # ----------------------------------------------------------------------------------------------------------------------; # Construct production of conditional decay_dm(dt|dterr) with empirical; # pdfErr(dterr); # (Alternatively you could also use the landau shape pdfDtErr); # ROOT.RooProdPdf model(""model"", ""model"",pdfDtErr,; # ROOT.RooFit.Conditional(decay_gm,dt)); # Sample, fit and plot product model; # ------------------------------------------------------------------; # Specify external dataset with dterr values to use model_dm as; # conditional pdf; # Fit conditional decay_dm(dt|dterr); # ---------------------------------------------------------------------; # Specify dterr as conditional observable; # Plot conditional decay_dm(dt|dterr); # ---------------------------------------------------------------------; # Make two-dimensional plot of conditional pdf in (dt,dterr); # Make projection of data an dt; # Draw all frames on canvas",MatchSource.CODE_COMMENT,tutorials/roofit/rf307_fullpereventerrors.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf307_fullpereventerrors.py
Deployability,integrat,integration,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Multidimensional models: normalization and integration of pdfs, construction of; ## cumulative distribution functions from pdfs in two dimensions; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Set up model; # ---------------------; # Create observables x,y; # Create pdf gaussx(x,-2,3), gaussy(y,2,2); # gxy = gx(x)*gy(y); # Retrieve raw & normalized values of RooFit pdfs; # --------------------------------------------------------------------------------------------------; # Return 'raw' unnormalized value of gx; # Return value of gxy normalized over x _and_ y in range [-10,10]; # Create object representing integral over gx; # which is used to calculate gx_Norm[x,y] == gx / gx_Int[x,y]; # NB: it is also possible to do the following; # Return value of gxy normalized over x in range [-10,10] (i.e. treating y; # as parameter); # Return value of gxy normalized over y in range [-10,10] (i.e. treating x; # as parameter); # Integrate normalized pdf over subrange; # ----------------------------------------------------------------------------; # Define a range named ""signal"" in x from -5,5; # Create an integral of gxy_Norm[x,y] over x and y in range ""signal""; # ROOT.This is the fraction of of pdf gxy_Norm[x,y] which is in the; # range named ""signal""; # Construct cumulative distribution function from pdf; # -----------------------------------------------------------------------------------------------------; # Create the cumulative distribution function of gx; # i.e. calculate Int[-10,x] gx(x') dx'; # Plot cdf of gx versus x",MatchSource.CODE_COMMENT,tutorials/roofit/rf308_normintegration2d.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf308_normintegration2d.py
Integrability,integrat,integration,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Multidimensional models: normalization and integration of pdfs, construction of; ## cumulative distribution functions from pdfs in two dimensions; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Set up model; # ---------------------; # Create observables x,y; # Create pdf gaussx(x,-2,3), gaussy(y,2,2); # gxy = gx(x)*gy(y); # Retrieve raw & normalized values of RooFit pdfs; # --------------------------------------------------------------------------------------------------; # Return 'raw' unnormalized value of gx; # Return value of gxy normalized over x _and_ y in range [-10,10]; # Create object representing integral over gx; # which is used to calculate gx_Norm[x,y] == gx / gx_Int[x,y]; # NB: it is also possible to do the following; # Return value of gxy normalized over x in range [-10,10] (i.e. treating y; # as parameter); # Return value of gxy normalized over y in range [-10,10] (i.e. treating x; # as parameter); # Integrate normalized pdf over subrange; # ----------------------------------------------------------------------------; # Define a range named ""signal"" in x from -5,5; # Create an integral of gxy_Norm[x,y] over x and y in range ""signal""; # ROOT.This is the fraction of of pdf gxy_Norm[x,y] which is in the; # range named ""signal""; # Construct cumulative distribution function from pdf; # -----------------------------------------------------------------------------------------------------; # Create the cumulative distribution function of gx; # i.e. calculate Int[-10,x] gx(x') dx'; # Plot cdf of gx versus x",MatchSource.CODE_COMMENT,tutorials/roofit/rf308_normintegration2d.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf308_normintegration2d.py
Availability,error,errors,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Multidimensional models: projecting pdf and data slices in discrete observables; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create B decay pdf with mixing; # ----------------------------------------------------------; # Decay time observables; # Discrete observables mixState (B0tag==B0reco?) and tagFlav; # (B0tag==B0(bar)?); # Define state labels of discrete observables; # Model parameters; # Build a gaussian resolution model; # Construct a decay pdf, with single gaussian resolution model; # Generate BMixing data with above set of event errors; # Plot full decay distribution; # ----------------------------------------------------------; # Create frame, data and pdf projection (integrated over tagFlav and; # mixState); # Plot decay distribution for mixed and unmixed slice of mixState; # -------------------------------------------------------------------------------------------; # Create frame, data (mixed only); # Position slice in mixState at ""mixed"" and plot slice of pdf in mixstate; # over data (integrated over tagFlav); # Create frame, data (unmixed only); # Position slice in mixState at ""unmixed"" and plot slice of pdf in; # mixstate over data (integrated over tagFlav)",MatchSource.CODE_COMMENT,tutorials/roofit/rf310_sliceplot.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf310_sliceplot.py
Deployability,integrat,integrated,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Multidimensional models: projecting pdf and data slices in discrete observables; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create B decay pdf with mixing; # ----------------------------------------------------------; # Decay time observables; # Discrete observables mixState (B0tag==B0reco?) and tagFlav; # (B0tag==B0(bar)?); # Define state labels of discrete observables; # Model parameters; # Build a gaussian resolution model; # Construct a decay pdf, with single gaussian resolution model; # Generate BMixing data with above set of event errors; # Plot full decay distribution; # ----------------------------------------------------------; # Create frame, data and pdf projection (integrated over tagFlav and; # mixState); # Plot decay distribution for mixed and unmixed slice of mixState; # -------------------------------------------------------------------------------------------; # Create frame, data (mixed only); # Position slice in mixState at ""mixed"" and plot slice of pdf in mixstate; # over data (integrated over tagFlav); # Create frame, data (unmixed only); # Position slice in mixState at ""unmixed"" and plot slice of pdf in; # mixstate over data (integrated over tagFlav)",MatchSource.CODE_COMMENT,tutorials/roofit/rf310_sliceplot.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf310_sliceplot.py
Integrability,integrat,integrated,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Multidimensional models: projecting pdf and data slices in discrete observables; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create B decay pdf with mixing; # ----------------------------------------------------------; # Decay time observables; # Discrete observables mixState (B0tag==B0reco?) and tagFlav; # (B0tag==B0(bar)?); # Define state labels of discrete observables; # Model parameters; # Build a gaussian resolution model; # Construct a decay pdf, with single gaussian resolution model; # Generate BMixing data with above set of event errors; # Plot full decay distribution; # ----------------------------------------------------------; # Create frame, data and pdf projection (integrated over tagFlav and; # mixState); # Plot decay distribution for mixed and unmixed slice of mixState; # -------------------------------------------------------------------------------------------; # Create frame, data (mixed only); # Position slice in mixState at ""mixed"" and plot slice of pdf in mixstate; # over data (integrated over tagFlav); # Create frame, data (unmixed only); # Position slice in mixState at ""unmixed"" and plot slice of pdf in; # mixstate over data (integrated over tagFlav)",MatchSource.CODE_COMMENT,tutorials/roofit/rf310_sliceplot.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf310_sliceplot.py
Deployability,continuous,continuous,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Multidimensional models: projecting pdf and data ranges in continuous observables; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create 3D pdf and data; # -------------------------------------------; # Create observables; # Create signal pdf gauss(x)*gauss(y)*gauss(z); # Create background pdf poly(x)*poly(y)*poly(z); # Create composite pdf sig+bkg; # Project pdf and data on x; # -------------------------------------------------; # Make plain projection of data and pdf on x observable; # Project pdf and data on x in signal range; # ----------------------------------------------------------------------------------; # Define signal region in y and z observables; # Make plot frame; # Plot subset of data in which all observables are inside ""sigRegion""; # For observables that do not have an explicit ""sigRegion"" range defined (e.g. observable); # an implicit definition is used that is identical to the full range (i.e.; # [-5,5] for x); # Project model on x, projected observables (y,z) only in ""sigRegion""",MatchSource.CODE_COMMENT,tutorials/roofit/rf311_rangeplot.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf311_rangeplot.py
Performance,perform,performing,"## \file; ## \ingroup tutorial_roofit; ## \notebook -nodraw; ## Multidimensional models: performing fits in multiple (disjoint) ranges in one or more dimensions; ##; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create 2D pdf and data; # -------------------------------------------; # Define observables x,y; # Construct the signal pdf gauss(x)*gauss(y); # Construct the background pdf (flat in x,y); # Construct the composite model sig+bkg; # Sample 10000 events in (x,y) from the model; # Define signal and sideband regions; # -------------------------------------------------------------------; # Construct the SideBand1,SideBand2, regions; #; # |; # +-------------+-----------+; # | | |; # | Side | Sig |; # | Band1 | nal |; # | | |; # --+-------------+-----------+--; # | |; # | Side |; # | Band2 |; # | |; # +-------------+-----------+; # |; # Perform fits in individual sideband regions; # -------------------------------------------------------------------------------------; # Perform fit in SideBand1 region (ROOT.RooAddPdf coefficients will be; # interpreted in full range); # Perform fit in SideBand2 region (ROOT.RooAddPdf coefficients will be; # interpreted in full range); # Perform fits in joint sideband regions; # -----------------------------------------------------------------------------; # Now perform fit to joint 'L-shaped' sideband region 'SB1|SB2'; # (ROOT.RooAddPdf coefficients will be interpreted in full range); # Print results for comparison",MatchSource.CODE_COMMENT,tutorials/roofit/rf312_multirangefit.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf312_multirangefit.py
Deployability,integrat,integration,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Multidimensional models: working with parameterized ranges to define non-rectangular; ## regions for fitting and integration; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create 3D pdf; # -------------------------; # Define observable (x,y,z); # Define 3 dimensional pdf; # Defined non-rectangular region R in (x, y, z); # -------------------------------------------------------------------------------------; #; # R = Z[0 - 0.1*Y^2] * Y[0.1*X - 0.9*X] * X[0 - 10]; #; # Construct range parameterized in ""R"" in y [ 0.1*x, 0.9*x ]; # Construct parameterized ranged ""R"" in z [ 0, 0.1*y^2 ]; # Calculate integral of normalized pdf in R; # ----------------------------------------------------------------------------------; # Create integral over normalized pdf model over x,y, in ""R"" region; # Plot value of integral as function of pdf parameter z0",MatchSource.CODE_COMMENT,tutorials/roofit/rf313_paramranges.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf313_paramranges.py
Integrability,integrat,integration,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Multidimensional models: working with parameterized ranges to define non-rectangular; ## regions for fitting and integration; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create 3D pdf; # -------------------------; # Define observable (x,y,z); # Define 3 dimensional pdf; # Defined non-rectangular region R in (x, y, z); # -------------------------------------------------------------------------------------; #; # R = Z[0 - 0.1*Y^2] * Y[0.1*X - 0.9*X] * X[0 - 10]; #; # Construct range parameterized in ""R"" in y [ 0.1*x, 0.9*x ]; # Construct parameterized ranged ""R"" in z [ 0, 0.1*y^2 ]; # Calculate integral of normalized pdf in R; # ----------------------------------------------------------------------------------; # Create integral over normalized pdf model over x,y, in ""R"" region; # Plot value of integral as function of pdf parameter z0",MatchSource.CODE_COMMENT,tutorials/roofit/rf313_paramranges.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf313_paramranges.py
Modifiability,parameteriz,parameterized,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Multidimensional models: working with parameterized ranges to define non-rectangular; ## regions for fitting and integration; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create 3D pdf; # -------------------------; # Define observable (x,y,z); # Define 3 dimensional pdf; # Defined non-rectangular region R in (x, y, z); # -------------------------------------------------------------------------------------; #; # R = Z[0 - 0.1*Y^2] * Y[0.1*X - 0.9*X] * X[0 - 10]; #; # Construct range parameterized in ""R"" in y [ 0.1*x, 0.9*x ]; # Construct parameterized ranged ""R"" in z [ 0, 0.1*y^2 ]; # Calculate integral of normalized pdf in R; # ----------------------------------------------------------------------------------; # Create integral over normalized pdf model over x,y, in ""R"" region; # Plot value of integral as function of pdf parameter z0",MatchSource.CODE_COMMENT,tutorials/roofit/rf313_paramranges.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf313_paramranges.py
Modifiability,parameteriz,parameterized,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Multidimensional models: working with parameterized ranges in a fit.; ## This an example of a fit with an acceptance that changes per-event; ##; ## `pdf = exp(-t/tau)` with `t[tmin,5]`; ##; ## where `t` and `tmin` are both observables in the dataset; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Define observables and decay pdf; # ---------------------------------------------------------------; # Declare observables; # Make parameterized range in t : [tmin,5]; # Make pdf; # Create input data; # ------------------------------------; # Generate complete dataset without acceptance cuts (for reference); # Generate a (fake) prototype dataset for acceptance limit values; # Generate dataset with t values that observe (t>tmin); # Fit pdf to data in acceptance region; # -----------------------------------------------------------------------; # Plot fitted pdf on full and accepted data; # ---------------------------------------------------------------------------------; # Make plot frame, datasets and overlay model; # Print fit results to demonstrate absence of bias",MatchSource.CODE_COMMENT,tutorials/roofit/rf314_paramfitrange.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf314_paramfitrange.py
Deployability,integrat,integration,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Multidimensional models: marginizalization of multi-dimensional pdfs through integration; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create pdf m(x,y) = gx(x|y) * g(y); # --------------------------------------------------------------; # Increase default precision of numeric integration; # as self exercise has high sensitivity to numeric integration precision; # Create observables; # Create function f(y) = a0 + a1*y; # Create gaussx(x,f(y),sx); # Create gaussy(y,0,2); # Create gaussx(x,sx|y) * gaussy(y); # Marginalize m(x,y) to m(x); # ----------------------------------------------------; # modelx(x) = Int model(x,y) dy; # Use marginalized pdf as regular 1D pdf; # -----------------------------------------------; # Sample 1000 events from modelx; # Fit modelx to toy data; # Plot modelx over data; # Make 2D histogram of model(x,y)",MatchSource.CODE_COMMENT,tutorials/roofit/rf315_projectpdf.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf315_projectpdf.py
Integrability,integrat,integration,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Multidimensional models: marginizalization of multi-dimensional pdfs through integration; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create pdf m(x,y) = gx(x|y) * g(y); # --------------------------------------------------------------; # Increase default precision of numeric integration; # as self exercise has high sensitivity to numeric integration precision; # Create observables; # Create function f(y) = a0 + a1*y; # Create gaussx(x,f(y),sx); # Create gaussy(y,0,2); # Create gaussx(x,sx|y) * gaussy(y); # Marginalize m(x,y) to m(x); # ----------------------------------------------------; # modelx(x) = Int model(x,y) dy; # Use marginalized pdf as regular 1D pdf; # -----------------------------------------------; # Sample 1000 events from modelx; # Fit modelx to toy data; # Plot modelx over data; # Make 2D histogram of model(x,y)",MatchSource.CODE_COMMENT,tutorials/roofit/rf315_projectpdf.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf315_projectpdf.py
Deployability,integrat,integrate,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Multidimensional models: using the likelihood ratio technique to construct a signal; ## enhanced one-dimensional projection of a multi-dimensional pdf; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create 3D pdf and data; # -------------------------------------------; # Create observables; # Create signal pdf gauss(x)*gauss(y)*gauss(z); # Create background pdf poly(x)*poly(y)*poly(z); # Create composite pdf sig+bkg; # Project pdf and data on x; # -------------------------------------------------; # Make plain projection of data and pdf on x observable; # Define projected signal likelihood ratio; # ----------------------------------------------------------------------------------; # Calculate projection of signal and total likelihood on (y,z) observables; # i.e. integrate signal and composite model over x; # Construct the log of the signal / signal+background probability; # Plot data with a LL ratio cut; # -------------------------------------------------------; # Calculate the llratio value for each event in the dataset; # Extract the subset of data with large signal likelihood; # Make plot frame; # Plot select data on frame; # Make MC projection of pdf with same LL ratio cut; # ---------------------------------------------------------------------------------------------; # Generate large number of events for MC integration of pdf projection; # Calculate LL ratio for each generated event and select MC events with; # llratio)0.7; # Project model on x, projected observables (y,z) with Monte Carlo technique; # on set of events with the same llratio cut as was applied to data",MatchSource.CODE_COMMENT,tutorials/roofit/rf316_llratioplot.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf316_llratioplot.py
Integrability,integrat,integrate,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Multidimensional models: using the likelihood ratio technique to construct a signal; ## enhanced one-dimensional projection of a multi-dimensional pdf; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create 3D pdf and data; # -------------------------------------------; # Create observables; # Create signal pdf gauss(x)*gauss(y)*gauss(z); # Create background pdf poly(x)*poly(y)*poly(z); # Create composite pdf sig+bkg; # Project pdf and data on x; # -------------------------------------------------; # Make plain projection of data and pdf on x observable; # Define projected signal likelihood ratio; # ----------------------------------------------------------------------------------; # Calculate projection of signal and total likelihood on (y,z) observables; # i.e. integrate signal and composite model over x; # Construct the log of the signal / signal+background probability; # Plot data with a LL ratio cut; # -------------------------------------------------------; # Calculate the llratio value for each event in the dataset; # Extract the subset of data with large signal likelihood; # Make plot frame; # Plot select data on frame; # Make MC projection of pdf with same LL ratio cut; # ---------------------------------------------------------------------------------------------; # Generate large number of events for MC integration of pdf projection; # Calculate LL ratio for each generated event and select MC events with; # llratio)0.7; # Project model on x, projected observables (y,z) with Monte Carlo technique; # on set of events with the same llratio cut as was applied to data",MatchSource.CODE_COMMENT,tutorials/roofit/rf316_llratioplot.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf316_llratioplot.py
Modifiability,enhance,enhanced,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Multidimensional models: using the likelihood ratio technique to construct a signal; ## enhanced one-dimensional projection of a multi-dimensional pdf; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create 3D pdf and data; # -------------------------------------------; # Create observables; # Create signal pdf gauss(x)*gauss(y)*gauss(z); # Create background pdf poly(x)*poly(y)*poly(z); # Create composite pdf sig+bkg; # Project pdf and data on x; # -------------------------------------------------; # Make plain projection of data and pdf on x observable; # Define projected signal likelihood ratio; # ----------------------------------------------------------------------------------; # Calculate projection of signal and total likelihood on (y,z) observables; # i.e. integrate signal and composite model over x; # Construct the log of the signal / signal+background probability; # Plot data with a LL ratio cut; # -------------------------------------------------------; # Calculate the llratio value for each event in the dataset; # Extract the subset of data with large signal likelihood; # Make plot frame; # Plot select data on frame; # Make MC projection of pdf with same LL ratio cut; # ---------------------------------------------------------------------------------------------; # Generate large number of events for MC integration of pdf projection; # Calculate LL ratio for each generated event and select MC events with; # llratio)0.7; # Project model on x, projected observables (y,z) with Monte Carlo technique; # on set of events with the same llratio cut as was applied to data",MatchSource.CODE_COMMENT,tutorials/roofit/rf316_llratioplot.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf316_llratioplot.py
Testability,log,log,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Multidimensional models: using the likelihood ratio technique to construct a signal; ## enhanced one-dimensional projection of a multi-dimensional pdf; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create 3D pdf and data; # -------------------------------------------; # Create observables; # Create signal pdf gauss(x)*gauss(y)*gauss(z); # Create background pdf poly(x)*poly(y)*poly(z); # Create composite pdf sig+bkg; # Project pdf and data on x; # -------------------------------------------------; # Make plain projection of data and pdf on x observable; # Define projected signal likelihood ratio; # ----------------------------------------------------------------------------------; # Calculate projection of signal and total likelihood on (y,z) observables; # i.e. integrate signal and composite model over x; # Construct the log of the signal / signal+background probability; # Plot data with a LL ratio cut; # -------------------------------------------------------; # Calculate the llratio value for each event in the dataset; # Extract the subset of data with large signal likelihood; # Make plot frame; # Plot select data on frame; # Make MC projection of pdf with same LL ratio cut; # ---------------------------------------------------------------------------------------------; # Generate large number of events for MC integration of pdf projection; # Calculate LL ratio for each generated event and select MC events with; # llratio)0.7; # Project model on x, projected observables (y,z) with Monte Carlo technique; # on set of events with the same llratio cut as was applied to data",MatchSource.CODE_COMMENT,tutorials/roofit/rf316_llratioplot.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf316_llratioplot.py
Energy Efficiency,reduce,reduce,"Unlike ROOT.RooAbsArgs (ROOT.RooAbsPdf, ROOT.RooFormulaVar,....) datasets are not attached to; # the variables they are constructed from. Instead they are attached to an internal; # clone of the supplied set of arguments; # Fill d with dummy values; # We must explicitly refer to x,y, here to pass the values because; # d is not linked to them (as explained above); # The get() function returns a pointer to the internal copy of the RooArgSet(x,y,c); # supplied in the constructor; # Get with an argument loads a specific data point in row and returns; # a pointer to row argset. get() always returns the same pointer, unless; # an invalid row number is specified. In that case a null ptr is returned; # Reducing, appending and merging; # -------------------------------------------------------------; # The reduce() function returns a dataset which is a subset of the; # original; # The merge() function adds two data set column-wise; # The append() function adds two datasets row-wise; # Operations on binned datasets; # ---------------------------------------------------------; # A binned dataset can be constructed empty, an unbinned dataset, or; # from a ROOT native histogram (TH1,2,3); # The binning of real variables (like x,y) is done using their fit range; # 'get/setRange()' and number of specified fit bins 'get/setBins()'.; # Category dimensions of binned datasets get one bin per defined category; # state; # plot projection of 2D binned data on y; # Examine the statistics of a binned dataset; # accounts for bin volume; # Locate a bin from a set of coordinates and retrieve its properties; # load bin center coordinates in internal buffer; # return weight of last loaded coordinates; # Reduce the 2-dimensional binned dataset to a 1-dimensional binned dataset; #; # All reduce() methods are interfaced in RooAbsData. All reduction techniques; # demonstrated on unbinned datasets can be applied to binned datasets as; # well.; # Add dh2 to yframe and redraw; # Saving and loading from",MatchSource.CODE_COMMENT,tutorials/roofit/rf402_datahandling.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf402_datahandling.py
Integrability,interface,interface,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Data and categories: tools for manipulation of (un)binned datasets; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # WVE Add reduction by range; # Binned (RooDataHist) and unbinned datasets (RooDataSet) share; # many properties and inherit from a common abstract base class; # (RooAbsData), provides an interface for all operations; # that can be performed regardless of the data format; # Basic operations on unbinned datasetss; # --------------------------------------------------------------; # ROOT.RooDataSet is an unbinned dataset (a collection of points in; # N-dimensional space); # Unlike ROOT.RooAbsArgs (ROOT.RooAbsPdf, ROOT.RooFormulaVar,....) datasets are not attached to; # the variables they are constructed from. Instead they are attached to an internal; # clone of the supplied set of arguments; # Fill d with dummy values; # We must explicitly refer to x,y, here to pass the values because; # d is not linked to them (as explained above); # The get() function returns a pointer to the internal copy of the RooArgSet(x,y,c); # supplied in the constructor; # Get with an argument loads a specific data point in row and returns; # a pointer to row argset. get() always returns the same pointer, unless; # an invalid row number is specified. In that case a null ptr is returned; # Reducing, appending and merging; # -------------------------------------------------------------; # The reduce() function returns a dataset which is a subset of the; # original; # The merge() function adds two data set column-wise; # The append() function adds two datasets row-wise; # Operations on binned datasets; # ---------------------------------------------------------; # A binned dataset can be constructed empty, an unbinned dataset, or; # from a ROOT native histogram (TH1,2,3); # The binning of real variables (like x,y) is done using their fit",MatchSource.CODE_COMMENT,tutorials/roofit/rf402_datahandling.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf402_datahandling.py
Modifiability,inherit,inherit,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Data and categories: tools for manipulation of (un)binned datasets; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # WVE Add reduction by range; # Binned (RooDataHist) and unbinned datasets (RooDataSet) share; # many properties and inherit from a common abstract base class; # (RooAbsData), provides an interface for all operations; # that can be performed regardless of the data format; # Basic operations on unbinned datasetss; # --------------------------------------------------------------; # ROOT.RooDataSet is an unbinned dataset (a collection of points in; # N-dimensional space); # Unlike ROOT.RooAbsArgs (ROOT.RooAbsPdf, ROOT.RooFormulaVar,....) datasets are not attached to; # the variables they are constructed from. Instead they are attached to an internal; # clone of the supplied set of arguments; # Fill d with dummy values; # We must explicitly refer to x,y, here to pass the values because; # d is not linked to them (as explained above); # The get() function returns a pointer to the internal copy of the RooArgSet(x,y,c); # supplied in the constructor; # Get with an argument loads a specific data point in row and returns; # a pointer to row argset. get() always returns the same pointer, unless; # an invalid row number is specified. In that case a null ptr is returned; # Reducing, appending and merging; # -------------------------------------------------------------; # The reduce() function returns a dataset which is a subset of the; # original; # The merge() function adds two data set column-wise; # The append() function adds two datasets row-wise; # Operations on binned datasets; # ---------------------------------------------------------; # A binned dataset can be constructed empty, an unbinned dataset, or; # from a ROOT native histogram (TH1,2,3); # The binning of real variables (like x,y) is done using their fit",MatchSource.CODE_COMMENT,tutorials/roofit/rf402_datahandling.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf402_datahandling.py
Performance,perform,performed,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Data and categories: tools for manipulation of (un)binned datasets; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # WVE Add reduction by range; # Binned (RooDataHist) and unbinned datasets (RooDataSet) share; # many properties and inherit from a common abstract base class; # (RooAbsData), provides an interface for all operations; # that can be performed regardless of the data format; # Basic operations on unbinned datasetss; # --------------------------------------------------------------; # ROOT.RooDataSet is an unbinned dataset (a collection of points in; # N-dimensional space); # Unlike ROOT.RooAbsArgs (ROOT.RooAbsPdf, ROOT.RooFormulaVar,....) datasets are not attached to; # the variables they are constructed from. Instead they are attached to an internal; # clone of the supplied set of arguments; # Fill d with dummy values; # We must explicitly refer to x,y, here to pass the values because; # d is not linked to them (as explained above); # The get() function returns a pointer to the internal copy of the RooArgSet(x,y,c); # supplied in the constructor; # Get with an argument loads a specific data point in row and returns; # a pointer to row argset. get() always returns the same pointer, unless; # an invalid row number is specified. In that case a null ptr is returned; # Reducing, appending and merging; # -------------------------------------------------------------; # The reduce() function returns a dataset which is a subset of the; # original; # The merge() function adds two data set column-wise; # The append() function adds two datasets row-wise; # Operations on binned datasets; # ---------------------------------------------------------; # A binned dataset can be constructed empty, an unbinned dataset, or; # from a ROOT native histogram (TH1,2,3); # The binning of real variables (like x,y) is done using their fit",MatchSource.CODE_COMMENT,tutorials/roofit/rf402_datahandling.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf402_datahandling.py
Availability,error,error,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## 'DATA AND CATEGORIES' RooFit tutorial macro #403; ##; ## Using weights in unbinned datasets; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C version); # Create observable and unweighted dataset; # -------------------------------------------; # Declare observable; # Construction a uniform pdf; # Sample 1000 events from pdf; # Calculate weight and make dataset weighted; # --------------------------------------------------; # Construct formula to calculate (fake) weight for events; # Add column with variable w to previously generated dataset; # Dataset d is now a dataset with two observable (x,w) with 1000 entries; # Instruct dataset wdata in interpret w as event weight rather than as; # observable; # Dataset d is now a dataset with one observable (x) with 1000 entries and; # a sum of weights of ~430K; # Unbinned ML fit to weighted data; # ---------------------------------------------------------------; # Construction quadratic polynomial pdf for fitting; # Fit quadratic polynomial to weighted data; # NOTE: A plain Maximum likelihood fit to weighted data does in general; # NOT result in correct error estimates, individual; # event weights represent Poisson statistics themselves.; #; # Fit with 'wrong' errors; # A first order correction to estimated parameter errors in an; # (unbinned) ML fit can be obtained by calculating the; # covariance matrix as; #; # V' = V C-1 V; #; # where V is the covariance matrix calculated from a fit; # to -logL = - sum [ w_i log f(x_i) ] and C is the covariance; # matrix calculated from -logL' = -sum [ w_i^2 log f(x_i) ]; # (i.e. the weights are applied squared); #; # A fit in self mode can be performed as follows:; # Plot weighted data and fit result; # ---------------------------------------------------------------; # Construct plot frame; # Plot data using sum-of-weights-squared error rather than Poisson",MatchSource.CODE_COMMENT,tutorials/roofit/rf403_weightedevts.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf403_weightedevts.py
Modifiability,variab,variable,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## 'DATA AND CATEGORIES' RooFit tutorial macro #403; ##; ## Using weights in unbinned datasets; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C version); # Create observable and unweighted dataset; # -------------------------------------------; # Declare observable; # Construction a uniform pdf; # Sample 1000 events from pdf; # Calculate weight and make dataset weighted; # --------------------------------------------------; # Construct formula to calculate (fake) weight for events; # Add column with variable w to previously generated dataset; # Dataset d is now a dataset with two observable (x,w) with 1000 entries; # Instruct dataset wdata in interpret w as event weight rather than as; # observable; # Dataset d is now a dataset with one observable (x) with 1000 entries and; # a sum of weights of ~430K; # Unbinned ML fit to weighted data; # ---------------------------------------------------------------; # Construction quadratic polynomial pdf for fitting; # Fit quadratic polynomial to weighted data; # NOTE: A plain Maximum likelihood fit to weighted data does in general; # NOT result in correct error estimates, individual; # event weights represent Poisson statistics themselves.; #; # Fit with 'wrong' errors; # A first order correction to estimated parameter errors in an; # (unbinned) ML fit can be obtained by calculating the; # covariance matrix as; #; # V' = V C-1 V; #; # where V is the covariance matrix calculated from a fit; # to -logL = - sum [ w_i log f(x_i) ] and C is the covariance; # matrix calculated from -logL' = -sum [ w_i^2 log f(x_i) ]; # (i.e. the weights are applied squared); #; # A fit in self mode can be performed as follows:; # Plot weighted data and fit result; # ---------------------------------------------------------------; # Construct plot frame; # Plot data using sum-of-weights-squared error rather than Poisson",MatchSource.CODE_COMMENT,tutorials/roofit/rf403_weightedevts.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf403_weightedevts.py
Performance,perform,performed,"emselves.; #; # Fit with 'wrong' errors; # A first order correction to estimated parameter errors in an; # (unbinned) ML fit can be obtained by calculating the; # covariance matrix as; #; # V' = V C-1 V; #; # where V is the covariance matrix calculated from a fit; # to -logL = - sum [ w_i log f(x_i) ] and C is the covariance; # matrix calculated from -logL' = -sum [ w_i^2 log f(x_i) ]; # (i.e. the weights are applied squared); #; # A fit in self mode can be performed as follows:; # Plot weighted data and fit result; # ---------------------------------------------------------------; # Construct plot frame; # Plot data using sum-of-weights-squared error rather than Poisson errors; # Overlay result of 2nd order polynomial fit to weighted data; # ML fit of pdf to equivalent unweighted dataset; # ---------------------------------------------------------------------; # Construct a pdf with the same shape as p0 after weighting; # Sample a dataset with the same number of events as data; # Sample a dataset with the same number of weights as data; # Fit the 2nd order polynomial to both unweighted datasets and save the; # results for comparison; # Chis2 fit of pdf to binned weighted dataset; # ---------------------------------------------------------------------------; # Construct binned clone of unbinned weighted dataset; # Perform chi2 fit to binned weighted dataset using sum-of-weights errors; #; # NB: Within the usual approximations of a chi2 fit, chi2 fit to weighted; # data using sum-of-weights-squared errors does give correct error; # estimates; # Plot chi^2 fit result on frame as well; # Compare fit results of chi2, L fits to (un)weighted data; # ------------------------------------------------------------; # Note that ML fit on 1Kevt of weighted data is closer to result of ML fit on 43Kevt of unweighted data; # than to 1Kevt of unweighted data, the reference chi^2 fit with SumW2 error gives a result closer to; # that of an unbinned ML fit to 1Kevt of unweighted data.",MatchSource.CODE_COMMENT,tutorials/roofit/rf403_weightedevts.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf403_weightedevts.py
Testability,log,logL,"-------------------------------------; # Construct formula to calculate (fake) weight for events; # Add column with variable w to previously generated dataset; # Dataset d is now a dataset with two observable (x,w) with 1000 entries; # Instruct dataset wdata in interpret w as event weight rather than as; # observable; # Dataset d is now a dataset with one observable (x) with 1000 entries and; # a sum of weights of ~430K; # Unbinned ML fit to weighted data; # ---------------------------------------------------------------; # Construction quadratic polynomial pdf for fitting; # Fit quadratic polynomial to weighted data; # NOTE: A plain Maximum likelihood fit to weighted data does in general; # NOT result in correct error estimates, individual; # event weights represent Poisson statistics themselves.; #; # Fit with 'wrong' errors; # A first order correction to estimated parameter errors in an; # (unbinned) ML fit can be obtained by calculating the; # covariance matrix as; #; # V' = V C-1 V; #; # where V is the covariance matrix calculated from a fit; # to -logL = - sum [ w_i log f(x_i) ] and C is the covariance; # matrix calculated from -logL' = -sum [ w_i^2 log f(x_i) ]; # (i.e. the weights are applied squared); #; # A fit in self mode can be performed as follows:; # Plot weighted data and fit result; # ---------------------------------------------------------------; # Construct plot frame; # Plot data using sum-of-weights-squared error rather than Poisson errors; # Overlay result of 2nd order polynomial fit to weighted data; # ML fit of pdf to equivalent unweighted dataset; # ---------------------------------------------------------------------; # Construct a pdf with the same shape as p0 after weighting; # Sample a dataset with the same number of events as data; # Sample a dataset with the same number of weights as data; # Fit the 2nd order polynomial to both unweighted datasets and save the; # results for comparison; # Chis2 fit of pdf to binned weighted dataset; # ",MatchSource.CODE_COMMENT,tutorials/roofit/rf403_weightedevts.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf403_weightedevts.py
Modifiability,variab,variables,"## \file; ## \ingroup tutorial_roofit; ## \notebook -nodraw; ## Data and categories: working with ROOT.RooCategory objects to describe discrete variables; ##; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Construct a category with labels; # --------------------------------------------; # Define a category with labels only; # Construct a category with labels and indices; # ------------------------------------------------; # Define a category with explicitly numbered states; # Generate dummy data for tabulation demo; # ------------------------------------------------; # Generate a dummy dataset; # Print tables of category contents of datasets; # --------------------------------------------------; # Tables are equivalent of plots for categories; # Create table for subset of events matching cut expression; # Create table for all (tagCat x b0flav) state combinations; # Retrieve number of events from table; # Number can be non-integer if source dataset has weighed events; # Retrieve fraction of events with ""Lepton"" tag; # Defining ranges for plotting, fitting on categories; # ------------------------------------------------------------------------------------------------------; # Define named range as comma separated list of labels; # Or add state names one by one; # Use category range in dataset reduction specification",MatchSource.CODE_COMMENT,tutorials/roofit/rf404_categories.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf404_categories.py
Modifiability,variab,variable,"dummy PDF in x; # Generate a dummy dataset; # Create a threshold real -> cat function; # --------------------------------------------------------------------------; # A RooThresholdCategory is a category function that maps regions in a real-valued; # input observable observables to state names. At construction time a 'default'; # state name must be specified to which all values of x are mapped that are not; # otherwise assigned; # Specify thresholds and state assignments one-by-one.; # Each statement specifies that all values _below_ the given value; # (and above any lower specified threshold) are mapped to the; # category state with the given name; #; # Background | SideBand | Signal | SideBand | Background; # 4.23 5.23 8.23 9.23; # Use threshold function to plot data regions; # ----------------------------------------------; # Add values of threshold function to dataset so that it can be used as; # observable; # Make plot of data in x; # Use calculated category to select sideband data; # Create a binning real -> cat function; # ----------------------------------------------------------------------; # A RooBinningCategory is a category function that maps bins of a (named) binning definition; # in a real-valued input observable observables to state names. The state names are automatically; # constructed from the variable name, binning name and the bin number. If no binning name; # is specified the default binning is mapped; # Use binning function for tabulation and plotting; # -----------------------------------------------------------------------------------------------; # Print table of xBins state multiplicity. Note that xBins does not need to be an observable in data; # it can be a function of observables in data as well; # Add values of xBins function to dataset so that it can be used as; # observable; # Define range ""alt"" as including bins 1,3,5,7,9; # Construct subset of data matching range ""alt"" but only for the first; # 5000 events and plot it on the frame",MatchSource.CODE_COMMENT,tutorials/roofit/rf405_realtocatfuncs.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf405_realtocatfuncs.py
Availability,error,error,"## \file; ## \ingroup tutorial_roofit; ## \notebook -nodraw; ## Data and categories: latex printing of lists and sets of RooArgSets; ##; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Setup composite pdf; # --------------------------------------; # Declare observable x; # Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and; # their parameters; # Sum the signal components into a composite signal pdf; # Build Chebychev polynomial pdf; # Build expontential pdf; # Sum the background components into a composite background pdf; # Sum the composite signal and background; # Make list of parameters before and after fit; # ----------------------------------------------------------------------------------------; # Make list of model parameters; # Save snapshot of prefit parameters; # Do fit to data, obtain error estimates on parameters; # Print LateX table of parameters of pdf; # --------------------------------------------------------------------------; # Print parameter list in LaTeX for (one column with names, column with; # values); # Print parameter list in LaTeX for (names values|names values); # Print two parameter lists side by side (name values initvalues); # Print two parameter lists side by side (name values initvalues|name; # values initvalues); # Write LaTex table to file",MatchSource.CODE_COMMENT,tutorials/roofit/rf407_latextables.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf407_latextables.py
Modifiability,variab,variables,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Fill RooDataSet/RooDataHist in RDataFrame.; ##; ## This tutorial shows how to fill RooFit data classes directly from RDataFrame.; ## Using two small helpers, we tell RDataFrame where the data has to go.; ##; ## \macro_code; ## \macro_output; ##; ## \date July 2021; ## \author Harshal Shende, Stephan Hageboeck (C++ version); # Set up; # ------------------------; # We create an RDataFrame with two columns filled with 2 million random numbers.; # We create RooFit variables that will represent the dataset.; # Booking the creation of RooDataSet / RooDataHist in RDataFrame; # ----------------------------------------------------------------; # Method 1:; # ---------; # We directly book the RooDataSetHelper action.; # We need to pass; # - the RDataFrame column types as template parameters; # - the constructor arguments for RooDataSet (they follow the same syntax as the usual RooDataSet constructors); # - the column names that RDataFrame should fill into the dataset; # NOTE: RDataFrame columns are matched to RooFit variables by position, *not by name*!; #; # The returned object is not yet a RooDataSet, but an RResultPtr that will be; # lazy-evaluated once you call GetValue() on it. We will only evaluate the; # RResultPtr once all other RDataFrame related actions are declared. This way; # we trigger the event loop computation only once, which will improve the; # runtime significantly.; #; # To learn more about lazy actions, see:; # https://root.cern/doc/master/classROOT_1_1RDataFrame.html#actions; # Method 2:; # ---------; # We first declare the RooDataHistHelper; # Then, we move it into an RDataFrame action:; # Run it and inspect the results; # -------------------------------; # At this point, all RDF actions were defined (namely, the `Book` operations),; # so we can get values from the RResultPtr objects, triggering the event loop; # and getting the actual RooFit data objects.; # Let's inspect the dataset / datahist.",MatchSource.CODE_COMMENT,tutorials/roofit/rf408_RDataFrameToRooFit.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf408_RDataFrameToRooFit.py
Usability,learn,learn,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Fill RooDataSet/RooDataHist in RDataFrame.; ##; ## This tutorial shows how to fill RooFit data classes directly from RDataFrame.; ## Using two small helpers, we tell RDataFrame where the data has to go.; ##; ## \macro_code; ## \macro_output; ##; ## \date July 2021; ## \author Harshal Shende, Stephan Hageboeck (C++ version); # Set up; # ------------------------; # We create an RDataFrame with two columns filled with 2 million random numbers.; # We create RooFit variables that will represent the dataset.; # Booking the creation of RooDataSet / RooDataHist in RDataFrame; # ----------------------------------------------------------------; # Method 1:; # ---------; # We directly book the RooDataSetHelper action.; # We need to pass; # - the RDataFrame column types as template parameters; # - the constructor arguments for RooDataSet (they follow the same syntax as the usual RooDataSet constructors); # - the column names that RDataFrame should fill into the dataset; # NOTE: RDataFrame columns are matched to RooFit variables by position, *not by name*!; #; # The returned object is not yet a RooDataSet, but an RResultPtr that will be; # lazy-evaluated once you call GetValue() on it. We will only evaluate the; # RResultPtr once all other RDataFrame related actions are declared. This way; # we trigger the event loop computation only once, which will improve the; # runtime significantly.; #; # To learn more about lazy actions, see:; # https://root.cern/doc/master/classROOT_1_1RDataFrame.html#actions; # Method 2:; # ---------; # We first declare the RooDataHistHelper; # Then, we move it into an RDataFrame action:; # Run it and inspect the results; # -------------------------------; # At this point, all RDF actions were defined (namely, the `Book` operations),; # so we can get values from the RResultPtr objects, triggering the event loop; # and getting the actual RooFit data objects.; # Let's inspect the dataset / datahist.",MatchSource.CODE_COMMENT,tutorials/roofit/rf408_RDataFrameToRooFit.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf408_RDataFrameToRooFit.py
Deployability,update,updated,"ian model with its parameters.; # Create a RooDataSet.; # Use RooDataSet.to_numpy() to export dataset to a dictionary of NumPy arrays.; # Real values will be of type `double`, categorical values of type `int`.; # We can verify that the mean and standard deviation matches our model specification.; # It is also possible to create a Pandas DataFrame directly from the numpy arrays:; # Now you can use the DataFrame e.g. for plotting. You can even combine this; # with the RooAbsReal.bins PyROOT function, which returns the binning from; # RooFit as a numpy array!; # Creating a dataset with NumPy and importing it to a RooDataSet; # --------------------------------------------------------------; # Now we create some Gaussian toy data with numpy, this time with a different; # mean.; # Import the data to a RooDataSet, passing a dictionary of arrays and the; # corresponding RooRealVars just like you would pass to the RooDataSet; # constructor.; # Let's fit the Gaussian to the data. The mean is updated accordingly.; # We can now plot the model and the dataset with RooFit.; # Draw RooFit plot on a canvas.; # Exporting a RooDataHist to NumPy arrays for histogram counts and bin edges; # --------------------------------------------------------------------------; # Create a binned clone of the dataset to show RooDataHist to NumPy export.; # You can also export a RooDataHist to numpy arrays with; # RooDataHist.to_numpy(). As output, you will get a multidimensional array with; # the histogram counts and a list of arrays with bin edges. This is comparable; # to the output of numpy.histogram (or numpy.histogramdd for the; # multidimensional case).; # Let's compare the output to the counts and bin edges we get with; # numpy.histogramdd when we pass it the original samples:; # The array values should be the same!; # Importing a RooDataHist from NumPy arrays with histogram counts and bin edges; # -----------------------------------------------------------------------------; # There is also a",MatchSource.CODE_COMMENT,tutorials/roofit/rf409_NumPyPandasToRooFit.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf409_NumPyPandasToRooFit.py
Integrability,interface,interface,"te some Gaussian toy data with numpy, this time with a different; # mean.; # Import the data to a RooDataSet, passing a dictionary of arrays and the; # corresponding RooRealVars just like you would pass to the RooDataSet; # constructor.; # Let's fit the Gaussian to the data. The mean is updated accordingly.; # We can now plot the model and the dataset with RooFit.; # Draw RooFit plot on a canvas.; # Exporting a RooDataHist to NumPy arrays for histogram counts and bin edges; # --------------------------------------------------------------------------; # Create a binned clone of the dataset to show RooDataHist to NumPy export.; # You can also export a RooDataHist to numpy arrays with; # RooDataHist.to_numpy(). As output, you will get a multidimensional array with; # the histogram counts and a list of arrays with bin edges. This is comparable; # to the output of numpy.histogram (or numpy.histogramdd for the; # multidimensional case).; # Let's compare the output to the counts and bin edges we get with; # numpy.histogramdd when we pass it the original samples:; # The array values should be the same!; # Importing a RooDataHist from NumPy arrays with histogram counts and bin edges; # -----------------------------------------------------------------------------; # There is also a `RooDataHist.from_numpy` function, again with an interface; # inspired by `numpy.histogramdd`. You need to pass at least the histogram; # counts and the list of variables. The binning is optional: the default; # binning of the RooRealVars is used if not explicitly specified.; # It's also possible to pass custom bin edges to `RooDataHist.from_numpy`, just; # like you pass them to `numpy.histogramdd` when you get the counts to fill the; # RooDataHist with:; # Alternatively, you can specify only the number of bins and the range if your; # binning is uniform. This is preferred over passing the full list of bin; # edges, because RooFit will know that the binning is uniform and do some; # optimizations.",MatchSource.CODE_COMMENT,tutorials/roofit/rf409_NumPyPandasToRooFit.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf409_NumPyPandasToRooFit.py
Modifiability,variab,variables,"te some Gaussian toy data with numpy, this time with a different; # mean.; # Import the data to a RooDataSet, passing a dictionary of arrays and the; # corresponding RooRealVars just like you would pass to the RooDataSet; # constructor.; # Let's fit the Gaussian to the data. The mean is updated accordingly.; # We can now plot the model and the dataset with RooFit.; # Draw RooFit plot on a canvas.; # Exporting a RooDataHist to NumPy arrays for histogram counts and bin edges; # --------------------------------------------------------------------------; # Create a binned clone of the dataset to show RooDataHist to NumPy export.; # You can also export a RooDataHist to numpy arrays with; # RooDataHist.to_numpy(). As output, you will get a multidimensional array with; # the histogram counts and a list of arrays with bin edges. This is comparable; # to the output of numpy.histogram (or numpy.histogramdd for the; # multidimensional case).; # Let's compare the output to the counts and bin edges we get with; # numpy.histogramdd when we pass it the original samples:; # The array values should be the same!; # Importing a RooDataHist from NumPy arrays with histogram counts and bin edges; # -----------------------------------------------------------------------------; # There is also a `RooDataHist.from_numpy` function, again with an interface; # inspired by `numpy.histogramdd`. You need to pass at least the histogram; # counts and the list of variables. The binning is optional: the default; # binning of the RooRealVars is used if not explicitly specified.; # It's also possible to pass custom bin edges to `RooDataHist.from_numpy`, just; # like you pass them to `numpy.histogramdd` when you get the counts to fill the; # RooDataHist with:; # Alternatively, you can specify only the number of bins and the range if your; # binning is uniform. This is preferred over passing the full list of bin; # edges, because RooFit will know that the binning is uniform and do some; # optimizations.",MatchSource.CODE_COMMENT,tutorials/roofit/rf409_NumPyPandasToRooFit.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf409_NumPyPandasToRooFit.py
Performance,optimiz,optimizations,"te some Gaussian toy data with numpy, this time with a different; # mean.; # Import the data to a RooDataSet, passing a dictionary of arrays and the; # corresponding RooRealVars just like you would pass to the RooDataSet; # constructor.; # Let's fit the Gaussian to the data. The mean is updated accordingly.; # We can now plot the model and the dataset with RooFit.; # Draw RooFit plot on a canvas.; # Exporting a RooDataHist to NumPy arrays for histogram counts and bin edges; # --------------------------------------------------------------------------; # Create a binned clone of the dataset to show RooDataHist to NumPy export.; # You can also export a RooDataHist to numpy arrays with; # RooDataHist.to_numpy(). As output, you will get a multidimensional array with; # the histogram counts and a list of arrays with bin edges. This is comparable; # to the output of numpy.histogram (or numpy.histogramdd for the; # multidimensional case).; # Let's compare the output to the counts and bin edges we get with; # numpy.histogramdd when we pass it the original samples:; # The array values should be the same!; # Importing a RooDataHist from NumPy arrays with histogram counts and bin edges; # -----------------------------------------------------------------------------; # There is also a `RooDataHist.from_numpy` function, again with an interface; # inspired by `numpy.histogramdd`. You need to pass at least the histogram; # counts and the list of variables. The binning is optional: the default; # binning of the RooRealVars is used if not explicitly specified.; # It's also possible to pass custom bin edges to `RooDataHist.from_numpy`, just; # like you pass them to `numpy.histogramdd` when you get the counts to fill the; # RooDataHist with:; # Alternatively, you can specify only the number of bins and the range if your; # binning is uniform. This is preferred over passing the full list of bin; # edges, because RooFit will know that the binning is uniform and do some; # optimizations.",MatchSource.CODE_COMMENT,tutorials/roofit/rf409_NumPyPandasToRooFit.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf409_NumPyPandasToRooFit.py
Deployability,integrat,integrated,"l sample model; # Construct the background pdf; # Construct the composite model; # Generate events for both samples; # ---------------------------------------------------------------; # Generate 1000 events in x and y from model; # Create index category and join samples; # ---------------------------------------------------------------------------; # Define category to distinguish physics and control samples events; # Construct combined dataset in (x,sample); # Construct a simultaneous pdf in (x, sample); # -----------------------------------------------------------------------------------; # Construct a simultaneous pdf using category sample as index: associate model; # with the physics state and model_ctl with the control state; # Perform a simultaneous fit; # ---------------------------------------------------; # Perform simultaneous fit of model to data and model_ctl to data_ctl; # Plot model slices on data slices; # ----------------------------------------------------------------; # Make a frame for the physics sample; # Plot all data tagged as physics sample; # Plot ""physics"" slice of simultaneous pdf.; # NB: You *must* project the sample index category with data using ProjWData as; # a RooSimultaneous makes no prediction on the shape in the index category and; # can thus not be integrated. In other words: Since the PDF doesn't know the; # number of events in the different category states, it doesn't know how much; # of each component it has to project out. This info is read from the data.; # The same plot for the control sample slice. We do this with a different; # approach this time, for illustration purposes. Here, we are slicing the; # dataset and then use the data slice for the projection, because then the; # RooFit::Slice() becomes unnecessary. This approach is more general,; # because you can plot sums of slices by using logical or in the Cut(); # command.; # The same plot for all the phase space. Here, we can just use the original; # combined dataset.",MatchSource.CODE_COMMENT,tutorials/roofit/rf501_simultaneouspdf.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf501_simultaneouspdf.py
Integrability,integrat,integrated,"l sample model; # Construct the background pdf; # Construct the composite model; # Generate events for both samples; # ---------------------------------------------------------------; # Generate 1000 events in x and y from model; # Create index category and join samples; # ---------------------------------------------------------------------------; # Define category to distinguish physics and control samples events; # Construct combined dataset in (x,sample); # Construct a simultaneous pdf in (x, sample); # -----------------------------------------------------------------------------------; # Construct a simultaneous pdf using category sample as index: associate model; # with the physics state and model_ctl with the control state; # Perform a simultaneous fit; # ---------------------------------------------------; # Perform simultaneous fit of model to data and model_ctl to data_ctl; # Plot model slices on data slices; # ----------------------------------------------------------------; # Make a frame for the physics sample; # Plot all data tagged as physics sample; # Plot ""physics"" slice of simultaneous pdf.; # NB: You *must* project the sample index category with data using ProjWData as; # a RooSimultaneous makes no prediction on the shape in the index category and; # can thus not be integrated. In other words: Since the PDF doesn't know the; # number of events in the different category states, it doesn't know how much; # of each component it has to project out. This info is read from the data.; # The same plot for the control sample slice. We do this with a different; # approach this time, for illustration purposes. Here, we are slicing the; # dataset and then use the data slice for the projection, because then the; # RooFit::Slice() becomes unnecessary. This approach is more general,; # because you can plot sums of slices by using logical or in the Cut(); # command.; # The same plot for all the phase space. Here, we can just use the original; # combined dataset.",MatchSource.CODE_COMMENT,tutorials/roofit/rf501_simultaneouspdf.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf501_simultaneouspdf.py
Safety,predict,prediction,"l sample model; # Construct the background pdf; # Construct the composite model; # Generate events for both samples; # ---------------------------------------------------------------; # Generate 1000 events in x and y from model; # Create index category and join samples; # ---------------------------------------------------------------------------; # Define category to distinguish physics and control samples events; # Construct combined dataset in (x,sample); # Construct a simultaneous pdf in (x, sample); # -----------------------------------------------------------------------------------; # Construct a simultaneous pdf using category sample as index: associate model; # with the physics state and model_ctl with the control state; # Perform a simultaneous fit; # ---------------------------------------------------; # Perform simultaneous fit of model to data and model_ctl to data_ctl; # Plot model slices on data slices; # ----------------------------------------------------------------; # Make a frame for the physics sample; # Plot all data tagged as physics sample; # Plot ""physics"" slice of simultaneous pdf.; # NB: You *must* project the sample index category with data using ProjWData as; # a RooSimultaneous makes no prediction on the shape in the index category and; # can thus not be integrated. In other words: Since the PDF doesn't know the; # number of events in the different category states, it doesn't know how much; # of each component it has to project out. This info is read from the data.; # The same plot for the control sample slice. We do this with a different; # approach this time, for illustration purposes. Here, we are slicing the; # dataset and then use the data slice for the projection, because then the; # RooFit::Slice() becomes unnecessary. This approach is more general,; # because you can plot sums of slices by using logical or in the Cut(); # command.; # The same plot for all the phase space. Here, we can just use the original; # combined dataset.",MatchSource.CODE_COMMENT,tutorials/roofit/rf501_simultaneouspdf.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf501_simultaneouspdf.py
Testability,log,logical,"l sample model; # Construct the background pdf; # Construct the composite model; # Generate events for both samples; # ---------------------------------------------------------------; # Generate 1000 events in x and y from model; # Create index category and join samples; # ---------------------------------------------------------------------------; # Define category to distinguish physics and control samples events; # Construct combined dataset in (x,sample); # Construct a simultaneous pdf in (x, sample); # -----------------------------------------------------------------------------------; # Construct a simultaneous pdf using category sample as index: associate model; # with the physics state and model_ctl with the control state; # Perform a simultaneous fit; # ---------------------------------------------------; # Perform simultaneous fit of model to data and model_ctl to data_ctl; # Plot model slices on data slices; # ----------------------------------------------------------------; # Make a frame for the physics sample; # Plot all data tagged as physics sample; # Plot ""physics"" slice of simultaneous pdf.; # NB: You *must* project the sample index category with data using ProjWData as; # a RooSimultaneous makes no prediction on the shape in the index category and; # can thus not be integrated. In other words: Since the PDF doesn't know the; # number of events in the different category states, it doesn't know how much; # of each component it has to project out. This info is read from the data.; # The same plot for the control sample slice. We do this with a different; # approach this time, for illustration purposes. Here, we are slicing the; # dataset and then use the data slice for the projection, because then the; # RooFit::Slice() becomes unnecessary. This approach is more general,; # because you can plot sums of slices by using logical or in the Cut(); # command.; # The same plot for all the phase space. Here, we can just use the original; # combined dataset.",MatchSource.CODE_COMMENT,tutorials/roofit/rf501_simultaneouspdf.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf501_simultaneouspdf.py
Deployability,configurat,configuration,"## \file rf505_asciicfg.py; ## \ingroup tutorial_roofit; ## \notebook -nodraw; ## Organization and simultaneous fits: reading and writing ASCII configuration files; ##; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create pdf; # ------------------; # Construct gauss(x,m,s); # Construct poly(x,p0); # model = f*gauss(x) + (1-f)*poly(x); # Fit model to toy data; # -----------------------------------------; # Write parameters to ASCII file; # -----------------------------------------------------------; # Obtain set of parameters; # Write parameters to file; # Read parameters from ASCII file; # ----------------------------------------------------------------; # Read parameters from file; # Read parameters from section 'Section2' of file; # Read parameters from section 'Section3' of file. Mark all; # variables that were processed with the ""READ"" attribute; # Print the list of parameters that were not read from Section3; # Read parameters from section 'Section4' of file, contains; # 'include file' statement of rf505_asciicfg_example.txt; # so that we effective read the same",MatchSource.CODE_COMMENT,tutorials/roofit/rf505_asciicfg.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf505_asciicfg.py
Modifiability,config,configuration,"## \file rf505_asciicfg.py; ## \ingroup tutorial_roofit; ## \notebook -nodraw; ## Organization and simultaneous fits: reading and writing ASCII configuration files; ##; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create pdf; # ------------------; # Construct gauss(x,m,s); # Construct poly(x,p0); # model = f*gauss(x) + (1-f)*poly(x); # Fit model to toy data; # -----------------------------------------; # Write parameters to ASCII file; # -----------------------------------------------------------; # Obtain set of parameters; # Write parameters to file; # Read parameters from ASCII file; # ----------------------------------------------------------------; # Read parameters from file; # Read parameters from section 'Section2' of file; # Read parameters from section 'Section3' of file. Mark all; # variables that were processed with the ""READ"" attribute; # Print the list of parameters that were not read from Section3; # Read parameters from section 'Section4' of file, contains; # 'include file' statement of rf505_asciicfg_example.txt; # so that we effective read the same",MatchSource.CODE_COMMENT,tutorials/roofit/rf505_asciicfg.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf505_asciicfg.py
Deployability,configurat,configuration,"## \file; ## \ingroup tutorial_roofit; ## \notebook -nodraw; ## Organization and simultaneous fits: tuning and customizing the ROOT.RooFit message logging facility; ##; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create pdf; # --------------------; # Construct gauss(x,m,s); # Construct poly(x,p0); # model = f*gauss(x) + (1-f)*poly(x); # Print configuration of message service; # ------------------------------------------; # Print streams configuration; # Adding integration topic to existing INFO stream; # ---------------------------------------------------; # Print streams configuration; # Add Integration topic to existing INFO stream; # Construct integral over gauss to demonstrate message stream; # Print streams configuration in verbose, also shows inactive streams; # Remove stream; # Examples of pdf value tracing; # -----------------------------------------------------------------------; # Show DEBUG level message on function tracing, ROOT.RooGaussian only; # Perform a fit to generate some tracing messages; # Reset message service to default stream configuration; # Show DEBUG level message on function tracing on all objects, output to; # file; # Perform a fit to generate some tracing messages; # Reset message service to default stream configuration; # Example of another debugging stream; # ---------------------------------------------------------------------; # Show DEBUG level messages on client/server link state management; # Clone composite pdf g to trigger some link state management activity; # Reset message service to default stream configuration",MatchSource.CODE_COMMENT,tutorials/roofit/rf506_msgservice.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf506_msgservice.py
Integrability,message,message,"## \file; ## \ingroup tutorial_roofit; ## \notebook -nodraw; ## Organization and simultaneous fits: tuning and customizing the ROOT.RooFit message logging facility; ##; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create pdf; # --------------------; # Construct gauss(x,m,s); # Construct poly(x,p0); # model = f*gauss(x) + (1-f)*poly(x); # Print configuration of message service; # ------------------------------------------; # Print streams configuration; # Adding integration topic to existing INFO stream; # ---------------------------------------------------; # Print streams configuration; # Add Integration topic to existing INFO stream; # Construct integral over gauss to demonstrate message stream; # Print streams configuration in verbose, also shows inactive streams; # Remove stream; # Examples of pdf value tracing; # -----------------------------------------------------------------------; # Show DEBUG level message on function tracing, ROOT.RooGaussian only; # Perform a fit to generate some tracing messages; # Reset message service to default stream configuration; # Show DEBUG level message on function tracing on all objects, output to; # file; # Perform a fit to generate some tracing messages; # Reset message service to default stream configuration; # Example of another debugging stream; # ---------------------------------------------------------------------; # Show DEBUG level messages on client/server link state management; # Clone composite pdf g to trigger some link state management activity; # Reset message service to default stream configuration",MatchSource.CODE_COMMENT,tutorials/roofit/rf506_msgservice.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf506_msgservice.py
Modifiability,config,configuration,"## \file; ## \ingroup tutorial_roofit; ## \notebook -nodraw; ## Organization and simultaneous fits: tuning and customizing the ROOT.RooFit message logging facility; ##; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create pdf; # --------------------; # Construct gauss(x,m,s); # Construct poly(x,p0); # model = f*gauss(x) + (1-f)*poly(x); # Print configuration of message service; # ------------------------------------------; # Print streams configuration; # Adding integration topic to existing INFO stream; # ---------------------------------------------------; # Print streams configuration; # Add Integration topic to existing INFO stream; # Construct integral over gauss to demonstrate message stream; # Print streams configuration in verbose, also shows inactive streams; # Remove stream; # Examples of pdf value tracing; # -----------------------------------------------------------------------; # Show DEBUG level message on function tracing, ROOT.RooGaussian only; # Perform a fit to generate some tracing messages; # Reset message service to default stream configuration; # Show DEBUG level message on function tracing on all objects, output to; # file; # Perform a fit to generate some tracing messages; # Reset message service to default stream configuration; # Example of another debugging stream; # ---------------------------------------------------------------------; # Show DEBUG level messages on client/server link state management; # Clone composite pdf g to trigger some link state management activity; # Reset message service to default stream configuration",MatchSource.CODE_COMMENT,tutorials/roofit/rf506_msgservice.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf506_msgservice.py
Testability,log,logging,"## \file; ## \ingroup tutorial_roofit; ## \notebook -nodraw; ## Organization and simultaneous fits: tuning and customizing the ROOT.RooFit message logging facility; ##; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create pdf; # --------------------; # Construct gauss(x,m,s); # Construct poly(x,p0); # model = f*gauss(x) + (1-f)*poly(x); # Print configuration of message service; # ------------------------------------------; # Print streams configuration; # Adding integration topic to existing INFO stream; # ---------------------------------------------------; # Print streams configuration; # Add Integration topic to existing INFO stream; # Construct integral over gauss to demonstrate message stream; # Print streams configuration in verbose, also shows inactive streams; # Remove stream; # Examples of pdf value tracing; # -----------------------------------------------------------------------; # Show DEBUG level message on function tracing, ROOT.RooGaussian only; # Perform a fit to generate some tracing messages; # Reset message service to default stream configuration; # Show DEBUG level message on function tracing on all objects, output to; # file; # Perform a fit to generate some tracing messages; # Reset message service to default stream configuration; # Example of another debugging stream; # ---------------------------------------------------------------------; # Show DEBUG level messages on client/server link state management; # Clone composite pdf g to trigger some link state management activity; # Reset message service to default stream configuration",MatchSource.CODE_COMMENT,tutorials/roofit/rf506_msgservice.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf506_msgservice.py
Availability,error,error,"------------------; # A ROOT.RooArgSet is a set of RooAbsArg objects. Each object in the set must have; # a unique name; # Set constructors exists with up to 9 initial arguments; # At any time objects can be added with add(); # Add up to 9 additional arguments in one call; # s.add(ROOT.RooArgSet(c, d)); # Sets can contain any type of RooAbsArg, pdf and functions; # Remove element d; # Accessing RooArgSet contents; # -------------------------------------------------------; # You can look up objects by name; # Construct a subset by name; # Construct asubset by attribute; # Construct the subset of overlapping contents with another set; # Owning RooArgSets; # ---------------------------------; # You can create a RooArgSet that owns copies of the objects instead of; # referencing the originals. A set either owns all of its components or none,; # so once addClone() is used, add() can no longer be used and will result in an; # error message; # A clone of a owning set is non-owning and its; # contents is owned by the originating owning set; # To make a clone of a set and its contents use; # the snapshot method; # If a set contains function objects, the head node; # is cloned in a snapshot. To make a snapshot of all; # servers of a function object do as follows. The result; # of a RooArgSet snapshot with deepCloning option is a set; # of cloned objects, all their clone (recursive) server; # dependencies, together form a self-consistent; # set that is free of external dependencies; # Set printing; # ------------------------; # Inline printing only show list of names of contained objects; # Plain print shows the same, by name of the set; # Standard printing shows one line for each item with the items name, name; # and value; # Verbose printing adds each items arguments, and 'extras' as defined by; # the object; # Using RooArgLists; # ---------------------------------; # List constructors exists with up to 9 initial arguments; # Lists have an explicit order and allow multiple a",MatchSource.CODE_COMMENT,tutorials/roofit/rf508_listsetmanip.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf508_listsetmanip.py
Integrability,message,message,"------------------; # A ROOT.RooArgSet is a set of RooAbsArg objects. Each object in the set must have; # a unique name; # Set constructors exists with up to 9 initial arguments; # At any time objects can be added with add(); # Add up to 9 additional arguments in one call; # s.add(ROOT.RooArgSet(c, d)); # Sets can contain any type of RooAbsArg, pdf and functions; # Remove element d; # Accessing RooArgSet contents; # -------------------------------------------------------; # You can look up objects by name; # Construct a subset by name; # Construct asubset by attribute; # Construct the subset of overlapping contents with another set; # Owning RooArgSets; # ---------------------------------; # You can create a RooArgSet that owns copies of the objects instead of; # referencing the originals. A set either owns all of its components or none,; # so once addClone() is used, add() can no longer be used and will result in an; # error message; # A clone of a owning set is non-owning and its; # contents is owned by the originating owning set; # To make a clone of a set and its contents use; # the snapshot method; # If a set contains function objects, the head node; # is cloned in a snapshot. To make a snapshot of all; # servers of a function object do as follows. The result; # of a RooArgSet snapshot with deepCloning option is a set; # of cloned objects, all their clone (recursive) server; # dependencies, together form a self-consistent; # set that is free of external dependencies; # Set printing; # ------------------------; # Inline printing only show list of names of contained objects; # Plain print shows the same, by name of the set; # Standard printing shows one line for each item with the items name, name; # and value; # Verbose printing adds each items arguments, and 'extras' as defined by; # the object; # Using RooArgLists; # ---------------------------------; # List constructors exists with up to 9 initial arguments; # Lists have an explicit order and allow multiple a",MatchSource.CODE_COMMENT,tutorials/roofit/rf508_listsetmanip.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf508_listsetmanip.py
Modifiability,variab,variables,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## 'ORGANIZATION AND SIMULTANEOUS FITS' RooFit tutorial macro #508; ##; ## RooArgSet and RooArgList tools and tricks; ##; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C version); # Create dummy objects; # ---------------------------------------; # Create some variables; # Create a category; # Create a pdf; # Creating, killing RooArgSets; # -------------------------------------------------------; # A ROOT.RooArgSet is a set of RooAbsArg objects. Each object in the set must have; # a unique name; # Set constructors exists with up to 9 initial arguments; # At any time objects can be added with add(); # Add up to 9 additional arguments in one call; # s.add(ROOT.RooArgSet(c, d)); # Sets can contain any type of RooAbsArg, pdf and functions; # Remove element d; # Accessing RooArgSet contents; # -------------------------------------------------------; # You can look up objects by name; # Construct a subset by name; # Construct asubset by attribute; # Construct the subset of overlapping contents with another set; # Owning RooArgSets; # ---------------------------------; # You can create a RooArgSet that owns copies of the objects instead of; # referencing the originals. A set either owns all of its components or none,; # so once addClone() is used, add() can no longer be used and will result in an; # error message; # A clone of a owning set is non-owning and its; # contents is owned by the originating owning set; # To make a clone of a set and its contents use; # the snapshot method; # If a set contains function objects, the head node; # is cloned in a snapshot. To make a snapshot of all; # servers of a function object do as follows. The result; # of a RooArgSet snapshot with deepCloning option is a set; # of cloned objects, all their clone (recursive) server; # dependencies, together form a self-consistent; # set that is free of external dependencies; # Set print",MatchSource.CODE_COMMENT,tutorials/roofit/rf508_listsetmanip.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf508_listsetmanip.py
Modifiability,variab,variable,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Organization and simultaneous fits: easy interactive access to workspace contents - CINT; ## to CLING code migration; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create pdf and fill workspace; # --------------------------------------------------------; # Declare observable x; # Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and; # their parameters; # Build Chebychev polynomial pdf; # Sum the signal components into a composite signal pdf; # Sum the composite signal and background; # Create and fill workspace; # ------------------------------------------------; # Create a workspace named 'w'; # With CINT w could exports its contents to; # a same-name C++ namespace in CINT 'namespace w'.; # but self does not work anymore in CLING.; # so self tutorial is an example on how to; # change the code; # Fill workspace with pdf and data in a separate function; # Print workspace contents; # self does not work anymore with CLING; # use normal workspace functionality; # Use workspace contents; # ----------------------------------------------; # Old syntax to use the name space prefix operator to access the workspace contents; #; # d = w.model.generate(w.x,1000); # r = w.model.fitTo(*d); # use normal workspace methods; # old syntax to access the variable x; # frame = w.x.frame(); # OLD syntax to omit x.; # NB: The 'w.' prefix can be omitted if namespace w is imported in local namespace; # in the usual C++ way; #; # using namespace w; # model.plotOn(frame); # model.plotOn(frame, Components=bkg, LineStyle=""--""); # correct syntax; # Draw the frame on the canvas",MatchSource.CODE_COMMENT,tutorials/roofit/rf509_wsinteractive.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf509_wsinteractive.py
Security,access,access,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Organization and simultaneous fits: easy interactive access to workspace contents - CINT; ## to CLING code migration; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create pdf and fill workspace; # --------------------------------------------------------; # Declare observable x; # Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and; # their parameters; # Build Chebychev polynomial pdf; # Sum the signal components into a composite signal pdf; # Sum the composite signal and background; # Create and fill workspace; # ------------------------------------------------; # Create a workspace named 'w'; # With CINT w could exports its contents to; # a same-name C++ namespace in CINT 'namespace w'.; # but self does not work anymore in CLING.; # so self tutorial is an example on how to; # change the code; # Fill workspace with pdf and data in a separate function; # Print workspace contents; # self does not work anymore with CLING; # use normal workspace functionality; # Use workspace contents; # ----------------------------------------------; # Old syntax to use the name space prefix operator to access the workspace contents; #; # d = w.model.generate(w.x,1000); # r = w.model.fitTo(*d); # use normal workspace methods; # old syntax to access the variable x; # frame = w.x.frame(); # OLD syntax to omit x.; # NB: The 'w.' prefix can be omitted if namespace w is imported in local namespace; # in the usual C++ way; #; # using namespace w; # model.plotOn(frame); # model.plotOn(frame, Components=bkg, LineStyle=""--""); # correct syntax; # Draw the frame on the canvas",MatchSource.CODE_COMMENT,tutorials/roofit/rf509_wsinteractive.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf509_wsinteractive.py
Availability,error,errors,"------; # Define named sets ""parameters"" and ""observables"", list which variables should be considered; # parameters and observables by the users convention; #; # Variables appearing in sets _must_ live in the workspace already, the autoImport flag; # of defineSet must be set to import them on the fly. Named sets contain only references; # to the original variables, the value of observables in named sets already; # reflect their 'current' value; # Encode reference value for parameters in workspace; # ---------------------------------------------------------------------------------------------------; # Define a parameter 'snapshot' in the p.d.f.; # Unlike a named set, parameter snapshot stores an independent set of values for; # a given set of variables in the workspace. The values can be stored and reloaded; # into the workspace variable objects using the loadSnapshot() and saveSnapshot(); # methods. A snapshot saves the value of each variable, errors that are stored; # with it as well as the 'Constant' flag that is used in fits to determine if a; # parameter is kept fixed or not.; # Do a dummy fit to a (supposedly) reference dataset here and store the results; # of that fit into a snapshot; # The kTRUE flag imports the values of the objects in (*params) into the workspace; # If not set, present values of the workspace parameters objects are stored; # Make another fit with the signal componentforced to zero; # and save those parameters too; # Create model and dataset; # -----------------------------------------------; # Exploit convention encoded in named set ""parameters"" and ""observables""; # to use workspace contents w/o need for introspected; # Generate data from p.d.f. in given observables; # Fit model to data; # Plot fitted model and data on frame of first (only) observable; # Overlay plot with model with reference parameters as stored in snapshots; # Draw the frame on the canvas; # Print workspace contents; # Workspace will remain in memory after macro finishes",MatchSource.CODE_COMMENT,tutorials/roofit/rf510_wsnamedsets.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf510_wsnamedsets.py
Modifiability,variab,variables,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## 'ORGANIZATION AND SIMULTANEOUS FITS' RooFit tutorial macro #510; ##; ## Working with named parameter sets and parameter snapshots in; ## workspaces; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C version); # Create model; # -----------------------; # Declare observable x; # Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and; # their parameters; # Build Chebychev polynomial p.d.f.; # Sum the signal components into a composite signal p.d.f.; # Sum the composite signal and background; # Import model into p.d.f.; # Encode definition of parameters in workspace; # ---------------------------------------------------------------------------------------; # Define named sets ""parameters"" and ""observables"", list which variables should be considered; # parameters and observables by the users convention; #; # Variables appearing in sets _must_ live in the workspace already, the autoImport flag; # of defineSet must be set to import them on the fly. Named sets contain only references; # to the original variables, the value of observables in named sets already; # reflect their 'current' value; # Encode reference value for parameters in workspace; # ---------------------------------------------------------------------------------------------------; # Define a parameter 'snapshot' in the p.d.f.; # Unlike a named set, parameter snapshot stores an independent set of values for; # a given set of variables in the workspace. The values can be stored and reloaded; # into the workspace variable objects using the loadSnapshot() and saveSnapshot(); # methods. A snapshot saves the value of each variable, errors that are stored; # with it as well as the 'Constant' flag that is used in fits to determine if a; # parameter is kept fixed or not.; # Do a dummy fit to a (supposedly) reference dataset here and store the results; # of that fit into a",MatchSource.CODE_COMMENT,tutorials/roofit/rf510_wsnamedsets.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf510_wsnamedsets.py
Performance,load,loadSnapshot,"port model into p.d.f.; # Encode definition of parameters in workspace; # ---------------------------------------------------------------------------------------; # Define named sets ""parameters"" and ""observables"", list which variables should be considered; # parameters and observables by the users convention; #; # Variables appearing in sets _must_ live in the workspace already, the autoImport flag; # of defineSet must be set to import them on the fly. Named sets contain only references; # to the original variables, the value of observables in named sets already; # reflect their 'current' value; # Encode reference value for parameters in workspace; # ---------------------------------------------------------------------------------------------------; # Define a parameter 'snapshot' in the p.d.f.; # Unlike a named set, parameter snapshot stores an independent set of values for; # a given set of variables in the workspace. The values can be stored and reloaded; # into the workspace variable objects using the loadSnapshot() and saveSnapshot(); # methods. A snapshot saves the value of each variable, errors that are stored; # with it as well as the 'Constant' flag that is used in fits to determine if a; # parameter is kept fixed or not.; # Do a dummy fit to a (supposedly) reference dataset here and store the results; # of that fit into a snapshot; # The kTRUE flag imports the values of the objects in (*params) into the workspace; # If not set, present values of the workspace parameters objects are stored; # Make another fit with the signal componentforced to zero; # and save those parameters too; # Create model and dataset; # -----------------------------------------------; # Exploit convention encoded in named set ""parameters"" and ""observables""; # to use workspace contents w/o need for introspected; # Generate data from p.d.f. in given observables; # Fit model to data; # Plot fitted model and data on frame of first (only) observable; # Overlay plot with model with refere",MatchSource.CODE_COMMENT,tutorials/roofit/rf510_wsnamedsets.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf510_wsnamedsets.py
Integrability,depend,depending,"## \file; ## \ingroup tutorial_roofit; ## \notebook -nodraw; ## Organization and simultaneous fits: basic use of the 'object factory' associated with a; ## workspace to rapidly build pdfs functions and their parameter components; ##; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Creating and adding basic pdfs; # ----------------------------------------------------------------; # Remake example pdf of tutorial rs502_wspacewrite.C:; #; # Basic pdf construction: ClassName.ObjectName(constructor arguments); # Variable construction : VarName[x,xlo,xhi], VarName[xlo,xhi], VarName[x]; # P.d.f. addition : SUM.ObjectName(coef1*pdf1,...coefM*pdfM,pdfN); #; # Use object factory to build pdf of tutorial rs502_wspacewrite; # Use object factory to build pdf of tutorial rs502_wspacewrite but; # - Contracted to a single line recursive expression,; # - Omitting explicit names for components that are not referred to explicitly later; # Advanced pdf constructor arguments; # ----------------------------------------------------------------; #; # P.d.f. constructor arguments may by any type of ROOT.RooAbsArg, also; #; # Double_t -. converted to ROOT.RooConst(...); # {a,b,c} -. converted to ROOT.RooArgSet() or ROOT.RooArgList() depending on required ctor arg; # dataset name -. converted to ROOT.RooAbsData reference for any dataset residing in the workspace; # enum -. Any enum label that belongs to an enum defined in the (base); # class; # Make a dummy dataset pdf 'model' and import it in the workspace; # Cannot call 'import' directly because this is a python keyword:; # Construct a KEYS pdf passing a dataset name and an enum type defining the; # mirroring strategy; # w.factory(""KeysPdf::k(x,data,NoMirror,0.2)""); # Workaround for pyROOT; # Print workspace contents",MatchSource.CODE_COMMENT,tutorials/roofit/rf511_wsfactory_basic.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf511_wsfactory_basic.py
Integrability,interface,interface,"## \file; ## \ingroup tutorial_roofit; ## \notebook -nodraw; ## Organization and simultaneous fits: illustration use of ROOT.RooCustomizer and; ## ROOT.RooSimWSTool interface in factory workspace tool in a complex standalone B physics example; ##; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Build a complex example pdf; # -----------------------------------------------------------; # Make signal model for CPV: A bmixing decay function in t (convoluted with a triple Gaussian resolution model); # times a Gaussian function the reconstructed mass; # Make background component: A plain decay function in t times an Argus; # function in the reconstructed mass; # Make composite model from the signal and background component; # Example of RooSimWSTool interface; # ------------------------------------------------------------------; # Introduce a flavour tagging category tagCat as observable with 4 states corresponding; # to 4 flavour tagging techniques with different performance that require different; # parameterizations of the fit model; #; # ROOT.RooSimWSTool operation:; # - Make 4 clones of model (for each tagCat) state, will gain an individual; # copy of parameters w, and biasC. The other parameters remain common; # - Make a simultaneous pdf of the 4 clones assigning each to the appropriate; # state of the tagCat index category; # ROOT.RooSimWSTool is interfaced as meta-type SIMCLONE in the factory. The $SplitParam(); # argument maps to the SplitParam() named argument in the; # ROOT.RooSimWSTool constructor; # Example of RooCustomizer interface; # -------------------------------------------------------------------; #; # Class ROOT.RooCustomizer makes clones of existing pdfs with certain prescribed; # modifications (branch of leaf node replacements); #; # Here we take our model (the original before ROOT.RooSimWSTool modifications); # and request that the parameter w (the mistag rate) is replaced wi",MatchSource.CODE_COMMENT,tutorials/roofit/rf513_wsfactory_tools.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf513_wsfactory_tools.py
Modifiability,parameteriz,parameterizations,"## \file; ## \ingroup tutorial_roofit; ## \notebook -nodraw; ## Organization and simultaneous fits: illustration use of ROOT.RooCustomizer and; ## ROOT.RooSimWSTool interface in factory workspace tool in a complex standalone B physics example; ##; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Build a complex example pdf; # -----------------------------------------------------------; # Make signal model for CPV: A bmixing decay function in t (convoluted with a triple Gaussian resolution model); # times a Gaussian function the reconstructed mass; # Make background component: A plain decay function in t times an Argus; # function in the reconstructed mass; # Make composite model from the signal and background component; # Example of RooSimWSTool interface; # ------------------------------------------------------------------; # Introduce a flavour tagging category tagCat as observable with 4 states corresponding; # to 4 flavour tagging techniques with different performance that require different; # parameterizations of the fit model; #; # ROOT.RooSimWSTool operation:; # - Make 4 clones of model (for each tagCat) state, will gain an individual; # copy of parameters w, and biasC. The other parameters remain common; # - Make a simultaneous pdf of the 4 clones assigning each to the appropriate; # state of the tagCat index category; # ROOT.RooSimWSTool is interfaced as meta-type SIMCLONE in the factory. The $SplitParam(); # argument maps to the SplitParam() named argument in the; # ROOT.RooSimWSTool constructor; # Example of RooCustomizer interface; # -------------------------------------------------------------------; #; # Class ROOT.RooCustomizer makes clones of existing pdfs with certain prescribed; # modifications (branch of leaf node replacements); #; # Here we take our model (the original before ROOT.RooSimWSTool modifications); # and request that the parameter w (the mistag rate) is replaced wi",MatchSource.CODE_COMMENT,tutorials/roofit/rf513_wsfactory_tools.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf513_wsfactory_tools.py
Performance,perform,performance,"## \file; ## \ingroup tutorial_roofit; ## \notebook -nodraw; ## Organization and simultaneous fits: illustration use of ROOT.RooCustomizer and; ## ROOT.RooSimWSTool interface in factory workspace tool in a complex standalone B physics example; ##; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Build a complex example pdf; # -----------------------------------------------------------; # Make signal model for CPV: A bmixing decay function in t (convoluted with a triple Gaussian resolution model); # times a Gaussian function the reconstructed mass; # Make background component: A plain decay function in t times an Argus; # function in the reconstructed mass; # Make composite model from the signal and background component; # Example of RooSimWSTool interface; # ------------------------------------------------------------------; # Introduce a flavour tagging category tagCat as observable with 4 states corresponding; # to 4 flavour tagging techniques with different performance that require different; # parameterizations of the fit model; #; # ROOT.RooSimWSTool operation:; # - Make 4 clones of model (for each tagCat) state, will gain an individual; # copy of parameters w, and biasC. The other parameters remain common; # - Make a simultaneous pdf of the 4 clones assigning each to the appropriate; # state of the tagCat index category; # ROOT.RooSimWSTool is interfaced as meta-type SIMCLONE in the factory. The $SplitParam(); # argument maps to the SplitParam() named argument in the; # ROOT.RooSimWSTool constructor; # Example of RooCustomizer interface; # -------------------------------------------------------------------; #; # Class ROOT.RooCustomizer makes clones of existing pdfs with certain prescribed; # modifications (branch of leaf node replacements); #; # Here we take our model (the original before ROOT.RooSimWSTool modifications); # and request that the parameter w (the mistag rate) is replaced wi",MatchSource.CODE_COMMENT,tutorials/roofit/rf513_wsfactory_tools.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf513_wsfactory_tools.py
Modifiability,variab,variable,"## \file; ## \ingroup tutorial_roofit; ## \notebook -nodraw; ## Using the RooCustomizer to create multiple PDFs that share a lot of properties, but have unique parameters for each category.; ## As an extra complication, some of the new parameters need to be functions of a mass parameter.; ##; ## \macro_code; ## \macro_output; ##; ## \date June 2021; ## \author Harshal Shende, Stephan Hageboeck (C++ version); # ""T"" prints the model as a tree; # Build the categories; # Start to customise the proto model that was defined above.; # ---------------------------------------------------------------------------; # We need two sets for bookkeeping of PDF nodes:; # 1. Each sample should have its own mean for the gaussian; # The customiser will make copies of `meanG` for each category.; # These will all appear in the set `newLeaves`, which will own the new nodes.; # 2. Each sample should have its own signal yield, but there is an extra complication:; # We need the yields 1 and 2 to be a function of the variable ""mass"".; # For this, we pre-define nodes with exactly the names that the customiser would have created automatically,; # that is, ""<nodeName>_<categoryName>"", and we register them in the set of customiser nodes.; # The customiser will pick them up instead of creating new ones.; # If we don't provide one (e.g. for ""yieldSig_Sample3""), it will be created automatically by cloning `yieldSig`.; # Instruct the customiser to replace all yieldSig nodes for each sample:; # Now we can start building the PDFs for all categories:; # And we inspect the two PDFs; # If we needed to set reasonable values for the means of the gaussians, this could be done as follows:",MatchSource.CODE_COMMENT,tutorials/roofit/rf514_RooCustomizer.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf514_RooCustomizer.py
Modifiability,config,config,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Code HistFactory Models in JSON.; ##; ## With the HS3 standard, it is possible to code RooFit-Models of any kind as JSON files.; ## In this tutorial, you can see how to code up a (simple) HistFactory-based model in JSON and import it into a RooWorkspace.; ##; ## \macro_code; ##; ## \date November 2021; ## \author Carsten Burgard; # start by creating an empty workspace; # the RooJSONFactoryWSTool is responsible for importing and exporting things to and from your workspace; # use it to import the information from your JSON file; # now, you can easily use your workspace to run your fit (as you usually would); # the model config is named after your pdf, i.e. <the pdf name>_modelConfig; # for resetting the parameters after the fit; # we are fitting a clone of the model now,; # reset parameters, such that we are not double-fitting the model in the; # closure check.; # in the end, you can again write to json; # the result will be not completely identical to the JSON file you used as an input, but it will work just the same; # You can again import it if you want and check for closure",MatchSource.CODE_COMMENT,tutorials/roofit/rf515_hfJSON.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf515_hfJSON.py
Usability,simpl,simple,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Code HistFactory Models in JSON.; ##; ## With the HS3 standard, it is possible to code RooFit-Models of any kind as JSON files.; ## In this tutorial, you can see how to code up a (simple) HistFactory-based model in JSON and import it into a RooWorkspace.; ##; ## \macro_code; ##; ## \date November 2021; ## \author Carsten Burgard; # start by creating an empty workspace; # the RooJSONFactoryWSTool is responsible for importing and exporting things to and from your workspace; # use it to import the information from your JSON file; # now, you can easily use your workspace to run your fit (as you usually would); # the model config is named after your pdf, i.e. <the pdf name>_modelConfig; # for resetting the parameters after the fit; # we are fitting a clone of the model now,; # reset parameters, such that we are not double-fitting the model in the; # closure check.; # in the end, you can again write to json; # the result will be not completely identical to the JSON file you used as an input, but it will work just the same; # You can again import it if you want and check for closure",MatchSource.CODE_COMMENT,tutorials/roofit/rf515_hfJSON.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf515_hfJSON.py
Availability,error,error,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## 'LIKELIHOOD AND MINIMIZATION' RooFit tutorial macro #601; ##; ## Interactive minimization with MINUIT; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C version); # Setup pdf and likelihood; # -----------------------------------------------; # Observable; # Model (intentional strong correlations); # Generate 1000 events; # Construct unbinned likelihood of model w.r.t. data; # Interactive minimization, error analysis; # -------------------------------------------------------------------------------; # Create MINUIT interface object; # Activate verbose logging of MINUIT parameter space stepping; # Call MIGRAD to minimize the likelihood; # Print values of all parameters, reflect values (and error estimates); # that are back propagated from MINUIT; # Disable verbose logging; # Run HESSE to calculate errors from d2L/dp2; # Print value (and error) of sigma_g2 parameter, reflects; # value and error back propagated from MINUIT; # Run MINOS on sigma_g2 parameter only; # Print value (and error) of sigma_g2 parameter, reflects; # value and error back propagated from MINUIT; # Saving results, contour plots; # ---------------------------------------------------------; # Save a snapshot of the fit result. ROOT.This object contains the initial; # fit parameters, final fit parameters, complete correlation; # matrix, EDM, minimized FCN , last MINUIT status code and; # the number of times the ROOT.RooFit function object has indicated evaluation; # problems (e.g. zero probabilities during likelihood evaluation); # Make contour plot of mx vs sx at 1,2, sigma; # Print the fit result snapshot; # Change parameter values, plotting; # -----------------------------------------------------------------; # At any moment you can manually change the value of a (constant); # parameter; # Rerun MIGRAD,HESSE; # Now fix sigma_g2; # Rerun MIGRAD,HESSE",MatchSource.CODE_COMMENT,tutorials/roofit/rf601_intminuit.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf601_intminuit.py
Integrability,interface,interface,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## 'LIKELIHOOD AND MINIMIZATION' RooFit tutorial macro #601; ##; ## Interactive minimization with MINUIT; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C version); # Setup pdf and likelihood; # -----------------------------------------------; # Observable; # Model (intentional strong correlations); # Generate 1000 events; # Construct unbinned likelihood of model w.r.t. data; # Interactive minimization, error analysis; # -------------------------------------------------------------------------------; # Create MINUIT interface object; # Activate verbose logging of MINUIT parameter space stepping; # Call MIGRAD to minimize the likelihood; # Print values of all parameters, reflect values (and error estimates); # that are back propagated from MINUIT; # Disable verbose logging; # Run HESSE to calculate errors from d2L/dp2; # Print value (and error) of sigma_g2 parameter, reflects; # value and error back propagated from MINUIT; # Run MINOS on sigma_g2 parameter only; # Print value (and error) of sigma_g2 parameter, reflects; # value and error back propagated from MINUIT; # Saving results, contour plots; # ---------------------------------------------------------; # Save a snapshot of the fit result. ROOT.This object contains the initial; # fit parameters, final fit parameters, complete correlation; # matrix, EDM, minimized FCN , last MINUIT status code and; # the number of times the ROOT.RooFit function object has indicated evaluation; # problems (e.g. zero probabilities during likelihood evaluation); # Make contour plot of mx vs sx at 1,2, sigma; # Print the fit result snapshot; # Change parameter values, plotting; # -----------------------------------------------------------------; # At any moment you can manually change the value of a (constant); # parameter; # Rerun MIGRAD,HESSE; # Now fix sigma_g2; # Rerun MIGRAD,HESSE",MatchSource.CODE_COMMENT,tutorials/roofit/rf601_intminuit.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf601_intminuit.py
Testability,log,logging,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## 'LIKELIHOOD AND MINIMIZATION' RooFit tutorial macro #601; ##; ## Interactive minimization with MINUIT; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C version); # Setup pdf and likelihood; # -----------------------------------------------; # Observable; # Model (intentional strong correlations); # Generate 1000 events; # Construct unbinned likelihood of model w.r.t. data; # Interactive minimization, error analysis; # -------------------------------------------------------------------------------; # Create MINUIT interface object; # Activate verbose logging of MINUIT parameter space stepping; # Call MIGRAD to minimize the likelihood; # Print values of all parameters, reflect values (and error estimates); # that are back propagated from MINUIT; # Disable verbose logging; # Run HESSE to calculate errors from d2L/dp2; # Print value (and error) of sigma_g2 parameter, reflects; # value and error back propagated from MINUIT; # Run MINOS on sigma_g2 parameter only; # Print value (and error) of sigma_g2 parameter, reflects; # value and error back propagated from MINUIT; # Saving results, contour plots; # ---------------------------------------------------------; # Save a snapshot of the fit result. ROOT.This object contains the initial; # fit parameters, final fit parameters, complete correlation; # matrix, EDM, minimized FCN , last MINUIT status code and; # the number of times the ROOT.RooFit function object has indicated evaluation; # problems (e.g. zero probabilities during likelihood evaluation); # Make contour plot of mx vs sx at 1,2, sigma; # Print the fit result snapshot; # Change parameter values, plotting; # -----------------------------------------------------------------; # At any moment you can manually change the value of a (constant); # parameter; # Rerun MIGRAD,HESSE; # Now fix sigma_g2; # Rerun MIGRAD,HESSE",MatchSource.CODE_COMMENT,tutorials/roofit/rf601_intminuit.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf601_intminuit.py
Availability,error,error,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## 'LIKELIHOOD AND MINIMIZATION' RooFit tutorial macro #602; ##; ## Setting up a chi^2 fit to a binned dataset; ##; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C version); # Set up model; # ---------------------; # Declare observable x; # Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and; # their parameters; # Build Chebychev polynomial p.d.f.; # Sum the signal components into a composite signal p.d.f.; # Sum the composite signal and background; # Create biuned dataset; # -----------------------------------------; # Construct a chi^2 of the data and the model.; # When a p.d.f. is used in a chi^2 fit, probability density scaled; # by the number of events in the dataset to obtain the fit function; # If model is an extended p.d.f, expected number events is used; # instead of the observed number of events.; # NB: It is also possible to fit a ROOT.RooAbsReal function to a ROOT.RooDataHist; # using chi2FitTo().; # Note that entries with zero bins are _not_ allowed; # for a proper chi^2 calculation and will give error; # messages",MatchSource.CODE_COMMENT,tutorials/roofit/rf602_chi2fit.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf602_chi2fit.py
Integrability,message,messages,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## 'LIKELIHOOD AND MINIMIZATION' RooFit tutorial macro #602; ##; ## Setting up a chi^2 fit to a binned dataset; ##; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C version); # Set up model; # ---------------------; # Declare observable x; # Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and; # their parameters; # Build Chebychev polynomial p.d.f.; # Sum the signal components into a composite signal p.d.f.; # Sum the composite signal and background; # Create biuned dataset; # -----------------------------------------; # Construct a chi^2 of the data and the model.; # When a p.d.f. is used in a chi^2 fit, probability density scaled; # by the number of events in the dataset to obtain the fit function; # If model is an extended p.d.f, expected number events is used; # instead of the observed number of events.; # NB: It is also possible to fit a ROOT.RooAbsReal function to a ROOT.RooDataHist; # using chi2FitTo().; # Note that entries with zero bins are _not_ allowed; # for a proper chi^2 calculation and will give error; # messages",MatchSource.CODE_COMMENT,tutorials/roofit/rf602_chi2fit.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf602_chi2fit.py
Modifiability,extend,extended,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## 'LIKELIHOOD AND MINIMIZATION' RooFit tutorial macro #602; ##; ## Setting up a chi^2 fit to a binned dataset; ##; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C version); # Set up model; # ---------------------; # Declare observable x; # Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and; # their parameters; # Build Chebychev polynomial p.d.f.; # Sum the signal components into a composite signal p.d.f.; # Sum the composite signal and background; # Create biuned dataset; # -----------------------------------------; # Construct a chi^2 of the data and the model.; # When a p.d.f. is used in a chi^2 fit, probability density scaled; # by the number of events in the dataset to obtain the fit function; # If model is an extended p.d.f, expected number events is used; # instead of the observed number of events.; # NB: It is also possible to fit a ROOT.RooAbsReal function to a ROOT.RooDataHist; # using chi2FitTo().; # Note that entries with zero bins are _not_ allowed; # for a proper chi^2 calculation and will give error; # messages",MatchSource.CODE_COMMENT,tutorials/roofit/rf602_chi2fit.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf602_chi2fit.py
Deployability,integrat,integration,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Likelihood and minimization: setting up a multi-core parallelized unbinned maximum likelihood fit; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create 3D pdf and data; # -------------------------------------------; # Create observables; # Create signal pdf gauss(x)*gauss(y)*gauss(z); # Create background pdf poly(x)*poly(y)*poly(z); # Create composite pdf sig+bkg; # Generate large dataset; # Parallel fitting; # -------------------------------; # In parallel mode the likelihood calculation is split in N pieces,; # that are calculated in parallel and added a posteriori before passing; # it back to MINUIT.; # Use four processes and time results both in wall time and CPU time; # Parallel MC projections; # ----------------------------------------------; # Construct signal, likelihood projection on (y,z) observables and; # likelihood ratio; # Calculate likelihood ratio for each event, subset of events with high; # signal likelihood; # Make plot frame and plot data; # Perform parallel projection using MC integration of pdf using given input dataSet.; # In self mode the data-weighted average of the pdf is calculated by splitting the; # input dataset in N equal pieces and calculating in parallel the weighted average; # one each subset. The N results of those calculations are then weighted into the; # final result; # Use four processes",MatchSource.CODE_COMMENT,tutorials/roofit/rf603_multicpu.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf603_multicpu.py
Integrability,integrat,integration,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Likelihood and minimization: setting up a multi-core parallelized unbinned maximum likelihood fit; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create 3D pdf and data; # -------------------------------------------; # Create observables; # Create signal pdf gauss(x)*gauss(y)*gauss(z); # Create background pdf poly(x)*poly(y)*poly(z); # Create composite pdf sig+bkg; # Generate large dataset; # Parallel fitting; # -------------------------------; # In parallel mode the likelihood calculation is split in N pieces,; # that are calculated in parallel and added a posteriori before passing; # it back to MINUIT.; # Use four processes and time results both in wall time and CPU time; # Parallel MC projections; # ----------------------------------------------; # Construct signal, likelihood projection on (y,z) observables and; # likelihood ratio; # Calculate likelihood ratio for each event, subset of events with high; # signal likelihood; # Make plot frame and plot data; # Perform parallel projection using MC integration of pdf using given input dataSet.; # In self mode the data-weighted average of the pdf is calculated by splitting the; # input dataset in N equal pieces and calculating in parallel the weighted average; # one each subset. The N results of those calculations are then weighted into the; # final result; # Use four processes",MatchSource.CODE_COMMENT,tutorials/roofit/rf603_multicpu.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf603_multicpu.py
Availability,error,error,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## 'LIKELIHOOD AND MINIMIZATION' RooFit tutorial macro #606; ##; ## Understanding and customizing error handling in likelihood evaluations; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C version); # Create model and dataset; # ----------------------------------------------; # Observable; # Parameters; # Pdf; # Sample 1000 events in m from argus; # Plot model and data; # --------------------------------------; # Fit model to data; # ---------------------------------; # The ARGUS background shape has a sharp kinematic cutoff at m=m0; # and is prone to evaluation errors if the cutoff parameter m0; # is floated: when the pdf cutoff value is lower than that in data; # events with m>m0 will have zero probability; # Perform unbinned ML fit. Print detailed error messages for up to; # 10 events per likelihood evaluation. The default error handling strategy; # is to return a very high value of the likelihood to MINUIT if errors occur,; # which will force MINUIT to retreat from the problematic area; # Perform another fit. In self configuration only the number of errors per; # likelihood evaluation is shown, it is greater than zero. The; # EvalErrorWall(kFALSE) arguments disables the default error handling strategy; # and will cause the actual (problematic) value of the likelihood to be passed; # to MINUIT.; #; # NB: Use of self option is NOT recommended as default strategt as broken -log(L) values; # can often be lower than 'good' ones because offending events are removed.; # ROOT.This may effectively create a False minimum in problem areas. ROOT.This is clearly; # illustrated in the second plot; # Plot likelihood as function of m0; # ------------------------------------------------------------------; # Construct likelihood function of model and data; # Plot likelihood in m0 in range that includes problematic values; # In self configuration no m",MatchSource.CODE_COMMENT,tutorials/roofit/rf606_nllerrorhandling.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf606_nllerrorhandling.py
Deployability,configurat,configuration,"; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C version); # Create model and dataset; # ----------------------------------------------; # Observable; # Parameters; # Pdf; # Sample 1000 events in m from argus; # Plot model and data; # --------------------------------------; # Fit model to data; # ---------------------------------; # The ARGUS background shape has a sharp kinematic cutoff at m=m0; # and is prone to evaluation errors if the cutoff parameter m0; # is floated: when the pdf cutoff value is lower than that in data; # events with m>m0 will have zero probability; # Perform unbinned ML fit. Print detailed error messages for up to; # 10 events per likelihood evaluation. The default error handling strategy; # is to return a very high value of the likelihood to MINUIT if errors occur,; # which will force MINUIT to retreat from the problematic area; # Perform another fit. In self configuration only the number of errors per; # likelihood evaluation is shown, it is greater than zero. The; # EvalErrorWall(kFALSE) arguments disables the default error handling strategy; # and will cause the actual (problematic) value of the likelihood to be passed; # to MINUIT.; #; # NB: Use of self option is NOT recommended as default strategt as broken -log(L) values; # can often be lower than 'good' ones because offending events are removed.; # ROOT.This may effectively create a False minimum in problem areas. ROOT.This is clearly; # illustrated in the second plot; # Plot likelihood as function of m0; # ------------------------------------------------------------------; # Construct likelihood function of model and data; # Plot likelihood in m0 in range that includes problematic values; # In self configuration no messages are printed for likelihood evaluation errors,; # but if an likelihood value evaluates with error, corresponding value; # on the curve will be set to the value given in EvalErrorValue().",MatchSource.CODE_COMMENT,tutorials/roofit/rf606_nllerrorhandling.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf606_nllerrorhandling.py
Integrability,message,messages,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## 'LIKELIHOOD AND MINIMIZATION' RooFit tutorial macro #606; ##; ## Understanding and customizing error handling in likelihood evaluations; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C version); # Create model and dataset; # ----------------------------------------------; # Observable; # Parameters; # Pdf; # Sample 1000 events in m from argus; # Plot model and data; # --------------------------------------; # Fit model to data; # ---------------------------------; # The ARGUS background shape has a sharp kinematic cutoff at m=m0; # and is prone to evaluation errors if the cutoff parameter m0; # is floated: when the pdf cutoff value is lower than that in data; # events with m>m0 will have zero probability; # Perform unbinned ML fit. Print detailed error messages for up to; # 10 events per likelihood evaluation. The default error handling strategy; # is to return a very high value of the likelihood to MINUIT if errors occur,; # which will force MINUIT to retreat from the problematic area; # Perform another fit. In self configuration only the number of errors per; # likelihood evaluation is shown, it is greater than zero. The; # EvalErrorWall(kFALSE) arguments disables the default error handling strategy; # and will cause the actual (problematic) value of the likelihood to be passed; # to MINUIT.; #; # NB: Use of self option is NOT recommended as default strategt as broken -log(L) values; # can often be lower than 'good' ones because offending events are removed.; # ROOT.This may effectively create a False minimum in problem areas. ROOT.This is clearly; # illustrated in the second plot; # Plot likelihood as function of m0; # ------------------------------------------------------------------; # Construct likelihood function of model and data; # Plot likelihood in m0 in range that includes problematic values; # In self configuration no m",MatchSource.CODE_COMMENT,tutorials/roofit/rf606_nllerrorhandling.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf606_nllerrorhandling.py
Modifiability,config,configuration,"; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C version); # Create model and dataset; # ----------------------------------------------; # Observable; # Parameters; # Pdf; # Sample 1000 events in m from argus; # Plot model and data; # --------------------------------------; # Fit model to data; # ---------------------------------; # The ARGUS background shape has a sharp kinematic cutoff at m=m0; # and is prone to evaluation errors if the cutoff parameter m0; # is floated: when the pdf cutoff value is lower than that in data; # events with m>m0 will have zero probability; # Perform unbinned ML fit. Print detailed error messages for up to; # 10 events per likelihood evaluation. The default error handling strategy; # is to return a very high value of the likelihood to MINUIT if errors occur,; # which will force MINUIT to retreat from the problematic area; # Perform another fit. In self configuration only the number of errors per; # likelihood evaluation is shown, it is greater than zero. The; # EvalErrorWall(kFALSE) arguments disables the default error handling strategy; # and will cause the actual (problematic) value of the likelihood to be passed; # to MINUIT.; #; # NB: Use of self option is NOT recommended as default strategt as broken -log(L) values; # can often be lower than 'good' ones because offending events are removed.; # ROOT.This may effectively create a False minimum in problem areas. ROOT.This is clearly; # illustrated in the second plot; # Plot likelihood as function of m0; # ------------------------------------------------------------------; # Construct likelihood function of model and data; # Plot likelihood in m0 in range that includes problematic values; # In self configuration no messages are printed for likelihood evaluation errors,; # but if an likelihood value evaluates with error, corresponding value; # on the curve will be set to the value given in EvalErrorValue().",MatchSource.CODE_COMMENT,tutorials/roofit/rf606_nllerrorhandling.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf606_nllerrorhandling.py
Testability,log,log,"; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C version); # Create model and dataset; # ----------------------------------------------; # Observable; # Parameters; # Pdf; # Sample 1000 events in m from argus; # Plot model and data; # --------------------------------------; # Fit model to data; # ---------------------------------; # The ARGUS background shape has a sharp kinematic cutoff at m=m0; # and is prone to evaluation errors if the cutoff parameter m0; # is floated: when the pdf cutoff value is lower than that in data; # events with m>m0 will have zero probability; # Perform unbinned ML fit. Print detailed error messages for up to; # 10 events per likelihood evaluation. The default error handling strategy; # is to return a very high value of the likelihood to MINUIT if errors occur,; # which will force MINUIT to retreat from the problematic area; # Perform another fit. In self configuration only the number of errors per; # likelihood evaluation is shown, it is greater than zero. The; # EvalErrorWall(kFALSE) arguments disables the default error handling strategy; # and will cause the actual (problematic) value of the likelihood to be passed; # to MINUIT.; #; # NB: Use of self option is NOT recommended as default strategt as broken -log(L) values; # can often be lower than 'good' ones because offending events are removed.; # ROOT.This may effectively create a False minimum in problem areas. ROOT.This is clearly; # illustrated in the second plot; # Plot likelihood as function of m0; # ------------------------------------------------------------------; # Construct likelihood function of model and data; # Plot likelihood in m0 in range that includes problematic values; # In self configuration no messages are printed for likelihood evaluation errors,; # but if an likelihood value evaluates with error, corresponding value; # on the curve will be set to the value given in EvalErrorValue().",MatchSource.CODE_COMMENT,tutorials/roofit/rf606_nllerrorhandling.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf606_nllerrorhandling.py
Usability,clear,clearly,"; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C version); # Create model and dataset; # ----------------------------------------------; # Observable; # Parameters; # Pdf; # Sample 1000 events in m from argus; # Plot model and data; # --------------------------------------; # Fit model to data; # ---------------------------------; # The ARGUS background shape has a sharp kinematic cutoff at m=m0; # and is prone to evaluation errors if the cutoff parameter m0; # is floated: when the pdf cutoff value is lower than that in data; # events with m>m0 will have zero probability; # Perform unbinned ML fit. Print detailed error messages for up to; # 10 events per likelihood evaluation. The default error handling strategy; # is to return a very high value of the likelihood to MINUIT if errors occur,; # which will force MINUIT to retreat from the problematic area; # Perform another fit. In self configuration only the number of errors per; # likelihood evaluation is shown, it is greater than zero. The; # EvalErrorWall(kFALSE) arguments disables the default error handling strategy; # and will cause the actual (problematic) value of the likelihood to be passed; # to MINUIT.; #; # NB: Use of self option is NOT recommended as default strategt as broken -log(L) values; # can often be lower than 'good' ones because offending events are removed.; # ROOT.This may effectively create a False minimum in problem areas. ROOT.This is clearly; # illustrated in the second plot; # Plot likelihood as function of m0; # ------------------------------------------------------------------; # Construct likelihood function of model and data; # Plot likelihood in m0 in range that includes problematic values; # In self configuration no messages are printed for likelihood evaluation errors,; # but if an likelihood value evaluates with error, corresponding value; # on the curve will be set to the value given in EvalErrorValue().",MatchSource.CODE_COMMENT,tutorials/roofit/rf606_nllerrorhandling.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf606_nllerrorhandling.py
Availability,error,error,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Likelihood and minimization: representing the parabolic approximation of the fit as a; ## multi-variate Gaussian on the parameters of the fitted pdf; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create model and dataset; # -----------------------------------------------; # Observable; # Model (intentional strong correlations); # Generate 1000 events; # Fit model to data; # ----------------------------------; # Create MV Gaussian pdf of fitted parameters; # ------------------------------------------------------------------------------------; # Some exercises with the parameter pdf; # -----------------------------------------------------------------------------; # Generate 100K points in the parameter space, from the MVGaussian pdf; # Sample a 3-D histogram of the pdf to be visualized as an error; # ellipsoid using the GLISO draw option; # Project 3D parameter pdf down to 3 permutations of two-dimensional pdfs; # The integrations corresponding to these projections are performed analytically; # by the MV Gaussian pdf; # Make 2D plots of the 3 two-dimensional pdf projections; # Draw the 'sigar'; # Draw the 2D projections of the 3D pdf; # Draw the distributions of parameter points sampled from the pdf",MatchSource.CODE_COMMENT,tutorials/roofit/rf608_fitresultaspdf.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf608_fitresultaspdf.py
Deployability,integrat,integrations,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Likelihood and minimization: representing the parabolic approximation of the fit as a; ## multi-variate Gaussian on the parameters of the fitted pdf; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create model and dataset; # -----------------------------------------------; # Observable; # Model (intentional strong correlations); # Generate 1000 events; # Fit model to data; # ----------------------------------; # Create MV Gaussian pdf of fitted parameters; # ------------------------------------------------------------------------------------; # Some exercises with the parameter pdf; # -----------------------------------------------------------------------------; # Generate 100K points in the parameter space, from the MVGaussian pdf; # Sample a 3-D histogram of the pdf to be visualized as an error; # ellipsoid using the GLISO draw option; # Project 3D parameter pdf down to 3 permutations of two-dimensional pdfs; # The integrations corresponding to these projections are performed analytically; # by the MV Gaussian pdf; # Make 2D plots of the 3 two-dimensional pdf projections; # Draw the 'sigar'; # Draw the 2D projections of the 3D pdf; # Draw the distributions of parameter points sampled from the pdf",MatchSource.CODE_COMMENT,tutorials/roofit/rf608_fitresultaspdf.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf608_fitresultaspdf.py
Integrability,integrat,integrations,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Likelihood and minimization: representing the parabolic approximation of the fit as a; ## multi-variate Gaussian on the parameters of the fitted pdf; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create model and dataset; # -----------------------------------------------; # Observable; # Model (intentional strong correlations); # Generate 1000 events; # Fit model to data; # ----------------------------------; # Create MV Gaussian pdf of fitted parameters; # ------------------------------------------------------------------------------------; # Some exercises with the parameter pdf; # -----------------------------------------------------------------------------; # Generate 100K points in the parameter space, from the MVGaussian pdf; # Sample a 3-D histogram of the pdf to be visualized as an error; # ellipsoid using the GLISO draw option; # Project 3D parameter pdf down to 3 permutations of two-dimensional pdfs; # The integrations corresponding to these projections are performed analytically; # by the MV Gaussian pdf; # Make 2D plots of the 3 two-dimensional pdf projections; # Draw the 'sigar'; # Draw the 2D projections of the 3D pdf; # Draw the distributions of parameter points sampled from the pdf",MatchSource.CODE_COMMENT,tutorials/roofit/rf608_fitresultaspdf.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf608_fitresultaspdf.py
Performance,perform,performed,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Likelihood and minimization: representing the parabolic approximation of the fit as a; ## multi-variate Gaussian on the parameters of the fitted pdf; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create model and dataset; # -----------------------------------------------; # Observable; # Model (intentional strong correlations); # Generate 1000 events; # Fit model to data; # ----------------------------------; # Create MV Gaussian pdf of fitted parameters; # ------------------------------------------------------------------------------------; # Some exercises with the parameter pdf; # -----------------------------------------------------------------------------; # Generate 100K points in the parameter space, from the MVGaussian pdf; # Sample a 3-D histogram of the pdf to be visualized as an error; # ellipsoid using the GLISO draw option; # Project 3D parameter pdf down to 3 permutations of two-dimensional pdfs; # The integrations corresponding to these projections are performed analytically; # by the MV Gaussian pdf; # Make 2D plots of the 3 two-dimensional pdf projections; # Draw the 'sigar'; # Draw the 2D projections of the 3D pdf; # Draw the distributions of parameter points sampled from the pdf",MatchSource.CODE_COMMENT,tutorials/roofit/rf608_fitresultaspdf.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf608_fitresultaspdf.py
Availability,error,errors,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Likelihood and minimization: setting up a chi^2 fit to an unbinned dataset with X,Y,err(Y); ## values (and optionally err(X) values); ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create dataset with X and Y values; # -------------------------------------------------------------------; # Make weighted XY dataset with asymmetric errors stored; # The StoreError() argument is essential as it makes; # the dataset store the error in addition to the values; # of the observables. If errors on one or more observables; # are asymmetric, can store the asymmetric error; # using the StoreAsymError() argument; # Fill an example dataset with X,err(X),Y,err(Y) values; # Set Y value and error; # Perform chi2 fit to X +/- dX and Y +/- dY values; # ---------------------------------------------------------------------------------------; # Make fit function; # Plot dataset in X-Y interpretation; # Fit chi^2 using X and Y errors; # Overlay fitted function; # Alternative: fit chi^2 integrating f(x) over ranges defined by X errors, rather; # than taking point at center of bin; # Overlay alternate fit result; # Draw the plot on a canvas",MatchSource.CODE_COMMENT,tutorials/roofit/rf609_xychi2fit.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf609_xychi2fit.py
Deployability,integrat,integrating,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Likelihood and minimization: setting up a chi^2 fit to an unbinned dataset with X,Y,err(Y); ## values (and optionally err(X) values); ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create dataset with X and Y values; # -------------------------------------------------------------------; # Make weighted XY dataset with asymmetric errors stored; # The StoreError() argument is essential as it makes; # the dataset store the error in addition to the values; # of the observables. If errors on one or more observables; # are asymmetric, can store the asymmetric error; # using the StoreAsymError() argument; # Fill an example dataset with X,err(X),Y,err(Y) values; # Set Y value and error; # Perform chi2 fit to X +/- dX and Y +/- dY values; # ---------------------------------------------------------------------------------------; # Make fit function; # Plot dataset in X-Y interpretation; # Fit chi^2 using X and Y errors; # Overlay fitted function; # Alternative: fit chi^2 integrating f(x) over ranges defined by X errors, rather; # than taking point at center of bin; # Overlay alternate fit result; # Draw the plot on a canvas",MatchSource.CODE_COMMENT,tutorials/roofit/rf609_xychi2fit.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf609_xychi2fit.py
Integrability,integrat,integrating,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Likelihood and minimization: setting up a chi^2 fit to an unbinned dataset with X,Y,err(Y); ## values (and optionally err(X) values); ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create dataset with X and Y values; # -------------------------------------------------------------------; # Make weighted XY dataset with asymmetric errors stored; # The StoreError() argument is essential as it makes; # the dataset store the error in addition to the values; # of the observables. If errors on one or more observables; # are asymmetric, can store the asymmetric error; # using the StoreAsymError() argument; # Fill an example dataset with X,err(X),Y,err(Y) values; # Set Y value and error; # Perform chi2 fit to X +/- dX and Y +/- dY values; # ---------------------------------------------------------------------------------------; # Make fit function; # Plot dataset in X-Y interpretation; # Fit chi^2 using X and Y errors; # Overlay fitted function; # Alternative: fit chi^2 integrating f(x) over ranges defined by X errors, rather; # than taking point at center of bin; # Overlay alternate fit result; # Draw the plot on a canvas",MatchSource.CODE_COMMENT,tutorials/roofit/rf609_xychi2fit.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf609_xychi2fit.py
Availability,error,errors,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Likelihood and minimization: visualization of errors from a covariance matrix; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Setup example fit; # ---------------------------------------; # Create sum of two Gaussians pdf with factory; # Create binned dataset; # Perform fit and save fit result; # Visualize fit error; # -------------------------------------; # Make plot frame; # Visualize 1-sigma error encoded in fit result 'r' as orange band using linear error propagation; # ROOT.This results in an error band that is by construction symmetric; #; # The linear error is calculated as; # error(x) = Z* F_a(x) * Corr(a,a') F_a'(x); #; # where F_a(x) = [ f(x,a+da) - f(x,a-da) ] / 2,; #; # with f(x) = the plotted curve; # 'da' = error taken from the fit result; # Corr(a,a') = the correlation matrix from the fit result; # Z = requested significance 'Z sigma band'; #; # The linear method is fast (required 2*N evaluations of the curve, N is the number of parameters),; # but may not be accurate in the presence of strong correlations (~>0.9) and at Z>2 due to linear and; # Gaussian approximations made; #; # Calculate error using sampling method and visualize as dashed red line.; #; # In self method a number of curves is calculated with variations of the parameter values, sampled; # from a multi-variate Gaussian pdf that is constructed from the fit results covariance matrix.; # The error(x) is determined by calculating a central interval that capture N% of the variations; # for each value of x, N% is controlled by Z (i.e. Z=1 gives N=68%). The number of sampling curves; # is chosen to be such that at least 100 curves are expected to be outside the N% interval, is minimally; # 100 (e.g. Z=1.Ncurve=356, Z=2.Ncurve=2156)) Intervals from the sampling method can be asymmetric,; # and may perform better in the presence of strong correla",MatchSource.CODE_COMMENT,tutorials/roofit/rf610_visualerror.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf610_visualerror.py
Energy Efficiency,reduce,reduced,"urate in the presence of strong correlations (~>0.9) and at Z>2 due to linear and; # Gaussian approximations made; #; # Calculate error using sampling method and visualize as dashed red line.; #; # In self method a number of curves is calculated with variations of the parameter values, sampled; # from a multi-variate Gaussian pdf that is constructed from the fit results covariance matrix.; # The error(x) is determined by calculating a central interval that capture N% of the variations; # for each value of x, N% is controlled by Z (i.e. Z=1 gives N=68%). The number of sampling curves; # is chosen to be such that at least 100 curves are expected to be outside the N% interval, is minimally; # 100 (e.g. Z=1.Ncurve=356, Z=2.Ncurve=2156)) Intervals from the sampling method can be asymmetric,; # and may perform better in the presence of strong correlations, may take; # (much) longer to calculate; # Perform the same type of error visualization on the background component only.; # The VisualizeError() option can generally applied to _any_ kind of; # plot (components, asymmetries, etc..); # Overlay central value; # Visualize partial fit error; # ------------------------------------------------------; # Make plot frame; # Visualize partial error. For partial error visualization the covariance matrix is first reduced as follows; # ___ -1; # Vred = V22 = V11 - V12 * V22 * V21; #; # Where V11,V12,V21, represent a block decomposition of the covariance matrix into observables that; # are propagated (labeled by index '1') and that are not propagated (labeled by index '2'), V22bar; # is the Shur complement of V22, as shown above; #; # (Note that Vred is _not_ a simple sub-matrix of V); # Propagate partial error due to shape parameters (m,m2) using linear and; # sampling method; # Make plot frame; # Propagate partial error due to yield parameter using linear and sampling; # method; # Make plot frame; # Propagate partial error due to yield parameter using linear and sampling; # method",MatchSource.CODE_COMMENT,tutorials/roofit/rf610_visualerror.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf610_visualerror.py
Performance,perform,perform,"nce 'Z sigma band'; #; # The linear method is fast (required 2*N evaluations of the curve, N is the number of parameters),; # but may not be accurate in the presence of strong correlations (~>0.9) and at Z>2 due to linear and; # Gaussian approximations made; #; # Calculate error using sampling method and visualize as dashed red line.; #; # In self method a number of curves is calculated with variations of the parameter values, sampled; # from a multi-variate Gaussian pdf that is constructed from the fit results covariance matrix.; # The error(x) is determined by calculating a central interval that capture N% of the variations; # for each value of x, N% is controlled by Z (i.e. Z=1 gives N=68%). The number of sampling curves; # is chosen to be such that at least 100 curves are expected to be outside the N% interval, is minimally; # 100 (e.g. Z=1.Ncurve=356, Z=2.Ncurve=2156)) Intervals from the sampling method can be asymmetric,; # and may perform better in the presence of strong correlations, may take; # (much) longer to calculate; # Perform the same type of error visualization on the background component only.; # The VisualizeError() option can generally applied to _any_ kind of; # plot (components, asymmetries, etc..); # Overlay central value; # Visualize partial fit error; # ------------------------------------------------------; # Make plot frame; # Visualize partial error. For partial error visualization the covariance matrix is first reduced as follows; # ___ -1; # Vred = V22 = V11 - V12 * V22 * V21; #; # Where V11,V12,V21, represent a block decomposition of the covariance matrix into observables that; # are propagated (labeled by index '1') and that are not propagated (labeled by index '2'), V22bar; # is the Shur complement of V22, as shown above; #; # (Note that Vred is _not_ a simple sub-matrix of V); # Propagate partial error due to shape parameters (m,m2) using linear and; # sampling method; # Make plot frame; # Propagate partial error due to yield paramet",MatchSource.CODE_COMMENT,tutorials/roofit/rf610_visualerror.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf610_visualerror.py
Usability,simpl,simple,"urate in the presence of strong correlations (~>0.9) and at Z>2 due to linear and; # Gaussian approximations made; #; # Calculate error using sampling method and visualize as dashed red line.; #; # In self method a number of curves is calculated with variations of the parameter values, sampled; # from a multi-variate Gaussian pdf that is constructed from the fit results covariance matrix.; # The error(x) is determined by calculating a central interval that capture N% of the variations; # for each value of x, N% is controlled by Z (i.e. Z=1 gives N=68%). The number of sampling curves; # is chosen to be such that at least 100 curves are expected to be outside the N% interval, is minimally; # 100 (e.g. Z=1.Ncurve=356, Z=2.Ncurve=2156)) Intervals from the sampling method can be asymmetric,; # and may perform better in the presence of strong correlations, may take; # (much) longer to calculate; # Perform the same type of error visualization on the background component only.; # The VisualizeError() option can generally applied to _any_ kind of; # plot (components, asymmetries, etc..); # Overlay central value; # Visualize partial fit error; # ------------------------------------------------------; # Make plot frame; # Visualize partial error. For partial error visualization the covariance matrix is first reduced as follows; # ___ -1; # Vred = V22 = V11 - V12 * V22 * V21; #; # Where V11,V12,V21, represent a block decomposition of the covariance matrix into observables that; # are propagated (labeled by index '1') and that are not propagated (labeled by index '2'), V22bar; # is the Shur complement of V22, as shown above; #; # (Note that Vred is _not_ a simple sub-matrix of V); # Propagate partial error due to shape parameters (m,m2) using linear and; # sampling method; # Make plot frame; # Propagate partial error due to yield parameter using linear and sampling; # method; # Make plot frame; # Propagate partial error due to yield parameter using linear and sampling; # method",MatchSource.CODE_COMMENT,tutorials/roofit/rf610_visualerror.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf610_visualerror.py
Availability,recover,recovery,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Likelihood and minimization: Recover from regions where the function is not defined.; ##; ## We demonstrate improved recovery from disallowed parameters. For this, we use a polynomial PDF of the form; ## \f[; ## \mathrm{Pol2} = \mathcal{N} \left( c + a_1 \cdot x + a_2 \cdot x^2 + 0.01 \cdot x^3 \right),; ## \f]; ## where \f$ \mathcal{N} \f$ is a normalisation factor. Unless the parameters are chosen carefully,; ## this function can be negative, and hence, it cannot be used as a PDF. In this case, RooFit passes; ## an error to the minimiser, which might try to recover.; ##; ## \macro_code; ## \macro_output; ##; ## \date June 2021; ## \author Harshal Shende, Stephan Hageboeck (C++ version); # Create a fit model:; # The polynomial is notoriously unstable, because it can quickly go negative.; # Since PDFs need to be positive, one often ends up with an unstable fit model.; # Create toy data with all-positive coefficients:; # For plotting.; # We create pointers to the plotted objects. We want these objects to leak out of the function,; # so we can still see them after it returns.; # Plotting a PDF with disallowed parameters doesn't work. We would get a lot of error messages.; # Therefore, we disable plotting messages in RooFit's message streams:; # RooFit before ROOT 6.24; # --------------------------------; # Before 6.24, RooFit wasn't able to recover from invalid parameters. The minimiser just errs around; # the starting values of the parameters without finding any improvement.; # Set up the parameters such that the PDF would come out negative. The PDF is now undefined.; # Perform a fit:; # This is how RooFit behaved prior to ROOT 6.24; # We are expecting a lot of evaluation errors. -1 switches off printing.; # RooFit since ROOT 6.24; # --------------------------------; # The minimiser gets information about the ""badness"" of the violation of the function definition. It uses this; # to find its way out of the disallo",MatchSource.CODE_COMMENT,tutorials/roofit/rf612_recoverFromInvalidParameters.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf612_recoverFromInvalidParameters.py
Integrability,message,messages,"se a polynomial PDF of the form; ## \f[; ## \mathrm{Pol2} = \mathcal{N} \left( c + a_1 \cdot x + a_2 \cdot x^2 + 0.01 \cdot x^3 \right),; ## \f]; ## where \f$ \mathcal{N} \f$ is a normalisation factor. Unless the parameters are chosen carefully,; ## this function can be negative, and hence, it cannot be used as a PDF. In this case, RooFit passes; ## an error to the minimiser, which might try to recover.; ##; ## \macro_code; ## \macro_output; ##; ## \date June 2021; ## \author Harshal Shende, Stephan Hageboeck (C++ version); # Create a fit model:; # The polynomial is notoriously unstable, because it can quickly go negative.; # Since PDFs need to be positive, one often ends up with an unstable fit model.; # Create toy data with all-positive coefficients:; # For plotting.; # We create pointers to the plotted objects. We want these objects to leak out of the function,; # so we can still see them after it returns.; # Plotting a PDF with disallowed parameters doesn't work. We would get a lot of error messages.; # Therefore, we disable plotting messages in RooFit's message streams:; # RooFit before ROOT 6.24; # --------------------------------; # Before 6.24, RooFit wasn't able to recover from invalid parameters. The minimiser just errs around; # the starting values of the parameters without finding any improvement.; # Set up the parameters such that the PDF would come out negative. The PDF is now undefined.; # Perform a fit:; # This is how RooFit behaved prior to ROOT 6.24; # We are expecting a lot of evaluation errors. -1 switches off printing.; # RooFit since ROOT 6.24; # --------------------------------; # The minimiser gets information about the ""badness"" of the violation of the function definition. It uses this; # to find its way out of the disallowed parameter regions.; # Reset the parameters such that the PDF is again undefined.; # Fit again, but pass recovery information to the minimiser:; # The magnitude of the recovery information can be chosen here.; # Higher v",MatchSource.CODE_COMMENT,tutorials/roofit/rf612_recoverFromInvalidParameters.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf612_recoverFromInvalidParameters.py
Safety,recover,recovery,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Likelihood and minimization: Recover from regions where the function is not defined.; ##; ## We demonstrate improved recovery from disallowed parameters. For this, we use a polynomial PDF of the form; ## \f[; ## \mathrm{Pol2} = \mathcal{N} \left( c + a_1 \cdot x + a_2 \cdot x^2 + 0.01 \cdot x^3 \right),; ## \f]; ## where \f$ \mathcal{N} \f$ is a normalisation factor. Unless the parameters are chosen carefully,; ## this function can be negative, and hence, it cannot be used as a PDF. In this case, RooFit passes; ## an error to the minimiser, which might try to recover.; ##; ## \macro_code; ## \macro_output; ##; ## \date June 2021; ## \author Harshal Shende, Stephan Hageboeck (C++ version); # Create a fit model:; # The polynomial is notoriously unstable, because it can quickly go negative.; # Since PDFs need to be positive, one often ends up with an unstable fit model.; # Create toy data with all-positive coefficients:; # For plotting.; # We create pointers to the plotted objects. We want these objects to leak out of the function,; # so we can still see them after it returns.; # Plotting a PDF with disallowed parameters doesn't work. We would get a lot of error messages.; # Therefore, we disable plotting messages in RooFit's message streams:; # RooFit before ROOT 6.24; # --------------------------------; # Before 6.24, RooFit wasn't able to recover from invalid parameters. The minimiser just errs around; # the starting values of the parameters without finding any improvement.; # Set up the parameters such that the PDF would come out negative. The PDF is now undefined.; # Perform a fit:; # This is how RooFit behaved prior to ROOT 6.24; # We are expecting a lot of evaluation errors. -1 switches off printing.; # RooFit since ROOT 6.24; # --------------------------------; # The minimiser gets information about the ""badness"" of the violation of the function definition. It uses this; # to find its way out of the disallo",MatchSource.CODE_COMMENT,tutorials/roofit/rf612_recoverFromInvalidParameters.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf612_recoverFromInvalidParameters.py
Integrability,depend,depend,"## \file; ## \ingroup tutorial_roofit; ## \notebook -js; ## This tutorial explains the concept of global observables in RooFit, and; ## showcases how their values can be stored either in the model or in the; ## dataset.; ##; ## ### Introduction; ##; ## Note: in this tutorial, we are multiplying the likelihood with an additional; ## likelihood to constrain the parameters with auxiliary measurements. This is; ## different from the `rf604_constraints` tutorial, where the likelihood is; ## multiplied with a Bayesian prior to constrain the parameters.; ##; ##; ## With RooFit, you usually optimize some model parameters `p` to maximize the; ## likelihood `L` given the per-event or per-bin observations `x`:; ##; ## \f[ L( x | p ) \f]; ##; ## Often, the parameters are constrained with some prior likelihood `C`, which; ## doesn't depend on the observables `x`:; ##; ## \f[ L'( x | p ) = L( x | p ) * C( p ) \f]; ##; ## Usually, these constraint terms depend on some auxiliary measurements of; ## other observables `g`. The constraint term is then the likelihood of the; ## so-called global observables:; ##; ## \f[ L'( x | p ) = L( x | p ) * C( g | p ) \f]; ##; ## For example, think of a model where the true luminosity `lumi` is a; ## nuisance parameter that is constrained by an auxiliary measurement; ## `lumi_obs` with uncertainty `lumi_obs_sigma`:; ##; ## \f[ L'(data | mu, lumi) = L(data | mu, lumi) * \text{Gauss}(lumi_obs | lumi, lumi_obs_sigma) \f]; ##; ## As a Gaussian is symmetric under exchange of the observable and the mean; ## parameter, you can also sometimes find this equivalent but less conventional; ## formulation for Gaussian constraints:; ##; ## \f[ L'(data | mu, lumi) = L(data | mu, lumi) * \text{Gauss}(lumi | lumi_obs, lumi_obs_sigma) \f]; ##; ## If you wanted to constrain a parameter that represents event counts, you; ## would use a Poissonian constraint, e.g.:; ##; ## \f[ L'(data | mu, count) = L(data | mu, count) * \text{Poisson}(count_obs | count) \f]; ##; ## Un",MatchSource.CODE_COMMENT,tutorials/roofit/rf613_global_observables.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf613_global_observables.py
Performance,optimiz,optimize,"## \file; ## \ingroup tutorial_roofit; ## \notebook -js; ## This tutorial explains the concept of global observables in RooFit, and; ## showcases how their values can be stored either in the model or in the; ## dataset.; ##; ## ### Introduction; ##; ## Note: in this tutorial, we are multiplying the likelihood with an additional; ## likelihood to constrain the parameters with auxiliary measurements. This is; ## different from the `rf604_constraints` tutorial, where the likelihood is; ## multiplied with a Bayesian prior to constrain the parameters.; ##; ##; ## With RooFit, you usually optimize some model parameters `p` to maximize the; ## likelihood `L` given the per-event or per-bin observations `x`:; ##; ## \f[ L( x | p ) \f]; ##; ## Often, the parameters are constrained with some prior likelihood `C`, which; ## doesn't depend on the observables `x`:; ##; ## \f[ L'( x | p ) = L( x | p ) * C( p ) \f]; ##; ## Usually, these constraint terms depend on some auxiliary measurements of; ## other observables `g`. The constraint term is then the likelihood of the; ## so-called global observables:; ##; ## \f[ L'( x | p ) = L( x | p ) * C( g | p ) \f]; ##; ## For example, think of a model where the true luminosity `lumi` is a; ## nuisance parameter that is constrained by an auxiliary measurement; ## `lumi_obs` with uncertainty `lumi_obs_sigma`:; ##; ## \f[ L'(data | mu, lumi) = L(data | mu, lumi) * \text{Gauss}(lumi_obs | lumi, lumi_obs_sigma) \f]; ##; ## As a Gaussian is symmetric under exchange of the observable and the mean; ## parameter, you can also sometimes find this equivalent but less conventional; ## formulation for Gaussian constraints:; ##; ## \f[ L'(data | mu, lumi) = L(data | mu, lumi) * \text{Gauss}(lumi | lumi_obs, lumi_obs_sigma) \f]; ##; ## If you wanted to constrain a parameter that represents event counts, you; ## would use a Poissonian constraint, e.g.:; ##; ## \f[ L'(data | mu, count) = L(data | mu, count) * \text{Poisson}(count_obs | count) \f]; ##; ## Un",MatchSource.CODE_COMMENT,tutorials/roofit/rf613_global_observables.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf613_global_observables.py
Deployability,integrat,integrating,"## \file; ## \ingroup tutorial_roofit; ## \notebook -js; ## A tutorial that explains you how to solve problems with binning effects and; ## numerical stability in binned fits.; ##; ## ### Introduction; ##; ## In this tutorial, you will learn three new things:; ##; ## 1. How to reduce the bias in binned fits by changing the definition of the; ## normalization integral; ##; ## 2. How to completely get rid of binning effects by integrating the pdf over each bin; ##; ## 3. How to improve the numeric stability of fits with a greatly different; ## number of events per bin, using a constant per-bin counterterm; ##; ## \macro_code; ## \macro_output; ##; ## \date January 2023; ## \author Jonas Rembser; """"""; Generate binned Asimov dataset for a continuous pdf.; One should in principle be able to use; pdf.generateBinned(x, n_events, RooFit::ExpectedData()).; Unfortunately it has a problem: it also has the bin bias that this tutorial; demonstrates, to if we would use it, the biases would cancel out.; """"""; """"""; Force numeric integration and do this numeric integration with the; RooBinIntegrator, which sums the function values at the bin centers.; """"""; """"""; Reset the integrator config to disable the RooBinIntegrator.; """"""; # Silence info output for this tutorial; # Exponential example; # -------------------; # Set up the observable; # fewer bins so we have larger binning effects for this demo; # Let's first look at the example of an exponential function; # Generate an Asimov dataset such that the only difference between the fit; # result and the true parameters comes from binning effects.; # If you do the fit the usual was in RooFit, you will get a bias in the; # result. This is because the continuous, normalized pdf is evaluated only; # at the bin centers.; # In the case of an exponential function, the bias that you get by; # evaluating the pdf only at the bin centers is a constant scale factor in; # each bin. Here, we can do a trick to get rid of the bias: we also; # evaluate th",MatchSource.CODE_COMMENT,tutorials/roofit/rf614_binned_fit_problems.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf614_binned_fit_problems.py
Energy Efficiency,reduce,reduce,"## \file; ## \ingroup tutorial_roofit; ## \notebook -js; ## A tutorial that explains you how to solve problems with binning effects and; ## numerical stability in binned fits.; ##; ## ### Introduction; ##; ## In this tutorial, you will learn three new things:; ##; ## 1. How to reduce the bias in binned fits by changing the definition of the; ## normalization integral; ##; ## 2. How to completely get rid of binning effects by integrating the pdf over each bin; ##; ## 3. How to improve the numeric stability of fits with a greatly different; ## number of events per bin, using a constant per-bin counterterm; ##; ## \macro_code; ## \macro_output; ##; ## \date January 2023; ## \author Jonas Rembser; """"""; Generate binned Asimov dataset for a continuous pdf.; One should in principle be able to use; pdf.generateBinned(x, n_events, RooFit::ExpectedData()).; Unfortunately it has a problem: it also has the bin bias that this tutorial; demonstrates, to if we would use it, the biases would cancel out.; """"""; """"""; Force numeric integration and do this numeric integration with the; RooBinIntegrator, which sums the function values at the bin centers.; """"""; """"""; Reset the integrator config to disable the RooBinIntegrator.; """"""; # Silence info output for this tutorial; # Exponential example; # -------------------; # Set up the observable; # fewer bins so we have larger binning effects for this demo; # Let's first look at the example of an exponential function; # Generate an Asimov dataset such that the only difference between the fit; # result and the true parameters comes from binning effects.; # If you do the fit the usual was in RooFit, you will get a bias in the; # result. This is because the continuous, normalized pdf is evaluated only; # at the bin centers.; # In the case of an exponential function, the bias that you get by; # evaluating the pdf only at the bin centers is a constant scale factor in; # each bin. Here, we can do a trick to get rid of the bias: we also; # evaluate th",MatchSource.CODE_COMMENT,tutorials/roofit/rf614_binned_fit_problems.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf614_binned_fit_problems.py
Integrability,integrat,integrating,"## \file; ## \ingroup tutorial_roofit; ## \notebook -js; ## A tutorial that explains you how to solve problems with binning effects and; ## numerical stability in binned fits.; ##; ## ### Introduction; ##; ## In this tutorial, you will learn three new things:; ##; ## 1. How to reduce the bias in binned fits by changing the definition of the; ## normalization integral; ##; ## 2. How to completely get rid of binning effects by integrating the pdf over each bin; ##; ## 3. How to improve the numeric stability of fits with a greatly different; ## number of events per bin, using a constant per-bin counterterm; ##; ## \macro_code; ## \macro_output; ##; ## \date January 2023; ## \author Jonas Rembser; """"""; Generate binned Asimov dataset for a continuous pdf.; One should in principle be able to use; pdf.generateBinned(x, n_events, RooFit::ExpectedData()).; Unfortunately it has a problem: it also has the bin bias that this tutorial; demonstrates, to if we would use it, the biases would cancel out.; """"""; """"""; Force numeric integration and do this numeric integration with the; RooBinIntegrator, which sums the function values at the bin centers.; """"""; """"""; Reset the integrator config to disable the RooBinIntegrator.; """"""; # Silence info output for this tutorial; # Exponential example; # -------------------; # Set up the observable; # fewer bins so we have larger binning effects for this demo; # Let's first look at the example of an exponential function; # Generate an Asimov dataset such that the only difference between the fit; # result and the true parameters comes from binning effects.; # If you do the fit the usual was in RooFit, you will get a bias in the; # result. This is because the continuous, normalized pdf is evaluated only; # at the bin centers.; # In the case of an exponential function, the bias that you get by; # evaluating the pdf only at the bin centers is a constant scale factor in; # each bin. Here, we can do a trick to get rid of the bias: we also; # evaluate th",MatchSource.CODE_COMMENT,tutorials/roofit/rf614_binned_fit_problems.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf614_binned_fit_problems.py
Modifiability,config,config,"# Introduction; ##; ## In this tutorial, you will learn three new things:; ##; ## 1. How to reduce the bias in binned fits by changing the definition of the; ## normalization integral; ##; ## 2. How to completely get rid of binning effects by integrating the pdf over each bin; ##; ## 3. How to improve the numeric stability of fits with a greatly different; ## number of events per bin, using a constant per-bin counterterm; ##; ## \macro_code; ## \macro_output; ##; ## \date January 2023; ## \author Jonas Rembser; """"""; Generate binned Asimov dataset for a continuous pdf.; One should in principle be able to use; pdf.generateBinned(x, n_events, RooFit::ExpectedData()).; Unfortunately it has a problem: it also has the bin bias that this tutorial; demonstrates, to if we would use it, the biases would cancel out.; """"""; """"""; Force numeric integration and do this numeric integration with the; RooBinIntegrator, which sums the function values at the bin centers.; """"""; """"""; Reset the integrator config to disable the RooBinIntegrator.; """"""; # Silence info output for this tutorial; # Exponential example; # -------------------; # Set up the observable; # fewer bins so we have larger binning effects for this demo; # Let's first look at the example of an exponential function; # Generate an Asimov dataset such that the only difference between the fit; # result and the true parameters comes from binning effects.; # If you do the fit the usual was in RooFit, you will get a bias in the; # result. This is because the continuous, normalized pdf is evaluated only; # at the bin centers.; # In the case of an exponential function, the bias that you get by; # evaluating the pdf only at the bin centers is a constant scale factor in; # each bin. Here, we can do a trick to get rid of the bias: we also; # evaluate the normalization integral for the pdf the same way, i.e.,; # summing the values of the unnormalized pdf at the bin centers. Like this; # the bias cancels out. You can achieve this by cus",MatchSource.CODE_COMMENT,tutorials/roofit/rf614_binned_fit_problems.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf614_binned_fit_problems.py
Performance,perform,performance,"he pdf only at the bin centers is a constant scale factor in; # each bin. Here, we can do a trick to get rid of the bias: we also; # evaluate the normalization integral for the pdf the same way, i.e.,; # summing the values of the unnormalized pdf at the bin centers. Like this; # the bias cancels out. You can achieve this by customizing the way how the; # pdf is integrated (see also the rf901_numintconfig tutorial).; # Power law example; # -----------------; # Let's not look at another example: a power law \f[x^a\f].; # Again, if you do a vanilla fit, you'll get a bias; # This time, the bias is not the same factor in each bin! This means our; # trick by sampling the integral in the same way doesn't cancel out the; # bias completely. The average bias is canceled, but there are per-bin; # biases that remain. Still, this method has some value: it is cheaper than; # rigurously correcting the bias by integrating the pdf in each bin. So if; # you know your per-bin bias variations are small or performance is an; # issue, this approach can be sufficient.; # To get rid of the binning effects in the general case, one can use the; # IntegrateBins() command argument. Now, the pdf is not evaluated at the; # bin centers, but numerically integrated over each bin and divided by the; # bin width. The parameter for IntegrateBins() is the required precision; # for the numeric integrals. This is computationally expensive, but the; # bias is now not a problem anymore.; # Improving numerical stability; # -----------------------------; # There is one more problem with binned fits that is related to the binning; # effects because often, a binned fit is affected by both problems.; #; # The issue is numerical stability for fits with a greatly different number; # of events in each bin. For each bin, you have a term \f[n\log(p)\f] in; # the NLL, where \f[n\f] is the number of observations in the bin, and; # \f[p\f] the predicted probability to have an event in that bin. The; # difference in the ",MatchSource.CODE_COMMENT,tutorials/roofit/rf614_binned_fit_problems.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf614_binned_fit_problems.py
Safety,predict,predicted,"sly correcting the bias by integrating the pdf in each bin. So if; # you know your per-bin bias variations are small or performance is an; # issue, this approach can be sufficient.; # To get rid of the binning effects in the general case, one can use the; # IntegrateBins() command argument. Now, the pdf is not evaluated at the; # bin centers, but numerically integrated over each bin and divided by the; # bin width. The parameter for IntegrateBins() is the required precision; # for the numeric integrals. This is computationally expensive, but the; # bias is now not a problem anymore.; # Improving numerical stability; # -----------------------------; # There is one more problem with binned fits that is related to the binning; # effects because often, a binned fit is affected by both problems.; #; # The issue is numerical stability for fits with a greatly different number; # of events in each bin. For each bin, you have a term \f[n\log(p)\f] in; # the NLL, where \f[n\f] is the number of observations in the bin, and; # \f[p\f] the predicted probability to have an event in that bin. The; # difference in the logarithms for each bin is small, but the difference in; # \f[n\f] can be orders of magnitudes! Therefore, when summing these terms,; # lots of numerical precision is lost for the bins with less events.; # We can study this with the example of an exponential plus a Gaussian. The; # Gaussian is only a faint signal in the tail of the exponential where; # there are not so many events. And we can't afford any precision loss for; # these bins, otherwise we can't fit the Gaussian.; # It's not about binning effects anymore, so reset the number of bins.; # Set the starting values for the Gaussian parameters away from the true; # value such that the fit is not trivial.; # You should see in the previous fit result that the fit did not converge:; # the `MINIMIZE` return code should be -1 (a successful fit has status code zero).; # To improve the situation, we can apply a numeric ",MatchSource.CODE_COMMENT,tutorials/roofit/rf614_binned_fit_problems.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf614_binned_fit_problems.py
Testability,log,log,"sly correcting the bias by integrating the pdf in each bin. So if; # you know your per-bin bias variations are small or performance is an; # issue, this approach can be sufficient.; # To get rid of the binning effects in the general case, one can use the; # IntegrateBins() command argument. Now, the pdf is not evaluated at the; # bin centers, but numerically integrated over each bin and divided by the; # bin width. The parameter for IntegrateBins() is the required precision; # for the numeric integrals. This is computationally expensive, but the; # bias is now not a problem anymore.; # Improving numerical stability; # -----------------------------; # There is one more problem with binned fits that is related to the binning; # effects because often, a binned fit is affected by both problems.; #; # The issue is numerical stability for fits with a greatly different number; # of events in each bin. For each bin, you have a term \f[n\log(p)\f] in; # the NLL, where \f[n\f] is the number of observations in the bin, and; # \f[p\f] the predicted probability to have an event in that bin. The; # difference in the logarithms for each bin is small, but the difference in; # \f[n\f] can be orders of magnitudes! Therefore, when summing these terms,; # lots of numerical precision is lost for the bins with less events.; # We can study this with the example of an exponential plus a Gaussian. The; # Gaussian is only a faint signal in the tail of the exponential where; # there are not so many events. And we can't afford any precision loss for; # these bins, otherwise we can't fit the Gaussian.; # It's not about binning effects anymore, so reset the number of bins.; # Set the starting values for the Gaussian parameters away from the true; # value such that the fit is not trivial.; # You should see in the previous fit result that the fit did not converge:; # the `MINIMIZE` return code should be -1 (a successful fit has status code zero).; # To improve the situation, we can apply a numeric ",MatchSource.CODE_COMMENT,tutorials/roofit/rf614_binned_fit_problems.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf614_binned_fit_problems.py
Usability,learn,learn,"## \file; ## \ingroup tutorial_roofit; ## \notebook -js; ## A tutorial that explains you how to solve problems with binning effects and; ## numerical stability in binned fits.; ##; ## ### Introduction; ##; ## In this tutorial, you will learn three new things:; ##; ## 1. How to reduce the bias in binned fits by changing the definition of the; ## normalization integral; ##; ## 2. How to completely get rid of binning effects by integrating the pdf over each bin; ##; ## 3. How to improve the numeric stability of fits with a greatly different; ## number of events per bin, using a constant per-bin counterterm; ##; ## \macro_code; ## \macro_output; ##; ## \date January 2023; ## \author Jonas Rembser; """"""; Generate binned Asimov dataset for a continuous pdf.; One should in principle be able to use; pdf.generateBinned(x, n_events, RooFit::ExpectedData()).; Unfortunately it has a problem: it also has the bin bias that this tutorial; demonstrates, to if we would use it, the biases would cancel out.; """"""; """"""; Force numeric integration and do this numeric integration with the; RooBinIntegrator, which sums the function values at the bin centers.; """"""; """"""; Reset the integrator config to disable the RooBinIntegrator.; """"""; # Silence info output for this tutorial; # Exponential example; # -------------------; # Set up the observable; # fewer bins so we have larger binning effects for this demo; # Let's first look at the example of an exponential function; # Generate an Asimov dataset such that the only difference between the fit; # result and the true parameters comes from binning effects.; # If you do the fit the usual was in RooFit, you will get a bias in the; # result. This is because the continuous, normalized pdf is evaluated only; # at the bin centers.; # In the case of an exponential function, the bias that you get by; # evaluating the pdf only at the bin centers is a constant scale factor in; # each bin. Here, we can do a trick to get rid of the bias: we also; # evaluate th",MatchSource.CODE_COMMENT,tutorials/roofit/rf614_binned_fit_problems.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf614_binned_fit_problems.py
Availability,error,error,"inning properly; # Define the sampled gausians; # Fill the histograms; # Make sure that every bin is filled and we don't get zero probability; # Add the pdf to the workspace; # Add the pdf to the grid; # Create the morphing and add it to the workspace; # Uncomment to see input plots for the first dimension (you might need to increase the morphed samples); # f1 = x_var.frame(Title=""linear morphing;x;pdf"", Range=(-4, 8)); # for i in range(n_grid):; # workspace[f""histpdf{i}""].plotOn(f1); # workspace[""morph""].plotOn(f1, LineColor=""r""); # c0 = ROOT.TCanvas(); # f1.Draw(); # input() # Wait for user input to proceed; # Class used in this case to demonstrate the use of SBI in Root; # Initializing the class SBI; # Choose the hyperparameters for training the neural network; # Defining the target / training data for different values of mean value mu; # Generating samples for the reference distribution; # Ensuring the normalization with generating as many reference data as target data; # Bringing the data in the right format for training; # Train the classifier; # Setting the training and toy data samples; the factor 5 to enable a fair comparison to morphing; # Define the ""observed"" data in a workspace; # using a workspace for easier processing inside the class; # The ""observed"" data; # Training the model; # Compute the likelihood ratio of the classifier for analysis purposes; # Compute the learned likelihood ratio; # Compute the real likelihood ratio; # Create the exact negative log likelihood functions for Gaussian model; # Create the learned pdf and NLL sum based on the learned likelihood ratio; # Compute the morphed nll; # Plot the negative logarithmic summed likelihood; # Silence some warnings; # Plot the likelihood functions; # Write the plots into one canvas to show, or into separate canvases for saving.; # Compute the minimum via minuit and display the results; # Adjust the error level in the minimization to work with likelihoods; # Hack to bypass ClearProxiedObjects()",MatchSource.CODE_COMMENT,tutorials/roofit/rf615_simulation_based_inference.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf615_simulation_based_inference.py
Integrability,message,messages,"fore, a classifier is trained to discriminate between; ## samples from a target distribution (here the Gaussian) $$x\sim p(x|\theta)$$ and a reference distribution (here the Uniform); ## $$x\sim p_{ref}(x|\theta)$$.; ##; ## The output of the classifier $$\hat{s}(\theta)$$ is a monotonic function of the likelihood ration and can be turned into an estimate of the likelihood ratio; ## via $$\hat{r}(\theta)=\frac{1-\hat{s}(\theta)}{\hat{s}(\theta)}.$$; ## This is called the likelihood ratio trick.; ##; ## In the end we compare the negative logarithmic likelihoods of the learned, morphed and analytical likelihood with minuit and as a plot.; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date July 2024; ## \author Robin Syring; # The samples used for training the classifier in this tutorial / rescale for more accuracy; # Kills warning messages; # Morphing as a baseline; # Define binning for morphing; # Number of 'sampled' gaussians, if you change it, adjust the binning properly; # Define the sampled gausians; # Fill the histograms; # Make sure that every bin is filled and we don't get zero probability; # Add the pdf to the workspace; # Add the pdf to the grid; # Create the morphing and add it to the workspace; # Uncomment to see input plots for the first dimension (you might need to increase the morphed samples); # f1 = x_var.frame(Title=""linear morphing;x;pdf"", Range=(-4, 8)); # for i in range(n_grid):; # workspace[f""histpdf{i}""].plotOn(f1); # workspace[""morph""].plotOn(f1, LineColor=""r""); # c0 = ROOT.TCanvas(); # f1.Draw(); # input() # Wait for user input to proceed; # Class used in this case to demonstrate the use of SBI in Root; # Initializing the class SBI; # Choose the hyperparameters for training the neural network; # Defining the target / training data for different values of mean value mu; # Generating samples for the reference distribution; # Ensuring the normalization with generating as many reference data as target data; # Bringing the data in t",MatchSource.CODE_COMMENT,tutorials/roofit/rf615_simulation_based_inference.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf615_simulation_based_inference.py
Testability,log,logarithmic,"and target; ## distribution.; ## We see how the neural networks generalize to unknown mean values.; ##; ## We compare the approach of using the likelihood ratio trick to morphing.; ##; ## An introduction of SBI can be found in https://arxiv.org/pdf/2010.06439.; ##; ## A short recap:; ## The idea of SBI is to fit a surrogate model to the data, in order to really; ## learn the likelihood function instead of calculating it. Therefore, a classifier is trained to discriminate between; ## samples from a target distribution (here the Gaussian) $$x\sim p(x|\theta)$$ and a reference distribution (here the Uniform); ## $$x\sim p_{ref}(x|\theta)$$.; ##; ## The output of the classifier $$\hat{s}(\theta)$$ is a monotonic function of the likelihood ration and can be turned into an estimate of the likelihood ratio; ## via $$\hat{r}(\theta)=\frac{1-\hat{s}(\theta)}{\hat{s}(\theta)}.$$; ## This is called the likelihood ratio trick.; ##; ## In the end we compare the negative logarithmic likelihoods of the learned, morphed and analytical likelihood with minuit and as a plot.; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date July 2024; ## \author Robin Syring; # The samples used for training the classifier in this tutorial / rescale for more accuracy; # Kills warning messages; # Morphing as a baseline; # Define binning for morphing; # Number of 'sampled' gaussians, if you change it, adjust the binning properly; # Define the sampled gausians; # Fill the histograms; # Make sure that every bin is filled and we don't get zero probability; # Add the pdf to the workspace; # Add the pdf to the grid; # Create the morphing and add it to the workspace; # Uncomment to see input plots for the first dimension (you might need to increase the morphed samples); # f1 = x_var.frame(Title=""linear morphing;x;pdf"", Range=(-4, 8)); # for i in range(n_grid):; # workspace[f""histpdf{i}""].plotOn(f1); # workspace[""morph""].plotOn(f1, LineColor=""r""); # c0 = ROOT.TCanvas(); # f1.Draw(); # input()",MatchSource.CODE_COMMENT,tutorials/roofit/rf615_simulation_based_inference.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf615_simulation_based_inference.py
Usability,simpl,simple,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Use Simulation Based Inference (SBI) in RooFit.; ##; ## This tutorial shows how to use SBI in ROOT. As reference distribution we; ## choose a simple uniform distribution. The target distribution is chosen to; ## be gaussian with different mean values.; ## The classifier is trained to discriminate between the reference and target; ## distribution.; ## We see how the neural networks generalize to unknown mean values.; ##; ## We compare the approach of using the likelihood ratio trick to morphing.; ##; ## An introduction of SBI can be found in https://arxiv.org/pdf/2010.06439.; ##; ## A short recap:; ## The idea of SBI is to fit a surrogate model to the data, in order to really; ## learn the likelihood function instead of calculating it. Therefore, a classifier is trained to discriminate between; ## samples from a target distribution (here the Gaussian) $$x\sim p(x|\theta)$$ and a reference distribution (here the Uniform); ## $$x\sim p_{ref}(x|\theta)$$.; ##; ## The output of the classifier $$\hat{s}(\theta)$$ is a monotonic function of the likelihood ration and can be turned into an estimate of the likelihood ratio; ## via $$\hat{r}(\theta)=\frac{1-\hat{s}(\theta)}{\hat{s}(\theta)}.$$; ## This is called the likelihood ratio trick.; ##; ## In the end we compare the negative logarithmic likelihoods of the learned, morphed and analytical likelihood with minuit and as a plot.; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date July 2024; ## \author Robin Syring; # The samples used for training the classifier in this tutorial / rescale for more accuracy; # Kills warning messages; # Morphing as a baseline; # Define binning for morphing; # Number of 'sampled' gaussians, if you change it, adjust the binning properly; # Define the sampled gausians; # Fill the histograms; # Make sure that every bin is filled and we don't get zero probability; # Add the pdf to the workspace; # Add the pdf to the grid; # Crea",MatchSource.CODE_COMMENT,tutorials/roofit/rf615_simulation_based_inference.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf615_simulation_based_inference.py
Integrability,message,messages,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Use Morphing in RooFit.; ##; ## This tutorial shows how to use template morphing inside RooFit. As input we have several; ## Gaussian distributions. The output is one gaussian, with a specific mean value.; ## Since likelihoods are often used within the framework of morphing, we provide a; ## way to estimate the negative log likelihood (nll).; ##; ## Based on example of Kyle Cranmer https://gist.github.com/cranmer/46fff8d22015e5a26619.; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date August 2024; ## \author Robin Syring; # Number of samples to fill the histograms; # Kills warning messages; # morphing as a baseline; # set up a frame for plotting; # define binning for morphing; # number of 'sampled' Gaussians, if you change it, adjust the binning properly; # Create the sampled Gaussian; # Fill the histograms; # Make sure that every bin is filled and we don't get zero probability; # Add the pdf to the workspace, the inOrder of 1 is necessary for calculation of the nll; # Adjust it to 0 to see binning; # Add the pdf to the grid and to the plot; # Create the morphing and add it to the workspace; # Normalizes the morphed object to be a pdf, set it false to prevent warning messages and gain computational speed up; # Creating the morphed pdf; # Define the ""observed"" data in a workspade; # The ""observed"" data; # Create the exact negative log likelihood functions for Gaussian model; # Compute the morphed nll; # TODO: Fix RooAddPdf::fixCoefNormalization(nset) warnings with new CPU backend; # Plot the negative logarithmic summed likelihood; # Compute the minimum via minuit and display the results",MatchSource.CODE_COMMENT,tutorials/roofit/rf616_morphing.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf616_morphing.py
Testability,log,log,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Use Morphing in RooFit.; ##; ## This tutorial shows how to use template morphing inside RooFit. As input we have several; ## Gaussian distributions. The output is one gaussian, with a specific mean value.; ## Since likelihoods are often used within the framework of morphing, we provide a; ## way to estimate the negative log likelihood (nll).; ##; ## Based on example of Kyle Cranmer https://gist.github.com/cranmer/46fff8d22015e5a26619.; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date August 2024; ## \author Robin Syring; # Number of samples to fill the histograms; # Kills warning messages; # morphing as a baseline; # set up a frame for plotting; # define binning for morphing; # number of 'sampled' Gaussians, if you change it, adjust the binning properly; # Create the sampled Gaussian; # Fill the histograms; # Make sure that every bin is filled and we don't get zero probability; # Add the pdf to the workspace, the inOrder of 1 is necessary for calculation of the nll; # Adjust it to 0 to see binning; # Add the pdf to the grid and to the plot; # Create the morphing and add it to the workspace; # Normalizes the morphed object to be a pdf, set it false to prevent warning messages and gain computational speed up; # Creating the morphed pdf; # Define the ""observed"" data in a workspade; # The ""observed"" data; # Create the exact negative log likelihood functions for Gaussian model; # Compute the morphed nll; # TODO: Fix RooAddPdf::fixCoefNormalization(nset) warnings with new CPU backend; # Plot the negative logarithmic summed likelihood; # Compute the minimum via minuit and display the results",MatchSource.CODE_COMMENT,tutorials/roofit/rf616_morphing.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf616_morphing.py
Integrability,message,messages,"istribution we; ## choose a simple uniform distribution. The target distribution is chosen to; ## be Gaussian with different mean values.; ## The classifier is trained to discriminate between the reference and target; ## distribution.; ## We see how the neural networks generalize to unknown mean values.; ##; ## Furthermore, we compare SBI to the approach of moment morphing. In this case,; ## we can conclude, that SBI is way more sample eficcient when it comes to; ## estimating the negative log likelihood ratio.; ##; ## For an introductory background see rf615_simulation_based_inference.py; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date July 2024; ## \author Robin Syring; # Kills warning messages; # Number of samples for morphing; # Number of 'sampled' Gaussians; # To have a fair comparison; # Morphing as baseline; # Define binning for morphing; # Set bins for each x variable; # Define mu values as input for morphing for each dimension; # Create a product of Gaussians for all dimensions; # Create a product PDF for the multidimensional Gaussian; # Iterate through each tuple; # Fill the histograms; # Ensure that every bin is filled and there are no zero probabilities; # Add the PDF to the grid; # Create the morphing function and add it to the ws; # Uncomment to see input plots for the first dimension (you might need to increase the morphed samples); # f1 = x_vars[0].frame(); # for i in range(n_bins):; # templates[(i, 0)].plotOn(f1); # ws[""morph""].plotOn(f1, LineColor=""r""); # c0 = ROOT.TCanvas(); # f1.Draw(); # input() # Wait for user input to proceed; # Define the observed mean values for the Gaussian distributions; # Class used in this case to demonstrate the use of SBI in Root; # Initializing the class SBI; # Choose the hyperparameters for training the neural network; # Defining the target / training data for different values of mean value mu; # Generating samples for the reference distribution; # Ensuring the normalization with generating as man",MatchSource.CODE_COMMENT,tutorials/roofit/rf617_simulation_based_inference_multidimensional.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf617_simulation_based_inference_multidimensional.py
Modifiability,variab,variable,"istribution we; ## choose a simple uniform distribution. The target distribution is chosen to; ## be Gaussian with different mean values.; ## The classifier is trained to discriminate between the reference and target; ## distribution.; ## We see how the neural networks generalize to unknown mean values.; ##; ## Furthermore, we compare SBI to the approach of moment morphing. In this case,; ## we can conclude, that SBI is way more sample eficcient when it comes to; ## estimating the negative log likelihood ratio.; ##; ## For an introductory background see rf615_simulation_based_inference.py; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date July 2024; ## \author Robin Syring; # Kills warning messages; # Number of samples for morphing; # Number of 'sampled' Gaussians; # To have a fair comparison; # Morphing as baseline; # Define binning for morphing; # Set bins for each x variable; # Define mu values as input for morphing for each dimension; # Create a product of Gaussians for all dimensions; # Create a product PDF for the multidimensional Gaussian; # Iterate through each tuple; # Fill the histograms; # Ensure that every bin is filled and there are no zero probabilities; # Add the PDF to the grid; # Create the morphing function and add it to the ws; # Uncomment to see input plots for the first dimension (you might need to increase the morphed samples); # f1 = x_vars[0].frame(); # for i in range(n_bins):; # templates[(i, 0)].plotOn(f1); # ws[""morph""].plotOn(f1, LineColor=""r""); # c0 = ROOT.TCanvas(); # f1.Draw(); # input() # Wait for user input to proceed; # Define the observed mean values for the Gaussian distributions; # Class used in this case to demonstrate the use of SBI in Root; # Initializing the class SBI; # Choose the hyperparameters for training the neural network; # Defining the target / training data for different values of mean value mu; # Generating samples for the reference distribution; # Ensuring the normalization with generating as man",MatchSource.CODE_COMMENT,tutorials/roofit/rf617_simulation_based_inference_multidimensional.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf617_simulation_based_inference_multidimensional.py
Testability,log,log,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Use Simulation Based Inference (SBI) in multiple dimensions in RooFit.; ##; ## This tutorial shows how to use SBI in higher dimension in ROOT.; ## This tutorial transfers the simple concepts of the 1D case introduced in; ## rf615_simulation_based_inference.py onto the higher dimensional case.; ##; ## Again as reference distribution we; ## choose a simple uniform distribution. The target distribution is chosen to; ## be Gaussian with different mean values.; ## The classifier is trained to discriminate between the reference and target; ## distribution.; ## We see how the neural networks generalize to unknown mean values.; ##; ## Furthermore, we compare SBI to the approach of moment morphing. In this case,; ## we can conclude, that SBI is way more sample eficcient when it comes to; ## estimating the negative log likelihood ratio.; ##; ## For an introductory background see rf615_simulation_based_inference.py; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date July 2024; ## \author Robin Syring; # Kills warning messages; # Number of samples for morphing; # Number of 'sampled' Gaussians; # To have a fair comparison; # Morphing as baseline; # Define binning for morphing; # Set bins for each x variable; # Define mu values as input for morphing for each dimension; # Create a product of Gaussians for all dimensions; # Create a product PDF for the multidimensional Gaussian; # Iterate through each tuple; # Fill the histograms; # Ensure that every bin is filled and there are no zero probabilities; # Add the PDF to the grid; # Create the morphing function and add it to the ws; # Uncomment to see input plots for the first dimension (you might need to increase the morphed samples); # f1 = x_vars[0].frame(); # for i in range(n_bins):; # templates[(i, 0)].plotOn(f1); # ws[""morph""].plotOn(f1, LineColor=""r""); # c0 = ROOT.TCanvas(); # f1.Draw(); # input() # Wait for user input to proceed; # Define the observed mean ",MatchSource.CODE_COMMENT,tutorials/roofit/rf617_simulation_based_inference_multidimensional.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf617_simulation_based_inference_multidimensional.py
Usability,simpl,simple,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Use Simulation Based Inference (SBI) in multiple dimensions in RooFit.; ##; ## This tutorial shows how to use SBI in higher dimension in ROOT.; ## This tutorial transfers the simple concepts of the 1D case introduced in; ## rf615_simulation_based_inference.py onto the higher dimensional case.; ##; ## Again as reference distribution we; ## choose a simple uniform distribution. The target distribution is chosen to; ## be Gaussian with different mean values.; ## The classifier is trained to discriminate between the reference and target; ## distribution.; ## We see how the neural networks generalize to unknown mean values.; ##; ## Furthermore, we compare SBI to the approach of moment morphing. In this case,; ## we can conclude, that SBI is way more sample eficcient when it comes to; ## estimating the negative log likelihood ratio.; ##; ## For an introductory background see rf615_simulation_based_inference.py; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date July 2024; ## \author Robin Syring; # Kills warning messages; # Number of samples for morphing; # Number of 'sampled' Gaussians; # To have a fair comparison; # Morphing as baseline; # Define binning for morphing; # Set bins for each x variable; # Define mu values as input for morphing for each dimension; # Create a product of Gaussians for all dimensions; # Create a product PDF for the multidimensional Gaussian; # Iterate through each tuple; # Fill the histograms; # Ensure that every bin is filled and there are no zero probabilities; # Add the PDF to the grid; # Create the morphing function and add it to the ws; # Uncomment to see input plots for the first dimension (you might need to increase the morphed samples); # f1 = x_vars[0].frame(); # for i in range(n_bins):; # templates[(i, 0)].plotOn(f1); # ws[""morph""].plotOn(f1, LineColor=""r""); # c0 = ROOT.TCanvas(); # f1.Draw(); # input() # Wait for user input to proceed; # Define the observed mean ",MatchSource.CODE_COMMENT,tutorials/roofit/rf617_simulation_based_inference_multidimensional.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf617_simulation_based_inference_multidimensional.py
Availability,error,error,"ing this decomposition, one is able to use the pairwise likelihood ratios.; ##; ## Since the only free parameter in our case is mu, the distributions are independent of this parameter and the dependence on the signal strength can be encoded into the weights.; ## Thus, the subratios simplify dramatically since they are independent of theta and these ratios can be pre-computed and the classifier does; ## not need to be parametrized.; ##; ## If you wish to see an analysis done with template histograms see 'hf001_example.py'.; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date September 2024; ## \author Robin Syring; # Get Dataframe from tutorial df106_HiggsToFourLeptons.py; # Adjust the path if running locally; # Initialize a dictionary to store counts and weight sums for each category; # Extract the relevant columns once and avoid repeated calls; # Loop over each sample category; # Normalize each weight; # Extract the weight_modified; # Store the count and weight sum in the dictionary; # Extract the mass for higgs and zz; # Prepare sample weights; # Putting sample weights together in the same manner as the training data; # For Training purposes we have to get rid of the negative weights, since xgb can't handle them; # Prepare the features and labels; # Train the Classifier to discriminate between higgs and zz; # Building a RooRealVar based on the observed data; # Define functions to compute the learned likelihood.; # Number of signals and background; # Define weight functions; # Define the likelihood ratio accordingly to mixture models; # Plot the likelihood; # llh.plotOn(frame1, ShiftToZero=False, LineColor=""kP6Blue""); # Prepare the observed data set and NLL; # Plot the nll computet by the mixture model; # Write the plots into one canvas to show, or into separate canvases for saving.; # Compute the minimum via minuit and display the results; # Adjust the error level in the minimization to work with likelihoods; # Hack to bypass ClearProxiedObjects()",MatchSource.CODE_COMMENT,tutorials/roofit/rf618_mixture_models.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf618_mixture_models.py
Integrability,depend,dependence,"s and a target; ## hypothesis, here the higgs samples. The data preparation is based on the tutorial 'df106_HiggsToFourLeptons.py'.; ##; ## An introduction to mixture models can be found here https://arxiv.org/pdf/1506.02169.; ##; ## A short summary:; ## We assume the whole probability distribution can be written as a mixture of several components, i.e.; ## $$p(x|\theta)= \sum_{c}w_{c}(\theta)p_{c}(x|\theta)$$; ## We can write the likelihood ratio in terms of pairwise classification problems; ## \begin{align*}; ## \frac{p(x|\mu)}{p(x|0)}&= \frac{\sum_{c}w_{c}(\mu)p_{c}(x|\mu)}{\sum_{c'}w_{c'}(0)p_{c'}(x|0)}\\; ## &=\sum_{c}\Bigg[\sum_{c'}\frac{w_{c'}(0)}{w_{c}(\mu)}\frac{p_{c'}(x|0)}{p_{c}(x|\mu)}\Bigg]^{-1},; ## \end{align*}; ## where mu is the signal strength, and a value of 0 corresponds to the background hypothesis. Using this decomposition, one is able to use the pairwise likelihood ratios.; ##; ## Since the only free parameter in our case is mu, the distributions are independent of this parameter and the dependence on the signal strength can be encoded into the weights.; ## Thus, the subratios simplify dramatically since they are independent of theta and these ratios can be pre-computed and the classifier does; ## not need to be parametrized.; ##; ## If you wish to see an analysis done with template histograms see 'hf001_example.py'.; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date September 2024; ## \author Robin Syring; # Get Dataframe from tutorial df106_HiggsToFourLeptons.py; # Adjust the path if running locally; # Initialize a dictionary to store counts and weight sums for each category; # Extract the relevant columns once and avoid repeated calls; # Loop over each sample category; # Normalize each weight; # Extract the weight_modified; # Store the count and weight sum in the dictionary; # Extract the mass for higgs and zz; # Prepare sample weights; # Putting sample weights together in the same manner as the training data; # For Traini",MatchSource.CODE_COMMENT,tutorials/roofit/rf618_mixture_models.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf618_mixture_models.py
Safety,avoid,avoid,"ing this decomposition, one is able to use the pairwise likelihood ratios.; ##; ## Since the only free parameter in our case is mu, the distributions are independent of this parameter and the dependence on the signal strength can be encoded into the weights.; ## Thus, the subratios simplify dramatically since they are independent of theta and these ratios can be pre-computed and the classifier does; ## not need to be parametrized.; ##; ## If you wish to see an analysis done with template histograms see 'hf001_example.py'.; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date September 2024; ## \author Robin Syring; # Get Dataframe from tutorial df106_HiggsToFourLeptons.py; # Adjust the path if running locally; # Initialize a dictionary to store counts and weight sums for each category; # Extract the relevant columns once and avoid repeated calls; # Loop over each sample category; # Normalize each weight; # Extract the weight_modified; # Store the count and weight sum in the dictionary; # Extract the mass for higgs and zz; # Prepare sample weights; # Putting sample weights together in the same manner as the training data; # For Training purposes we have to get rid of the negative weights, since xgb can't handle them; # Prepare the features and labels; # Train the Classifier to discriminate between higgs and zz; # Building a RooRealVar based on the observed data; # Define functions to compute the learned likelihood.; # Number of signals and background; # Define weight functions; # Define the likelihood ratio accordingly to mixture models; # Plot the likelihood; # llh.plotOn(frame1, ShiftToZero=False, LineColor=""kP6Blue""); # Prepare the observed data set and NLL; # Plot the nll computet by the mixture model; # Write the plots into one canvas to show, or into separate canvases for saving.; # Compute the minimum via minuit and display the results; # Adjust the error level in the minimization to work with likelihoods; # Hack to bypass ClearProxiedObjects()",MatchSource.CODE_COMMENT,tutorials/roofit/rf618_mixture_models.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf618_mixture_models.py
Usability,simpl,simplify," found here https://arxiv.org/pdf/1506.02169.; ##; ## A short summary:; ## We assume the whole probability distribution can be written as a mixture of several components, i.e.; ## $$p(x|\theta)= \sum_{c}w_{c}(\theta)p_{c}(x|\theta)$$; ## We can write the likelihood ratio in terms of pairwise classification problems; ## \begin{align*}; ## \frac{p(x|\mu)}{p(x|0)}&= \frac{\sum_{c}w_{c}(\mu)p_{c}(x|\mu)}{\sum_{c'}w_{c'}(0)p_{c'}(x|0)}\\; ## &=\sum_{c}\Bigg[\sum_{c'}\frac{w_{c'}(0)}{w_{c}(\mu)}\frac{p_{c'}(x|0)}{p_{c}(x|\mu)}\Bigg]^{-1},; ## \end{align*}; ## where mu is the signal strength, and a value of 0 corresponds to the background hypothesis. Using this decomposition, one is able to use the pairwise likelihood ratios.; ##; ## Since the only free parameter in our case is mu, the distributions are independent of this parameter and the dependence on the signal strength can be encoded into the weights.; ## Thus, the subratios simplify dramatically since they are independent of theta and these ratios can be pre-computed and the classifier does; ## not need to be parametrized.; ##; ## If you wish to see an analysis done with template histograms see 'hf001_example.py'.; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date September 2024; ## \author Robin Syring; # Get Dataframe from tutorial df106_HiggsToFourLeptons.py; # Adjust the path if running locally; # Initialize a dictionary to store counts and weight sums for each category; # Extract the relevant columns once and avoid repeated calls; # Loop over each sample category; # Normalize each weight; # Extract the weight_modified; # Store the count and weight sum in the dictionary; # Extract the mass for higgs and zz; # Prepare sample weights; # Putting sample weights together in the same manner as the training data; # For Training purposes we have to get rid of the negative weights, since xgb can't handle them; # Prepare the features and labels; # Train the Classifier to discriminate between higgs and zz;",MatchSource.CODE_COMMENT,tutorials/roofit/rf618_mixture_models.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf618_mixture_models.py
Modifiability,variab,variables,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Special pdf's: unbinned maximum likelihood fit of an efficiency eff(x) function to a; ## dataset D(x,cut), cut is a category encoding a selection, which the efficiency as function; ## of x should be described by eff(x); ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Construct efficiency function e(x); # -------------------------------------------------------------------; # Declare variables x,mean, with associated name, title, value and allowed; # range; # Efficiency function eff(x;a,b); # Construct conditional efficiency pdf E(cut|x); # ------------------------------------------------------------------------------------------; # Acceptance state cut (1 or 0); # Construct efficiency pdf eff(cut|x); # Generate data (x, cut) from a toy model; # -----------------------------------------------------------------------------; # Construct global shape pdf shape(x) and product model(x,cut) = eff(cut|x)*shape(x); # (These are _only_ needed to generate some toy MC here to be used later); # Generate some toy data from model; # Fit conditional efficiency pdf to data; # --------------------------------------------------------------------------; # Fit conditional efficiency pdf to data; # Plot fitted, data efficiency; # --------------------------------------------------------; # Plot distribution of all events and accepted fraction of events on frame; # Plot accept/reject efficiency on data overlay fitted efficiency curve; # needs ROOT version >= 5.21; # Draw all frames on a canvas",MatchSource.CODE_COMMENT,tutorials/roofit/rf701_efficiencyfit.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf701_efficiencyfit.py
Modifiability,variab,variables,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Special pdf's: unbinned maximum likelihood fit of an efficiency eff(x) function; ## to a dataset D(x,cut), cut is a category encoding a selection whose efficiency as function; ## of x should be described by eff(x); ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Construct efficiency function e(x,y); # -----------------------------------------------------------------------; # Declare variables x,mean, with associated name, title, value and allowed; # range; # Efficiency function eff(x;a,b); # Acceptance state cut (1 or 0); # Construct conditional efficiency pdf E(cut|x,y); # ---------------------------------------------------------------------------------------------; # Construct efficiency pdf eff(cut|x); # Generate data(x,y,cut) from a toy model; # -------------------------------------------------------------------------------; # Construct global shape pdf shape(x) and product model(x,cut) = eff(cut|x)*shape(x); # (These are _only_ needed to generate some toy MC here to be used later); # Generate some toy data from model; # Fit conditional efficiency pdf to data; # --------------------------------------------------------------------------; # Fit conditional efficiency pdf to data; # Plot fitted, data efficiency; # --------------------------------------------------------; # Make 2D histograms of all data, data and efficiency function; # Some adjustsment for good visualization; # Draw all frames on a canvas",MatchSource.CODE_COMMENT,tutorials/roofit/rf702_efficiencyfit_2D.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf702_efficiencyfit_2D.py
Availability,error,error,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Special pdf's: using a product of an (acceptance) efficiency and a pdf as pdf; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Define observables and decay pdf; # ---------------------------------------------------------------; # Declare observables; # Make pdf; # Define efficiency function; # ---------------------------------------------------; # Use error function to simulate turn-on slope; # Define decay pdf with efficiency; # ---------------------------------------------------------------; # Multiply pdf(t) with efficiency in t; # Plot efficiency, pdf; # ----------------------------------------; # Generate toy data, fit model eff to data; # ------------------------------------------------------------------------------; # Generate events. If the input pdf has an internal generator, internal generator; # is used and an accept/reject sampling on the efficiency is applied.; # Fit pdf. The normalization integral is calculated numerically.; # Plot generated data and overlay fitted pdf",MatchSource.CODE_COMMENT,tutorials/roofit/rf703_effpdfprod.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf703_effpdfprod.py
Modifiability,variab,variable,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## 'SPECIAL PDFS' RooFit tutorial macro #705; ##; ## Linear interpolation between p.d.f shapes using the 'Alex Read' algorithm; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C version); # Create end point pdf shapes; # ------------------------------------------------------; # Observable; # Lower end point shape: a Gaussian; # Upper end point shape: a Polynomial; # Create interpolating pdf; # -----------------------------------------------; # Create interpolation variable; # Specify sampling density on observable and interpolation variable; # Construct interpolating pdf in (x,a) represent g1(x) at a=a_min; # and g2(x) at a=a_max; # Plot interpolating pdf aat various alphas a l p h a; # -----------------------------------------------------------------------------; # Show end points as blue curves; # Show interpolated shapes in red; # Show 2D distribution of pdf(x,alpha); # -----------------------------------------------------------------------; # Create 2D histogram; # Fit pdf to dataset with alpha=0.8; # -----------------------------------------------------------------; # Generate a toy dataset alpha = 0.8; # Fit pdf to toy data; # Plot fitted pdf and data overlaid; # Scan -log(L) vs alpha; # -----------------------------------------; # Show scan -log(L) of dataset w.r.t alpha; # Make 2D pdf of histogram",MatchSource.CODE_COMMENT,tutorials/roofit/rf705_linearmorph.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf705_linearmorph.py
Testability,log,log,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## 'SPECIAL PDFS' RooFit tutorial macro #705; ##; ## Linear interpolation between p.d.f shapes using the 'Alex Read' algorithm; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C version); # Create end point pdf shapes; # ------------------------------------------------------; # Observable; # Lower end point shape: a Gaussian; # Upper end point shape: a Polynomial; # Create interpolating pdf; # -----------------------------------------------; # Create interpolation variable; # Specify sampling density on observable and interpolation variable; # Construct interpolating pdf in (x,a) represent g1(x) at a=a_min; # and g2(x) at a=a_max; # Plot interpolating pdf aat various alphas a l p h a; # -----------------------------------------------------------------------------; # Show end points as blue curves; # Show interpolated shapes in red; # Show 2D distribution of pdf(x,alpha); # -----------------------------------------------------------------------; # Create 2D histogram; # Fit pdf to dataset with alpha=0.8; # -----------------------------------------------------------------; # Generate a toy dataset alpha = 0.8; # Fit pdf to toy data; # Plot fitted pdf and data overlaid; # Scan -log(L) vs alpha; # -----------------------------------------; # Show scan -log(L) of dataset w.r.t alpha; # Make 2D pdf of histogram",MatchSource.CODE_COMMENT,tutorials/roofit/rf705_linearmorph.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf705_linearmorph.py
Deployability,configurat,configuration,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Special pdf's: using non-parametric (multi-dimensional) kernel estimation pdfs; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create low stats 1D dataset; # -------------------------------------------------------; # Create a toy pdf for sampling; # Sample 500 events from p; # Create 1D kernel estimation pdf; # ---------------------------------------------------------------; # Create adaptive kernel estimation pdf. In self configuration the input data; # is mirrored over the boundaries to minimize edge effects in distribution; # that do not fall to zero towards the edges; # An adaptive kernel estimation pdf on the same data without mirroring option; # for comparison; # Adaptive kernel estimation pdf with increased bandwidth scale factor; # (promotes smoothness over detail preservation); # Plot kernel estimation pdfs with and without mirroring over data; # Plot kernel estimation pdfs with regular and increased bandwidth; # Create low status 2D dataset; # -------------------------------------------------------; # Construct a 2D toy pdf for sampleing; # Create 2D kernel estimation pdf; # ---------------------------------------------------------------; # Create 2D adaptive kernel estimation pdf with mirroring; # Create 2D adaptive kernel estimation pdf with mirroring and double; # bandwidth; # Create a histogram of the data; # Create histogram of the 2d kernel estimation pdfs",MatchSource.CODE_COMMENT,tutorials/roofit/rf707_kernelestimation.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf707_kernelestimation.py
Energy Efficiency,adapt,adaptive,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Special pdf's: using non-parametric (multi-dimensional) kernel estimation pdfs; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create low stats 1D dataset; # -------------------------------------------------------; # Create a toy pdf for sampling; # Sample 500 events from p; # Create 1D kernel estimation pdf; # ---------------------------------------------------------------; # Create adaptive kernel estimation pdf. In self configuration the input data; # is mirrored over the boundaries to minimize edge effects in distribution; # that do not fall to zero towards the edges; # An adaptive kernel estimation pdf on the same data without mirroring option; # for comparison; # Adaptive kernel estimation pdf with increased bandwidth scale factor; # (promotes smoothness over detail preservation); # Plot kernel estimation pdfs with and without mirroring over data; # Plot kernel estimation pdfs with regular and increased bandwidth; # Create low status 2D dataset; # -------------------------------------------------------; # Construct a 2D toy pdf for sampleing; # Create 2D kernel estimation pdf; # ---------------------------------------------------------------; # Create 2D adaptive kernel estimation pdf with mirroring; # Create 2D adaptive kernel estimation pdf with mirroring and double; # bandwidth; # Create a histogram of the data; # Create histogram of the 2d kernel estimation pdfs",MatchSource.CODE_COMMENT,tutorials/roofit/rf707_kernelestimation.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf707_kernelestimation.py
Modifiability,adapt,adaptive,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Special pdf's: using non-parametric (multi-dimensional) kernel estimation pdfs; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create low stats 1D dataset; # -------------------------------------------------------; # Create a toy pdf for sampling; # Sample 500 events from p; # Create 1D kernel estimation pdf; # ---------------------------------------------------------------; # Create adaptive kernel estimation pdf. In self configuration the input data; # is mirrored over the boundaries to minimize edge effects in distribution; # that do not fall to zero towards the edges; # An adaptive kernel estimation pdf on the same data without mirroring option; # for comparison; # Adaptive kernel estimation pdf with increased bandwidth scale factor; # (promotes smoothness over detail preservation); # Plot kernel estimation pdfs with and without mirroring over data; # Plot kernel estimation pdfs with regular and increased bandwidth; # Create low status 2D dataset; # -------------------------------------------------------; # Construct a 2D toy pdf for sampleing; # Create 2D kernel estimation pdf; # ---------------------------------------------------------------; # Create 2D adaptive kernel estimation pdf with mirroring; # Create 2D adaptive kernel estimation pdf with mirroring and double; # bandwidth; # Create a histogram of the data; # Create histogram of the 2d kernel estimation pdfs",MatchSource.CODE_COMMENT,tutorials/roofit/rf707_kernelestimation.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf707_kernelestimation.py
Availability,down,down,"ssian signal on top of a uniform background; # Generate the data to be fitted; # Make histogram templates for signal and background.; # Let's take a signal distribution with low statistics and a more accurate; # background distribution.; # Normally, these come from Monte Carlo simulations, but we will just generate them.; # Case 0 - 'Rigid templates'; # Construct histogram shapes for signal and background; # Construct scale factors for adding the two distributions; # Construct the sum model; # Case 1 - 'Barlow Beeston'; # Construct parameterized histogram shapes for signal and background; # Construct the sum of these; # Construct the subsidiary poisson measurements constraining the histogram parameters; # These ensure that the bin contents of the histograms are only allowed to vary within; # the statistical uncertainty of the Monte Carlo.; # Construct the joint model with template PDFs and constraints; # Case 2 - 'Barlow Beeston' light (one parameter per bin for all samples); # Construct the histogram shapes, using the same parameters for signal and background; # This requires passing the first histogram to the second, so that their common parameters; # can be re-used.; # The first ParamHistFunc will create one parameter per bin, such as `p_ph_sig2_gamma_bin_0`.; # This allows bin 0 to fluctuate up and down.; # Then, the SAME parameters are connected to the background histogram, so the bins fluctuate; # synchronously. This reduces the number of parameters.; # As before, construct the sum of signal2 and background2; # Construct the subsidiary poisson measurements constraining the statistical fluctuations; # Construct the joint model; # ************ Fit all models to data and plot *********************; # Plot data to enable automatic determination of model0 normalisation:; # Plot data again to show it on top of model0 error bands:; # Plot model components; # Plot data again to show it on top of error bands:; # Plot data again to show it on top of model0 error bands:",MatchSource.CODE_COMMENT,tutorials/roofit/rf709_BarlowBeeston.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf709_BarlowBeeston.py
Energy Efficiency,reduce,reduces,"ssian signal on top of a uniform background; # Generate the data to be fitted; # Make histogram templates for signal and background.; # Let's take a signal distribution with low statistics and a more accurate; # background distribution.; # Normally, these come from Monte Carlo simulations, but we will just generate them.; # Case 0 - 'Rigid templates'; # Construct histogram shapes for signal and background; # Construct scale factors for adding the two distributions; # Construct the sum model; # Case 1 - 'Barlow Beeston'; # Construct parameterized histogram shapes for signal and background; # Construct the sum of these; # Construct the subsidiary poisson measurements constraining the histogram parameters; # These ensure that the bin contents of the histograms are only allowed to vary within; # the statistical uncertainty of the Monte Carlo.; # Construct the joint model with template PDFs and constraints; # Case 2 - 'Barlow Beeston' light (one parameter per bin for all samples); # Construct the histogram shapes, using the same parameters for signal and background; # This requires passing the first histogram to the second, so that their common parameters; # can be re-used.; # The first ParamHistFunc will create one parameter per bin, such as `p_ph_sig2_gamma_bin_0`.; # This allows bin 0 to fluctuate up and down.; # Then, the SAME parameters are connected to the background histogram, so the bins fluctuate; # synchronously. This reduces the number of parameters.; # As before, construct the sum of signal2 and background2; # Construct the subsidiary poisson measurements constraining the statistical fluctuations; # Construct the joint model; # ************ Fit all models to data and plot *********************; # Plot data to enable automatic determination of model0 normalisation:; # Plot data again to show it on top of model0 error bands:; # Plot model components; # Plot data again to show it on top of error bands:; # Plot data again to show it on top of model0 error bands:",MatchSource.CODE_COMMENT,tutorials/roofit/rf709_BarlowBeeston.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf709_BarlowBeeston.py
Modifiability,parameteriz,parameterized,"## \file; ## \ingroup tutorial_roofit; ## \notebook -js; ## Implementing the Barlow-Beeston method for taking into account the statistical; ## uncertainty of a Monte-Carlo fit template.; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## Based on a demo by Wouter Verkerke; ## \date June 2021; ## \author Harshal Shende, Stephan Hageboeck (C++ version); # First, construct a likelihood model with a Gaussian signal on top of a uniform background; # Generate the data to be fitted; # Make histogram templates for signal and background.; # Let's take a signal distribution with low statistics and a more accurate; # background distribution.; # Normally, these come from Monte Carlo simulations, but we will just generate them.; # Case 0 - 'Rigid templates'; # Construct histogram shapes for signal and background; # Construct scale factors for adding the two distributions; # Construct the sum model; # Case 1 - 'Barlow Beeston'; # Construct parameterized histogram shapes for signal and background; # Construct the sum of these; # Construct the subsidiary poisson measurements constraining the histogram parameters; # These ensure that the bin contents of the histograms are only allowed to vary within; # the statistical uncertainty of the Monte Carlo.; # Construct the joint model with template PDFs and constraints; # Case 2 - 'Barlow Beeston' light (one parameter per bin for all samples); # Construct the histogram shapes, using the same parameters for signal and background; # This requires passing the first histogram to the second, so that their common parameters; # can be re-used.; # The first ParamHistFunc will create one parameter per bin, such as `p_ph_sig2_gamma_bin_0`.; # This allows bin 0 to fluctuate up and down.; # Then, the SAME parameters are connected to the background histogram, so the bins fluctuate; # synchronously. This reduces the number of parameters.; # As before, construct the sum of signal2 and background2; # Construct the subsidiary poisson measurements",MatchSource.CODE_COMMENT,tutorials/roofit/rf709_BarlowBeeston.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf709_BarlowBeeston.py
Integrability,wrap,wrapped,"## \file; ## \ingroup tutorial_roofit; ## \notebook -js; ## Morphing effective field theory distributions with RooLagrangianMorphFunc.; ## A morphing function as a function of one coefficient is setup and can be used; ## to obtain the distribution for any value of the coefficient.; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date January 2022; ## \author Rahul Balasubramanian; # Create functions; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Setup observable that is to be morphed; # Setup two couplings that enters the morphing function; # kSM -> SM coupling set to constant (1); # cHq3 -> EFT parameter with NewPhysics attribute set to true; # Inputs to setup config; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Set Config; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Create morphing function; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Get morphed distribution at cHq3 = 0.01, 0.5; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Extract input templates for plotting; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Plot input templates; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Plot morphed templates for cHq3=0.01,0.25,0.5; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Create wrapped pdf to generate 2D dataset of cHq3 as a function of pTV; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Draw plots on canvas; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -",MatchSource.CODE_COMMENT,tutorials/roofit/rf711_lagrangianmorph.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf711_lagrangianmorph.py
Modifiability,coupling,couplings,"## \file; ## \ingroup tutorial_roofit; ## \notebook -js; ## Morphing effective field theory distributions with RooLagrangianMorphFunc.; ## A morphing function as a function of one coefficient is setup and can be used; ## to obtain the distribution for any value of the coefficient.; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date January 2022; ## \author Rahul Balasubramanian; # Create functions; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Setup observable that is to be morphed; # Setup two couplings that enters the morphing function; # kSM -> SM coupling set to constant (1); # cHq3 -> EFT parameter with NewPhysics attribute set to true; # Inputs to setup config; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Set Config; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Create morphing function; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Get morphed distribution at cHq3 = 0.01, 0.5; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Extract input templates for plotting; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Plot input templates; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Plot morphed templates for cHq3=0.01,0.25,0.5; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Create wrapped pdf to generate 2D dataset of cHq3 as a function of pTV; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Draw plots on canvas; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -",MatchSource.CODE_COMMENT,tutorials/roofit/rf711_lagrangianmorph.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf711_lagrangianmorph.py
Availability,error,error,"## \file; ## \ingroup tutorial_roofit; ## \notebook -js; ## Performing a simple fit with RooLagrangianMorphFunc; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date January 2022; ## \author Rahul Balasubramanian; # Create functions; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Setup three EFT coefficient and constant SM modifier; # Inputs to setup config; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Set Config; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Create morphing function; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Create pseudo data histogram to fit at cHq3 = 0.01, cHl3 = 1.0, cHDD = 0.2; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # reset parameters to zeros before fit; # set error to set initial step size in fit; # Wrap pdf on morphfunc and fit to data histogram; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # wrapper pdf to normalise morphing function to a morphing pdf; # run the fit; # Get the correlation matrix; # Extract postfit distribution and plot with initial histogram; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Draw plots on canvas; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -",MatchSource.CODE_COMMENT,tutorials/roofit/rf712_lagrangianmorphfit.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf712_lagrangianmorphfit.py
Integrability,wrap,wrapper,"## \file; ## \ingroup tutorial_roofit; ## \notebook -js; ## Performing a simple fit with RooLagrangianMorphFunc; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date January 2022; ## \author Rahul Balasubramanian; # Create functions; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Setup three EFT coefficient and constant SM modifier; # Inputs to setup config; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Set Config; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Create morphing function; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Create pseudo data histogram to fit at cHq3 = 0.01, cHl3 = 1.0, cHDD = 0.2; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # reset parameters to zeros before fit; # set error to set initial step size in fit; # Wrap pdf on morphfunc and fit to data histogram; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # wrapper pdf to normalise morphing function to a morphing pdf; # run the fit; # Get the correlation matrix; # Extract postfit distribution and plot with initial histogram; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Draw plots on canvas; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -",MatchSource.CODE_COMMENT,tutorials/roofit/rf712_lagrangianmorphfit.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf712_lagrangianmorphfit.py
Modifiability,config,config,"## \file; ## \ingroup tutorial_roofit; ## \notebook -js; ## Performing a simple fit with RooLagrangianMorphFunc; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date January 2022; ## \author Rahul Balasubramanian; # Create functions; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Setup three EFT coefficient and constant SM modifier; # Inputs to setup config; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Set Config; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Create morphing function; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Create pseudo data histogram to fit at cHq3 = 0.01, cHl3 = 1.0, cHDD = 0.2; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # reset parameters to zeros before fit; # set error to set initial step size in fit; # Wrap pdf on morphfunc and fit to data histogram; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # wrapper pdf to normalise morphing function to a morphing pdf; # run the fit; # Get the correlation matrix; # Extract postfit distribution and plot with initial histogram; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Draw plots on canvas; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -",MatchSource.CODE_COMMENT,tutorials/roofit/rf712_lagrangianmorphfit.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf712_lagrangianmorphfit.py
Usability,simpl,simple,"## \file; ## \ingroup tutorial_roofit; ## \notebook -js; ## Performing a simple fit with RooLagrangianMorphFunc; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date January 2022; ## \author Rahul Balasubramanian; # Create functions; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Setup three EFT coefficient and constant SM modifier; # Inputs to setup config; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Set Config; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Create morphing function; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Create pseudo data histogram to fit at cHq3 = 0.01, cHl3 = 1.0, cHDD = 0.2; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # reset parameters to zeros before fit; # set error to set initial step size in fit; # Wrap pdf on morphfunc and fit to data histogram; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # wrapper pdf to normalise morphing function to a morphing pdf; # run the fit; # Get the correlation matrix; # Extract postfit distribution and plot with initial histogram; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -; # Draw plots on canvas; # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -",MatchSource.CODE_COMMENT,tutorials/roofit/rf712_lagrangianmorphfit.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf712_lagrangianmorphfit.py
Availability,error,error,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Validation and MC studies: toy Monte Carlo study that perform cycles of event generation and fitting; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create model; # -----------------------; # Declare observable x; # Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and; # their parameters; # Build Chebychev polynomial pdf; # Sum the signal components into a composite signal pdf; # Sum the composite signal and background; # Create manager; # ---------------------------; # Instantiate ROOT.RooMCStudy manager on model with x as observable and given choice of fit options; #; # The Silence() option kills all messages below the PROGRESS level, only a single message; # per sample executed, any error message that occur during fitting; #; # The Extended() option has two effects:; # 1) The extended ML term is included in the likelihood and; # 2) A poisson fluctuation is introduced on the number of generated events; #; # The FitOptions() given here are passed to the fitting stage of each toy experiment.; # If Save() is specified, fit result of each experiment is saved by the manager; #; # A Binned() option is added in self example to bin the data between generation and fitting; # to speed up the study at the expemse of some precision; # Generate and fit events; # ---------------------------------------------; # Generate and fit 1000 samples of Poisson(nExpected) events; # Explore results of study; # ------------------------------------------------; # Make plots of the distributions of mean, error on mean and the pull of; # mean; # Plot distribution of minimized likelihood; # Make some histograms from the parameter dataset; # Access some of the saved fit results from individual toys; # Draw all plots on a canvas; # Make ROOT.RooMCStudy object available on command line after; # macro finishes",MatchSource.CODE_COMMENT,tutorials/roofit/rf801_mcstudy.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf801_mcstudy.py
Integrability,message,messages,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Validation and MC studies: toy Monte Carlo study that perform cycles of event generation and fitting; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create model; # -----------------------; # Declare observable x; # Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and; # their parameters; # Build Chebychev polynomial pdf; # Sum the signal components into a composite signal pdf; # Sum the composite signal and background; # Create manager; # ---------------------------; # Instantiate ROOT.RooMCStudy manager on model with x as observable and given choice of fit options; #; # The Silence() option kills all messages below the PROGRESS level, only a single message; # per sample executed, any error message that occur during fitting; #; # The Extended() option has two effects:; # 1) The extended ML term is included in the likelihood and; # 2) A poisson fluctuation is introduced on the number of generated events; #; # The FitOptions() given here are passed to the fitting stage of each toy experiment.; # If Save() is specified, fit result of each experiment is saved by the manager; #; # A Binned() option is added in self example to bin the data between generation and fitting; # to speed up the study at the expemse of some precision; # Generate and fit events; # ---------------------------------------------; # Generate and fit 1000 samples of Poisson(nExpected) events; # Explore results of study; # ------------------------------------------------; # Make plots of the distributions of mean, error on mean and the pull of; # mean; # Plot distribution of minimized likelihood; # Make some histograms from the parameter dataset; # Access some of the saved fit results from individual toys; # Draw all plots on a canvas; # Make ROOT.RooMCStudy object available on command line after; # macro finishes",MatchSource.CODE_COMMENT,tutorials/roofit/rf801_mcstudy.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf801_mcstudy.py
Modifiability,extend,extended,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Validation and MC studies: toy Monte Carlo study that perform cycles of event generation and fitting; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create model; # -----------------------; # Declare observable x; # Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and; # their parameters; # Build Chebychev polynomial pdf; # Sum the signal components into a composite signal pdf; # Sum the composite signal and background; # Create manager; # ---------------------------; # Instantiate ROOT.RooMCStudy manager on model with x as observable and given choice of fit options; #; # The Silence() option kills all messages below the PROGRESS level, only a single message; # per sample executed, any error message that occur during fitting; #; # The Extended() option has two effects:; # 1) The extended ML term is included in the likelihood and; # 2) A poisson fluctuation is introduced on the number of generated events; #; # The FitOptions() given here are passed to the fitting stage of each toy experiment.; # If Save() is specified, fit result of each experiment is saved by the manager; #; # A Binned() option is added in self example to bin the data between generation and fitting; # to speed up the study at the expemse of some precision; # Generate and fit events; # ---------------------------------------------; # Generate and fit 1000 samples of Poisson(nExpected) events; # Explore results of study; # ------------------------------------------------; # Make plots of the distributions of mean, error on mean and the pull of; # mean; # Plot distribution of minimized likelihood; # Make some histograms from the parameter dataset; # Access some of the saved fit results from individual toys; # Draw all plots on a canvas; # Make ROOT.RooMCStudy object available on command line after; # macro finishes",MatchSource.CODE_COMMENT,tutorials/roofit/rf801_mcstudy.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf801_mcstudy.py
Performance,perform,perform,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Validation and MC studies: toy Monte Carlo study that perform cycles of event generation and fitting; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create model; # -----------------------; # Declare observable x; # Create two Gaussian PDFs g1(x,mean1,sigma) anf g2(x,mean2,sigma) and; # their parameters; # Build Chebychev polynomial pdf; # Sum the signal components into a composite signal pdf; # Sum the composite signal and background; # Create manager; # ---------------------------; # Instantiate ROOT.RooMCStudy manager on model with x as observable and given choice of fit options; #; # The Silence() option kills all messages below the PROGRESS level, only a single message; # per sample executed, any error message that occur during fitting; #; # The Extended() option has two effects:; # 1) The extended ML term is included in the likelihood and; # 2) A poisson fluctuation is introduced on the number of generated events; #; # The FitOptions() given here are passed to the fitting stage of each toy experiment.; # If Save() is specified, fit result of each experiment is saved by the manager; #; # A Binned() option is added in self example to bin the data between generation and fitting; # to speed up the study at the expemse of some precision; # Generate and fit events; # ---------------------------------------------; # Generate and fit 1000 samples of Poisson(nExpected) events; # Explore results of study; # ------------------------------------------------; # Make plots of the distributions of mean, error on mean and the pull of; # mean; # Plot distribution of minimized likelihood; # Make some histograms from the parameter dataset; # Access some of the saved fit results from individual toys; # Draw all plots on a canvas; # Make ROOT.RooMCStudy object available on command line after; # macro finishes",MatchSource.CODE_COMMENT,tutorials/roofit/rf801_mcstudy.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf801_mcstudy.py
Availability,avail,available,"## \ingroup tutorial_roofit; ## \notebook; ##; ## 'VALIDATION AND MC STUDIES' RooFit tutorial macro #802; ##; ## RooMCStudy: using separate fit and generator models, the chi^2 calculator model; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \author Clemens Lange; # Create model; # -----------------------; # Observables, parameters; # Create Gaussian pdf; # Create manager with chi^2 add-on module; # ----------------------------------------------------------------------------; # Create study manager for binned likelihood fits of a Gaussian pdf in 10; # bins; # Add chi^2 calculator module to mcs; # Generate 1000 samples of 1000 events; # Number of bins for chi2 plots; # Fill histograms with distributions chi2 and prob(chi2,ndf) that; # are calculated by ROOT.RooChiMCSModule; # Create manager with separate fit model; # ----------------------------------------------------------------------------; # Create alternate pdf with shifted mean; # Create study manager with separate generation and fit model. ROOT.This configuration; # is set up to generate bad fits as the fit and generator model have different means; # and the mean parameter is not floating in the fit; # Add chi^2 calculator module to mcs; # Generate 1000 samples of 1000 events; # Request a the pull plot of mean. The pulls will be one-sided because; # `mean` is limited to 1.8.; # Note that RooFit will have trouble to compute the pulls because the parameters; # are called `mean` in the fit, but `mean2` in the generator model. It is not obvious; # that these are related. RooFit will nevertheless compute pulls, but complain that; # this is risky.; # Fill histograms with distributions chi2 and prob(chi2,ndf) that; # are calculated by ROOT.RooChiMCSModule; # Make RooMCStudy object available on command line after; # macro finishes",MatchSource.CODE_COMMENT,tutorials/roofit/rf802_mcstudy_addons.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf802_mcstudy_addons.py
Deployability,configurat,configuration,"## \ingroup tutorial_roofit; ## \notebook; ##; ## 'VALIDATION AND MC STUDIES' RooFit tutorial macro #802; ##; ## RooMCStudy: using separate fit and generator models, the chi^2 calculator model; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \author Clemens Lange; # Create model; # -----------------------; # Observables, parameters; # Create Gaussian pdf; # Create manager with chi^2 add-on module; # ----------------------------------------------------------------------------; # Create study manager for binned likelihood fits of a Gaussian pdf in 10; # bins; # Add chi^2 calculator module to mcs; # Generate 1000 samples of 1000 events; # Number of bins for chi2 plots; # Fill histograms with distributions chi2 and prob(chi2,ndf) that; # are calculated by ROOT.RooChiMCSModule; # Create manager with separate fit model; # ----------------------------------------------------------------------------; # Create alternate pdf with shifted mean; # Create study manager with separate generation and fit model. ROOT.This configuration; # is set up to generate bad fits as the fit and generator model have different means; # and the mean parameter is not floating in the fit; # Add chi^2 calculator module to mcs; # Generate 1000 samples of 1000 events; # Request a the pull plot of mean. The pulls will be one-sided because; # `mean` is limited to 1.8.; # Note that RooFit will have trouble to compute the pulls because the parameters; # are called `mean` in the fit, but `mean2` in the generator model. It is not obvious; # that these are related. RooFit will nevertheless compute pulls, but complain that; # this is risky.; # Fill histograms with distributions chi2 and prob(chi2,ndf) that; # are calculated by ROOT.RooChiMCSModule; # Make RooMCStudy object available on command line after; # macro finishes",MatchSource.CODE_COMMENT,tutorials/roofit/rf802_mcstudy_addons.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf802_mcstudy_addons.py
Modifiability,config,configuration,"## \ingroup tutorial_roofit; ## \notebook; ##; ## 'VALIDATION AND MC STUDIES' RooFit tutorial macro #802; ##; ## RooMCStudy: using separate fit and generator models, the chi^2 calculator model; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \author Clemens Lange; # Create model; # -----------------------; # Observables, parameters; # Create Gaussian pdf; # Create manager with chi^2 add-on module; # ----------------------------------------------------------------------------; # Create study manager for binned likelihood fits of a Gaussian pdf in 10; # bins; # Add chi^2 calculator module to mcs; # Generate 1000 samples of 1000 events; # Number of bins for chi2 plots; # Fill histograms with distributions chi2 and prob(chi2,ndf) that; # are calculated by ROOT.RooChiMCSModule; # Create manager with separate fit model; # ----------------------------------------------------------------------------; # Create alternate pdf with shifted mean; # Create study manager with separate generation and fit model. ROOT.This configuration; # is set up to generate bad fits as the fit and generator model have different means; # and the mean parameter is not floating in the fit; # Add chi^2 calculator module to mcs; # Generate 1000 samples of 1000 events; # Request a the pull plot of mean. The pulls will be one-sided because; # `mean` is limited to 1.8.; # Note that RooFit will have trouble to compute the pulls because the parameters; # are called `mean` in the fit, but `mean2` in the generator model. It is not obvious; # that these are related. RooFit will nevertheless compute pulls, but complain that; # this is risky.; # Fill histograms with distributions chi2 and prob(chi2,ndf) that; # are calculated by ROOT.RooChiMCSModule; # Make RooMCStudy object available on command line after; # macro finishes",MatchSource.CODE_COMMENT,tutorials/roofit/rf802_mcstudy_addons.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf802_mcstudy_addons.py
Safety,risk,risky,"## \ingroup tutorial_roofit; ## \notebook; ##; ## 'VALIDATION AND MC STUDIES' RooFit tutorial macro #802; ##; ## RooMCStudy: using separate fit and generator models, the chi^2 calculator model; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \author Clemens Lange; # Create model; # -----------------------; # Observables, parameters; # Create Gaussian pdf; # Create manager with chi^2 add-on module; # ----------------------------------------------------------------------------; # Create study manager for binned likelihood fits of a Gaussian pdf in 10; # bins; # Add chi^2 calculator module to mcs; # Generate 1000 samples of 1000 events; # Number of bins for chi2 plots; # Fill histograms with distributions chi2 and prob(chi2,ndf) that; # are calculated by ROOT.RooChiMCSModule; # Create manager with separate fit model; # ----------------------------------------------------------------------------; # Create alternate pdf with shifted mean; # Create study manager with separate generation and fit model. ROOT.This configuration; # is set up to generate bad fits as the fit and generator model have different means; # and the mean parameter is not floating in the fit; # Add chi^2 calculator module to mcs; # Generate 1000 samples of 1000 events; # Request a the pull plot of mean. The pulls will be one-sided because; # `mean` is limited to 1.8.; # Note that RooFit will have trouble to compute the pulls because the parameters; # are called `mean` in the fit, but `mean2` in the generator model. It is not obvious; # that these are related. RooFit will nevertheless compute pulls, but complain that; # this is risky.; # Fill histograms with distributions chi2 and prob(chi2,ndf) that; # are calculated by ROOT.RooChiMCSModule; # Make RooMCStudy object available on command line after; # macro finishes",MatchSource.CODE_COMMENT,tutorials/roofit/rf802_mcstudy_addons.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf802_mcstudy_addons.py
Availability,avail,available,"## \ingroup tutorial_roofit; ## \notebook; ##; ## 'VALIDATION AND MC STUDIES' RooFit tutorial macro #803; ##; ## RooMCStudy: Using the randomizer and profile likelihood add-on models; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \author Clemens Lange; # Create model; # -----------------------; # Simulation of signal and background of top quark decaying into; # 3 jets with background; # Observable; # Signal component (Gaussian); # Background component (Chebychev); # Composite model; # Create manager; # ---------------------------; # Configure manager to perform binned extended likelihood fits (Binned=True, Extended=True) on data generated; # with a Poisson fluctuation on Nobs (Extended=True); # Customize manager; # ---------------------------------; # Add module that randomizes the summed value of nsig+nbkg; # sampling from a uniform distribution between 0 and 1000; #; # In general one can randomize a single parameter, a; # sum of N parameters, either a uniform or a Gaussian; # distribution. Multiple randomization can be executed; # by a single randomizer module; # Add profile likelihood calculation of significance. Redo each; # fit while keeping parameter nsig fixed to zero. For each toy,; # the difference in -log(L) of both fits is stored, well; # a simple significance interpretation of the delta(-logL); # Dnll = 0.5 sigma^2; # Run manager, make plots; # ---------------------------------------------; # Run 1000 experiments. ROOT.This configuration will generate a fair number; # of (harmless) MINUIT warnings due to the instability of the Chebychev polynomial fit; # at low statistics.; # Make some plots; # Draw plots on canvas; # Make ROOT.RooMCStudy object available on command line after; # macro finishes",MatchSource.CODE_COMMENT,tutorials/roofit/rf803_mcstudy_addons2.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf803_mcstudy_addons2.py
Deployability,configurat,configuration,"## \ingroup tutorial_roofit; ## \notebook; ##; ## 'VALIDATION AND MC STUDIES' RooFit tutorial macro #803; ##; ## RooMCStudy: Using the randomizer and profile likelihood add-on models; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \author Clemens Lange; # Create model; # -----------------------; # Simulation of signal and background of top quark decaying into; # 3 jets with background; # Observable; # Signal component (Gaussian); # Background component (Chebychev); # Composite model; # Create manager; # ---------------------------; # Configure manager to perform binned extended likelihood fits (Binned=True, Extended=True) on data generated; # with a Poisson fluctuation on Nobs (Extended=True); # Customize manager; # ---------------------------------; # Add module that randomizes the summed value of nsig+nbkg; # sampling from a uniform distribution between 0 and 1000; #; # In general one can randomize a single parameter, a; # sum of N parameters, either a uniform or a Gaussian; # distribution. Multiple randomization can be executed; # by a single randomizer module; # Add profile likelihood calculation of significance. Redo each; # fit while keeping parameter nsig fixed to zero. For each toy,; # the difference in -log(L) of both fits is stored, well; # a simple significance interpretation of the delta(-logL); # Dnll = 0.5 sigma^2; # Run manager, make plots; # ---------------------------------------------; # Run 1000 experiments. ROOT.This configuration will generate a fair number; # of (harmless) MINUIT warnings due to the instability of the Chebychev polynomial fit; # at low statistics.; # Make some plots; # Draw plots on canvas; # Make ROOT.RooMCStudy object available on command line after; # macro finishes",MatchSource.CODE_COMMENT,tutorials/roofit/rf803_mcstudy_addons2.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf803_mcstudy_addons2.py
Modifiability,extend,extended,"## \ingroup tutorial_roofit; ## \notebook; ##; ## 'VALIDATION AND MC STUDIES' RooFit tutorial macro #803; ##; ## RooMCStudy: Using the randomizer and profile likelihood add-on models; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \author Clemens Lange; # Create model; # -----------------------; # Simulation of signal and background of top quark decaying into; # 3 jets with background; # Observable; # Signal component (Gaussian); # Background component (Chebychev); # Composite model; # Create manager; # ---------------------------; # Configure manager to perform binned extended likelihood fits (Binned=True, Extended=True) on data generated; # with a Poisson fluctuation on Nobs (Extended=True); # Customize manager; # ---------------------------------; # Add module that randomizes the summed value of nsig+nbkg; # sampling from a uniform distribution between 0 and 1000; #; # In general one can randomize a single parameter, a; # sum of N parameters, either a uniform or a Gaussian; # distribution. Multiple randomization can be executed; # by a single randomizer module; # Add profile likelihood calculation of significance. Redo each; # fit while keeping parameter nsig fixed to zero. For each toy,; # the difference in -log(L) of both fits is stored, well; # a simple significance interpretation of the delta(-logL); # Dnll = 0.5 sigma^2; # Run manager, make plots; # ---------------------------------------------; # Run 1000 experiments. ROOT.This configuration will generate a fair number; # of (harmless) MINUIT warnings due to the instability of the Chebychev polynomial fit; # at low statistics.; # Make some plots; # Draw plots on canvas; # Make ROOT.RooMCStudy object available on command line after; # macro finishes",MatchSource.CODE_COMMENT,tutorials/roofit/rf803_mcstudy_addons2.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf803_mcstudy_addons2.py
Performance,perform,perform,"## \ingroup tutorial_roofit; ## \notebook; ##; ## 'VALIDATION AND MC STUDIES' RooFit tutorial macro #803; ##; ## RooMCStudy: Using the randomizer and profile likelihood add-on models; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \author Clemens Lange; # Create model; # -----------------------; # Simulation of signal and background of top quark decaying into; # 3 jets with background; # Observable; # Signal component (Gaussian); # Background component (Chebychev); # Composite model; # Create manager; # ---------------------------; # Configure manager to perform binned extended likelihood fits (Binned=True, Extended=True) on data generated; # with a Poisson fluctuation on Nobs (Extended=True); # Customize manager; # ---------------------------------; # Add module that randomizes the summed value of nsig+nbkg; # sampling from a uniform distribution between 0 and 1000; #; # In general one can randomize a single parameter, a; # sum of N parameters, either a uniform or a Gaussian; # distribution. Multiple randomization can be executed; # by a single randomizer module; # Add profile likelihood calculation of significance. Redo each; # fit while keeping parameter nsig fixed to zero. For each toy,; # the difference in -log(L) of both fits is stored, well; # a simple significance interpretation of the delta(-logL); # Dnll = 0.5 sigma^2; # Run manager, make plots; # ---------------------------------------------; # Run 1000 experiments. ROOT.This configuration will generate a fair number; # of (harmless) MINUIT warnings due to the instability of the Chebychev polynomial fit; # at low statistics.; # Make some plots; # Draw plots on canvas; # Make ROOT.RooMCStudy object available on command line after; # macro finishes",MatchSource.CODE_COMMENT,tutorials/roofit/rf803_mcstudy_addons2.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf803_mcstudy_addons2.py
Testability,log,log,"## \ingroup tutorial_roofit; ## \notebook; ##; ## 'VALIDATION AND MC STUDIES' RooFit tutorial macro #803; ##; ## RooMCStudy: Using the randomizer and profile likelihood add-on models; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \author Clemens Lange; # Create model; # -----------------------; # Simulation of signal and background of top quark decaying into; # 3 jets with background; # Observable; # Signal component (Gaussian); # Background component (Chebychev); # Composite model; # Create manager; # ---------------------------; # Configure manager to perform binned extended likelihood fits (Binned=True, Extended=True) on data generated; # with a Poisson fluctuation on Nobs (Extended=True); # Customize manager; # ---------------------------------; # Add module that randomizes the summed value of nsig+nbkg; # sampling from a uniform distribution between 0 and 1000; #; # In general one can randomize a single parameter, a; # sum of N parameters, either a uniform or a Gaussian; # distribution. Multiple randomization can be executed; # by a single randomizer module; # Add profile likelihood calculation of significance. Redo each; # fit while keeping parameter nsig fixed to zero. For each toy,; # the difference in -log(L) of both fits is stored, well; # a simple significance interpretation of the delta(-logL); # Dnll = 0.5 sigma^2; # Run manager, make plots; # ---------------------------------------------; # Run 1000 experiments. ROOT.This configuration will generate a fair number; # of (harmless) MINUIT warnings due to the instability of the Chebychev polynomial fit; # at low statistics.; # Make some plots; # Draw plots on canvas; # Make ROOT.RooMCStudy object available on command line after; # macro finishes",MatchSource.CODE_COMMENT,tutorials/roofit/rf803_mcstudy_addons2.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf803_mcstudy_addons2.py
Usability,simpl,simple,"## \ingroup tutorial_roofit; ## \notebook; ##; ## 'VALIDATION AND MC STUDIES' RooFit tutorial macro #803; ##; ## RooMCStudy: Using the randomizer and profile likelihood add-on models; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \author Clemens Lange; # Create model; # -----------------------; # Simulation of signal and background of top quark decaying into; # 3 jets with background; # Observable; # Signal component (Gaussian); # Background component (Chebychev); # Composite model; # Create manager; # ---------------------------; # Configure manager to perform binned extended likelihood fits (Binned=True, Extended=True) on data generated; # with a Poisson fluctuation on Nobs (Extended=True); # Customize manager; # ---------------------------------; # Add module that randomizes the summed value of nsig+nbkg; # sampling from a uniform distribution between 0 and 1000; #; # In general one can randomize a single parameter, a; # sum of N parameters, either a uniform or a Gaussian; # distribution. Multiple randomization can be executed; # by a single randomizer module; # Add profile likelihood calculation of significance. Redo each; # fit while keeping parameter nsig fixed to zero. For each toy,; # the difference in -log(L) of both fits is stored, well; # a simple significance interpretation of the delta(-logL); # Dnll = 0.5 sigma^2; # Run manager, make plots; # ---------------------------------------------; # Run 1000 experiments. ROOT.This configuration will generate a fair number; # of (harmless) MINUIT warnings due to the instability of the Chebychev polynomial fit; # at low statistics.; # Make some plots; # Draw plots on canvas; # Make ROOT.RooMCStudy object available on command line after; # macro finishes",MatchSource.CODE_COMMENT,tutorials/roofit/rf803_mcstudy_addons2.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf803_mcstudy_addons2.py
Deployability,configurat,configuration,"## \file; ## \ingroup tutorial_roofit; ## \notebook -nodraw; ## Numeric algorithm tuning: configuration and customization of how numeric (partial) integrals are executed; ##; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Adjust global 1D integration precision; # ----------------------------------------------------------------------------; # Print current global default configuration for numeric integration; # strategies; # Example: Change global precision for 1D integrals from 1e-7 to 1e-6; #; # The relative epsilon (change as fraction of current best integral estimate) and; # absolute epsilon (absolute change w.r.t last best integral estimate) can be specified; # separately. For most pdf integrals the relative change criterium is the most important,; # however for certain non-pdf functions that integrate out to zero a separate absolute; # change criterium is necessary to declare convergence of the integral; #; # NB: ROOT.This change is for illustration only. In general the precision should be at least 1e-7; # for normalization integrals for MINUIT to succeed.; #; # N u m e r i c i n t e g r a t i o n o f l a n d a u p d f; # ------------------------------------------------------------------; # Disable analytic integration from demonstration purposes; # Activate debug-level messages for topic integration to be able to follow; # actions below; # Calculate integral over landau with default choice of numeric integrator; # setprecision(15); # Same with custom configuration; # -----------------------------------------------------------; # Construct a custom configuration which uses the adaptive Gauss-Kronrod technique; # for closed 1D integrals; # Calculate integral over landau with custom integral specification; # Adjusting default config for a specific pdf; # -------------------------------------------------------------------------------------; # Another possibility: associate custom numeric int",MatchSource.CODE_COMMENT,tutorials/roofit/rf901_numintconfig.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf901_numintconfig.py
Energy Efficiency,adapt,adaptive,"tive change criterium is the most important,; # however for certain non-pdf functions that integrate out to zero a separate absolute; # change criterium is necessary to declare convergence of the integral; #; # NB: ROOT.This change is for illustration only. In general the precision should be at least 1e-7; # for normalization integrals for MINUIT to succeed.; #; # N u m e r i c i n t e g r a t i o n o f l a n d a u p d f; # ------------------------------------------------------------------; # Disable analytic integration from demonstration purposes; # Activate debug-level messages for topic integration to be able to follow; # actions below; # Calculate integral over landau with default choice of numeric integrator; # setprecision(15); # Same with custom configuration; # -----------------------------------------------------------; # Construct a custom configuration which uses the adaptive Gauss-Kronrod technique; # for closed 1D integrals; # Calculate integral over landau with custom integral specification; # Adjusting default config for a specific pdf; # -------------------------------------------------------------------------------------; # Another possibility: associate custom numeric integration configuration; # as default for object 'landau'; # Calculate integral over landau custom numeric integrator specified as; # object default; # Another possibility: Change global default for 1D numeric integration; # strategy on finite domains; # Adjusting parameters of a specific technique; # ---------------------------------------------------------------------------------------; # Adjust maximum number of steps of ROOT.RooIntegrator1D in the global; # default configuration; # Example of how to change the parameters of a numeric integrator; # (Each config section is a ROOT.RooArgSet with ROOT.RooRealVars holding real-valued parameters; # and ROOT.RooCategories holding parameters with a finite set of options); # Example of how to print set of possible values for ""method"" cat",MatchSource.CODE_COMMENT,tutorials/roofit/rf901_numintconfig.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf901_numintconfig.py
Integrability,integrat,integration,"## \file; ## \ingroup tutorial_roofit; ## \notebook -nodraw; ## Numeric algorithm tuning: configuration and customization of how numeric (partial) integrals are executed; ##; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Adjust global 1D integration precision; # ----------------------------------------------------------------------------; # Print current global default configuration for numeric integration; # strategies; # Example: Change global precision for 1D integrals from 1e-7 to 1e-6; #; # The relative epsilon (change as fraction of current best integral estimate) and; # absolute epsilon (absolute change w.r.t last best integral estimate) can be specified; # separately. For most pdf integrals the relative change criterium is the most important,; # however for certain non-pdf functions that integrate out to zero a separate absolute; # change criterium is necessary to declare convergence of the integral; #; # NB: ROOT.This change is for illustration only. In general the precision should be at least 1e-7; # for normalization integrals for MINUIT to succeed.; #; # N u m e r i c i n t e g r a t i o n o f l a n d a u p d f; # ------------------------------------------------------------------; # Disable analytic integration from demonstration purposes; # Activate debug-level messages for topic integration to be able to follow; # actions below; # Calculate integral over landau with default choice of numeric integrator; # setprecision(15); # Same with custom configuration; # -----------------------------------------------------------; # Construct a custom configuration which uses the adaptive Gauss-Kronrod technique; # for closed 1D integrals; # Calculate integral over landau with custom integral specification; # Adjusting default config for a specific pdf; # -------------------------------------------------------------------------------------; # Another possibility: associate custom numeric int",MatchSource.CODE_COMMENT,tutorials/roofit/rf901_numintconfig.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf901_numintconfig.py
Modifiability,config,configuration,"## \file; ## \ingroup tutorial_roofit; ## \notebook -nodraw; ## Numeric algorithm tuning: configuration and customization of how numeric (partial) integrals are executed; ##; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Adjust global 1D integration precision; # ----------------------------------------------------------------------------; # Print current global default configuration for numeric integration; # strategies; # Example: Change global precision for 1D integrals from 1e-7 to 1e-6; #; # The relative epsilon (change as fraction of current best integral estimate) and; # absolute epsilon (absolute change w.r.t last best integral estimate) can be specified; # separately. For most pdf integrals the relative change criterium is the most important,; # however for certain non-pdf functions that integrate out to zero a separate absolute; # change criterium is necessary to declare convergence of the integral; #; # NB: ROOT.This change is for illustration only. In general the precision should be at least 1e-7; # for normalization integrals for MINUIT to succeed.; #; # N u m e r i c i n t e g r a t i o n o f l a n d a u p d f; # ------------------------------------------------------------------; # Disable analytic integration from demonstration purposes; # Activate debug-level messages for topic integration to be able to follow; # actions below; # Calculate integral over landau with default choice of numeric integrator; # setprecision(15); # Same with custom configuration; # -----------------------------------------------------------; # Construct a custom configuration which uses the adaptive Gauss-Kronrod technique; # for closed 1D integrals; # Calculate integral over landau with custom integral specification; # Adjusting default config for a specific pdf; # -------------------------------------------------------------------------------------; # Another possibility: associate custom numeric int",MatchSource.CODE_COMMENT,tutorials/roofit/rf901_numintconfig.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf901_numintconfig.py
Deployability,configurat,configuration,"## \file; ## \ingroup tutorial_roofit; ## \notebook -nodraw; ## Numeric algorithm tuning: configuration and customization of how MC sampling algorithms; ## on specific pdfs are executed; ##; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Adjust global MC sampling strategy; # ------------------------------------------------------------------; # Example pdf for use below; # Change global strategy for 1D sampling problems without conditional observable; # (1st kFALSE) and without discrete observable (2nd kFALSE) from ROOT.RooFoamGenerator,; # ( an interface to the ROOT.TFoam MC generator with adaptive subdivisioning strategy ) to ROOT.RooAcceptReject,; # a plain accept/reject sampling algorithm [ ROOT.RooFit default before; # ROOT 5.23/04 ]; # Generate 10Kevt using ROOT.RooAcceptReject; # Adjusting default config for a specific pdf; # -------------------------------------------------------------------------------------; # Another possibility: associate custom MC sampling configuration as default for object 'model'; # The kTRUE argument will install a clone of the default configuration as specialized configuration; # for self model if none existed so far; # Adjusting parameters of a specific technique; # ---------------------------------------------------------------------------------------; # Adjust maximum number of steps of ROOT.RooIntegrator1D in the global; # default configuration; # Example of how to change the parameters of a numeric integrator; # (Each config section is a ROOT.RooArgSet with ROOT.RooRealVars holding real-valued parameters; # and ROOT.RooCategories holding parameters with a finite set of options); # Generate 10Kevt using ROOT.RooFoamGenerator (FOAM verbosity increased; # with above chatLevel adjustment for illustration purposes)",MatchSource.CODE_COMMENT,tutorials/roofit/rf902_numgenconfig.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf902_numgenconfig.py
Energy Efficiency,adapt,adaptive,"## \file; ## \ingroup tutorial_roofit; ## \notebook -nodraw; ## Numeric algorithm tuning: configuration and customization of how MC sampling algorithms; ## on specific pdfs are executed; ##; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Adjust global MC sampling strategy; # ------------------------------------------------------------------; # Example pdf for use below; # Change global strategy for 1D sampling problems without conditional observable; # (1st kFALSE) and without discrete observable (2nd kFALSE) from ROOT.RooFoamGenerator,; # ( an interface to the ROOT.TFoam MC generator with adaptive subdivisioning strategy ) to ROOT.RooAcceptReject,; # a plain accept/reject sampling algorithm [ ROOT.RooFit default before; # ROOT 5.23/04 ]; # Generate 10Kevt using ROOT.RooAcceptReject; # Adjusting default config for a specific pdf; # -------------------------------------------------------------------------------------; # Another possibility: associate custom MC sampling configuration as default for object 'model'; # The kTRUE argument will install a clone of the default configuration as specialized configuration; # for self model if none existed so far; # Adjusting parameters of a specific technique; # ---------------------------------------------------------------------------------------; # Adjust maximum number of steps of ROOT.RooIntegrator1D in the global; # default configuration; # Example of how to change the parameters of a numeric integrator; # (Each config section is a ROOT.RooArgSet with ROOT.RooRealVars holding real-valued parameters; # and ROOT.RooCategories holding parameters with a finite set of options); # Generate 10Kevt using ROOT.RooFoamGenerator (FOAM verbosity increased; # with above chatLevel adjustment for illustration purposes)",MatchSource.CODE_COMMENT,tutorials/roofit/rf902_numgenconfig.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf902_numgenconfig.py
Integrability,interface,interface,"## \file; ## \ingroup tutorial_roofit; ## \notebook -nodraw; ## Numeric algorithm tuning: configuration and customization of how MC sampling algorithms; ## on specific pdfs are executed; ##; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Adjust global MC sampling strategy; # ------------------------------------------------------------------; # Example pdf for use below; # Change global strategy for 1D sampling problems without conditional observable; # (1st kFALSE) and without discrete observable (2nd kFALSE) from ROOT.RooFoamGenerator,; # ( an interface to the ROOT.TFoam MC generator with adaptive subdivisioning strategy ) to ROOT.RooAcceptReject,; # a plain accept/reject sampling algorithm [ ROOT.RooFit default before; # ROOT 5.23/04 ]; # Generate 10Kevt using ROOT.RooAcceptReject; # Adjusting default config for a specific pdf; # -------------------------------------------------------------------------------------; # Another possibility: associate custom MC sampling configuration as default for object 'model'; # The kTRUE argument will install a clone of the default configuration as specialized configuration; # for self model if none existed so far; # Adjusting parameters of a specific technique; # ---------------------------------------------------------------------------------------; # Adjust maximum number of steps of ROOT.RooIntegrator1D in the global; # default configuration; # Example of how to change the parameters of a numeric integrator; # (Each config section is a ROOT.RooArgSet with ROOT.RooRealVars holding real-valued parameters; # and ROOT.RooCategories holding parameters with a finite set of options); # Generate 10Kevt using ROOT.RooFoamGenerator (FOAM verbosity increased; # with above chatLevel adjustment for illustration purposes)",MatchSource.CODE_COMMENT,tutorials/roofit/rf902_numgenconfig.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf902_numgenconfig.py
Modifiability,config,configuration,"## \file; ## \ingroup tutorial_roofit; ## \notebook -nodraw; ## Numeric algorithm tuning: configuration and customization of how MC sampling algorithms; ## on specific pdfs are executed; ##; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Adjust global MC sampling strategy; # ------------------------------------------------------------------; # Example pdf for use below; # Change global strategy for 1D sampling problems without conditional observable; # (1st kFALSE) and without discrete observable (2nd kFALSE) from ROOT.RooFoamGenerator,; # ( an interface to the ROOT.TFoam MC generator with adaptive subdivisioning strategy ) to ROOT.RooAcceptReject,; # a plain accept/reject sampling algorithm [ ROOT.RooFit default before; # ROOT 5.23/04 ]; # Generate 10Kevt using ROOT.RooAcceptReject; # Adjusting default config for a specific pdf; # -------------------------------------------------------------------------------------; # Another possibility: associate custom MC sampling configuration as default for object 'model'; # The kTRUE argument will install a clone of the default configuration as specialized configuration; # for self model if none existed so far; # Adjusting parameters of a specific technique; # ---------------------------------------------------------------------------------------; # Adjust maximum number of steps of ROOT.RooIntegrator1D in the global; # default configuration; # Example of how to change the parameters of a numeric integrator; # (Each config section is a ROOT.RooArgSet with ROOT.RooRealVars holding real-valued parameters; # and ROOT.RooCategories holding parameters with a finite set of options); # Generate 10Kevt using ROOT.RooFoamGenerator (FOAM verbosity increased; # with above chatLevel adjustment for illustration purposes)",MatchSource.CODE_COMMENT,tutorials/roofit/rf902_numgenconfig.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf902_numgenconfig.py
Availability,avail,available,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Numeric algorithm tuning: caching of slow numeric integrals and parameterizations of slow numeric integrals; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create, save or load workspace with pdf; # -----------------------------------------------------------------------------------; #; # Mode = 0 : Create workspace for plain running (no integral caching); # Mode = 1 : Generate workspace with precalculated integral and store it on file; # Mode = 2 : Load previously stored workspace from file; # Create empty workspace workspace; # Make a difficult to normalize pdf in 3 dimensions that is; # integrated numerically.; # Instruct model to precalculate normalization integral that integrate at least; # two dimensions numerically. In self specific case the integral value for; # all values of parameter 'a' are stored in a histogram and available for use; # in subsequent fitting and plotting operations (interpolation is; # applied); # w.pdf(""model"").setNormValueCaching(3); # Evaluate pdf once to trigger filling of cache; # Load preexisting workspace from file in mode==2; # Return created or loaded workspace; # Mode = 0 : Run plain fit (slow); # Mode = 1 : Generate workspace with precalculated integral and store it on file (prepare for accelerated running); # Mode = 2 : Run fit from previously stored workspace including cached; # integrals (fast, run in mode=1 first); # Create, save or load workspace with pdf; # -----------------------------------------------------------------------------------; # Make/load workspace, here in mode 1; # Show workspace that was created; # Show plot of cached integral values; # Use pdf from workspace for generation and fitting; # -----------------------------------------------------------------------------------; # ROOT.This is always slow (need to find maximum function value; # empirically in 3D sp",MatchSource.CODE_COMMENT,tutorials/roofit/rf903_numintcache.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf903_numintcache.py
Deployability,integrat,integrated,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Numeric algorithm tuning: caching of slow numeric integrals and parameterizations of slow numeric integrals; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create, save or load workspace with pdf; # -----------------------------------------------------------------------------------; #; # Mode = 0 : Create workspace for plain running (no integral caching); # Mode = 1 : Generate workspace with precalculated integral and store it on file; # Mode = 2 : Load previously stored workspace from file; # Create empty workspace workspace; # Make a difficult to normalize pdf in 3 dimensions that is; # integrated numerically.; # Instruct model to precalculate normalization integral that integrate at least; # two dimensions numerically. In self specific case the integral value for; # all values of parameter 'a' are stored in a histogram and available for use; # in subsequent fitting and plotting operations (interpolation is; # applied); # w.pdf(""model"").setNormValueCaching(3); # Evaluate pdf once to trigger filling of cache; # Load preexisting workspace from file in mode==2; # Return created or loaded workspace; # Mode = 0 : Run plain fit (slow); # Mode = 1 : Generate workspace with precalculated integral and store it on file (prepare for accelerated running); # Mode = 2 : Run fit from previously stored workspace including cached; # integrals (fast, run in mode=1 first); # Create, save or load workspace with pdf; # -----------------------------------------------------------------------------------; # Make/load workspace, here in mode 1; # Show workspace that was created; # Show plot of cached integral values; # Use pdf from workspace for generation and fitting; # -----------------------------------------------------------------------------------; # ROOT.This is always slow (need to find maximum function value; # empirically in 3D sp",MatchSource.CODE_COMMENT,tutorials/roofit/rf903_numintcache.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf903_numintcache.py
Integrability,integrat,integrated,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Numeric algorithm tuning: caching of slow numeric integrals and parameterizations of slow numeric integrals; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create, save or load workspace with pdf; # -----------------------------------------------------------------------------------; #; # Mode = 0 : Create workspace for plain running (no integral caching); # Mode = 1 : Generate workspace with precalculated integral and store it on file; # Mode = 2 : Load previously stored workspace from file; # Create empty workspace workspace; # Make a difficult to normalize pdf in 3 dimensions that is; # integrated numerically.; # Instruct model to precalculate normalization integral that integrate at least; # two dimensions numerically. In self specific case the integral value for; # all values of parameter 'a' are stored in a histogram and available for use; # in subsequent fitting and plotting operations (interpolation is; # applied); # w.pdf(""model"").setNormValueCaching(3); # Evaluate pdf once to trigger filling of cache; # Load preexisting workspace from file in mode==2; # Return created or loaded workspace; # Mode = 0 : Run plain fit (slow); # Mode = 1 : Generate workspace with precalculated integral and store it on file (prepare for accelerated running); # Mode = 2 : Run fit from previously stored workspace including cached; # integrals (fast, run in mode=1 first); # Create, save or load workspace with pdf; # -----------------------------------------------------------------------------------; # Make/load workspace, here in mode 1; # Show workspace that was created; # Show plot of cached integral values; # Use pdf from workspace for generation and fitting; # -----------------------------------------------------------------------------------; # ROOT.This is always slow (need to find maximum function value; # empirically in 3D sp",MatchSource.CODE_COMMENT,tutorials/roofit/rf903_numintcache.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf903_numintcache.py
Modifiability,parameteriz,parameterizations,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Numeric algorithm tuning: caching of slow numeric integrals and parameterizations of slow numeric integrals; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create, save or load workspace with pdf; # -----------------------------------------------------------------------------------; #; # Mode = 0 : Create workspace for plain running (no integral caching); # Mode = 1 : Generate workspace with precalculated integral and store it on file; # Mode = 2 : Load previously stored workspace from file; # Create empty workspace workspace; # Make a difficult to normalize pdf in 3 dimensions that is; # integrated numerically.; # Instruct model to precalculate normalization integral that integrate at least; # two dimensions numerically. In self specific case the integral value for; # all values of parameter 'a' are stored in a histogram and available for use; # in subsequent fitting and plotting operations (interpolation is; # applied); # w.pdf(""model"").setNormValueCaching(3); # Evaluate pdf once to trigger filling of cache; # Load preexisting workspace from file in mode==2; # Return created or loaded workspace; # Mode = 0 : Run plain fit (slow); # Mode = 1 : Generate workspace with precalculated integral and store it on file (prepare for accelerated running); # Mode = 2 : Run fit from previously stored workspace including cached; # integrals (fast, run in mode=1 first); # Create, save or load workspace with pdf; # -----------------------------------------------------------------------------------; # Make/load workspace, here in mode 1; # Show workspace that was created; # Show plot of cached integral values; # Use pdf from workspace for generation and fitting; # -----------------------------------------------------------------------------------; # ROOT.This is always slow (need to find maximum function value; # empirically in 3D sp",MatchSource.CODE_COMMENT,tutorials/roofit/rf903_numintcache.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf903_numintcache.py
Performance,load,load,"## \file; ## \ingroup tutorial_roofit; ## \notebook; ## Numeric algorithm tuning: caching of slow numeric integrals and parameterizations of slow numeric integrals; ##; ## \macro_image; ## \macro_code; ## \macro_output; ##; ## \date February 2018; ## \authors Clemens Lange, Wouter Verkerke (C++ version); # Create, save or load workspace with pdf; # -----------------------------------------------------------------------------------; #; # Mode = 0 : Create workspace for plain running (no integral caching); # Mode = 1 : Generate workspace with precalculated integral and store it on file; # Mode = 2 : Load previously stored workspace from file; # Create empty workspace workspace; # Make a difficult to normalize pdf in 3 dimensions that is; # integrated numerically.; # Instruct model to precalculate normalization integral that integrate at least; # two dimensions numerically. In self specific case the integral value for; # all values of parameter 'a' are stored in a histogram and available for use; # in subsequent fitting and plotting operations (interpolation is; # applied); # w.pdf(""model"").setNormValueCaching(3); # Evaluate pdf once to trigger filling of cache; # Load preexisting workspace from file in mode==2; # Return created or loaded workspace; # Mode = 0 : Run plain fit (slow); # Mode = 1 : Generate workspace with precalculated integral and store it on file (prepare for accelerated running); # Mode = 2 : Run fit from previously stored workspace including cached; # integrals (fast, run in mode=1 first); # Create, save or load workspace with pdf; # -----------------------------------------------------------------------------------; # Make/load workspace, here in mode 1; # Show workspace that was created; # Show plot of cached integral values; # Use pdf from workspace for generation and fitting; # -----------------------------------------------------------------------------------; # ROOT.This is always slow (need to find maximum function value; # empirically in 3D sp",MatchSource.CODE_COMMENT,tutorials/roofit/rf903_numintcache.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roofit/rf903_numintcache.py
Availability,error,errors," ## joint distribution for ""x"" and ""y"" can be factorized.; ## Generally, these regions have many events, so it the ratio can be; ## measured very precisely there. So we extend the model to describe the; ## left two boxes... denoted with ""bar"".; ## - In the upper left we observe nonbar events and expect bbar events; ## - In the bottom left we observe noffbar events and expect tau bbar events; ## Note again we have:; ##; ## ~~~{.cpp}; ## tau ~ <expectation off bar> / <expectation on bar>; ## ~~~; ##; ## One can further expand the model to account for the systematic associated; ## to assuming the distribution of ""x"" and ""y"" factorizes (eg. that; ## tau is the same for off/on and offbar/onbar). This can be done in several; ## ways, but here we introduce an additional parameter rho, which so that; ## one set of models will use tau and the other tau*rho. The choice is arbitrary,; ## but it has consequences on the numerical stability of the algorithms.; ## The ""bar"" measurements typically have more events (& smaller relative errors).; ## If we choose; ##; ## ~~~{.cpp}; ## <expectation noffbar> = tau * rho * <expectation noonbar>; ## ~~~; ##; ## the product tau*rho will be known very precisely (~1/sqrt(bbar)) and the contour; ## in those parameters will be narrow and have a non-trivial tau~1/rho shape.; ## However, if we choose to put rho on the non/noff measurements (where the; ## product will have an error `~1/sqrt(b))`, the contours will be more amenable; ## to numerical techniques. Thus, here we choose to define:; ##; ## ~~~{.cpp}; ## tau := <expectation off bar> / (<expectation on bar>); ## rho := <expectation off> / (<expectation on> * tau); ##; ## ^ y; ## |; ## |---------------------------+; ## | | |; ## | nonbar | non |; ## | bbar | s+b |; ## | | |; ## |---------------+-----------|; ## | | |; ## | noffbar | noff |; ## | tau bbar | tau b rho |; ## | | |; ## +-----------------------------> x; ## ~~~; ##; ## Left in this way, the problem is under-constrained. However, o",MatchSource.CODE_COMMENT,tutorials/roostats/FourBinInstructional.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/FourBinInstructional.py
Energy Efficiency,efficient,efficient,"ons:; # wspace.defineSet(""poi"",""s,rho""); # test simpler cases where parameters are known.; # wspace[""tau""].setConstant(); # wspace[""rho""].setConstant(); # wspace[""b""].setConstant(); # wspace[""bbar""].setConstant(); # inspect workspace; # wspace.Print(); # ----------------------------------------------------------; # Generate toy data; # generate toy data assuming current value of the parameters; # import into workspace.; # add Verbose() to see how it's being generated; # data.Print(""v""); # ----------------------------------; # Now the statistical tests; # model config; # wspace.writeToFile(""FourBin.root""); # -------------------------------------------------; # If you want to see the covariance matrix uncomment; # wspace[""model""].fitTo(data); # use ProfileLikelihood; # get ugly print out of the way. Fix.; # use FeldmaCousins (takes ~20 min); # number counting: dataset always has 1 entry with N events observed; # takes 7 minutes; # use BayesianCalculator (only 1-d parameter of interest, slow for this problem); # use MCMCCalculator (takes about 1 min); # Want an efficient proposal function, so derive it from covariance; # matrix of fit; # auto-create mean vars and add mappings; # first N steps to be ignored as burn-in; # make a central interval; # ----------------------------------; # Make some plots; # the plot takes a long time and print lots of error; # using a scan it is better; # ----------------------------------; # query intervals; # Profile Likelihood interval on s = [12.1902, 88.6871]; # Feldman Cousins interval on s = [18.75 +/- 2.45, 83.75 +/- 2.45]; # MCMC interval on s = [15.7628, 84.7266]; # TODO: The calculators have to be destructed first. Otherwise, we can get; # segmentation faults depending on the destruction order, which is random in; # Python. Probably the issue is that some object has a non-owning pointer to; # another object, which it uses in its destructor. This should be fixed either; # in the design of RooStats in C++, or with phythonizations.",MatchSource.CODE_COMMENT,tutorials/roostats/FourBinInstructional.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/FourBinInstructional.py
Integrability,depend,depends,"le is a generalization of the on/off problem.; ##; ## This example is a generalization of the on/off problem.; ## It's a common setup for SUSY searches. Imagine that one has two; ## variables ""x"" and ""y"" (eg. missing ET and SumET), see figure.; ## The signal region has high values of both of these variables (top right).; ## One can see low values of ""x"" or ""y"" acting as side-bands. If we; ## just used ""y"" as a sideband, we would have the on/off problem.; ## - In the signal region we observe non events and expect s+b events.; ## - In the region with low values of ""y"" (bottom right); ## we observe noff events and expect tau*b events.; ## Note the significance of tau. In the background only case:; ##; ## ~~~{.cpp}; ## tau ~ <expectation off> / <expectation on>; ## ~~~; ##; ## If tau is known, this model is sufficient, but often tau is not known exactly.; ## So one can use low values of ""x"" as an additional constraint for tau.; ## Note that this technique critically depends on the notion that the; ## joint distribution for ""x"" and ""y"" can be factorized.; ## Generally, these regions have many events, so it the ratio can be; ## measured very precisely there. So we extend the model to describe the; ## left two boxes... denoted with ""bar"".; ## - In the upper left we observe nonbar events and expect bbar events; ## - In the bottom left we observe noffbar events and expect tau bbar events; ## Note again we have:; ##; ## ~~~{.cpp}; ## tau ~ <expectation off bar> / <expectation on bar>; ## ~~~; ##; ## One can further expand the model to account for the systematic associated; ## to assuming the distribution of ""x"" and ""y"" factorizes (eg. that; ## tau is the same for off/on and offbar/onbar). This can be done in several; ## ways, but here we introduce an additional parameter rho, which so that; ## one set of models will use tau and the other tau*rho. The choice is arbitrary,; ## but it has consequences on the numerical stability of the algorithms.; ## The ""bar"" measurements typica",MatchSource.CODE_COMMENT,tutorials/roostats/FourBinInstructional.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/FourBinInstructional.py
Modifiability,variab,variables,"## \file; ## \ingroup tutorial_roostats; ## \notebook; ## This example is a generalization of the on/off problem.; ##; ## This example is a generalization of the on/off problem.; ## It's a common setup for SUSY searches. Imagine that one has two; ## variables ""x"" and ""y"" (eg. missing ET and SumET), see figure.; ## The signal region has high values of both of these variables (top right).; ## One can see low values of ""x"" or ""y"" acting as side-bands. If we; ## just used ""y"" as a sideband, we would have the on/off problem.; ## - In the signal region we observe non events and expect s+b events.; ## - In the region with low values of ""y"" (bottom right); ## we observe noff events and expect tau*b events.; ## Note the significance of tau. In the background only case:; ##; ## ~~~{.cpp}; ## tau ~ <expectation off> / <expectation on>; ## ~~~; ##; ## If tau is known, this model is sufficient, but often tau is not known exactly.; ## So one can use low values of ""x"" as an additional constraint for tau.; ## Note that this technique critically depends on the notion that the; ## joint distribution for ""x"" and ""y"" can be factorized.; ## Generally, these regions have many events, so it the ratio can be; ## measured very precisely there. So we extend the model to describe the; ## left two boxes... denoted with ""bar"".; ## - In the upper left we observe nonbar events and expect bbar events; ## - In the bottom left we observe noffbar events and expect tau bbar events; ## Note again we have:; ##; ## ~~~{.cpp}; ## tau ~ <expectation off bar> / <expectation on bar>; ## ~~~; ##; ## One can further expand the model to account for the systematic associated; ## to assuming the distribution of ""x"" and ""y"" factorizes (eg. that; ## tau is the same for off/on and offbar/onbar). This can be done in several; ## ways, but here we introduce an additional parameter rho, which so that; ## one set of models will use tau and the other tau*rho. The choice is arbitrary,; ## but it has consequences on the nume",MatchSource.CODE_COMMENT,tutorials/roostats/FourBinInstructional.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/FourBinInstructional.py
Testability,test,test,"o hold the model; ## - use workspace factory to quickly create the terms of the model; ## - use workspace factory to define total model (a prod pdf); ## - create a RooStats ModelConfig to specify observables, parameters of interest; ## - add to the ModelConfig a prior on the parameters for Bayesian techniques; ## note, the pdf it is factorized for parameters of interest & nuisance params; ## - visualize the model; ## - write the workspace to a file; ## - use several of RooStats IntervalCalculators & compare results; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \date July 2022; ## \authors Artem Busorgin, Kyle Cranmer and Tanja Rommerskirchen (C++ version); # let's time this challenging example; # set RooFit random seed for reproducible results; # make model; # ----------------------------------; # Control some interesting variations; # define parameers of interest; # for 1-d plots; # for 2-d plots to inspect correlations:; # wspace.defineSet(""poi"",""s,rho""); # test simpler cases where parameters are known.; # wspace[""tau""].setConstant(); # wspace[""rho""].setConstant(); # wspace[""b""].setConstant(); # wspace[""bbar""].setConstant(); # inspect workspace; # wspace.Print(); # ----------------------------------------------------------; # Generate toy data; # generate toy data assuming current value of the parameters; # import into workspace.; # add Verbose() to see how it's being generated; # data.Print(""v""); # ----------------------------------; # Now the statistical tests; # model config; # wspace.writeToFile(""FourBin.root""); # -------------------------------------------------; # If you want to see the covariance matrix uncomment; # wspace[""model""].fitTo(data); # use ProfileLikelihood; # get ugly print out of the way. Fix.; # use FeldmaCousins (takes ~20 min); # number counting: dataset always has 1 entry with N events observed; # takes 7 minutes; # use BayesianCalculator (only 1-d parameter of interest, slow for this problem); # use MCMCCalculator (takes a",MatchSource.CODE_COMMENT,tutorials/roostats/FourBinInstructional.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/FourBinInstructional.py
Usability,simpl,simpler,"o hold the model; ## - use workspace factory to quickly create the terms of the model; ## - use workspace factory to define total model (a prod pdf); ## - create a RooStats ModelConfig to specify observables, parameters of interest; ## - add to the ModelConfig a prior on the parameters for Bayesian techniques; ## note, the pdf it is factorized for parameters of interest & nuisance params; ## - visualize the model; ## - write the workspace to a file; ## - use several of RooStats IntervalCalculators & compare results; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \date July 2022; ## \authors Artem Busorgin, Kyle Cranmer and Tanja Rommerskirchen (C++ version); # let's time this challenging example; # set RooFit random seed for reproducible results; # make model; # ----------------------------------; # Control some interesting variations; # define parameers of interest; # for 1-d plots; # for 2-d plots to inspect correlations:; # wspace.defineSet(""poi"",""s,rho""); # test simpler cases where parameters are known.; # wspace[""tau""].setConstant(); # wspace[""rho""].setConstant(); # wspace[""b""].setConstant(); # wspace[""bbar""].setConstant(); # inspect workspace; # wspace.Print(); # ----------------------------------------------------------; # Generate toy data; # generate toy data assuming current value of the parameters; # import into workspace.; # add Verbose() to see how it's being generated; # data.Print(""v""); # ----------------------------------; # Now the statistical tests; # model config; # wspace.writeToFile(""FourBin.root""); # -------------------------------------------------; # If you want to see the covariance matrix uncomment; # wspace[""model""].fitTo(data); # use ProfileLikelihood; # get ugly print out of the way. Fix.; # use FeldmaCousins (takes ~20 min); # number counting: dataset always has 1 entry with N events observed; # takes 7 minutes; # use BayesianCalculator (only 1-d parameter of interest, slow for this problem); # use MCMCCalculator (takes a",MatchSource.CODE_COMMENT,tutorials/roostats/FourBinInstructional.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/FourBinInstructional.py
Deployability,integrat,integration,"# \file; # \ingroup tutorial_roostats; # \notebook -js; # Example demonstrating usage of HybridCalcultor; #; # A hypothesis testing example based on number counting; # with background uncertainty.; #; # NOTE: This example must be run with the ACLIC (the + option ) due to the; # new class that is defined.; #; # This example:; # - demonstrates the usage of the HybridCalcultor (Part 4-6); # - demonstrates the numerical integration of RooFit (Part 2); # - validates the RooStats against an example with a known analytic answer; # - demonstrates usage of different test statistics; # - explains subtle choices in the prior used for hybrid methods; # - demonstrates usage of different priors for the nuisance parameters; # - demonstrates usage of PROOF; #; # The basic setup here is that a main measurement has observed x events with an; # expectation of s+b. One can choose an ad hoc prior for the uncertainty on b,; # or try to base it on an auxiliary measurement. In this case, the auxiliary; # measurement (aka control measurement, sideband) is another counting experiment; # with measurement y and expectation tau*b. With an 'original prior' on b,; # called \f$\eta(b)\f$ then one can obtain a posterior from the auxiliary measurement; # \f$\pi(b) = \eta(b) * Pois(y|tau*b).\f$ This is a principled choice for a prior; # on b in the main measurement of x, which can then be treated in a hybrid; # Bayesian/Frequentist way. Additionally, one can try to treat the two; # measurements simultaneously, which is detailed in Part 6 of the tutorial.; #; # This tutorial is related to the FourBin.C tutorial in the modeling, but; # focuses on hypothesis testing instead of interval estimation.; #; # More background on this 'prototype problem' can be found in the; # following papers:; #; # - Evaluation of three methods for calculating statistical significance; # when incorporating a systematic uncertainty into a test of the; # background-only hypothesis for a Poisson process; # Authors: Robert D. Cous",MatchSource.CODE_COMMENT,tutorials/roostats/HybridInstructional.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/HybridInstructional.py
Energy Efficiency,green,green,"ent threshold on messages.; # ----------------------------------------------------; # P A R T 2 : D I R E C T I N T E G R A T I O N; # ====================================================; # This is not the 'RooStats' way, but in this case the distribution; # of the test statistic is simply x and can be calculated directly; # from the PDF using RooFit's built-in integration.; # Note, this does not generalize to situations in which the test statistic; # depends on many events (rows in a dataset).; # construct the Bayesian-averaged model (eg. a projection pdf); # $p'(x|s) = \int db p(x|s+b) * [ p(y|b) * prior(b) ]$; # lower message level; # plot it, red is averaged model, green is b known exactly, blue is s+b av model; # compare analytic calculation of Z_Bi; # with the numerical RooFit implementation of Z_Gamma; # for an example with x = 150, y = 100; # numeric RooFit Z_Gamma; # get ugly print messages out of the way; # set it back; # ---------------------------------------------; # P A R T 3 : A N A L Y T I C R E S U L T; # =============================================; # In this special case, the integrals are known analytically; # and they are implemented in NumberCountingUtils; # analytic Z_Bi; # -------------------------------------------------------------; # P A R T 4 : U S I N G H Y B R I D C A L C U L A T O R; # =============================================================; # Now we demonstrate the RooStats HybridCalculator.; #; # Like all RooStats calculators it needs the data and a ModelConfig; # for the relevant hypotheses. Since we are doing hypothesis testing; # we need a ModelConfig for the null (background only) and the alternate; # (signal+background) hypotheses. We also need to specify the PDF,; # the parameters of interest, and the observables. Furthermore, since; # the parameter of interest is floating, we need to specify which values; # of the parameter corresponds to the null and alternate (eg. s=0 and s=50); #; # define some sets of variables obs",MatchSource.CODE_COMMENT,tutorials/roostats/HybridInstructional.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/HybridInstructional.py
Integrability,integrat,integration,"# \file; # \ingroup tutorial_roostats; # \notebook -js; # Example demonstrating usage of HybridCalcultor; #; # A hypothesis testing example based on number counting; # with background uncertainty.; #; # NOTE: This example must be run with the ACLIC (the + option ) due to the; # new class that is defined.; #; # This example:; # - demonstrates the usage of the HybridCalcultor (Part 4-6); # - demonstrates the numerical integration of RooFit (Part 2); # - validates the RooStats against an example with a known analytic answer; # - demonstrates usage of different test statistics; # - explains subtle choices in the prior used for hybrid methods; # - demonstrates usage of different priors for the nuisance parameters; # - demonstrates usage of PROOF; #; # The basic setup here is that a main measurement has observed x events with an; # expectation of s+b. One can choose an ad hoc prior for the uncertainty on b,; # or try to base it on an auxiliary measurement. In this case, the auxiliary; # measurement (aka control measurement, sideband) is another counting experiment; # with measurement y and expectation tau*b. With an 'original prior' on b,; # called \f$\eta(b)\f$ then one can obtain a posterior from the auxiliary measurement; # \f$\pi(b) = \eta(b) * Pois(y|tau*b).\f$ This is a principled choice for a prior; # on b in the main measurement of x, which can then be treated in a hybrid; # Bayesian/Frequentist way. Additionally, one can try to treat the two; # measurements simultaneously, which is detailed in Part 6 of the tutorial.; #; # This tutorial is related to the FourBin.C tutorial in the modeling, but; # focuses on hypothesis testing instead of interval estimation.; #; # More background on this 'prototype problem' can be found in the; # following papers:; #; # - Evaluation of three methods for calculating statistical significance; # when incorporating a systematic uncertainty into a test of the; # background-only hypothesis for a Poisson process; # Authors: Robert D. Cous",MatchSource.CODE_COMMENT,tutorials/roostats/HybridInstructional.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/HybridInstructional.py
Modifiability,config,configuration,"ich is detailed in Part 6 of the tutorial.; #; # This tutorial is related to the FourBin.C tutorial in the modeling, but; # focuses on hypothesis testing instead of interval estimation.; #; # More background on this 'prototype problem' can be found in the; # following papers:; #; # - Evaluation of three methods for calculating statistical significance; # when incorporating a systematic uncertainty into a test of the; # background-only hypothesis for a Poisson process; # Authors: Robert D. Cousins, James T. Linnemann, Jordan Tucker; # http://arxiv.org/abs/physics/0702156; # NIM A 595 (2008) 480--501; #; # - Statistical Challenges for Searches for New Physics at the LHC; # Author: Kyle Cranmer; # http://arxiv.org/abs/physics/0511028; #; # - Measures of Significance in HEP and Astrophysics; # Authors J. T. Linnemann; # http://arxiv.org/abs/physics/0312059; #; # \macro_image; # \macro_output; # \macro_code; #; # \authors Kyle Cranmer, Wouter Verkerke, Sven Kreiss, and Jolly Chen (Python translation); # User configuration parameters; # ratio Ntoys Null/ntoys ALT; # ----------------------------------; # A New Test Statistic Class for this example.; # It simply returns the sum of the values in a particular; # column of a dataset.; # You can ignore this class and focus on the macro below; """"""; using namespace RooFit;; using namespace RooStats;. class BinCountTestStat : public TestStatistic {; public:; BinCountTestStat(void) : fColumnName(""tmp"") {}; BinCountTestStat(string columnName) : fColumnName(columnName) {}. virtual Double_t Evaluate(RooAbsData &data, RooArgSet & /*nullPOI*/); {; // This is the main method in the interface; Double_t value = 0.0;; for (int i = 0; i < data.numEntries(); i++) {; value += data.get(i)->getRealValue(fColumnName.c_str());; }; return value;; }; virtual const TString GetVarName() const { return fColumnName; }. private:; string fColumnName;. protected:; ClassDef(BinCountTestStat, 1); };; """"""; # ----------------------------------; # The Actual Tu",MatchSource.CODE_COMMENT,tutorials/roostats/HybridInstructional.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/HybridInstructional.py
Safety,avoid,avoid,"};; """"""; # ----------------------------------; # The Actual Tutorial Macro; # This tutorial has 6 parts; # Table of Contents; # Setup; # 1. Make the model for the 'prototype problem'; # Special cases; # 2. Use RooFit's direct integration to get p-value & significance; # 3. Use RooStats analytic solution for this problem; # RooStats HybridCalculator -- can be generalized; # 4. RooStats ToyMC version of 2. & 3.; # 5. RooStats ToyMC with an equivalent test statistic; # 6. RooStats ToyMC with simultaneous control & main measurement; # It takes ~4 min without PROOF and ~2 min with PROOF on 4 cores.; # Of course, everything looks nicer with more toys, which takes longer.; # ----------------------------------------------------; # P A R T 1 : D I R E C T I N T E G R A T I O N; # ====================================================; # Make model for prototype on/off problem; # $Pois(x | s+b) * Pois(y | tau b )$; # for Z_Gamma, use uniform prior on b.; # We will control the output level in a few places to avoid; # verbose progress messages. We start by keeping track; # of the current threshold on messages.; # ----------------------------------------------------; # P A R T 2 : D I R E C T I N T E G R A T I O N; # ====================================================; # This is not the 'RooStats' way, but in this case the distribution; # of the test statistic is simply x and can be calculated directly; # from the PDF using RooFit's built-in integration.; # Note, this does not generalize to situations in which the test statistic; # depends on many events (rows in a dataset).; # construct the Bayesian-averaged model (eg. a projection pdf); # $p'(x|s) = \int db p(x|s+b) * [ p(y|b) * prior(b) ]$; # lower message level; # plot it, red is averaged model, green is b known exactly, blue is s+b av model; # compare analytic calculation of Z_Bi; # with the numerical RooFit implementation of Z_Gamma; # for an example with x = 150, y = 100; # numeric RooFit Z_Gamma; # get ugly print messages ",MatchSource.CODE_COMMENT,tutorials/roostats/HybridInstructional.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/HybridInstructional.py
Security,validat,validates,"# \file; # \ingroup tutorial_roostats; # \notebook -js; # Example demonstrating usage of HybridCalcultor; #; # A hypothesis testing example based on number counting; # with background uncertainty.; #; # NOTE: This example must be run with the ACLIC (the + option ) due to the; # new class that is defined.; #; # This example:; # - demonstrates the usage of the HybridCalcultor (Part 4-6); # - demonstrates the numerical integration of RooFit (Part 2); # - validates the RooStats against an example with a known analytic answer; # - demonstrates usage of different test statistics; # - explains subtle choices in the prior used for hybrid methods; # - demonstrates usage of different priors for the nuisance parameters; # - demonstrates usage of PROOF; #; # The basic setup here is that a main measurement has observed x events with an; # expectation of s+b. One can choose an ad hoc prior for the uncertainty on b,; # or try to base it on an auxiliary measurement. In this case, the auxiliary; # measurement (aka control measurement, sideband) is another counting experiment; # with measurement y and expectation tau*b. With an 'original prior' on b,; # called \f$\eta(b)\f$ then one can obtain a posterior from the auxiliary measurement; # \f$\pi(b) = \eta(b) * Pois(y|tau*b).\f$ This is a principled choice for a prior; # on b in the main measurement of x, which can then be treated in a hybrid; # Bayesian/Frequentist way. Additionally, one can try to treat the two; # measurements simultaneously, which is detailed in Part 6 of the tutorial.; #; # This tutorial is related to the FourBin.C tutorial in the modeling, but; # focuses on hypothesis testing instead of interval estimation.; #; # More background on this 'prototype problem' can be found in the; # following papers:; #; # - Evaluation of three methods for calculating statistical significance; # when incorporating a systematic uncertainty into a test of the; # background-only hypothesis for a Poisson process; # Authors: Robert D. Cous",MatchSource.CODE_COMMENT,tutorials/roostats/HybridInstructional.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/HybridInstructional.py
Testability,test,testing,"# \file; # \ingroup tutorial_roostats; # \notebook -js; # Example demonstrating usage of HybridCalcultor; #; # A hypothesis testing example based on number counting; # with background uncertainty.; #; # NOTE: This example must be run with the ACLIC (the + option ) due to the; # new class that is defined.; #; # This example:; # - demonstrates the usage of the HybridCalcultor (Part 4-6); # - demonstrates the numerical integration of RooFit (Part 2); # - validates the RooStats against an example with a known analytic answer; # - demonstrates usage of different test statistics; # - explains subtle choices in the prior used for hybrid methods; # - demonstrates usage of different priors for the nuisance parameters; # - demonstrates usage of PROOF; #; # The basic setup here is that a main measurement has observed x events with an; # expectation of s+b. One can choose an ad hoc prior for the uncertainty on b,; # or try to base it on an auxiliary measurement. In this case, the auxiliary; # measurement (aka control measurement, sideband) is another counting experiment; # with measurement y and expectation tau*b. With an 'original prior' on b,; # called \f$\eta(b)\f$ then one can obtain a posterior from the auxiliary measurement; # \f$\pi(b) = \eta(b) * Pois(y|tau*b).\f$ This is a principled choice for a prior; # on b in the main measurement of x, which can then be treated in a hybrid; # Bayesian/Frequentist way. Additionally, one can try to treat the two; # measurements simultaneously, which is detailed in Part 6 of the tutorial.; #; # This tutorial is related to the FourBin.C tutorial in the modeling, but; # focuses on hypothesis testing instead of interval estimation.; #; # More background on this 'prototype problem' can be found in the; # following papers:; #; # - Evaluation of three methods for calculating statistical significance; # when incorporating a systematic uncertainty into a test of the; # background-only hypothesis for a Poisson process; # Authors: Robert D. Cous",MatchSource.CODE_COMMENT,tutorials/roostats/HybridInstructional.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/HybridInstructional.py
Usability,simpl,simply,"kground on this 'prototype problem' can be found in the; # following papers:; #; # - Evaluation of three methods for calculating statistical significance; # when incorporating a systematic uncertainty into a test of the; # background-only hypothesis for a Poisson process; # Authors: Robert D. Cousins, James T. Linnemann, Jordan Tucker; # http://arxiv.org/abs/physics/0702156; # NIM A 595 (2008) 480--501; #; # - Statistical Challenges for Searches for New Physics at the LHC; # Author: Kyle Cranmer; # http://arxiv.org/abs/physics/0511028; #; # - Measures of Significance in HEP and Astrophysics; # Authors J. T. Linnemann; # http://arxiv.org/abs/physics/0312059; #; # \macro_image; # \macro_output; # \macro_code; #; # \authors Kyle Cranmer, Wouter Verkerke, Sven Kreiss, and Jolly Chen (Python translation); # User configuration parameters; # ratio Ntoys Null/ntoys ALT; # ----------------------------------; # A New Test Statistic Class for this example.; # It simply returns the sum of the values in a particular; # column of a dataset.; # You can ignore this class and focus on the macro below; """"""; using namespace RooFit;; using namespace RooStats;. class BinCountTestStat : public TestStatistic {; public:; BinCountTestStat(void) : fColumnName(""tmp"") {}; BinCountTestStat(string columnName) : fColumnName(columnName) {}. virtual Double_t Evaluate(RooAbsData &data, RooArgSet & /*nullPOI*/); {; // This is the main method in the interface; Double_t value = 0.0;; for (int i = 0; i < data.numEntries(); i++) {; value += data.get(i)->getRealValue(fColumnName.c_str());; }; return value;; }; virtual const TString GetVarName() const { return fColumnName; }. private:; string fColumnName;. protected:; ClassDef(BinCountTestStat, 1); };; """"""; # ----------------------------------; # The Actual Tutorial Macro; # This tutorial has 6 parts; # Table of Contents; # Setup; # 1. Make the model for the 'prototype problem'; # Special cases; # 2. Use RooFit's direct integration to get p-value & signific",MatchSource.CODE_COMMENT,tutorials/roostats/HybridInstructional.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/HybridInstructional.py
Deployability,integrat,integration,"onding to N events; # and the extended term is Pois(N|S+B); #; # - fractional form: non-extended PDF of some discriminating variable m:; # eg: P(m) ~ s*fs(m) + (1-s)*fb(m), where s is a signal fraction; # in this case the dataset has N rows corresponding to N events; # and there is no extended term; #; # - number counting form: in which there is no discriminating variable; # and the counts are modeled directly (see HybridInstructional); # eg: P(N) = Pois(N|S+B); # in this case the dataset has 1 row corresponding to N events; # and the extended term is the PDF itself.; #; # Here we convert the number counting form into the standard form by; # introducing a dummy discriminating variable m with a uniform distribution.; #; # This example:; # - demonstrates the usage of the HybridCalcultor (Part 4-6); # - demonstrates the numerical integration of RooFit (Part 2); # - validates the RooStats against an example with a known analytic answer; # - demonstrates usage of different test statistics; # - explains subtle choices in the prior used for hybrid methods; # - demonstrates usage of different priors for the nuisance parameters; # - demonstrates usage of PROOF; #; # The basic setup here is that a main measurement has observed x events with an; # expectation of s+b. One can choose an ad hoc prior for the uncertainty on b,; # or try to base it on an auxiliary measurement. In this case, the auxiliary; # measurement (aka control measurement, sideband) is another counting experiment; # with measurement y and expectation tau*b. With an 'original prior' on b,; # called \f$ \eta(b) \f$ then one can obtain a posterior from the auxiliary measurement; # \f$ \pi(b) = \eta(b) * Pois(y|tau*b) \f$. This is a principled choice for a prior; # on b in the main measurement of x, which can then be treated in a hybrid; # Bayesian/Frequentist way. Additionally, one can try to treat the two; # measurements simultaneously, which is detailed in Part 6 of the tutorial.; #; # This tutorial is related ",MatchSource.CODE_COMMENT,tutorials/roostats/HybridStandardForm.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/HybridStandardForm.py
Integrability,integrat,integration,"onding to N events; # and the extended term is Pois(N|S+B); #; # - fractional form: non-extended PDF of some discriminating variable m:; # eg: P(m) ~ s*fs(m) + (1-s)*fb(m), where s is a signal fraction; # in this case the dataset has N rows corresponding to N events; # and there is no extended term; #; # - number counting form: in which there is no discriminating variable; # and the counts are modeled directly (see HybridInstructional); # eg: P(N) = Pois(N|S+B); # in this case the dataset has 1 row corresponding to N events; # and the extended term is the PDF itself.; #; # Here we convert the number counting form into the standard form by; # introducing a dummy discriminating variable m with a uniform distribution.; #; # This example:; # - demonstrates the usage of the HybridCalcultor (Part 4-6); # - demonstrates the numerical integration of RooFit (Part 2); # - validates the RooStats against an example with a known analytic answer; # - demonstrates usage of different test statistics; # - explains subtle choices in the prior used for hybrid methods; # - demonstrates usage of different priors for the nuisance parameters; # - demonstrates usage of PROOF; #; # The basic setup here is that a main measurement has observed x events with an; # expectation of s+b. One can choose an ad hoc prior for the uncertainty on b,; # or try to base it on an auxiliary measurement. In this case, the auxiliary; # measurement (aka control measurement, sideband) is another counting experiment; # with measurement y and expectation tau*b. With an 'original prior' on b,; # called \f$ \eta(b) \f$ then one can obtain a posterior from the auxiliary measurement; # \f$ \pi(b) = \eta(b) * Pois(y|tau*b) \f$. This is a principled choice for a prior; # on b in the main measurement of x, which can then be treated in a hybrid; # Bayesian/Frequentist way. Additionally, one can try to treat the two; # measurements simultaneously, which is detailed in Part 6 of the tutorial.; #; # This tutorial is related ",MatchSource.CODE_COMMENT,tutorials/roostats/HybridStandardForm.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/HybridStandardForm.py
Modifiability,extend,extended,"# \file; # \ingroup tutorial_roostats; # \notebook -js; # A hypothesis testing example based on number counting with background uncertainty.; #; # A hypothesis testing example based on number counting; # with background uncertainty.; #; # NOTE: This example is like HybridInstructional, but the model is more clearly; # generalizable to an analysis with shapes. There is a lot of flexibility; # for how one models a problem in RooFit/RooStats. Models come in a few; # common forms:; # - standard form: extended PDF of some discriminating variable m:; # eg: P(m) ~ S*fs(m) + B*fb(m), with S+B events expected; # in this case the dataset has N rows corresponding to N events; # and the extended term is Pois(N|S+B); #; # - fractional form: non-extended PDF of some discriminating variable m:; # eg: P(m) ~ s*fs(m) + (1-s)*fb(m), where s is a signal fraction; # in this case the dataset has N rows corresponding to N events; # and there is no extended term; #; # - number counting form: in which there is no discriminating variable; # and the counts are modeled directly (see HybridInstructional); # eg: P(N) = Pois(N|S+B); # in this case the dataset has 1 row corresponding to N events; # and the extended term is the PDF itself.; #; # Here we convert the number counting form into the standard form by; # introducing a dummy discriminating variable m with a uniform distribution.; #; # This example:; # - demonstrates the usage of the HybridCalcultor (Part 4-6); # - demonstrates the numerical integration of RooFit (Part 2); # - validates the RooStats against an example with a known analytic answer; # - demonstrates usage of different test statistics; # - explains subtle choices in the prior used for hybrid methods; # - demonstrates usage of different priors for the nuisance parameters; # - demonstrates usage of PROOF; #; # The basic setup here is that a main measurement has observed x events with an; # expectation of s+b. One can choose an ad hoc prior for the uncertainty on b,; # or try to ",MatchSource.CODE_COMMENT,tutorials/roostats/HybridStandardForm.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/HybridStandardForm.py
Safety,avoid,avoid,"otype problem'; # Special cases; # 2. NOT RELEVANT HERE; # 3. Use RooStats analytic solution for this problem; # RooStats HybridCalculator -- can be generalized; # 4. RooStats ToyMC version of 2. & 3.; # 5. RooStats ToyMC with an equivalent test statistic; # 6. RooStats ToyMC with simultaneous control & main measurement; # Part 4 takes ~4 min without PROOF.; # Part 5 takes about ~2 min with PROOF on 4 cores.; # Of course, everything looks nicer with more toys, which takes longer.; # -----------------------------------------------------; # P A R T 1 : D I R E C T I N T E G R A T I O N; # ====================================================; # Make model for prototype on/off problem; # Pois(x | s+b) * Pois(y | tau b ); # for Z_Gamma, use uniform prior on b.; # replace the pdf in 'number counting form'; # w.factory(""Poisson::px(x[150,0,500],sum::splusb(s[0,0,100],b[100,0,300]))""); # with one in standard form. Now x is encoded in event count; # m is a dummy discriminating variable; # We will control the output level in a few places to avoid; # verbose progress messages. We start by keeping track; # of the current threshold on messages.; # -----------------------------------------------; # P A R T 3 : A N A L Y T I C R E S U L T; # ==============================================; # In this special case, the integrals are known analytically; # and they are implemented in RooStats::NumberCountingUtils; # analytic Z_Bi; # --------------------------------------------------------------; # P A R T 4 : U S I N G H Y B R I D C A L C U L A T O R; # ==============================================================; # Now we demonstrate the RooStats HybridCalculator.; #; # Like all RooStats calculators it needs the data and a ModelConfig; # for the relevant hypotheses. Since we are doing hypothesis testing; # we need a ModelConfig for the null (background only) and the alternate; # (signal+background) hypotheses. We also need to specify the PDF,; # the parameters of interest, and the ob",MatchSource.CODE_COMMENT,tutorials/roostats/HybridStandardForm.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/HybridStandardForm.py
Security,validat,validates,"onding to N events; # and the extended term is Pois(N|S+B); #; # - fractional form: non-extended PDF of some discriminating variable m:; # eg: P(m) ~ s*fs(m) + (1-s)*fb(m), where s is a signal fraction; # in this case the dataset has N rows corresponding to N events; # and there is no extended term; #; # - number counting form: in which there is no discriminating variable; # and the counts are modeled directly (see HybridInstructional); # eg: P(N) = Pois(N|S+B); # in this case the dataset has 1 row corresponding to N events; # and the extended term is the PDF itself.; #; # Here we convert the number counting form into the standard form by; # introducing a dummy discriminating variable m with a uniform distribution.; #; # This example:; # - demonstrates the usage of the HybridCalcultor (Part 4-6); # - demonstrates the numerical integration of RooFit (Part 2); # - validates the RooStats against an example with a known analytic answer; # - demonstrates usage of different test statistics; # - explains subtle choices in the prior used for hybrid methods; # - demonstrates usage of different priors for the nuisance parameters; # - demonstrates usage of PROOF; #; # The basic setup here is that a main measurement has observed x events with an; # expectation of s+b. One can choose an ad hoc prior for the uncertainty on b,; # or try to base it on an auxiliary measurement. In this case, the auxiliary; # measurement (aka control measurement, sideband) is another counting experiment; # with measurement y and expectation tau*b. With an 'original prior' on b,; # called \f$ \eta(b) \f$ then one can obtain a posterior from the auxiliary measurement; # \f$ \pi(b) = \eta(b) * Pois(y|tau*b) \f$. This is a principled choice for a prior; # on b in the main measurement of x, which can then be treated in a hybrid; # Bayesian/Frequentist way. Additionally, one can try to treat the two; # measurements simultaneously, which is detailed in Part 6 of the tutorial.; #; # This tutorial is related ",MatchSource.CODE_COMMENT,tutorials/roostats/HybridStandardForm.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/HybridStandardForm.py
Testability,test,testing,"# \file; # \ingroup tutorial_roostats; # \notebook -js; # A hypothesis testing example based on number counting with background uncertainty.; #; # A hypothesis testing example based on number counting; # with background uncertainty.; #; # NOTE: This example is like HybridInstructional, but the model is more clearly; # generalizable to an analysis with shapes. There is a lot of flexibility; # for how one models a problem in RooFit/RooStats. Models come in a few; # common forms:; # - standard form: extended PDF of some discriminating variable m:; # eg: P(m) ~ S*fs(m) + B*fb(m), with S+B events expected; # in this case the dataset has N rows corresponding to N events; # and the extended term is Pois(N|S+B); #; # - fractional form: non-extended PDF of some discriminating variable m:; # eg: P(m) ~ s*fs(m) + (1-s)*fb(m), where s is a signal fraction; # in this case the dataset has N rows corresponding to N events; # and there is no extended term; #; # - number counting form: in which there is no discriminating variable; # and the counts are modeled directly (see HybridInstructional); # eg: P(N) = Pois(N|S+B); # in this case the dataset has 1 row corresponding to N events; # and the extended term is the PDF itself.; #; # Here we convert the number counting form into the standard form by; # introducing a dummy discriminating variable m with a uniform distribution.; #; # This example:; # - demonstrates the usage of the HybridCalcultor (Part 4-6); # - demonstrates the numerical integration of RooFit (Part 2); # - validates the RooStats against an example with a known analytic answer; # - demonstrates usage of different test statistics; # - explains subtle choices in the prior used for hybrid methods; # - demonstrates usage of different priors for the nuisance parameters; # - demonstrates usage of PROOF; #; # The basic setup here is that a main measurement has observed x events with an; # expectation of s+b. One can choose an ad hoc prior for the uncertainty on b,; # or try to ",MatchSource.CODE_COMMENT,tutorials/roostats/HybridStandardForm.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/HybridStandardForm.py
Usability,clear,clearly,"# \file; # \ingroup tutorial_roostats; # \notebook -js; # A hypothesis testing example based on number counting with background uncertainty.; #; # A hypothesis testing example based on number counting; # with background uncertainty.; #; # NOTE: This example is like HybridInstructional, but the model is more clearly; # generalizable to an analysis with shapes. There is a lot of flexibility; # for how one models a problem in RooFit/RooStats. Models come in a few; # common forms:; # - standard form: extended PDF of some discriminating variable m:; # eg: P(m) ~ S*fs(m) + B*fb(m), with S+B events expected; # in this case the dataset has N rows corresponding to N events; # and the extended term is Pois(N|S+B); #; # - fractional form: non-extended PDF of some discriminating variable m:; # eg: P(m) ~ s*fs(m) + (1-s)*fb(m), where s is a signal fraction; # in this case the dataset has N rows corresponding to N events; # and there is no extended term; #; # - number counting form: in which there is no discriminating variable; # and the counts are modeled directly (see HybridInstructional); # eg: P(N) = Pois(N|S+B); # in this case the dataset has 1 row corresponding to N events; # and the extended term is the PDF itself.; #; # Here we convert the number counting form into the standard form by; # introducing a dummy discriminating variable m with a uniform distribution.; #; # This example:; # - demonstrates the usage of the HybridCalcultor (Part 4-6); # - demonstrates the numerical integration of RooFit (Part 2); # - validates the RooStats against an example with a known analytic answer; # - demonstrates usage of different test statistics; # - explains subtle choices in the prior used for hybrid methods; # - demonstrates usage of different priors for the nuisance parameters; # - demonstrates usage of PROOF; #; # The basic setup here is that a main measurement has observed x events with an; # expectation of s+b. One can choose an ad hoc prior for the uncertainty on b,; # or try to ",MatchSource.CODE_COMMENT,tutorials/roostats/HybridStandardForm.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/HybridStandardForm.py
Availability,fault,faults,"idence intervals with four techniques.; ## The model is a Normal Gaussian G(x|mu,sigma) with 100 samples of x.; ## The answer is known analytically, so this is a good example to validate; ## the RooStats tools.; ##; ## - expected interval is [-0.162917, 0.229075]; ## - plc interval is [-0.162917, 0.229075]; ## - fc interval is [-0.17 , 0.23] // stepsize is 0.01; ## - bc interval is [-0.162918, 0.229076]; ## - mcmc interval is [-0.166999, 0.230224]; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \date July 2022; ## \authors Artem Busorgin, Kyle Cranmer (C++ version); # Time this macro; # set RooFit random seed for reproducible results; # make a simple model via the workspace factory; # specify components of model for statistical tools; # create a toy dataset; # for convenience later on; # set confidence level; # example use profile likelihood calculator; # example use of Feldman-Cousins; # number of points to test per parameter; # make it go faster; # Here, we consider only ensembles with 100 events; # The PDF could be extended and this could be removed; # example use of BayesianCalculator; # now we also need to specify a prior in the ModelConfig; # example usage of BayesianCalculator; # example use of MCMCInterval; # special options; # bins used internally for representing posterior; # first N steps to be ignored as burn-in; # how long to run chain; # for central interval; # for this example we know the expected intervals; # Use the intervals; # make a reasonable style; # some plots; # plot the data; # plot the profile likelihood; # plot the MCMC interval; # TODO: The BayesianCalculator and MCMCCalculator have to be destructed first.; # Otherwise, we can get segmentation faults depending on the destruction order,; # which is random in Python. Probably the issue is that some object has a; # non-owning pointer to another object, which it uses in its destructor. This; # should be fixed either in the design of RooStats in C++, or with; # phythonizations.",MatchSource.CODE_COMMENT,tutorials/roostats/IntervalExamples.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/IntervalExamples.py
Integrability,depend,depending,"idence intervals with four techniques.; ## The model is a Normal Gaussian G(x|mu,sigma) with 100 samples of x.; ## The answer is known analytically, so this is a good example to validate; ## the RooStats tools.; ##; ## - expected interval is [-0.162917, 0.229075]; ## - plc interval is [-0.162917, 0.229075]; ## - fc interval is [-0.17 , 0.23] // stepsize is 0.01; ## - bc interval is [-0.162918, 0.229076]; ## - mcmc interval is [-0.166999, 0.230224]; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \date July 2022; ## \authors Artem Busorgin, Kyle Cranmer (C++ version); # Time this macro; # set RooFit random seed for reproducible results; # make a simple model via the workspace factory; # specify components of model for statistical tools; # create a toy dataset; # for convenience later on; # set confidence level; # example use profile likelihood calculator; # example use of Feldman-Cousins; # number of points to test per parameter; # make it go faster; # Here, we consider only ensembles with 100 events; # The PDF could be extended and this could be removed; # example use of BayesianCalculator; # now we also need to specify a prior in the ModelConfig; # example usage of BayesianCalculator; # example use of MCMCInterval; # special options; # bins used internally for representing posterior; # first N steps to be ignored as burn-in; # how long to run chain; # for central interval; # for this example we know the expected intervals; # Use the intervals; # make a reasonable style; # some plots; # plot the data; # plot the profile likelihood; # plot the MCMC interval; # TODO: The BayesianCalculator and MCMCCalculator have to be destructed first.; # Otherwise, we can get segmentation faults depending on the destruction order,; # which is random in Python. Probably the issue is that some object has a; # non-owning pointer to another object, which it uses in its destructor. This; # should be fixed either in the design of RooStats in C++, or with; # phythonizations.",MatchSource.CODE_COMMENT,tutorials/roostats/IntervalExamples.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/IntervalExamples.py
Modifiability,extend,extended,"idence intervals with four techniques.; ## The model is a Normal Gaussian G(x|mu,sigma) with 100 samples of x.; ## The answer is known analytically, so this is a good example to validate; ## the RooStats tools.; ##; ## - expected interval is [-0.162917, 0.229075]; ## - plc interval is [-0.162917, 0.229075]; ## - fc interval is [-0.17 , 0.23] // stepsize is 0.01; ## - bc interval is [-0.162918, 0.229076]; ## - mcmc interval is [-0.166999, 0.230224]; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \date July 2022; ## \authors Artem Busorgin, Kyle Cranmer (C++ version); # Time this macro; # set RooFit random seed for reproducible results; # make a simple model via the workspace factory; # specify components of model for statistical tools; # create a toy dataset; # for convenience later on; # set confidence level; # example use profile likelihood calculator; # example use of Feldman-Cousins; # number of points to test per parameter; # make it go faster; # Here, we consider only ensembles with 100 events; # The PDF could be extended and this could be removed; # example use of BayesianCalculator; # now we also need to specify a prior in the ModelConfig; # example usage of BayesianCalculator; # example use of MCMCInterval; # special options; # bins used internally for representing posterior; # first N steps to be ignored as burn-in; # how long to run chain; # for central interval; # for this example we know the expected intervals; # Use the intervals; # make a reasonable style; # some plots; # plot the data; # plot the profile likelihood; # plot the MCMC interval; # TODO: The BayesianCalculator and MCMCCalculator have to be destructed first.; # Otherwise, we can get segmentation faults depending on the destruction order,; # which is random in Python. Probably the issue is that some object has a; # non-owning pointer to another object, which it uses in its destructor. This; # should be fixed either in the design of RooStats in C++, or with; # phythonizations.",MatchSource.CODE_COMMENT,tutorials/roostats/IntervalExamples.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/IntervalExamples.py
Security,validat,validate,"## \file; ## \ingroup tutorial_roostats; ## \notebook; ## Example showing confidence intervals with four techniques.; ##; ## An example that shows confidence intervals with four techniques.; ## The model is a Normal Gaussian G(x|mu,sigma) with 100 samples of x.; ## The answer is known analytically, so this is a good example to validate; ## the RooStats tools.; ##; ## - expected interval is [-0.162917, 0.229075]; ## - plc interval is [-0.162917, 0.229075]; ## - fc interval is [-0.17 , 0.23] // stepsize is 0.01; ## - bc interval is [-0.162918, 0.229076]; ## - mcmc interval is [-0.166999, 0.230224]; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \date July 2022; ## \authors Artem Busorgin, Kyle Cranmer (C++ version); # Time this macro; # set RooFit random seed for reproducible results; # make a simple model via the workspace factory; # specify components of model for statistical tools; # create a toy dataset; # for convenience later on; # set confidence level; # example use profile likelihood calculator; # example use of Feldman-Cousins; # number of points to test per parameter; # make it go faster; # Here, we consider only ensembles with 100 events; # The PDF could be extended and this could be removed; # example use of BayesianCalculator; # now we also need to specify a prior in the ModelConfig; # example usage of BayesianCalculator; # example use of MCMCInterval; # special options; # bins used internally for representing posterior; # first N steps to be ignored as burn-in; # how long to run chain; # for central interval; # for this example we know the expected intervals; # Use the intervals; # make a reasonable style; # some plots; # plot the data; # plot the profile likelihood; # plot the MCMC interval; # TODO: The BayesianCalculator and MCMCCalculator have to be destructed first.; # Otherwise, we can get segmentation faults depending on the destruction order,; # which is random in Python. Probably the issue is that some object has a; # non-owning poi",MatchSource.CODE_COMMENT,tutorials/roostats/IntervalExamples.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/IntervalExamples.py
Testability,test,test,"idence intervals with four techniques.; ## The model is a Normal Gaussian G(x|mu,sigma) with 100 samples of x.; ## The answer is known analytically, so this is a good example to validate; ## the RooStats tools.; ##; ## - expected interval is [-0.162917, 0.229075]; ## - plc interval is [-0.162917, 0.229075]; ## - fc interval is [-0.17 , 0.23] // stepsize is 0.01; ## - bc interval is [-0.162918, 0.229076]; ## - mcmc interval is [-0.166999, 0.230224]; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \date July 2022; ## \authors Artem Busorgin, Kyle Cranmer (C++ version); # Time this macro; # set RooFit random seed for reproducible results; # make a simple model via the workspace factory; # specify components of model for statistical tools; # create a toy dataset; # for convenience later on; # set confidence level; # example use profile likelihood calculator; # example use of Feldman-Cousins; # number of points to test per parameter; # make it go faster; # Here, we consider only ensembles with 100 events; # The PDF could be extended and this could be removed; # example use of BayesianCalculator; # now we also need to specify a prior in the ModelConfig; # example usage of BayesianCalculator; # example use of MCMCInterval; # special options; # bins used internally for representing posterior; # first N steps to be ignored as burn-in; # how long to run chain; # for central interval; # for this example we know the expected intervals; # Use the intervals; # make a reasonable style; # some plots; # plot the data; # plot the profile likelihood; # plot the MCMC interval; # TODO: The BayesianCalculator and MCMCCalculator have to be destructed first.; # Otherwise, we can get segmentation faults depending on the destruction order,; # which is random in Python. Probably the issue is that some object has a; # non-owning pointer to another object, which it uses in its destructor. This; # should be fixed either in the design of RooStats in C++, or with; # phythonizations.",MatchSource.CODE_COMMENT,tutorials/roostats/IntervalExamples.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/IntervalExamples.py
Usability,simpl,simple,"idence intervals with four techniques.; ## The model is a Normal Gaussian G(x|mu,sigma) with 100 samples of x.; ## The answer is known analytically, so this is a good example to validate; ## the RooStats tools.; ##; ## - expected interval is [-0.162917, 0.229075]; ## - plc interval is [-0.162917, 0.229075]; ## - fc interval is [-0.17 , 0.23] // stepsize is 0.01; ## - bc interval is [-0.162918, 0.229076]; ## - mcmc interval is [-0.166999, 0.230224]; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \date July 2022; ## \authors Artem Busorgin, Kyle Cranmer (C++ version); # Time this macro; # set RooFit random seed for reproducible results; # make a simple model via the workspace factory; # specify components of model for statistical tools; # create a toy dataset; # for convenience later on; # set confidence level; # example use profile likelihood calculator; # example use of Feldman-Cousins; # number of points to test per parameter; # make it go faster; # Here, we consider only ensembles with 100 events; # The PDF could be extended and this could be removed; # example use of BayesianCalculator; # now we also need to specify a prior in the ModelConfig; # example usage of BayesianCalculator; # example use of MCMCInterval; # special options; # bins used internally for representing posterior; # first N steps to be ignored as burn-in; # how long to run chain; # for central interval; # for this example we know the expected intervals; # Use the intervals; # make a reasonable style; # some plots; # plot the data; # plot the profile likelihood; # plot the MCMC interval; # TODO: The BayesianCalculator and MCMCCalculator have to be destructed first.; # Otherwise, we can get segmentation faults depending on the destruction order,; # which is random in Python. Probably the issue is that some object has a; # non-owning pointer to another object, which it uses in its destructor. This; # should be fixed either in the design of RooStats in C++, or with; # phythonizations.",MatchSource.CODE_COMMENT,tutorials/roostats/IntervalExamples.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/IntervalExamples.py
Availability,fault,faults,"## \file; ## \ingroup tutorial_roostats; ## \notebook; ## Comparison of MCMC and PLC in a multi-variate gaussian problem; ##; ## This tutorial produces an N-dimensional multivariate Gaussian; ## with a non-trivial covariance matrix. By default N=4 (called ""dim"").; ##; ## A subset of these are considered parameters of interest.; ## This problem is tractable analytically.; ##; ## We use this mainly as a test of Markov Chain Monte Carlo; ## and we compare the result to the profile likelihood ratio.; ##; ## We use the proposal helper to create a customized; ## proposal function for this problem.; ##; ## For N=4 and 2 parameters of interest it takes about 10-20 seconds; ## and the acceptance rate is 37%; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \date July 2022; ## \authors Artem Busorgin, Kevin Belasco and Kyle Cranmer (C++ version); # let's time this challenging example; # make the observable and means; # put them into the list of parameters of interest; # make a covariance matrix that is all 1's; # now make the multivariate Gaussian; # --------------------; # make a toy dataset; # now create the model config for this problem; # -------------------------------------------------------; # Setup calculators; # MCMC; # we want to setup an efficient proposal function; # using the covariance matrix from a fit to the data; # now create the calculator; # now setup the profile likelihood calculator; # make some plots; # MCMC interval on p0: [-0.2, 0.6]; # MCMC interval on p1: [-0.2, 0.6]; # TODO: The MCMCCalculator has to be destructed first. Otherwise, we can get; # segmentation faults depending on the destruction order, which is random in; # Python. Probably the issue is that some object has a non-owning pointer to; # another object, which it uses in its destructor. This should be fixed either; # in the design of RooStats in C++, or with phythonizations.",MatchSource.CODE_COMMENT,tutorials/roostats/MultivariateGaussianTest.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/MultivariateGaussianTest.py
Energy Efficiency,efficient,efficient,"## \file; ## \ingroup tutorial_roostats; ## \notebook; ## Comparison of MCMC and PLC in a multi-variate gaussian problem; ##; ## This tutorial produces an N-dimensional multivariate Gaussian; ## with a non-trivial covariance matrix. By default N=4 (called ""dim"").; ##; ## A subset of these are considered parameters of interest.; ## This problem is tractable analytically.; ##; ## We use this mainly as a test of Markov Chain Monte Carlo; ## and we compare the result to the profile likelihood ratio.; ##; ## We use the proposal helper to create a customized; ## proposal function for this problem.; ##; ## For N=4 and 2 parameters of interest it takes about 10-20 seconds; ## and the acceptance rate is 37%; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \date July 2022; ## \authors Artem Busorgin, Kevin Belasco and Kyle Cranmer (C++ version); # let's time this challenging example; # make the observable and means; # put them into the list of parameters of interest; # make a covariance matrix that is all 1's; # now make the multivariate Gaussian; # --------------------; # make a toy dataset; # now create the model config for this problem; # -------------------------------------------------------; # Setup calculators; # MCMC; # we want to setup an efficient proposal function; # using the covariance matrix from a fit to the data; # now create the calculator; # now setup the profile likelihood calculator; # make some plots; # MCMC interval on p0: [-0.2, 0.6]; # MCMC interval on p1: [-0.2, 0.6]; # TODO: The MCMCCalculator has to be destructed first. Otherwise, we can get; # segmentation faults depending on the destruction order, which is random in; # Python. Probably the issue is that some object has a non-owning pointer to; # another object, which it uses in its destructor. This should be fixed either; # in the design of RooStats in C++, or with phythonizations.",MatchSource.CODE_COMMENT,tutorials/roostats/MultivariateGaussianTest.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/MultivariateGaussianTest.py
Integrability,depend,depending,"## \file; ## \ingroup tutorial_roostats; ## \notebook; ## Comparison of MCMC and PLC in a multi-variate gaussian problem; ##; ## This tutorial produces an N-dimensional multivariate Gaussian; ## with a non-trivial covariance matrix. By default N=4 (called ""dim"").; ##; ## A subset of these are considered parameters of interest.; ## This problem is tractable analytically.; ##; ## We use this mainly as a test of Markov Chain Monte Carlo; ## and we compare the result to the profile likelihood ratio.; ##; ## We use the proposal helper to create a customized; ## proposal function for this problem.; ##; ## For N=4 and 2 parameters of interest it takes about 10-20 seconds; ## and the acceptance rate is 37%; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \date July 2022; ## \authors Artem Busorgin, Kevin Belasco and Kyle Cranmer (C++ version); # let's time this challenging example; # make the observable and means; # put them into the list of parameters of interest; # make a covariance matrix that is all 1's; # now make the multivariate Gaussian; # --------------------; # make a toy dataset; # now create the model config for this problem; # -------------------------------------------------------; # Setup calculators; # MCMC; # we want to setup an efficient proposal function; # using the covariance matrix from a fit to the data; # now create the calculator; # now setup the profile likelihood calculator; # make some plots; # MCMC interval on p0: [-0.2, 0.6]; # MCMC interval on p1: [-0.2, 0.6]; # TODO: The MCMCCalculator has to be destructed first. Otherwise, we can get; # segmentation faults depending on the destruction order, which is random in; # Python. Probably the issue is that some object has a non-owning pointer to; # another object, which it uses in its destructor. This should be fixed either; # in the design of RooStats in C++, or with phythonizations.",MatchSource.CODE_COMMENT,tutorials/roostats/MultivariateGaussianTest.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/MultivariateGaussianTest.py
Modifiability,config,config,"## \file; ## \ingroup tutorial_roostats; ## \notebook; ## Comparison of MCMC and PLC in a multi-variate gaussian problem; ##; ## This tutorial produces an N-dimensional multivariate Gaussian; ## with a non-trivial covariance matrix. By default N=4 (called ""dim"").; ##; ## A subset of these are considered parameters of interest.; ## This problem is tractable analytically.; ##; ## We use this mainly as a test of Markov Chain Monte Carlo; ## and we compare the result to the profile likelihood ratio.; ##; ## We use the proposal helper to create a customized; ## proposal function for this problem.; ##; ## For N=4 and 2 parameters of interest it takes about 10-20 seconds; ## and the acceptance rate is 37%; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \date July 2022; ## \authors Artem Busorgin, Kevin Belasco and Kyle Cranmer (C++ version); # let's time this challenging example; # make the observable and means; # put them into the list of parameters of interest; # make a covariance matrix that is all 1's; # now make the multivariate Gaussian; # --------------------; # make a toy dataset; # now create the model config for this problem; # -------------------------------------------------------; # Setup calculators; # MCMC; # we want to setup an efficient proposal function; # using the covariance matrix from a fit to the data; # now create the calculator; # now setup the profile likelihood calculator; # make some plots; # MCMC interval on p0: [-0.2, 0.6]; # MCMC interval on p1: [-0.2, 0.6]; # TODO: The MCMCCalculator has to be destructed first. Otherwise, we can get; # segmentation faults depending on the destruction order, which is random in; # Python. Probably the issue is that some object has a non-owning pointer to; # another object, which it uses in its destructor. This should be fixed either; # in the design of RooStats in C++, or with phythonizations.",MatchSource.CODE_COMMENT,tutorials/roostats/MultivariateGaussianTest.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/MultivariateGaussianTest.py
Testability,test,test,"## \file; ## \ingroup tutorial_roostats; ## \notebook; ## Comparison of MCMC and PLC in a multi-variate gaussian problem; ##; ## This tutorial produces an N-dimensional multivariate Gaussian; ## with a non-trivial covariance matrix. By default N=4 (called ""dim"").; ##; ## A subset of these are considered parameters of interest.; ## This problem is tractable analytically.; ##; ## We use this mainly as a test of Markov Chain Monte Carlo; ## and we compare the result to the profile likelihood ratio.; ##; ## We use the proposal helper to create a customized; ## proposal function for this problem.; ##; ## For N=4 and 2 parameters of interest it takes about 10-20 seconds; ## and the acceptance rate is 37%; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \date July 2022; ## \authors Artem Busorgin, Kevin Belasco and Kyle Cranmer (C++ version); # let's time this challenging example; # make the observable and means; # put them into the list of parameters of interest; # make a covariance matrix that is all 1's; # now make the multivariate Gaussian; # --------------------; # make a toy dataset; # now create the model config for this problem; # -------------------------------------------------------; # Setup calculators; # MCMC; # we want to setup an efficient proposal function; # using the covariance matrix from a fit to the data; # now create the calculator; # now setup the profile likelihood calculator; # make some plots; # MCMC interval on p0: [-0.2, 0.6]; # MCMC interval on p1: [-0.2, 0.6]; # TODO: The MCMCCalculator has to be destructed first. Otherwise, we can get; # segmentation faults depending on the destruction order, which is random in; # Python. Probably the issue is that some object has a non-owning pointer to; # another object, which it uses in its destructor. This should be fixed either; # in the design of RooStats in C++, or with phythonizations.",MatchSource.CODE_COMMENT,tutorials/roostats/MultivariateGaussianTest.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/MultivariateGaussianTest.py
Modifiability,config,configuring,"-------------------------------; # An example of setting a limit in a number counting experiment with uncertainty on background and signal; # to time the macro; # --------------------------------------; # The Model building stage; # --------------------------------------; # counting model; # 5% signal efficiency uncertainty; # 10% background efficiency uncertainty; # product of terms; # get the model; # get the observable; # get the signal we care about; # get the background and set it to a constant. Uncertainty included in ratioBkgEff; # get uncertain parameter to constrain; # get uncertain parameter to constrain; # need to constrain these in the fit (should change default behavior); # global observables for signal efficiency; # global obs for background efficiency; # Create an example dataset with 160 observed events; # not necessary; # Now let's make some confidence intervals for s, our parameter of interest; # wspace.writeToFile(""rs101_ws.root""); # Make sure we reference the data in the workspace from now on; # First, let's use a Calculator based on the Profile Likelihood Ratio; # Let's make a plot; # Second, use a Calculator based on the Feldman Cousins technique; # number counting analysis: dataset always has 1 entry with N events observed; # number of points to test per parameter; # fc.SaveBeltToFile(True) # optional; # Third, use a Calculator based on Markov Chain monte carlo; # Before configuring the calculator, let's make a ProposalFunction; # that will achieve a high acceptance rate; # steps to propose in the chain; # 95% CL; # ignore first N steps in chain as ""burn in""; # find a ""central"" interval; # Get Lower and Upper limits from Profile Calculator; # Get Lower and Upper limits from FeldmanCousins with profile construction; # Plot MCMC interval and print some statistics; # 3-d plot of the parameter points; # also plot the points in the markov chain; # 3-d box proportional to posterior; # the points used in the profile construction; # print timing info",MatchSource.CODE_COMMENT,tutorials/roostats/rs101_limitexample.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/rs101_limitexample.py
Testability,test,test,"-------------------------------; # An example of setting a limit in a number counting experiment with uncertainty on background and signal; # to time the macro; # --------------------------------------; # The Model building stage; # --------------------------------------; # counting model; # 5% signal efficiency uncertainty; # 10% background efficiency uncertainty; # product of terms; # get the model; # get the observable; # get the signal we care about; # get the background and set it to a constant. Uncertainty included in ratioBkgEff; # get uncertain parameter to constrain; # get uncertain parameter to constrain; # need to constrain these in the fit (should change default behavior); # global observables for signal efficiency; # global obs for background efficiency; # Create an example dataset with 160 observed events; # not necessary; # Now let's make some confidence intervals for s, our parameter of interest; # wspace.writeToFile(""rs101_ws.root""); # Make sure we reference the data in the workspace from now on; # First, let's use a Calculator based on the Profile Likelihood Ratio; # Let's make a plot; # Second, use a Calculator based on the Feldman Cousins technique; # number counting analysis: dataset always has 1 entry with N events observed; # number of points to test per parameter; # fc.SaveBeltToFile(True) # optional; # Third, use a Calculator based on Markov Chain monte carlo; # Before configuring the calculator, let's make a ProposalFunction; # that will achieve a high acceptance rate; # steps to propose in the chain; # 95% CL; # ignore first N steps in chain as ""burn in""; # find a ""central"" interval; # Get Lower and Upper limits from Profile Calculator; # Get Lower and Upper limits from FeldmanCousins with profile construction; # Plot MCMC interval and print some statistics; # 3-d plot of the parameter points; # also plot the points in the markov chain; # 3-d box proportional to posterior; # the points used in the profile construction; # print timing info",MatchSource.CODE_COMMENT,tutorials/roostats/rs101_limitexample.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/rs101_limitexample.py
Availability,error,errors,"e AddSWeight; # method).; #; # sigmaZ = ws[""sigmaZ""); # qcdMassDecayConst = ws[""qcdMassDecayConst""); # sigmaZ.setConstant(); # qcdMassDecayConst.setConstant(); # Now we use the SPlot class to add SWeights for the isolation variable to; # our data set based on fitting the yields to the invariant mass variable.; # Any keyword arguments will be forwarded to the underlying call to RooAbsPdf::fitTo().; # Check that our weights have the desired properties; # import this new dataset with sWeights; # Here we make plots of the discriminating variable (invMass) after the fit; # and of the control variable (isolation) after unfolding with sPlot.; # make our canvas; # get what we need out of the workspace; # note, we get the dataset with sWeights; # create weighted data sets; # plot invMass for data with full model and individual components overlaid; # cdata = TCanvas(); # Now use the sWeights to show isolation distribution for Z and QCD.; # The SPlot class can make this easier, but here we demonstrate in more; # detail how the sWeights are used. The SPlot class should make this; # very easy and needs some more development.; # Plot isolation for Z component.; # Do this by plotting all events weighted by the sWeight for the Z component.; # The SPlot class adds a new variable that has the name of the corresponding; # yield + ""_sw"".; # Since the data are weighted, we use SumW2 to compute the errors.; # Plot isolation for QCD component.; # Eg. plot all events weighted by the sWeight for the QCD component.; # The SPlot class adds a new variable that has the name of the corresponding; # yield + ""_sw"".; # Create a workspace to manage the project.; # add the signal and background models to the workspace.; # Inside this function you will find a description of our model.; # add some toy data to the workspace; # do sPlot.; # This will make a new dataset with sWeights added for every event.; # Make some plots showing the discriminating variable and; # the control variable after unfolding.",MatchSource.CODE_COMMENT,tutorials/roostats/rs301_splot.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/rs301_splot.py
Modifiability,variab,variable,"# \file; # \ingroup roostats_python_tutorials; # \notebook -js; # SPlot tutorial; #; # This tutorial shows an example of using SPlot to unfold two distributions.; # The physics context for the example is that we want to know; # the isolation distribution for real electrons from Z events; # and fake electrons from QCD. Isolation is our 'control' variable.; # To unfold them, we need a model for an uncorrelated variable that; # discriminates between Z and QCD. To do this, we use the invariant; # mass of two electrons. We model the Z with a Gaussian and the QCD; # with a falling exponential.; #; # Note, since we don't have real data in this tutorial, we need to generate; # toy data. To do that we need a model for the isolation variable for; # both Z and QCD. This is only used to generate the toy data, and would; # not be needed if we had real data.; #; # \macro_image; # \macro_code; # \macro_output; #; # \authors P. P., Kyle Cranmer (C++ version); # Make models for signal (Higgs) and background (Z+jets and QCD); # In real life, this part requires an intelligent modeling; # of signal and background -- this is only an example.; # set range of observable; # make a ROOT.RooRealVar for the observables; # --------------------------------------; # make 2-d model for Z including the invariant mass; # distribution and an isolation distribution which we want to; # unfold from QCD.; # mass model for Z; # we know Z mass; # we leave the width of the Z free during the fit in this example.; # isolation model for Z. Only used to generate toy MC.; # the exponential is of the form exp(c*x). If we want; # the isolation to decay an e-fold every R GeV, we use; # c = -1/R.; # make the combined Z model; # --------------------------------------; # make QCD model; # mass model for QCD.; # the exponential is of the form exp(c*x). If we want; # the mass to decay an e-fold every R GeV, we use; # c = -1/R.; # We can leave this parameter free during the fit.; # isolation model for QCD. Only used to g",MatchSource.CODE_COMMENT,tutorials/roostats/rs301_splot.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/rs301_splot.py
Testability,test,test,"## \file; ## \ingroup tutorial_roostats; ## \notebook; ## Produces an interval on the mean signal in a number counting experiment with known background using the; ## Feldman-Cousins technique.; ##; ## Using the RooStats FeldmanCousins tool with 200 bins; ## it takes 1 min and the interval is [0.2625, 10.6125]; ## with a step size of 0.075.; ## The interval in Feldman & Cousins's original paper is [.29, 10.81] Phys.Rev.D57:3873-3889,1998.; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \date July 2022; ## \authors Artem Busorgin, Kyle Cranmer (C++ version); # to time the macro... about 30 s; # make a simple model; # with a limit on mu>=0; # create a toy dataset; # show use of Feldman-Cousins; # set size of test; # number counting analysis: dataset always has 1 entry with N events observed; # number of points to test per parameter; # use the Feldman-Cousins tool; # make a canvas for plots; # using 200 bins it takes 1 min and the answer is; # interval is [0.2625, 10.6125] with a step size of .075; # The interval in Feldman & Cousins's original paper is [.29, 10.81]; # Phys.Rev.D57:3873-3889,1998.; # No dedicated plotting class yet, so do it by hand:; # loop over points to test; # get a parameter point from the list of points to test.",MatchSource.CODE_COMMENT,tutorials/roostats/rs401c_FeldmanCousins.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/rs401c_FeldmanCousins.py
Usability,simpl,simple,"## \file; ## \ingroup tutorial_roostats; ## \notebook; ## Produces an interval on the mean signal in a number counting experiment with known background using the; ## Feldman-Cousins technique.; ##; ## Using the RooStats FeldmanCousins tool with 200 bins; ## it takes 1 min and the interval is [0.2625, 10.6125]; ## with a step size of 0.075.; ## The interval in Feldman & Cousins's original paper is [.29, 10.81] Phys.Rev.D57:3873-3889,1998.; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \date July 2022; ## \authors Artem Busorgin, Kyle Cranmer (C++ version); # to time the macro... about 30 s; # make a simple model; # with a limit on mu>=0; # create a toy dataset; # show use of Feldman-Cousins; # set size of test; # number counting analysis: dataset always has 1 entry with N events observed; # number of points to test per parameter; # use the Feldman-Cousins tool; # make a canvas for plots; # using 200 bins it takes 1 min and the answer is; # interval is [0.2625, 10.6125] with a step size of .075; # The interval in Feldman & Cousins's original paper is [.29, 10.81]; # Phys.Rev.D57:3873-3889,1998.; # No dedicated plotting class yet, so do it by hand:; # loop over points to test; # get a parameter point from the list of points to test.",MatchSource.CODE_COMMENT,tutorials/roostats/rs401c_FeldmanCousins.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/rs401c_FeldmanCousins.py
Modifiability,extend,extended,"## \file; ## \ingroup tutorial_roostats; ## \notebook -js; ## High Level Factory: creation of a simple model; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \date July 2022; ## \authors Artem Busorgin, Danilo Piparo (C++ version); # --- Build the datacard and dump to file---; # --- Take elements out of the internal workspace ---; # --- Generate a toyMC sample from composite PDF ---; # --- Perform extended ML fit of composite PDF to toy data ---; # --- Plot toy data and composite PDF overlaid ---",MatchSource.CODE_COMMENT,tutorials/roostats/rs601_HLFactoryexample.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/rs601_HLFactoryexample.py
Usability,simpl,simple,"## \file; ## \ingroup tutorial_roostats; ## \notebook -js; ## High Level Factory: creation of a simple model; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \date July 2022; ## \authors Artem Busorgin, Danilo Piparo (C++ version); # --- Build the datacard and dump to file---; # --- Take elements out of the internal workspace ---; # --- Generate a toyMC sample from composite PDF ---; # --- Perform extended ML fit of composite PDF to toy data ---; # --- Plot toy data and composite PDF overlaid ---",MatchSource.CODE_COMMENT,tutorials/roostats/rs601_HLFactoryexample.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/rs601_HLFactoryexample.py
Availability,fault,faults,"## \file; ## \ingroup tutorial_roostats; ## \notebook; ## Bayesian calculator: basic example; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \date July 2022; ## \authors Artem Busorgin, Gregory Schott (C++ version); # pdf*priorNuisance; # observed number of events; # create a data set with n observed events; # to suppress messages when pdf goes to zero; # observe one event while expecting one background event -> the 95% CL upper limit on s is 4.10; # observe one event while expecting zero background event -> the 95% CL upper limit on s is 4.74; # TODO: The BayesianCalculator has to be destructed first. Otherwise, we can get; # segmentation faults depending on the destruction order, which is random in; # Python. Probably the issue is that some object has a non-owning pointer to; # another object, which it uses in its destructor. This should be fixed either; # in the design of RooStats in C++, or with phythonizations.",MatchSource.CODE_COMMENT,tutorials/roostats/rs701_BayesianCalculator.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/rs701_BayesianCalculator.py
Integrability,message,messages,"## \file; ## \ingroup tutorial_roostats; ## \notebook; ## Bayesian calculator: basic example; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \date July 2022; ## \authors Artem Busorgin, Gregory Schott (C++ version); # pdf*priorNuisance; # observed number of events; # create a data set with n observed events; # to suppress messages when pdf goes to zero; # observe one event while expecting one background event -> the 95% CL upper limit on s is 4.10; # observe one event while expecting zero background event -> the 95% CL upper limit on s is 4.74; # TODO: The BayesianCalculator has to be destructed first. Otherwise, we can get; # segmentation faults depending on the destruction order, which is random in; # Python. Probably the issue is that some object has a non-owning pointer to; # another object, which it uses in its destructor. This should be fixed either; # in the design of RooStats in C++, or with phythonizations.",MatchSource.CODE_COMMENT,tutorials/roostats/rs701_BayesianCalculator.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/rs701_BayesianCalculator.py
Availability,toler,tolerance,"## \file; ## \ingroup tutorial_roostats; ## \notebook -js; ## Example of the BernsteinCorrection utility in RooStats.; ##; ## The idea is that one has a distribution coming either from data or Monte Carlo; ## (called ""reality"" in the macro) and a nominal model that is not sufficiently; ## flexible to take into account the real distribution. One wants to take into; ## account the systematic associated with this imperfect modeling by augmenting; ## the nominal model with some correction term (in this case a polynomial).; ## The BernsteinCorrection utility will import into your workspace a corrected model; ## given by nominal(x) * poly_N(x), where poly_N is an n-th order polynomial in; ## the Bernstein basis. The degree N of the polynomial is chosen by specifying the tolerance; ## one has in adding an extra term to the polynomial.; ## The Bernstein basis is nice because it only has positive-definite terms; ## and works well with PDFs.; ## Finally, the macro makes a plot of:; ## - the data (drawn from 'reality'),; ## - the best fit of the nominal model (blue); ## - and the best fit corrected model.; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \date June 2022; ## \authors Artem Busorgin, Kyle Cranmer (C++ version); # set range of observable; # make a RooRealVar for the observable; # true model; # nominal model; # use Minuit2 if ROOT was built with support for it:; # The tolerance sets the probability to add an unnecessary term.; # lower tolerance will add fewer terms, while higher tolerance; # will add more terms and provide a more flexible function.; # plot the best fit nominal model in blue; # plot the best fit corrected model in red; # fit corrected model; # plot the correction term (* norm constant) in dashed green; # should make norm constant just be 1, not depend on binning of data; # this is a switch to check the sampling distribution; # of -2 log LR for two comparisons:; # the first is for n-1 vs. n degree polynomial corrections; # the second is ",MatchSource.CODE_COMMENT,tutorials/roostats/rs_bernsteinCorrection.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/rs_bernsteinCorrection.py
Energy Efficiency,green,green,"s not sufficiently; ## flexible to take into account the real distribution. One wants to take into; ## account the systematic associated with this imperfect modeling by augmenting; ## the nominal model with some correction term (in this case a polynomial).; ## The BernsteinCorrection utility will import into your workspace a corrected model; ## given by nominal(x) * poly_N(x), where poly_N is an n-th order polynomial in; ## the Bernstein basis. The degree N of the polynomial is chosen by specifying the tolerance; ## one has in adding an extra term to the polynomial.; ## The Bernstein basis is nice because it only has positive-definite terms; ## and works well with PDFs.; ## Finally, the macro makes a plot of:; ## - the data (drawn from 'reality'),; ## - the best fit of the nominal model (blue); ## - and the best fit corrected model.; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \date June 2022; ## \authors Artem Busorgin, Kyle Cranmer (C++ version); # set range of observable; # make a RooRealVar for the observable; # true model; # nominal model; # use Minuit2 if ROOT was built with support for it:; # The tolerance sets the probability to add an unnecessary term.; # lower tolerance will add fewer terms, while higher tolerance; # will add more terms and provide a more flexible function.; # plot the best fit nominal model in blue; # plot the best fit corrected model in red; # fit corrected model; # plot the correction term (* norm constant) in dashed green; # should make norm constant just be 1, not depend on binning of data; # this is a switch to check the sampling distribution; # of -2 log LR for two comparisons:; # the first is for n-1 vs. n degree polynomial corrections; # the second is for n vs. n+1 degree polynomial corrections; # Here we choose n to be the one chosen by the tolerance; # criterion above, eg. n = ""degree"" in the code.; # Setting this to true is takes about 10 min.; # increase this value for sensible results; # check sampling dist",MatchSource.CODE_COMMENT,tutorials/roostats/rs_bernsteinCorrection.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/rs_bernsteinCorrection.py
Integrability,depend,depend,"s not sufficiently; ## flexible to take into account the real distribution. One wants to take into; ## account the systematic associated with this imperfect modeling by augmenting; ## the nominal model with some correction term (in this case a polynomial).; ## The BernsteinCorrection utility will import into your workspace a corrected model; ## given by nominal(x) * poly_N(x), where poly_N is an n-th order polynomial in; ## the Bernstein basis. The degree N of the polynomial is chosen by specifying the tolerance; ## one has in adding an extra term to the polynomial.; ## The Bernstein basis is nice because it only has positive-definite terms; ## and works well with PDFs.; ## Finally, the macro makes a plot of:; ## - the data (drawn from 'reality'),; ## - the best fit of the nominal model (blue); ## - and the best fit corrected model.; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \date June 2022; ## \authors Artem Busorgin, Kyle Cranmer (C++ version); # set range of observable; # make a RooRealVar for the observable; # true model; # nominal model; # use Minuit2 if ROOT was built with support for it:; # The tolerance sets the probability to add an unnecessary term.; # lower tolerance will add fewer terms, while higher tolerance; # will add more terms and provide a more flexible function.; # plot the best fit nominal model in blue; # plot the best fit corrected model in red; # fit corrected model; # plot the correction term (* norm constant) in dashed green; # should make norm constant just be 1, not depend on binning of data; # this is a switch to check the sampling distribution; # of -2 log LR for two comparisons:; # the first is for n-1 vs. n degree polynomial corrections; # the second is for n vs. n+1 degree polynomial corrections; # Here we choose n to be the one chosen by the tolerance; # criterion above, eg. n = ""degree"" in the code.; # Setting this to true is takes about 10 min.; # increase this value for sensible results; # check sampling dist",MatchSource.CODE_COMMENT,tutorials/roostats/rs_bernsteinCorrection.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/rs_bernsteinCorrection.py
Modifiability,flexible,flexible,"## \file; ## \ingroup tutorial_roostats; ## \notebook -js; ## Example of the BernsteinCorrection utility in RooStats.; ##; ## The idea is that one has a distribution coming either from data or Monte Carlo; ## (called ""reality"" in the macro) and a nominal model that is not sufficiently; ## flexible to take into account the real distribution. One wants to take into; ## account the systematic associated with this imperfect modeling by augmenting; ## the nominal model with some correction term (in this case a polynomial).; ## The BernsteinCorrection utility will import into your workspace a corrected model; ## given by nominal(x) * poly_N(x), where poly_N is an n-th order polynomial in; ## the Bernstein basis. The degree N of the polynomial is chosen by specifying the tolerance; ## one has in adding an extra term to the polynomial.; ## The Bernstein basis is nice because it only has positive-definite terms; ## and works well with PDFs.; ## Finally, the macro makes a plot of:; ## - the data (drawn from 'reality'),; ## - the best fit of the nominal model (blue); ## - and the best fit corrected model.; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \date June 2022; ## \authors Artem Busorgin, Kyle Cranmer (C++ version); # set range of observable; # make a RooRealVar for the observable; # true model; # nominal model; # use Minuit2 if ROOT was built with support for it:; # The tolerance sets the probability to add an unnecessary term.; # lower tolerance will add fewer terms, while higher tolerance; # will add more terms and provide a more flexible function.; # plot the best fit nominal model in blue; # plot the best fit corrected model in red; # fit corrected model; # plot the correction term (* norm constant) in dashed green; # should make norm constant just be 1, not depend on binning of data; # this is a switch to check the sampling distribution; # of -2 log LR for two comparisons:; # the first is for n-1 vs. n degree polynomial corrections; # the second is ",MatchSource.CODE_COMMENT,tutorials/roostats/rs_bernsteinCorrection.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/rs_bernsteinCorrection.py
Testability,log,log,"s not sufficiently; ## flexible to take into account the real distribution. One wants to take into; ## account the systematic associated with this imperfect modeling by augmenting; ## the nominal model with some correction term (in this case a polynomial).; ## The BernsteinCorrection utility will import into your workspace a corrected model; ## given by nominal(x) * poly_N(x), where poly_N is an n-th order polynomial in; ## the Bernstein basis. The degree N of the polynomial is chosen by specifying the tolerance; ## one has in adding an extra term to the polynomial.; ## The Bernstein basis is nice because it only has positive-definite terms; ## and works well with PDFs.; ## Finally, the macro makes a plot of:; ## - the data (drawn from 'reality'),; ## - the best fit of the nominal model (blue); ## - and the best fit corrected model.; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \date June 2022; ## \authors Artem Busorgin, Kyle Cranmer (C++ version); # set range of observable; # make a RooRealVar for the observable; # true model; # nominal model; # use Minuit2 if ROOT was built with support for it:; # The tolerance sets the probability to add an unnecessary term.; # lower tolerance will add fewer terms, while higher tolerance; # will add more terms and provide a more flexible function.; # plot the best fit nominal model in blue; # plot the best fit corrected model in red; # fit corrected model; # plot the correction term (* norm constant) in dashed green; # should make norm constant just be 1, not depend on binning of data; # this is a switch to check the sampling distribution; # of -2 log LR for two comparisons:; # the first is for n-1 vs. n degree polynomial corrections; # the second is for n vs. n+1 degree polynomial corrections; # Here we choose n to be the one chosen by the tolerance; # criterion above, eg. n = ""degree"" in the code.; # Setting this to true is takes about 10 min.; # increase this value for sensible results; # check sampling dist",MatchSource.CODE_COMMENT,tutorials/roostats/rs_bernsteinCorrection.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/rs_bernsteinCorrection.py
Energy Efficiency,energy,energy,"## \file; ## \ingroup tutorial_roostats; ## \notebook -nodraw; ## 'Number Counting Utils' RooStats tutorial; ##; ## This tutorial shows an example of the RooStats standalone; ## utilities that calculate the p-value or Z value (eg. significance in; ## 1-sided Gaussian standard deviations) for a number counting experiment.; ## This is a hypothesis test between background only and signal-plus-background.; ## The background estimate has uncertainty derived from an auxiliary or sideband; ## measurement.; ##; ## Documentation for these utilities can be found here:; ## https://root.cern.ch/doc/master/namespaceNumberCountingUtils.html; ##; ##; ## This problem is often called a proto-type problem for high energy physics.; ## In some references it is referred to as the on/off problem.; ##; ## The problem is treated in a fully frequentist fashion by; ## interpreting the relative background uncertainty as; ## being due to an auxiliary or sideband observation; ## that is also Poisson distributed with only background.; ## Finally, one considers the test as a ratio of Poisson means; ## where an interval is well known based on the conditioning on the total; ## number of events and the binomial distribution.; ## For more on this, see; ## - http://arxiv.org/abs/0905.3831; ## - http://arxiv.org/abs/physics/physics/0702156; ## - http://arxiv.org/abs/physics/0511028; ##; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \date July 2022; ## \authors Artem Busorgin, Kyle Cranmer (C++ version); # From the root prompt, you can see the full list of functions by using tab-completion; # ~~~{.bash}; # root [0] RooStats::NumberCountingUtils:: <tab>; # BinomialExpZ; # BinomialWithTauExpZ; # BinomialObsZ; # BinomialWithTauObsZ; # BinomialExpP; # BinomialWithTauExpP; # BinomialObsP; # BinomialWithTauObsP; # ~~~; # For each of the utilities you can inspect the arguments by tab completion; # ~~~{.bash}; # root [1] NumberCountingUtils::BinomialExpZ( <tab>; # Double_t BinomialExpZ(Double_t s",MatchSource.CODE_COMMENT,tutorials/roostats/rs_numbercountingutils.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/rs_numbercountingutils.py
Testability,test,test,"## \file; ## \ingroup tutorial_roostats; ## \notebook -nodraw; ## 'Number Counting Utils' RooStats tutorial; ##; ## This tutorial shows an example of the RooStats standalone; ## utilities that calculate the p-value or Z value (eg. significance in; ## 1-sided Gaussian standard deviations) for a number counting experiment.; ## This is a hypothesis test between background only and signal-plus-background.; ## The background estimate has uncertainty derived from an auxiliary or sideband; ## measurement.; ##; ## Documentation for these utilities can be found here:; ## https://root.cern.ch/doc/master/namespaceNumberCountingUtils.html; ##; ##; ## This problem is often called a proto-type problem for high energy physics.; ## In some references it is referred to as the on/off problem.; ##; ## The problem is treated in a fully frequentist fashion by; ## interpreting the relative background uncertainty as; ## being due to an auxiliary or sideband observation; ## that is also Poisson distributed with only background.; ## Finally, one considers the test as a ratio of Poisson means; ## where an interval is well known based on the conditioning on the total; ## number of events and the binomial distribution.; ## For more on this, see; ## - http://arxiv.org/abs/0905.3831; ## - http://arxiv.org/abs/physics/physics/0702156; ## - http://arxiv.org/abs/physics/0511028; ##; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \date July 2022; ## \authors Artem Busorgin, Kyle Cranmer (C++ version); # From the root prompt, you can see the full list of functions by using tab-completion; # ~~~{.bash}; # root [0] RooStats::NumberCountingUtils:: <tab>; # BinomialExpZ; # BinomialWithTauExpZ; # BinomialObsZ; # BinomialWithTauObsZ; # BinomialExpP; # BinomialWithTauExpP; # BinomialObsP; # BinomialWithTauObsP; # ~~~; # For each of the utilities you can inspect the arguments by tab completion; # ~~~{.bash}; # root [1] NumberCountingUtils::BinomialExpZ( <tab>; # Double_t BinomialExpZ(Double_t s",MatchSource.CODE_COMMENT,tutorials/roostats/rs_numbercountingutils.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/rs_numbercountingutils.py
Security,access,access,"## \file; ## \ingroup tutorial_roostats; ## \notebook -js; ## Standard demo of the ProfileInspector class; ## StandardProfileInspectorDemo; ##; ## This is a standard demo that can be used with any ROOT file; ## prepared in the standard way. You specify:; ## - name for input ROOT file; ## - name of workspace inside ROOT file that holds model and data; ## - name of ModelConfig that specifies details for calculator tools; ## - name of dataset; ##; ## With the values provided below this script will attempt to run the; ## standard hist2workspace example and read the ROOT file; ## that it produces.; ##; ## The actual heart of the demo is only about 10 lines long.; ##; ## The ProfileInspector plots the conditional maximum likelihood estimate; ## of each nuisance parameter in the model vs. the parameter of interest.; ## (aka. profiled value of nuisance parameter vs. parameter of interest); ## (aka. best fit nuisance parameter with p.o.i fixed vs. parameter of interest); ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \authors Akeem Hart, Kyle Cranmer (C++ Version); # -------------------------------------------------------; # First part is just to access a user-defined file; # or create the standard example file if it doesn't exist; # if file does not exists generate with histfactory; # Normally this would be run on the command line; # -------------------------------------------------------; # Tutorial starts here; # -------------------------------------------------------; # get the workspace out of the file; # get the modelConfig out of the file; # get the modelConfig out of the file; # -----------------------------; # now use the profile inspector; # now make plots",MatchSource.CODE_COMMENT,tutorials/roostats/StandardProfileInspectorDemo.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/StandardProfileInspectorDemo.py
Modifiability,config,config,"## \file; ## \ingroup tutorial_roostats; ## \notebook; ## Standard demo of the Profile Likelihood calculator; ## StandardProfileLikelihoodDemo; ##; ## This is a standard demo that can be used with any ROOT file; ## prepared in the standard way. You specify:; ## - name for input ROOT file; ## - name of workspace inside ROOT file that holds model and data; ## - name of ModelConfig that specifies details for calculator tools; ## - name of dataset; ## With the values provided below the macro will attempt to run the; ## standard hist2workspace example and read the ROOT file; ## that it produces.; ##; ## The actual heart of the demo is only about 10 lines long.; ##; ## The ProfileLikelihoodCalculator is based on Wilks's theorem; ## and the asymptotic properties of the profile likelihood ratio; ## (eg. that it is chi-square distributed for the true value).; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \authors Akeem Hart, Kyle Cranmer (C++ Version); # if file does not exists generate with histfactory; # Normally this would be run on the command line; # -------------------------------------------------------; # Tutorial starts here; # -------------------------------------------------------; # get the workspace out of the file; # get the modelConfig out of the file; # get the modelConfig out of the file; # ---------------------------------------------; # create and use the ProfileLikelihoodCalculator; # to find and plot the 95% confidence interval; # on the parameter of interest as specified; # in the model config; # print out the interval on the first Parameter of Interest; # make a plot; # do not use too many points, it could become very slow for some models; # use option TF1 if too slow (plot.Draw(""tf1""); # if requested perform also an hypothesis test for the significance",MatchSource.CODE_COMMENT,tutorials/roostats/StandardProfileLikelihoodDemo.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/StandardProfileLikelihoodDemo.py
Performance,perform,perform,"## \file; ## \ingroup tutorial_roostats; ## \notebook; ## Standard demo of the Profile Likelihood calculator; ## StandardProfileLikelihoodDemo; ##; ## This is a standard demo that can be used with any ROOT file; ## prepared in the standard way. You specify:; ## - name for input ROOT file; ## - name of workspace inside ROOT file that holds model and data; ## - name of ModelConfig that specifies details for calculator tools; ## - name of dataset; ## With the values provided below the macro will attempt to run the; ## standard hist2workspace example and read the ROOT file; ## that it produces.; ##; ## The actual heart of the demo is only about 10 lines long.; ##; ## The ProfileLikelihoodCalculator is based on Wilks's theorem; ## and the asymptotic properties of the profile likelihood ratio; ## (eg. that it is chi-square distributed for the true value).; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \authors Akeem Hart, Kyle Cranmer (C++ Version); # if file does not exists generate with histfactory; # Normally this would be run on the command line; # -------------------------------------------------------; # Tutorial starts here; # -------------------------------------------------------; # get the workspace out of the file; # get the modelConfig out of the file; # get the modelConfig out of the file; # ---------------------------------------------; # create and use the ProfileLikelihoodCalculator; # to find and plot the 95% confidence interval; # on the parameter of interest as specified; # in the model config; # print out the interval on the first Parameter of Interest; # make a plot; # do not use too many points, it could become very slow for some models; # use option TF1 if too slow (plot.Draw(""tf1""); # if requested perform also an hypothesis test for the significance",MatchSource.CODE_COMMENT,tutorials/roostats/StandardProfileLikelihoodDemo.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/StandardProfileLikelihoodDemo.py
Testability,test,test,"## \file; ## \ingroup tutorial_roostats; ## \notebook; ## Standard demo of the Profile Likelihood calculator; ## StandardProfileLikelihoodDemo; ##; ## This is a standard demo that can be used with any ROOT file; ## prepared in the standard way. You specify:; ## - name for input ROOT file; ## - name of workspace inside ROOT file that holds model and data; ## - name of ModelConfig that specifies details for calculator tools; ## - name of dataset; ## With the values provided below the macro will attempt to run the; ## standard hist2workspace example and read the ROOT file; ## that it produces.; ##; ## The actual heart of the demo is only about 10 lines long.; ##; ## The ProfileLikelihoodCalculator is based on Wilks's theorem; ## and the asymptotic properties of the profile likelihood ratio; ## (eg. that it is chi-square distributed for the true value).; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \authors Akeem Hart, Kyle Cranmer (C++ Version); # if file does not exists generate with histfactory; # Normally this would be run on the command line; # -------------------------------------------------------; # Tutorial starts here; # -------------------------------------------------------; # get the workspace out of the file; # get the modelConfig out of the file; # get the modelConfig out of the file; # ---------------------------------------------; # create and use the ProfileLikelihoodCalculator; # to find and plot the 95% confidence interval; # on the parameter of interest as specified; # in the model config; # print out the interval on the first Parameter of Interest; # make a plot; # do not use too many points, it could become very slow for some models; # use option TF1 if too slow (plot.Draw(""tf1""); # if requested perform also an hypothesis test for the significance",MatchSource.CODE_COMMENT,tutorials/roostats/StandardProfileLikelihoodDemo.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/StandardProfileLikelihoodDemo.py
Testability,test,test,"# \file; # \ingroup tutorial_roostats; # \notebook -js; #; # \macro_image; # \macro_output; # \macro_code; #; # \author Lorenzo Moneta; # k <2, must use sum; # kk > 2 can use bessel; # kk > 2, force sum; # a normal ""central"" chi-square for comparison when lambda->0; # w.var(""kk"").setVal(4.) # test a large kk",MatchSource.CODE_COMMENT,tutorials/roostats/TestNonCentral.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/TestNonCentral.py
Deployability,install,installed,"ce; # interval for any particular dataset by finding the intersection; # of the observed test statistic and the confidence belt. First; # this is done on the observed data to get an observed 1-sided upper limt.; #; # Finally, there expected limit and bands (from background-only) are; # formed by generating background-only data and finding the upper limit.; # The background-only is defined as such that the nuisance parameters are; # fixed to their best fit value based on the data with the signal rate fixed to 0.; # The bands are done by hand for now, will later be part of the RooStats tools.; #; # On a technical note, this technique IS the generalization of Feldman-Cousins; # with nuisance parameters.; #; # Building the confidence belt can be computationally expensive.; # Once it is built, one could save it to a file and use it in a separate step.; #; # We can use PROOF to speed things along in parallel, however,; # the test statistic has to be installed on the workers; # so either turn off PROOF or include the modified test statistic; # in your $ROOTSYS/roofit/roostats/inc directory,; # add the additional line to the LinkDef.h file,; # and recompile root.; #; # Note, if you have a boundary on the parameter of interest (eg. cross-section); # the threshold on the two-sided test statistic starts off at moderate values and plateaus.; #; # [#0] PROGRESS:Generation -- generated toys: 500 / 999; # NeymanConstruction: Prog: 12/50 total MC = 39 this test stat = 0; # SigXsecOverSM=0.69 alpha_syst1=0.136515 alpha_syst3=0.425415 beta_syst2=1.08496 [-1e+30, 0.011215] in interval = 1; #; # this tells you the values of the parameters being used to generate the pseudo-experiments; # and the threshold in this case is 0.011215. One would expect for 95% that the threshold; # would be ~1.35 once the cross-section is far enough away from 0 that it is essentially; # unaffected by the boundary. As one reaches the last points in the scan, the; # threshold starts to get artificially high. Th",MatchSource.CODE_COMMENT,tutorials/roostats/TwoSidedFrequentistUpperLimitWithBands.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/TwoSidedFrequentistUpperLimitWithBands.py
Energy Efficiency,power,power-constraint,"ol needs to throw toy MC the PDF needs to be; # extended or the tool needs to know how many entries in a dataset; # per pseudo experiment.; # In the 'number counting form' where the entries in the dataset; # are counts, and not values of discriminating variables, the; # datasets typically only have one entry and the PDF is not; # extended.; # Now get the interval; # print out the interval on the first Parameter of Interest; # get observed UL and value of test statistic evaluated there; # Ask the calculator which points were scanned; # make a histogram of parameter vs. threshold; # loop through the points that were tested and ask confidence belt; # what the upper/lower thresholds were.; # For FeldmanCousins, the lower cut off is always 0; # print(get threshold""); # -------------------------------------------------------; # Now we generate the expected bands and power-constraint; # First: find parameter point for mu=0, with conditional MLEs for nuisance parameters; # this will do fit and set nuisance parameters to profiled values; # comment this out for the original conditional ensemble; # Now we generate background only and find distribution of upper limits; # set parameters back to values for generating pseudo data; # print(""\n get current nuis, set vals, print again""); # poiAndNuisance.Print(""v""); # now generate a toy dataset for the main measurement; # print(""generating extended dataset""); # generate global observables; # need to be careful for simpdf.; # In ROOT 5.28 there is a problem with generating global observables; # with a simultaneous PDF. In 5.29 there is a solution with; # RooSimultaneous::generateSimGlobal, but this may change to; # the standard generate interface in 5.30.; # get test stat at observed UL in observed data; # toyData.get().Print(""v""); # print(""obsTSatObsUL "", obsTSatObsUL, ""toyTS "", toyTSatObsUL); # not sure about <= part yet; # not sure about <= part yet; # loop over points in belt to find upper limit for this toy data; # thisTS = profil",MatchSource.CODE_COMMENT,tutorials/roostats/TwoSidedFrequentistUpperLimitWithBands.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/TwoSidedFrequentistUpperLimitWithBands.py
Integrability,interface,interface," belt; # what the upper/lower thresholds were.; # For FeldmanCousins, the lower cut off is always 0; # print(get threshold""); # -------------------------------------------------------; # Now we generate the expected bands and power-constraint; # First: find parameter point for mu=0, with conditional MLEs for nuisance parameters; # this will do fit and set nuisance parameters to profiled values; # comment this out for the original conditional ensemble; # Now we generate background only and find distribution of upper limits; # set parameters back to values for generating pseudo data; # print(""\n get current nuis, set vals, print again""); # poiAndNuisance.Print(""v""); # now generate a toy dataset for the main measurement; # print(""generating extended dataset""); # generate global observables; # need to be careful for simpdf.; # In ROOT 5.28 there is a problem with generating global observables; # with a simultaneous PDF. In 5.29 there is a solution with; # RooSimultaneous::generateSimGlobal, but this may change to; # the standard generate interface in 5.30.; # get test stat at observed UL in observed data; # toyData.get().Print(""v""); # print(""obsTSatObsUL "", obsTSatObsUL, ""toyTS "", toyTSatObsUL); # not sure about <= part yet; # not sure about <= part yet; # loop over points in belt to find upper limit for this toy data; # thisTS = profile.getVal(); # print(f""poi = {firstPOI.getVal()} max is {arMax} this profile = {thisTS}""); # print(""thisTS = "", thisTS); # for few events, data is often the same, and UL is often the same; # print(""thisUL = "", thisUL); # if you want to see a plot of the sampling distribution for a particular scan point:; #; # sampPlot = ROOT.RooStats.SamplingDistPlot(); # indexInScan = 0; # tmpPoint = parameterScan.get(indexInScan).clone(""temp""); # firstPOI.setVal( tmpPoint.getRealValue(firstPOI.GetName()) ); # toymcsampler.SetParametersForTestStat(tmpPOI); # samp = toymcsampler.GetSamplingDistribution(tmpPoint); # sampPlot.AddSamplingDistribution(samp); #",MatchSource.CODE_COMMENT,tutorials/roostats/TwoSidedFrequentistUpperLimitWithBands.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/TwoSidedFrequentistUpperLimitWithBands.py
Modifiability,config,configuration,"er range of the; # parameter should be well above the expected upper limit... but not too high or one will; # need a very large value of nPointsToScan to resolve the relevant region. This can be; # improved, but this is the first version of this script.; #; # Important note: when the model includes external constraint terms, like a Gaussian; # constraint to a nuisance parameter centered around some nominal value there is; # a subtlety. The asymptotic results are all based on the assumption that all the; # measurements fluctuate... including the nominal values from auxiliary measurements.; # If these do not fluctuate, this corresponds to an ""conditional ensemble"". The; # result is that the distribution of the test statistic can become very non-chi^2.; # This results in thresholds that become very large.; #; # \macro_image; # \macro_output; # \macro_code; #; # \authors Kyle Cranmer, contributions from Aaron Armbruster, Haoshuang Ji, Haichen Wang, Daniel Whiteson, and Jolly Chen (Python translation); # -------------------------------------------------------; # User configuration parameters; # degrade/improve number of pseudo-experiments used to define the confidence belt.; # value of 1 corresponds to default number of toys in the tail, which is 50/(1-confidenceLevel); # number of steps in the parameter of interest; # number of toys used to define the expected limit and band; # -------------------------------------------------------; # First part is just to access a user-defined file; # or create the standard example file if it doesn't exist; # note opposite return code; # if file does not exists generate with histfactory; # Normally this would be run on the command line; # Try to open the file; # -------------------------------------------------------; # Now get the data and workspace; # get the workspace out of the file; # get the modelConfig out of the file; # get the modelConfig out of the file; # -------------------------------------------------------; # Now get th",MatchSource.CODE_COMMENT,tutorials/roostats/TwoSidedFrequentistUpperLimitWithBands.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/TwoSidedFrequentistUpperLimitWithBands.py
Security,access,access,"esponds to an ""conditional ensemble"". The; # result is that the distribution of the test statistic can become very non-chi^2.; # This results in thresholds that become very large.; #; # \macro_image; # \macro_output; # \macro_code; #; # \authors Kyle Cranmer, contributions from Aaron Armbruster, Haoshuang Ji, Haichen Wang, Daniel Whiteson, and Jolly Chen (Python translation); # -------------------------------------------------------; # User configuration parameters; # degrade/improve number of pseudo-experiments used to define the confidence belt.; # value of 1 corresponds to default number of toys in the tail, which is 50/(1-confidenceLevel); # number of steps in the parameter of interest; # number of toys used to define the expected limit and band; # -------------------------------------------------------; # First part is just to access a user-defined file; # or create the standard example file if it doesn't exist; # note opposite return code; # if file does not exists generate with histfactory; # Normally this would be run on the command line; # Try to open the file; # -------------------------------------------------------; # Now get the data and workspace; # get the workspace out of the file; # get the modelConfig out of the file; # get the modelConfig out of the file; # -------------------------------------------------------; # Now get the POI for convenience; # you may want to adjust the range of your POI; # firstPOI.setMin(0); # firstPOI.setMax(10); # -------------------------------------------------------; # create and use the FeldmanCousins tool; # to find and plot the 95% confidence interval; # on the parameter of interest as specified; # in the model config; # REMEMBER, we will change the test statistic; # so this is NOT a Feldman-Cousins interval; # improve sampling that defines confidence belt; # fc.UseAdaptiveSampling(True) # speed it up a bit, but don't use for expected limits; # set how many points per parameter of interest to scan; # save the inform",MatchSource.CODE_COMMENT,tutorials/roostats/TwoSidedFrequentistUpperLimitWithBands.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/TwoSidedFrequentistUpperLimitWithBands.py
Testability,test,test,"# \file; # \ingroup tutorial_roostats; # \notebook -js; # TwoSidedFrequentistUpperLimitWithBands; #; #; # This is a standard demo that can be used with any ROOT file; # prepared in the standard way. You specify:; # - name for input ROOT file; # - name of workspace inside ROOT file that holds model and data; # - name of ModelConfig that specifies details for calculator tools; # - name of dataset; #; # With default parameters the macro will attempt to run the; # standard hist2workspace example and read the ROOT file; # that it produces.; #; # You may want to control:; # ~~~{.cpp}; # double confidenceLevel=0.95;; # double additionalToysFac = 1.;; # int nPointsToScan = 12;; # int nToyMC = 200;; # ~~~; #; # This uses a modified version of the profile likelihood ratio as; # a test statistic for upper limits (eg. test stat = 0 if muhat>mu).; #; # Based on the observed data, one defines a set of parameter points; # to be tested based on the value of the parameter of interest; # and the conditional MLE (eg. profiled) values of the nuisance parameters.; #; # At each parameter point, pseudo-experiments are generated using this; # fixed reference model and then the test statistic is evaluated.; # The auxiliary measurements (global observables) associated with the; # constraint terms in nuisance parameters are also fluctuated in the; # process of generating the pseudo-experiments in a frequentist manner; # forming an 'unconditional ensemble'. One could form a 'conditional'; # ensemble in which these auxiliary measurements are fixed. Note that the; # nuisance parameters are not randomized, which is a Bayesian procedure.; # Note, the nuisance parameters are floating in the fits. For each point,; # the threshold that defines the 95% acceptance region is found. This; # forms a ""Confidence Belt"".; #; # After constructing the confidence belt, one can find the confidence; # interval for any particular dataset by finding the intersection; # of the observed test statistic and the confiden",MatchSource.CODE_COMMENT,tutorials/roostats/TwoSidedFrequentistUpperLimitWithBands.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/TwoSidedFrequentistUpperLimitWithBands.py
Integrability,message,messages,"## \file; ## \ingroup tutorial_roostats; ## \notebook -js; ## Demonstrate Z_Bi = Z_Gamma; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \date July 2022; ## \authors Artem Busorgin, Kyle Cranmer and Wouter Verkerke (C++ version); # Make model for prototype on/off problem; # Pois(x | s+b) * Pois(y | tau b ); # for Z_Gamma, use uniform prior on b.; # construct the Bayesian-averaged model (eg. a projection pdf); # p'(x|s) = \int db p(x|s+b) * [ p(y|b) * prior(b) ]; # plot it, blue is averaged model, red is b known exactly; # compare analytic calculation of Z_Bi; # with the numerical RooFit implementation of Z_Gamma; # for an example with x = 150, y = 100; # numeric RooFit Z_Gamma; # get ugly print messages out of the way; # analytic Z_Bi",MatchSource.CODE_COMMENT,tutorials/roostats/Zbi_Zgamma.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/roostats/Zbi_Zgamma.py
Testability,test,test,"## \file; ## \ingroup tutorial_sql; ## \notebook -nodraw; ## Create a runcatalog table in a MySQL test database.; ##; ## Based on the code sqlcreatedb.C by Sergey Linev; ##; ## \macro_code; ##; ## \author Juan Fernando Jaramillo Botero; # read in runcatalog table definition; # open connection to MySQL server on localhost; # create new table (delete old one first if exists)",MatchSource.CODE_COMMENT,tutorials/sql/sqlcreatedb.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/sql/sqlcreatedb.py
Availability,avail,available,"## \file; ## \ingroup tutorial_sql; ##; ## Query example to MySQL test database.; ## Example of query by using the test database made in MySQL, you need the; ## database test installed in localhost, with user nobody without password.; ##; ## Based on sqlselect.C by Sergey Linev; ##; ## \macro_code; ##; ## \author Juan Fernando Jaramillo Botero; # list databases available on server; # list tables in database ""test"" (the permission tables); # list columns in table ""runcatalog"" in database ""mysql""; # start timer; # query database and print results; # sql = ""select dataset,rawfilepath from test.runcatalog "" \; # ""WHERE tag&(1<<2) AND (run=490001 OR run=300122)""; # stop timer and print results",MatchSource.CODE_COMMENT,tutorials/sql/sqlselect.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/sql/sqlselect.py
Deployability,install,installed,"## \file; ## \ingroup tutorial_sql; ##; ## Query example to MySQL test database.; ## Example of query by using the test database made in MySQL, you need the; ## database test installed in localhost, with user nobody without password.; ##; ## Based on sqlselect.C by Sergey Linev; ##; ## \macro_code; ##; ## \author Juan Fernando Jaramillo Botero; # list databases available on server; # list tables in database ""test"" (the permission tables); # list columns in table ""runcatalog"" in database ""mysql""; # start timer; # query database and print results; # sql = ""select dataset,rawfilepath from test.runcatalog "" \; # ""WHERE tag&(1<<2) AND (run=490001 OR run=300122)""; # stop timer and print results",MatchSource.CODE_COMMENT,tutorials/sql/sqlselect.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/sql/sqlselect.py
Security,password,password,"## \file; ## \ingroup tutorial_sql; ##; ## Query example to MySQL test database.; ## Example of query by using the test database made in MySQL, you need the; ## database test installed in localhost, with user nobody without password.; ##; ## Based on sqlselect.C by Sergey Linev; ##; ## \macro_code; ##; ## \author Juan Fernando Jaramillo Botero; # list databases available on server; # list tables in database ""test"" (the permission tables); # list columns in table ""runcatalog"" in database ""mysql""; # start timer; # query database and print results; # sql = ""select dataset,rawfilepath from test.runcatalog "" \; # ""WHERE tag&(1<<2) AND (run=490001 OR run=300122)""; # stop timer and print results",MatchSource.CODE_COMMENT,tutorials/sql/sqlselect.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/sql/sqlselect.py
Testability,test,test,"## \file; ## \ingroup tutorial_sql; ##; ## Query example to MySQL test database.; ## Example of query by using the test database made in MySQL, you need the; ## database test installed in localhost, with user nobody without password.; ##; ## Based on sqlselect.C by Sergey Linev; ##; ## \macro_code; ##; ## \author Juan Fernando Jaramillo Botero; # list databases available on server; # list tables in database ""test"" (the permission tables); # list columns in table ""runcatalog"" in database ""mysql""; # start timer; # query database and print results; # sql = ""select dataset,rawfilepath from test.runcatalog "" \; # ""WHERE tag&(1<<2) AND (run=490001 OR run=300122)""; # stop timer and print results",MatchSource.CODE_COMMENT,tutorials/sql/sqlselect.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/sql/sqlselect.py
Safety,predict,prediction,"### \file; ### \ingroup tutorial_tmva; ### \notebook -nodraw; ### Example of getting batches of events from a ROOT dataset into a basic; ### PyTorch workflow.; ###; ### \macro_code; ### \macro_output; ### \author Dante Niewenhuis; # Returns two generators that return training and validation batches; # as PyTorch tensors.; # Get a list of the columns used for training; # Initialize PyTorch model; # Loop through the training set and train model; # Make prediction and calculate loss; # improve model; # Calculate accuracy; #################################################################; # Validation; #################################################################; # Evaluate the model on the validation set; # Make prediction and calculate accuracy",MatchSource.CODE_COMMENT,tutorials/tmva/RBatchGenerator_PyTorch.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/RBatchGenerator_PyTorch.py
Security,validat,validation,"### \file; ### \ingroup tutorial_tmva; ### \notebook -nodraw; ### Example of getting batches of events from a ROOT dataset into a basic; ### PyTorch workflow.; ###; ### \macro_code; ### \macro_output; ### \author Dante Niewenhuis; # Returns two generators that return training and validation batches; # as PyTorch tensors.; # Get a list of the columns used for training; # Initialize PyTorch model; # Loop through the training set and train model; # Make prediction and calculate loss; # improve model; # Calculate accuracy; #################################################################; # Validation; #################################################################; # Evaluate the model on the validation set; # Make prediction and calculate accuracy",MatchSource.CODE_COMMENT,tutorials/tmva/RBatchGenerator_PyTorch.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/RBatchGenerator_PyTorch.py
Security,validat,validation,"### \file; ### \ingroup tutorial_tmva; ### \notebook -nodraw; ### Example of getting batches of events from a ROOT dataset into a basic; ### TensorFlow workflow.; ###; ### \macro_code; ### \macro_output; ### \author Dante Niewenhuis; # Returns two TF.Dataset for training and validation batches.; # Get a list of the columns used for training; ##############################################################################; # AI example; ##############################################################################; # Define TensorFlow model; # input shape required; # Train model",MatchSource.CODE_COMMENT,tutorials/tmva/RBatchGenerator_TensorFlow.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/RBatchGenerator_TensorFlow.py
Modifiability,variab,variables,"## \file; ## \ingroup tutorial_tmva; ## \notebook -nodraw; ## This tutorial illustrates how to prepare ROOT datasets to be nicely readable; ## by most machine learning methods. This requires filtering the initial complex; ## datasets and writing the data in a flat format.; ##; ## \macro_code; ## \macro_output; ##; ## \date August 2019; ## \author Stefan Wunsch; """"""; Reduce initial dataset to only events which shall be used for training; """"""; """"""; Define the variables which shall be used for training; """"""; # Load dataset, filter the required events and define the training variables; # Book cutflow report; # Split dataset by event number for training and testing; # Print cutflow report",MatchSource.CODE_COMMENT,tutorials/tmva/tmva100_DataPreparation.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/tmva100_DataPreparation.py
Testability,test,testing,"## \file; ## \ingroup tutorial_tmva; ## \notebook -nodraw; ## This tutorial illustrates how to prepare ROOT datasets to be nicely readable; ## by most machine learning methods. This requires filtering the initial complex; ## datasets and writing the data in a flat format.; ##; ## \macro_code; ## \macro_output; ##; ## \date August 2019; ## \author Stefan Wunsch; """"""; Reduce initial dataset to only events which shall be used for training; """"""; """"""; Define the variables which shall be used for training; """"""; # Load dataset, filter the required events and define the training variables; # Book cutflow report; # Split dataset by event number for training and testing; # Print cutflow report",MatchSource.CODE_COMMENT,tutorials/tmva/tmva100_DataPreparation.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/tmva100_DataPreparation.py
Usability,learn,learning,"## \file; ## \ingroup tutorial_tmva; ## \notebook -nodraw; ## This tutorial illustrates how to prepare ROOT datasets to be nicely readable; ## by most machine learning methods. This requires filtering the initial complex; ## datasets and writing the data in a flat format.; ##; ## \macro_code; ## \macro_output; ##; ## \date August 2019; ## \author Stefan Wunsch; """"""; Reduce initial dataset to only events which shall be used for training; """"""; """"""; Define the variables which shall be used for training; """"""; # Load dataset, filter the required events and define the training variables; # Book cutflow report; # Split dataset by event number for training and testing; # Print cutflow report",MatchSource.CODE_COMMENT,tutorials/tmva/tmva100_DataPreparation.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/tmva100_DataPreparation.py
Safety,avoid,avoid,"## \file; ## \ingroup tutorial_tmva; ## \notebook -nodraw; ## This tutorial show how you can train a machine learning model with any package; ## reading the training data directly from ROOT files. Using XGBoost, we illustrate; ## how you can convert an externally trained model in a format serializable and readable; ## with the fast tree inference engine offered by TMVA.; ##; ## \macro_code; ## \macro_output; ##; ## \date August 2019; ## \author Stefan Wunsch; # XGBoost has to be imported before ROOT to avoid crashes because of clashing; # std::regexp symbols that are exported by cppyy.; # See also: https://github.com/wlav/cppyy/issues/227; # Read data from ROOT files; # Convert inputs to format readable by machine learning tools; # Create labels; # Compute weights balancing both classes; # Load data; # Fit xgboost model; # Save model in TMVA format",MatchSource.CODE_COMMENT,tutorials/tmva/tmva101_Training.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/tmva101_Training.py
Usability,learn,learning,"## \file; ## \ingroup tutorial_tmva; ## \notebook -nodraw; ## This tutorial show how you can train a machine learning model with any package; ## reading the training data directly from ROOT files. Using XGBoost, we illustrate; ## how you can convert an externally trained model in a format serializable and readable; ## with the fast tree inference engine offered by TMVA.; ##; ## \macro_code; ## \macro_output; ##; ## \date August 2019; ## \author Stefan Wunsch; # XGBoost has to be imported before ROOT to avoid crashes because of clashing; # std::regexp symbols that are exported by cppyy.; # See also: https://github.com/wlav/cppyy/issues/227; # Read data from ROOT files; # Convert inputs to format readable by machine learning tools; # Create labels; # Compute weights balancing both classes; # Load data; # Fit xgboost model; # Save model in TMVA format",MatchSource.CODE_COMMENT,tutorials/tmva/tmva101_Training.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/tmva101_Training.py
Safety,predict,prediction,"## \file; ## \ingroup tutorial_tmva; ## \notebook -nodraw; ## This tutorial illustrates how you can test a trained BDT model using the fast; ## tree inference engine offered by TMVA and external tools such as scikit-learn.; ##; ## \macro_code; ## \macro_output; ##; ## \date August 2019; ## \author Stefan Wunsch; # Load data; # Load trained model; # Make prediction; # Compute ROC using sklearn; # Plot ROC",MatchSource.CODE_COMMENT,tutorials/tmva/tmva102_Testing.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/tmva102_Testing.py
Testability,test,test,"## \file; ## \ingroup tutorial_tmva; ## \notebook -nodraw; ## This tutorial illustrates how you can test a trained BDT model using the fast; ## tree inference engine offered by TMVA and external tools such as scikit-learn.; ##; ## \macro_code; ## \macro_output; ##; ## \date August 2019; ## \author Stefan Wunsch; # Load data; # Load trained model; # Make prediction; # Compute ROC using sklearn; # Plot ROC",MatchSource.CODE_COMMENT,tutorials/tmva/tmva102_Testing.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/tmva102_Testing.py
Usability,learn,learn,"## \file; ## \ingroup tutorial_tmva; ## \notebook -nodraw; ## This tutorial illustrates how you can test a trained BDT model using the fast; ## tree inference engine offered by TMVA and external tools such as scikit-learn.; ##; ## \macro_code; ## \macro_output; ##; ## \date August 2019; ## \author Stefan Wunsch; # Load data; # Load trained model; # Make prediction; # Compute ROC using sklearn; # Plot ROC",MatchSource.CODE_COMMENT,tutorials/tmva/tmva102_Testing.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/tmva102_Testing.py
Availability,avail,available,"for DataLoader::PrepareTrainingAndTestTree; # DataSetInfo : [dataset] : Added class ""Signal""; # : Add Tree sig_tree of type Signal with 10000 events; # DataSetInfo : [dataset] : Added class ""Background""; # : Add Tree bkg_tree of type Background with 10000 events; # signalTree.Print();; # Booking Methods; # Here we book the TMVA methods. We book a Boosted Decision Tree method (BDT); # Boosted Decision Trees; #### Booking Deep Neural Network; # Here we book the DNN of TMVA. See the example TMVA_Higgs_Classification.C for a detailed description of the; # options; # Training strategies; # one can catenate several training strings with different parameters (e.g. learning rates or regularizations; # parameters) The training string must be concatenated with the `|` delimiter; # + ""|"" + trainingString2 + ...; # Build now the full DNN Option string; # use GPU if available; ### Book Convolutional Neural Network in TMVA; # For building a CNN one needs to define; # - Input Layout : number of channels (in this case = 1) | image height | image width; # - Batch Layout : batch size | number of channels | image size = (height*width); # Then one add Convolutional layers and MaxPool layers.; # - For Convolutional layer the option string has to be:; # - CONV | number of units | filter height | filter width | stride height | stride width | padding height | paddig; # width | activation function; # - note in this case we are using a filer 3x3 and padding=1 and stride=1 so we get the output dimension of the; # conv layer equal to the input; # - note we use after the first convolutional layer a batch normalization layer. This seems to help significantly the; # convergence; # - For the MaxPool layer:; # - MAXPOOL | pool height | pool width | stride height | stride width; # The RESHAPE layer is needed to flatten the output before the Dense layer; # Note that to run the CNN is required to have CPU or GPU support; # Training strategies.; ## New DL (CNN); # use GPU if available; ### Book Convolut",MatchSource.CODE_COMMENT,tutorials/tmva/TMVA_CNN_Classification.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/TMVA_CNN_Classification.py
Deployability,configurat,configuration,"e of clashing; # std::regexp symbols that are exported by cppyy.; # See also: https://github.com/wlav/cppyy/issues/227; # image size (nh x nw); # number of events we use to fill each image; # 5% difference in the sigma; # create signal and background trees with a single branch; # an std::vector<float> of size nh x nw containing the image data; # generate random means in range [3,7] to be not too much on the border; # add some noise in each bin; # use a larger value to get better results; # use max 4 threads; # maximum number of epochs used for training; # do enable MT running; # switch OFF MT in OpenBLAS; ## Create TMVA Factory; # Create the Factory class. Later you can choose the methods; # whose performance you'd like to investigate.; # The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; # - The first argument is the base of the name of all the output; # weight files in the directory weight/ that will be created with the; # method parameters; # - The second argument is the output file for the training results; # - The third argument is a string option defining some general configuration for the TMVA session.; # For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the; # option string; # - note that we disable any pre-transformation of the input variables and we avoid computing correlations between; # input variables; ## Declare DataLoader(s); # The next step is to declare the DataLoader class that deals with input variables; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; # In this case the input data consists of an image of 16x16 pixels. Each single pixel is a branch in a ROOT TTree; ## Setup Dataset(s); # Define input data file and signal and background trees; # if the input file does not exist create it; # inputFileName = ",MatchSource.CODE_COMMENT,tutorials/tmva/TMVA_CNN_Classification.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/TMVA_CNN_Classification.py
Energy Efficiency,allocate,allocates,"+ ""|"" + trainingString2 + ...; # Build now the full DNN Option string; # use GPU if available; ### Book Convolutional Neural Network in TMVA; # For building a CNN one needs to define; # - Input Layout : number of channels (in this case = 1) | image height | image width; # - Batch Layout : batch size | number of channels | image size = (height*width); # Then one add Convolutional layers and MaxPool layers.; # - For Convolutional layer the option string has to be:; # - CONV | number of units | filter height | filter width | stride height | stride width | padding height | paddig; # width | activation function; # - note in this case we are using a filer 3x3 and padding=1 and stride=1 so we get the output dimension of the; # conv layer equal to the input; # - note we use after the first convolutional layer a batch normalization layer. This seems to help significantly the; # convergence; # - For the MaxPool layer:; # - MAXPOOL | pool height | pool width | stride height | stride width; # The RESHAPE layer is needed to flatten the output before the Dense layer; # Note that to run the CNN is required to have CPU or GPU support; # Training strategies.; ## New DL (CNN); # use GPU if available; ### Book Convolutional Neural Network in Keras using a generated model; # check that pytorch can be imported and file defining the model exists; #cmd = str(ROOT.TMVA.Python_Executable()) + "" "" + pyTorchFileName; #os.system(cmd); #import PyTorch_Generate_CNN_Model; # create python script which can be executed; # create 2 conv2d layer + maxpool + dense; # from keras.initializers import TruncatedNormal; # from keras import initializations; # from keras.callbacks import ReduceLROnPlateau; # stride for maxpool is equal to pool size; # model.add(Dropout(0.2)); # book PyKeras method only if Keras model could be created; # needed for RTX NVidia card and to avoid TF allocates all GPU memory; ## Train Methods; ## Test and Evaluate Methods; ## Plot ROC Curve; # close outputfile to save output file",MatchSource.CODE_COMMENT,tutorials/tmva/TMVA_CNN_Classification.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/TMVA_CNN_Classification.py
Modifiability,config,configuration,"e of clashing; # std::regexp symbols that are exported by cppyy.; # See also: https://github.com/wlav/cppyy/issues/227; # image size (nh x nw); # number of events we use to fill each image; # 5% difference in the sigma; # create signal and background trees with a single branch; # an std::vector<float> of size nh x nw containing the image data; # generate random means in range [3,7] to be not too much on the border; # add some noise in each bin; # use a larger value to get better results; # use max 4 threads; # maximum number of epochs used for training; # do enable MT running; # switch OFF MT in OpenBLAS; ## Create TMVA Factory; # Create the Factory class. Later you can choose the methods; # whose performance you'd like to investigate.; # The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; # - The first argument is the base of the name of all the output; # weight files in the directory weight/ that will be created with the; # method parameters; # - The second argument is the output file for the training results; # - The third argument is a string option defining some general configuration for the TMVA session.; # For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the; # option string; # - note that we disable any pre-transformation of the input variables and we avoid computing correlations between; # input variables; ## Declare DataLoader(s); # The next step is to declare the DataLoader class that deals with input variables; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; # In this case the input data consists of an image of 16x16 pixels. Each single pixel is a branch in a ROOT TTree; ## Setup Dataset(s); # Define input data file and signal and background trees; # if the input file does not exist create it; # inputFileName = ",MatchSource.CODE_COMMENT,tutorials/tmva/TMVA_CNN_Classification.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/TMVA_CNN_Classification.py
Performance,perform,performance,"h a location (means in X and Y) different for each event; ## The difference between signal and background is in the gaussian width.; ## The width for the background gaussian is slightly larger than the signal width by few % values; # PyTorch has to be imported before ROOT to avoid crashes because of clashing; # std::regexp symbols that are exported by cppyy.; # See also: https://github.com/wlav/cppyy/issues/227; # image size (nh x nw); # number of events we use to fill each image; # 5% difference in the sigma; # create signal and background trees with a single branch; # an std::vector<float> of size nh x nw containing the image data; # generate random means in range [3,7] to be not too much on the border; # add some noise in each bin; # use a larger value to get better results; # use max 4 threads; # maximum number of epochs used for training; # do enable MT running; # switch OFF MT in OpenBLAS; ## Create TMVA Factory; # Create the Factory class. Later you can choose the methods; # whose performance you'd like to investigate.; # The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; # - The first argument is the base of the name of all the output; # weight files in the directory weight/ that will be created with the; # method parameters; # - The second argument is the output file for the training results; # - The third argument is a string option defining some general configuration for the TMVA session.; # For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the; # option string; # - note that we disable any pre-transformation of the input variables and we avoid computing correlations between; # input variables; ## Declare DataLoader(s); # The next step is to declare the DataLoader class that deals with input variables; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be par",MatchSource.CODE_COMMENT,tutorials/tmva/TMVA_CNN_Classification.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/TMVA_CNN_Classification.py
Safety,avoid,avoid,"## \file; ## \ingroup tutorial_tmva; ## \notebook; ## TMVA Classification Example Using a Convolutional Neural Network; ##; ## This is an example of using a CNN in TMVA. We do classification using a toy image data set; ## that is generated when running the example macro; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \author Harshal Shende; # TMVA Classification Example Using a Convolutional Neural Network; ## Helper function to create input images data; ## we create a signal and background 2D histograms from 2d gaussians; ## with a location (means in X and Y) different for each event; ## The difference between signal and background is in the gaussian width.; ## The width for the background gaussian is slightly larger than the signal width by few % values; # PyTorch has to be imported before ROOT to avoid crashes because of clashing; # std::regexp symbols that are exported by cppyy.; # See also: https://github.com/wlav/cppyy/issues/227; # image size (nh x nw); # number of events we use to fill each image; # 5% difference in the sigma; # create signal and background trees with a single branch; # an std::vector<float> of size nh x nw containing the image data; # generate random means in range [3,7] to be not too much on the border; # add some noise in each bin; # use a larger value to get better results; # use max 4 threads; # maximum number of epochs used for training; # do enable MT running; # switch OFF MT in OpenBLAS; ## Create TMVA Factory; # Create the Factory class. Later you can choose the methods; # whose performance you'd like to investigate.; # The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; # - The first argument is the base of the name of all the output; # weight files in the directory weight/ that will be created with the; # method parameters; # - The second argument is the output file for the training results; # - The third argument is a string option defining some general co",MatchSource.CODE_COMMENT,tutorials/tmva/TMVA_CNN_Classification.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/TMVA_CNN_Classification.py
Testability,test,test,"neral configuration for the TMVA session.; # For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the; # option string; # - note that we disable any pre-transformation of the input variables and we avoid computing correlations between; # input variables; ## Declare DataLoader(s); # The next step is to declare the DataLoader class that deals with input variables; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; # In this case the input data consists of an image of 16x16 pixels. Each single pixel is a branch in a ROOT TTree; ## Setup Dataset(s); # Define input data file and signal and background trees; # if the input file does not exist create it; # inputFileName = ""tmva_class_example.root""; # --- Register the training and test trees; # global event weights per tree (see below for setting event-wise weights); # You can add an arbitrary number of signal or background trees; ## add event variables (image); ## use new method (from ROOT 6.20 to add a variable array for all image data); # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; # Apply additional cuts on the signal and background samples (can be different); # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; # for example: TCut mycutb = ""abs(var1)<0.5"";; # Tell the factory how to use the training and testing events; # If no numbers of events are given, half of the events in the tree are used; # for training, and the other half for testing:; # loader.PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; # It is possible also to specify the number of training and testing events,; # note we",MatchSource.CODE_COMMENT,tutorials/tmva/TMVA_CNN_Classification.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/TMVA_CNN_Classification.py
Usability,learn,learning,"ng:; # loader.PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; # It is possible also to specify the number of training and testing events,; # note we disable the computation of the correlation matrix of the input variables; # build the string options for DataLoader::PrepareTrainingAndTestTree; # DataSetInfo : [dataset] : Added class ""Signal""; # : Add Tree sig_tree of type Signal with 10000 events; # DataSetInfo : [dataset] : Added class ""Background""; # : Add Tree bkg_tree of type Background with 10000 events; # signalTree.Print();; # Booking Methods; # Here we book the TMVA methods. We book a Boosted Decision Tree method (BDT); # Boosted Decision Trees; #### Booking Deep Neural Network; # Here we book the DNN of TMVA. See the example TMVA_Higgs_Classification.C for a detailed description of the; # options; # Training strategies; # one can catenate several training strings with different parameters (e.g. learning rates or regularizations; # parameters) The training string must be concatenated with the `|` delimiter; # + ""|"" + trainingString2 + ...; # Build now the full DNN Option string; # use GPU if available; ### Book Convolutional Neural Network in TMVA; # For building a CNN one needs to define; # - Input Layout : number of channels (in this case = 1) | image height | image width; # - Batch Layout : batch size | number of channels | image size = (height*width); # Then one add Convolutional layers and MaxPool layers.; # - For Convolutional layer the option string has to be:; # - CONV | number of units | filter height | filter width | stride height | stride width | padding height | paddig; # width | activation function; # - note in this case we are using a filer 3x3 and padding=1 and stride=1 so we get the output dimension of the; # conv layer equal to the input; # - note we use after the first convolutional layer a batch normalization layer. This seems to help significantly the; # convergence; # - For the MaxPool layer:; # - MAXPOOL | pool height | pool",MatchSource.CODE_COMMENT,tutorials/tmva/TMVA_CNN_Classification.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/TMVA_CNN_Classification.py
Availability,avail,available,"files in the directory weight/ that will be created with the; ## method parameters; ## - The second argument is the output file for the training results; ## - The third argument is a string option defining some general configuration for the TMVA session. For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the option string; # options to control used methods; # likelihood based discriminant; # likelihood based discriminant; # Fischer discriminant; # Multi Layer Perceptron (old TMVA NN implementation); # Boosted Decision Tree; # TMVA Deep learning ( CPU or GPU); # Use Keras Deep Learning via PyMVA; # cannot use Keras if PYMVA is not available; ## Setup Dataset(s); # Define now input data file and signal and background trees; # file exists; # --- Register the training and test trees; ## Declare DataLoader(s); # The next step is to declare the DataLoader class that deals with input variables; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; # We set now the input data trees in the TMVA DataLoader class; # global event weights per tree (see below for setting event-wise weights); # You can add an arbitrary number of signal or background trees; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; # Apply additional cuts on the signal and background samples (can be different); # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; # for example: TCut mycutb = ""abs(var1)<0.5"";; # Tell the factory how to use the training and testing events; #; # If no numbers of events are given, half of the events in the tree are used; # for training, and the other half for ",MatchSource.CODE_COMMENT,tutorials/tmva/TMVA_Higgs_Classification.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/TMVA_Higgs_Classification.py
Deployability,configurat,configuration,"## \file; ## \ingroup tutorial_tmva; ## \notebook; ## Classification example of TMVA based on public Higgs UCI dataset; ##; ## The UCI data set is a public HIGGS data set , see http://archive.ics.uci.edu/ml/datasets/HIGGS; ## used in this paper: Baldi, P., P. Sadowski, and D. Whiteson. “Searching for Exotic Particles in High-energy Physics; ## with Deep Learning.” Nature Communications 5 (July 2, 2014).; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \author Harshal Shende; ## Declare Factory; ## Create the Factory class. Later you can choose the methods; ## whose performance you'd like to investigate.; ## The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ## - The first argument is the base of the name of all the output; ## weightfiles in the directory weight/ that will be created with the; ## method parameters; ## - The second argument is the output file for the training results; ## - The third argument is a string option defining some general configuration for the TMVA session. For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the option string; # options to control used methods; # likelihood based discriminant; # likelihood based discriminant; # Fischer discriminant; # Multi Layer Perceptron (old TMVA NN implementation); # Boosted Decision Tree; # TMVA Deep learning ( CPU or GPU); # Use Keras Deep Learning via PyMVA; # cannot use Keras if PYMVA is not available; ## Setup Dataset(s); # Define now input data file and signal and background trees; # file exists; # --- Register the training and test trees; ## Declare DataLoader(s); # The next step is to declare the DataLoader class that deals with input variables; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; # We set now the input data trees in the TMVA Data",MatchSource.CODE_COMMENT,tutorials/tmva/TMVA_Higgs_Classification.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/TMVA_Higgs_Classification.py
Energy Efficiency,energy,energy,"## \file; ## \ingroup tutorial_tmva; ## \notebook; ## Classification example of TMVA based on public Higgs UCI dataset; ##; ## The UCI data set is a public HIGGS data set , see http://archive.ics.uci.edu/ml/datasets/HIGGS; ## used in this paper: Baldi, P., P. Sadowski, and D. Whiteson. “Searching for Exotic Particles in High-energy Physics; ## with Deep Learning.” Nature Communications 5 (July 2, 2014).; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \author Harshal Shende; ## Declare Factory; ## Create the Factory class. Later you can choose the methods; ## whose performance you'd like to investigate.; ## The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ## - The first argument is the base of the name of all the output; ## weightfiles in the directory weight/ that will be created with the; ## method parameters; ## - The second argument is the output file for the training results; ## - The third argument is a string option defining some general configuration for the TMVA session. For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the option string; # options to control used methods; # likelihood based discriminant; # likelihood based discriminant; # Fischer discriminant; # Multi Layer Perceptron (old TMVA NN implementation); # Boosted Decision Tree; # TMVA Deep learning ( CPU or GPU); # Use Keras Deep Learning via PyMVA; # cannot use Keras if PYMVA is not available; ## Setup Dataset(s); # Define now input data file and signal and background trees; # file exists; # --- Register the training and test trees; ## Declare DataLoader(s); # The next step is to declare the DataLoader class that deals with input variables; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; # We set now the input data trees in the TMVA Data",MatchSource.CODE_COMMENT,tutorials/tmva/TMVA_Higgs_Classification.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/TMVA_Higgs_Classification.py
Modifiability,config,configuration,"## \file; ## \ingroup tutorial_tmva; ## \notebook; ## Classification example of TMVA based on public Higgs UCI dataset; ##; ## The UCI data set is a public HIGGS data set , see http://archive.ics.uci.edu/ml/datasets/HIGGS; ## used in this paper: Baldi, P., P. Sadowski, and D. Whiteson. “Searching for Exotic Particles in High-energy Physics; ## with Deep Learning.” Nature Communications 5 (July 2, 2014).; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \author Harshal Shende; ## Declare Factory; ## Create the Factory class. Later you can choose the methods; ## whose performance you'd like to investigate.; ## The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ## - The first argument is the base of the name of all the output; ## weightfiles in the directory weight/ that will be created with the; ## method parameters; ## - The second argument is the output file for the training results; ## - The third argument is a string option defining some general configuration for the TMVA session. For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the option string; # options to control used methods; # likelihood based discriminant; # likelihood based discriminant; # Fischer discriminant; # Multi Layer Perceptron (old TMVA NN implementation); # Boosted Decision Tree; # TMVA Deep learning ( CPU or GPU); # Use Keras Deep Learning via PyMVA; # cannot use Keras if PYMVA is not available; ## Setup Dataset(s); # Define now input data file and signal and background trees; # file exists; # --- Register the training and test trees; ## Declare DataLoader(s); # The next step is to declare the DataLoader class that deals with input variables; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; # We set now the input data trees in the TMVA Data",MatchSource.CODE_COMMENT,tutorials/tmva/TMVA_Higgs_Classification.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/TMVA_Higgs_Classification.py
Performance,perform,performance,"## \file; ## \ingroup tutorial_tmva; ## \notebook; ## Classification example of TMVA based on public Higgs UCI dataset; ##; ## The UCI data set is a public HIGGS data set , see http://archive.ics.uci.edu/ml/datasets/HIGGS; ## used in this paper: Baldi, P., P. Sadowski, and D. Whiteson. “Searching for Exotic Particles in High-energy Physics; ## with Deep Learning.” Nature Communications 5 (July 2, 2014).; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \author Harshal Shende; ## Declare Factory; ## Create the Factory class. Later you can choose the methods; ## whose performance you'd like to investigate.; ## The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ## - The first argument is the base of the name of all the output; ## weightfiles in the directory weight/ that will be created with the; ## method parameters; ## - The second argument is the output file for the training results; ## - The third argument is a string option defining some general configuration for the TMVA session. For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the option string; # options to control used methods; # likelihood based discriminant; # likelihood based discriminant; # Fischer discriminant; # Multi Layer Perceptron (old TMVA NN implementation); # Boosted Decision Tree; # TMVA Deep learning ( CPU or GPU); # Use Keras Deep Learning via PyMVA; # cannot use Keras if PYMVA is not available; ## Setup Dataset(s); # Define now input data file and signal and background trees; # file exists; # --- Register the training and test trees; ## Declare DataLoader(s); # The next step is to declare the DataLoader class that deals with input variables; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; # We set now the input data trees in the TMVA Data",MatchSource.CODE_COMMENT,tutorials/tmva/TMVA_Higgs_Classification.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/TMVA_Higgs_Classification.py
Safety,avoid,avoid,"""|""`` separator.; # - Optimizer; # - Learning rate; # - Momentum (valid for SGD and RMSPROP); # - Regularization and Weight Decay; # - Dropout; # - Max number of epochs; # - Convergence steps. if the test error will not decrease after that value the training will stop; # - Batch size (This value must be the same specified in the input layout); # - Test Repetitions (the interval when the test error will be computed); #### 3. Define general DNN options; # We define the general DNN options concatenating in the final string the previously defined layout and training strategy.; # Note we use the ``"":""`` separator to separate the different higher level options, as in the other TMVA methods.; # In addition to input layout, batch layout and training strategy we add now:; # - Type of Loss function (e.g. CROSSENTROPY); # - Weight Initizalization (e.g XAVIER, XAVIERUNIFORM, NORMAL ); # - Variable Transformation; # - Type of Architecture (e.g. CPU, GPU, Standard); # We can then book the DL method using the built option string; # Define DNN layout; # Define Training strategies; # one can catenate several training strategies; # ADAM default parameters; # training2 = ROOT.TString(""LearningRate=1e-3,Momentum=0.9""; # ""ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,""; # ""MaxEpochs=20,WeightDecay=1e-4,Regularization=None,""; # ""Optimizer=SGD,DropConfig=0.0+0.0+0.0+0.""); # General Options.; # Keras DL; # create Keras model with 4 layers of 64 units and relu activations; # book PyKeras method only if Keras model could be created; # GpuOptions=""allow_growth=True"",; # ) # needed for RTX NVidia card and to avoid TF allocates all GPU memory; ## Train Methods; # Here we train all the previously booked methods.; ## Test all methods; # Now we test and evaluate all methods using the test data set; # after we get the ROC curve and we display; # at the end we close the output file which contains the evaluation result of all methods and it can be used by TMVAGUI; # to display additional plots",MatchSource.CODE_COMMENT,tutorials/tmva/TMVA_Higgs_Classification.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/TMVA_Higgs_Classification.py
Testability,test,test,"files in the directory weight/ that will be created with the; ## method parameters; ## - The second argument is the output file for the training results; ## - The third argument is a string option defining some general configuration for the TMVA session. For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the option string; # options to control used methods; # likelihood based discriminant; # likelihood based discriminant; # Fischer discriminant; # Multi Layer Perceptron (old TMVA NN implementation); # Boosted Decision Tree; # TMVA Deep learning ( CPU or GPU); # Use Keras Deep Learning via PyMVA; # cannot use Keras if PYMVA is not available; ## Setup Dataset(s); # Define now input data file and signal and background trees; # file exists; # --- Register the training and test trees; ## Declare DataLoader(s); # The next step is to declare the DataLoader class that deals with input variables; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; # We set now the input data trees in the TMVA DataLoader class; # global event weights per tree (see below for setting event-wise weights); # You can add an arbitrary number of signal or background trees; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; # Apply additional cuts on the signal and background samples (can be different); # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; # for example: TCut mycutb = ""abs(var1)<0.5"";; # Tell the factory how to use the training and testing events; #; # If no numbers of events are given, half of the events in the tree are used; # for training, and the other half for ",MatchSource.CODE_COMMENT,tutorials/tmva/TMVA_Higgs_Classification.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/TMVA_Higgs_Classification.py
Usability,learn,learning,"files in the directory weight/ that will be created with the; ## method parameters; ## - The second argument is the output file for the training results; ## - The third argument is a string option defining some general configuration for the TMVA session. For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the option string; # options to control used methods; # likelihood based discriminant; # likelihood based discriminant; # Fischer discriminant; # Multi Layer Perceptron (old TMVA NN implementation); # Boosted Decision Tree; # TMVA Deep learning ( CPU or GPU); # Use Keras Deep Learning via PyMVA; # cannot use Keras if PYMVA is not available; ## Setup Dataset(s); # Define now input data file and signal and background trees; # file exists; # --- Register the training and test trees; ## Declare DataLoader(s); # The next step is to declare the DataLoader class that deals with input variables; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; # We set now the input data trees in the TMVA DataLoader class; # global event weights per tree (see below for setting event-wise weights); # You can add an arbitrary number of signal or background trees; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; # Apply additional cuts on the signal and background samples (can be different); # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; # for example: TCut mycutb = ""abs(var1)<0.5"";; # Tell the factory how to use the training and testing events; #; # If no numbers of events are given, half of the events in the tree are used; # for training, and the other half for ",MatchSource.CODE_COMMENT,tutorials/tmva/TMVA_Higgs_Classification.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/TMVA_Higgs_Classification.py
Availability,avail,available,"on using a toy time dependent data set; ## that is generated when running this example macro; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \author Harshal Shende; # TMVA Classification Example Using a Recurrent Neural Network; # This is an example of using a RNN in TMVA.; # We do the classification using a toy data set containing a time series of data sample ntimes; # and with dimension ndim that is generated when running the provided function `MakeTimeData (nevents, ntime, ndim)`; # use max 4 threads; # do enable MT running; # switch off MT in OpenBLAS to avoid conflict with tbb; ## Helper function to generate the time data set; ## make some time data but not of fixed length.; ## use a poisson with mu = 5 and truncated at 10; # ntime = 10;; # ndim = 30; // number of dim/time; # std::cout << j*10+k << "" "";; ## macro for performing a classification using a Recurrent Neural Network; ## @param use_type; ## use_type = 0 use Simple RNN network; ## use_type = 1 use LSTM network; ## use_type = 2 use GRU; ## use_type = 3 build 3 different networks with RNN, LSTM and GRU; # total events to be generated for signal or background; # use GPU for TMVA if available; # if file does not exists create it; # Create a ROOT output file where TMVA will store ntuples, histograms, etc.; ## Declare Factory; # Create the Factory class. Later you can choose the methods; # whose performance you'd like to investigate.; # The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to; # pass; # - The first argument is the base of the name of all the output; # weightfiles in the directory weight/ that will be created with the; # method parameters; # - The second argument is the output file for the training results; #; # - The third argument is a string option defining some general configuration for the TMVA session.; # For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in; # the opti",MatchSource.CODE_COMMENT,tutorials/tmva/TMVA_RNN_Classification.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/TMVA_RNN_Classification.py
Deployability,configurat,configuration,"of fixed length.; ## use a poisson with mu = 5 and truncated at 10; # ntime = 10;; # ndim = 30; // number of dim/time; # std::cout << j*10+k << "" "";; ## macro for performing a classification using a Recurrent Neural Network; ## @param use_type; ## use_type = 0 use Simple RNN network; ## use_type = 1 use LSTM network; ## use_type = 2 use GRU; ## use_type = 3 build 3 different networks with RNN, LSTM and GRU; # total events to be generated for signal or background; # use GPU for TMVA if available; # if file does not exists create it; # Create a ROOT output file where TMVA will store ntuples, histograms, etc.; ## Declare Factory; # Create the Factory class. Later you can choose the methods; # whose performance you'd like to investigate.; # The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to; # pass; # - The first argument is the base of the name of all the output; # weightfiles in the directory weight/ that will be created with the; # method parameters; # - The second argument is the output file for the training results; #; # - The third argument is a string option defining some general configuration for the TMVA session.; # For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in; # the option string; # // Creating the factory object; ## add variables - use new AddVariablesArray function; # check given input; # Apply additional cuts on the signal and background samples (can be different); # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; # build the string options for DataLoader::PrepareTrainingAndTestTree; ## Book TMVA recurrent models; # Book the different types of recurrent models in TMVA (SimpleRNN, LSTM or GRU); ## Define RNN layer layout; ## it should be LayerType (RNN or LSTM or GRU) | number of units | number of inputs | time steps | remember output (typically no=0 | return full sequence; ## Defining Training strategies. Different training ",MatchSource.CODE_COMMENT,tutorials/tmva/TMVA_RNN_Classification.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/TMVA_RNN_Classification.py
Integrability,depend,dependent,"## \file; ## \ingroup tutorial_tmva; ## \notebook; ## TMVA Classification Example Using a Recurrent Neural Network; ##; ## This is an example of using a RNN in TMVA. We do classification using a toy time dependent data set; ## that is generated when running this example macro; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \author Harshal Shende; # TMVA Classification Example Using a Recurrent Neural Network; # This is an example of using a RNN in TMVA.; # We do the classification using a toy data set containing a time series of data sample ntimes; # and with dimension ndim that is generated when running the provided function `MakeTimeData (nevents, ntime, ndim)`; # use max 4 threads; # do enable MT running; # switch off MT in OpenBLAS to avoid conflict with tbb; ## Helper function to generate the time data set; ## make some time data but not of fixed length.; ## use a poisson with mu = 5 and truncated at 10; # ntime = 10;; # ndim = 30; // number of dim/time; # std::cout << j*10+k << "" "";; ## macro for performing a classification using a Recurrent Neural Network; ## @param use_type; ## use_type = 0 use Simple RNN network; ## use_type = 1 use LSTM network; ## use_type = 2 use GRU; ## use_type = 3 build 3 different networks with RNN, LSTM and GRU; # total events to be generated for signal or background; # use GPU for TMVA if available; # if file does not exists create it; # Create a ROOT output file where TMVA will store ntuples, histograms, etc.; ## Declare Factory; # Create the Factory class. Later you can choose the methods; # whose performance you'd like to investigate.; # The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to; # pass; # - The first argument is the base of the name of all the output; # weightfiles in the directory weight/ that will be created with the; # method parameters; # - The second argument is the output file for the training results; #; # - The third argument is a string opti",MatchSource.CODE_COMMENT,tutorials/tmva/TMVA_RNN_Classification.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/TMVA_RNN_Classification.py
Modifiability,config,configuration,"of fixed length.; ## use a poisson with mu = 5 and truncated at 10; # ntime = 10;; # ndim = 30; // number of dim/time; # std::cout << j*10+k << "" "";; ## macro for performing a classification using a Recurrent Neural Network; ## @param use_type; ## use_type = 0 use Simple RNN network; ## use_type = 1 use LSTM network; ## use_type = 2 use GRU; ## use_type = 3 build 3 different networks with RNN, LSTM and GRU; # total events to be generated for signal or background; # use GPU for TMVA if available; # if file does not exists create it; # Create a ROOT output file where TMVA will store ntuples, histograms, etc.; ## Declare Factory; # Create the Factory class. Later you can choose the methods; # whose performance you'd like to investigate.; # The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to; # pass; # - The first argument is the base of the name of all the output; # weightfiles in the directory weight/ that will be created with the; # method parameters; # - The second argument is the output file for the training results; #; # - The third argument is a string option defining some general configuration for the TMVA session.; # For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in; # the option string; # // Creating the factory object; ## add variables - use new AddVariablesArray function; # check given input; # Apply additional cuts on the signal and background samples (can be different); # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; # build the string options for DataLoader::PrepareTrainingAndTestTree; ## Book TMVA recurrent models; # Book the different types of recurrent models in TMVA (SimpleRNN, LSTM or GRU); ## Define RNN layer layout; ## it should be LayerType (RNN or LSTM or GRU) | number of units | number of inputs | time steps | remember output (typically no=0 | return full sequence; ## Defining Training strategies. Different training ",MatchSource.CODE_COMMENT,tutorials/tmva/TMVA_RNN_Classification.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/TMVA_RNN_Classification.py
Performance,perform,performing,"on using a toy time dependent data set; ## that is generated when running this example macro; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \author Harshal Shende; # TMVA Classification Example Using a Recurrent Neural Network; # This is an example of using a RNN in TMVA.; # We do the classification using a toy data set containing a time series of data sample ntimes; # and with dimension ndim that is generated when running the provided function `MakeTimeData (nevents, ntime, ndim)`; # use max 4 threads; # do enable MT running; # switch off MT in OpenBLAS to avoid conflict with tbb; ## Helper function to generate the time data set; ## make some time data but not of fixed length.; ## use a poisson with mu = 5 and truncated at 10; # ntime = 10;; # ndim = 30; // number of dim/time; # std::cout << j*10+k << "" "";; ## macro for performing a classification using a Recurrent Neural Network; ## @param use_type; ## use_type = 0 use Simple RNN network; ## use_type = 1 use LSTM network; ## use_type = 2 use GRU; ## use_type = 3 build 3 different networks with RNN, LSTM and GRU; # total events to be generated for signal or background; # use GPU for TMVA if available; # if file does not exists create it; # Create a ROOT output file where TMVA will store ntuples, histograms, etc.; ## Declare Factory; # Create the Factory class. Later you can choose the methods; # whose performance you'd like to investigate.; # The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to; # pass; # - The first argument is the base of the name of all the output; # weightfiles in the directory weight/ that will be created with the; # method parameters; # - The second argument is the output file for the training results; #; # - The third argument is a string option defining some general configuration for the TMVA session.; # For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in; # the opti",MatchSource.CODE_COMMENT,tutorials/tmva/TMVA_RNN_Classification.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/TMVA_RNN_Classification.py
Safety,avoid,avoid,"## \file; ## \ingroup tutorial_tmva; ## \notebook; ## TMVA Classification Example Using a Recurrent Neural Network; ##; ## This is an example of using a RNN in TMVA. We do classification using a toy time dependent data set; ## that is generated when running this example macro; ##; ## \macro_image; ## \macro_output; ## \macro_code; ##; ## \author Harshal Shende; # TMVA Classification Example Using a Recurrent Neural Network; # This is an example of using a RNN in TMVA.; # We do the classification using a toy data set containing a time series of data sample ntimes; # and with dimension ndim that is generated when running the provided function `MakeTimeData (nevents, ntime, ndim)`; # use max 4 threads; # do enable MT running; # switch off MT in OpenBLAS to avoid conflict with tbb; ## Helper function to generate the time data set; ## make some time data but not of fixed length.; ## use a poisson with mu = 5 and truncated at 10; # ntime = 10;; # ndim = 30; // number of dim/time; # std::cout << j*10+k << "" "";; ## macro for performing a classification using a Recurrent Neural Network; ## @param use_type; ## use_type = 0 use Simple RNN network; ## use_type = 1 use LSTM network; ## use_type = 2 use GRU; ## use_type = 3 build 3 different networks with RNN, LSTM and GRU; # total events to be generated for signal or background; # use GPU for TMVA if available; # if file does not exists create it; # Create a ROOT output file where TMVA will store ntuples, histograms, etc.; ## Declare Factory; # Create the Factory class. Later you can choose the methods; # whose performance you'd like to investigate.; # The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to; # pass; # - The first argument is the base of the name of all the output; # weightfiles in the directory weight/ that will be created with the; # method parameters; # - The second argument is the output file for the training results; #; # - The third argument is a string opti",MatchSource.CODE_COMMENT,tutorials/tmva/TMVA_RNN_Classification.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/TMVA_RNN_Classification.py
Testability,test,test,"ay function; # check given input; # Apply additional cuts on the signal and background samples (can be different); # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; # build the string options for DataLoader::PrepareTrainingAndTestTree; ## Book TMVA recurrent models; # Book the different types of recurrent models in TMVA (SimpleRNN, LSTM or GRU); ## Define RNN layer layout; ## it should be LayerType (RNN or LSTM or GRU) | number of units | number of inputs | time steps | remember output (typically no=0 | return full sequence; ## Defining Training strategies. Different training strings can be concatenate. Use however only one; ## define the inputlayout string for RNN; ## the input data should be organize as following:; ##/ input layout for RNN: time x ndim; ## add after RNN a reshape layer (needed top flatten the output) and a dense layer with 64 units and a last one; ## Note the last layer is linear because when using Crossentropy a Sigmoid is applied already; ## Define the full RNN Noption string adding the final options for all network; ## Book TMVA fully connected dense layer models; # Method DL with Dense Layer; # Training strategies.; # + ""|"" + trainingString2; # General Options.; ## Book Keras recurrent models; # Book the different types of recurrent models in Keras (SimpleRNN, LSTM or GRU); # create python script which can be executed; # create 2 conv2d layer + maxpool + dense; # from keras.initializers import TruncatedNormal; # from keras import initializations; # add recurrent neural network depending on type / Use option to return the full output; # m.AddLine(""model.add(BatchNormalization())"");; # needed if returning the full time output sequence; # book PyKeras method only if Keras model could be created; # use BDT in case not using Keras or TMVA DL; ## Book TMVA BDT; ## Train all methods; # ---- Evaluate all MVAs using the set of test events; # ----- Evaluate and compare performance of all configured MVAs; # check method; # plot ROC curve",MatchSource.CODE_COMMENT,tutorials/tmva/TMVA_RNN_Classification.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/TMVA_RNN_Classification.py
Performance,optimiz,optimize,"# defining graph properties; # method for returning dictionary of graph data; # method to instantiate mlp model to be added in GNN; # defining GraphIndependent class with MLP edge, node, and global models.; # defining Graph network class with MLP edge, node, and global models.; # defining a Encode-Process-Decode module for LHCb toy model; # Instantiating EncodeProcessDecode Model; # Initializing randomized input data; #input_graphs is a tuple representing the initial data; # Initializing randomized input data for core; # note that the core network has as input a double number of features; #initialize graph data for decoder (input is LATENT_SIZE); # Make prediction of GNN; #print(""---> Input:\n"",input_graph_data); #print(""\n\n------> Input core data:\n"",input_core_graph_data); #print(""\n\n---> Output:\n"",output_gn); # Make SOFIE Model; # Compile now the generated C++ code from SOFIE; '''#pragma cling optimize(2); #include ""gnn_encoder.hxx""; #include ""gnn_core.hxx""; #include ""gnn_decoder.hxx""; #include ""gnn_output_transform.hxx""'''; #helper function to print SOFIE GNN data structure; # Build SOFIE GNN Model and run inference; # copy the input data; # running inference on sofie; # Test both GNN on some simulated events; # Run graph_nets model; # First we convert input data to the required input format; # Function to run the graph net; # running SOFIE-GNN; #print(""inference event...."",i); #compute differences between SOFIE and GNN",MatchSource.CODE_COMMENT,tutorials/tmva/TMVA_SOFIE_GNN.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/TMVA_SOFIE_GNN.py
Safety,predict,prediction,"# defining graph properties; # method for returning dictionary of graph data; # method to instantiate mlp model to be added in GNN; # defining GraphIndependent class with MLP edge, node, and global models.; # defining Graph network class with MLP edge, node, and global models.; # defining a Encode-Process-Decode module for LHCb toy model; # Instantiating EncodeProcessDecode Model; # Initializing randomized input data; #input_graphs is a tuple representing the initial data; # Initializing randomized input data for core; # note that the core network has as input a double number of features; #initialize graph data for decoder (input is LATENT_SIZE); # Make prediction of GNN; #print(""---> Input:\n"",input_graph_data); #print(""\n\n------> Input core data:\n"",input_core_graph_data); #print(""\n\n---> Output:\n"",output_gn); # Make SOFIE Model; # Compile now the generated C++ code from SOFIE; '''#pragma cling optimize(2); #include ""gnn_encoder.hxx""; #include ""gnn_core.hxx""; #include ""gnn_decoder.hxx""; #include ""gnn_output_transform.hxx""'''; #helper function to print SOFIE GNN data structure; # Build SOFIE GNN Model and run inference; # copy the input data; # running inference on sofie; # Test both GNN on some simulated events; # Run graph_nets model; # First we convert input data to the required input format; # Function to run the graph net; # running SOFIE-GNN; #print(""inference event...."",i); #compute differences between SOFIE and GNN",MatchSource.CODE_COMMENT,tutorials/tmva/TMVA_SOFIE_GNN.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/TMVA_SOFIE_GNN.py
Safety,predict,prediction,"## Tutorial showing how to parse a GNN from GraphNet and make a SOFIE model; ## The tutorial also generate some data which can serve as input for the tutorial TMVA_SOFIE_GNN_Application.C; #for getting time and memory; # defining graph properties. Number of edges/modes are the maximum; #print the used memory in MB; #get memory of current process; #divide by 1024 * 1024 to get memory in MB; # method for returning dictionary of graph data; # generate graph data with a fixed number of nodes/edges; # method to instantiate mlp model to be added in GNN; # defining GraphIndependent class with MLP edge, node, and global models.; # defining Graph network class with MLP edge, node, and global models.; # defining a Encode-Process-Decode module for LHCb toy model; ########################################################################################################; # Instantiating EncodeProcessDecode Model; # Initializing randomized input data with maximum number of nodes/edges; #input_graphs is a tuple representing the initial data; # Initializing randomized input data for core; # note that the core network has as input a double number of features; #initialize graph data for decoder (input is LATENT_SIZE); # Make prediction of GNN. This will initialize the GNN with weights; #print(""---> Input:\n"",input_graph_data); #print(""\n\n------> Input core data:\n"",input_core_graph_data); #print(""\n\n---> Output:\n"",output_gn); # Make SOFIE Model, the model will be made using a maximum number of nodes/edges which are inside GraphData; ####################################################################################################################################; #generate data and save in a ROOT TTree; #; #need to store each element since annot store RTensor; #make sure dtype of graphData['receivers'] and senders is int32; #; #evaluate graph net on these events; #; #do a first evaluation",MatchSource.CODE_COMMENT,tutorials/tmva/TMVA_SOFIE_GNN_Parser.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/TMVA_SOFIE_GNN_Parser.py
Testability,test,test,"### \file; ### \ingroup tutorial_tmva; ### \notebook -nodraw; ### Example of inference with SOFIE using a set of models trained with Keras.; ### This tutorial shows how to store several models in a single header file and; ### the weights in a ROOT binary file.; ### The models are then evaluated using the RDataFrame; ### First, generate the input model by running `TMVA_Higgs_Classification.C`.; ###; ### This tutorial parses the input model and runs the inference using ROOT's JITing capability.; ###; ### \macro_code; ### \macro_output; ### \author Lorenzo Moneta; ## generate and train Keras models with different architectures; #get the input data; #print(sigData); # stack all the 7 numpy array in a single array (nevents x nvars); # make SOFIE inference on background data; #split data in training and test data; ### run the models; ## create models and train them; #evaluate with SOFIE the 3 trained models; #Generating inference code using a ROOT binary file; # add option to append to the same file the generated headers (pass True for append flag); #model.PrintGenerated(); #need to remove existing header file since we are appending on same one; #compile the generated code; #run the inference on the test data; ## draw also ROC curves",MatchSource.CODE_COMMENT,tutorials/tmva/TMVA_SOFIE_Models.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/TMVA_SOFIE_Models.py
Performance,optimiz,optimizer,"#!/usr/bin/env python; ## \file; ## \ingroup tutorial_tmva_keras; ## \notebook -nodraw; ## This tutorial shows how to do classification in TMVA with neural networks; ## trained with keras.; ##; ## \macro_code; ##; ## \date 2017; ## \author TMVA Team; # Setup TMVA; # Load data; # Generate model; # Define model; # Set loss and optimizer; # Store model to file; # Book methods; # Run training, test and evaluation",MatchSource.CODE_COMMENT,tutorials/tmva/keras/ClassificationKeras.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/keras/ClassificationKeras.py
Testability,test,test,"#!/usr/bin/env python; ## \file; ## \ingroup tutorial_tmva_keras; ## \notebook -nodraw; ## This tutorial shows how to do classification in TMVA with neural networks; ## trained with keras.; ##; ## \macro_code; ##; ## \date 2017; ## \author TMVA Team; # Setup TMVA; # Load data; # Generate model; # Define model; # Set loss and optimizer; # Store model to file; # Book methods; # Run training, test and evaluation",MatchSource.CODE_COMMENT,tutorials/tmva/keras/ClassificationKeras.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/keras/ClassificationKeras.py
Performance,optimiz,optimizer,"#!/usr/bin/env python; ## \file; ## \ingroup tutorial_tmva_keras; ## \notebook -nodraw; ## This tutorial shows how to do multiclass classification in TMVA with neural; ## networks trained with keras.; ##; ## \macro_code; ##; ## \date 2017; ## \author TMVA Team; # Setup TMVA; # Load data; # Generate model; # Define model; # Set loss and optimizer; # Store model to file; # Book methods; # Run TMVA",MatchSource.CODE_COMMENT,tutorials/tmva/keras/MulticlassKeras.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/keras/MulticlassKeras.py
Performance,optimiz,optimizer,"#!/usr/bin/env python; ## \file; ## \ingroup tutorial_tmva_keras; ## \notebook -nodraw; ## This tutorial shows how to do regression in TMVA with neural networks; ## trained with keras.; ##; ## \macro_code; ##; ## \date 2017; ## \author TMVA Team; # Setup TMVA; # Load data; #use only 1000 events since evaluation is very slow (especially on MacOS). Increase it to get meaningful results; # Generate model; # Define model; # Set loss and optimizer; # Store model to file; # Book methods; # Run TMVA",MatchSource.CODE_COMMENT,tutorials/tmva/keras/RegressionKeras.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/keras/RegressionKeras.py
Safety,avoid,avoid,"#!/usr/bin/env python; ## \file; ## \ingroup tutorial_tmva_pytorch; ## \notebook -nodraw; ## This tutorial shows how to apply a trained model to new data.; ##; ## \macro_code; ##; ## \date 2020; ## \author Anirudh Dagar <anirudhdagar6@gmail.com> - IIT, Roorkee; # PyTorch has to be imported before ROOT to avoid crashes because of clashing; # std::regexp symbols that are exported by cppyy.; # See also: https://github.com/wlav/cppyy/issues/227; # Setup TMVA; # Load data; # Define predict function; # Set to eval mode; # Book methods; # Print some example classifications",MatchSource.CODE_COMMENT,tutorials/tmva/pytorch/ApplicationClassificationPyTorch.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/pytorch/ApplicationClassificationPyTorch.py
Safety,avoid,avoid,"#!/usr/bin/env python; ## \file; ## \ingroup tutorial_tmva_pytorch; ## \notebook -nodraw; ## This tutorial shows how to apply a trained model to new data (regression).; ##; ## \macro_code; ##; ## \date 2020; ## \author Anirudh Dagar <anirudhdagar6@gmail.com> - IIT, Roorkee; # PyTorch has to be imported before ROOT to avoid crashes because of clashing; # std::regexp symbols that are exported by cppyy.; # See also: https://github.com/wlav/cppyy/issues/227; # Setup TMVA; # Load data; # Book methods; # Define predict function; # Set to eval mode; # Print some example regressions",MatchSource.CODE_COMMENT,tutorials/tmva/pytorch/ApplicationRegressionPyTorch.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/pytorch/ApplicationRegressionPyTorch.py
Safety,avoid,avoid,"#!/usr/bin/env python; ## \file; ## \ingroup tutorial_tmva_pytorch; ## \notebook -nodraw; ## This tutorial shows how to do classification in TMVA with neural networks; ## trained with PyTorch.; ##; ## \macro_code; ##; ## \date 2020; ## \author Anirudh Dagar <anirudhdagar6@gmail.com> - IIT, Roorkee; # PyTorch has to be imported before ROOT to avoid crashes because of clashing; # std::regexp symbols that are exported by cppyy.; # See also: https://github.com/wlav/cppyy/issues/227; # Setup TMVA; # create factory without output file since it is not needed; # Load data; # Generate model; # Define model; # Construct loss function and Optimizer.; # Define train function; # Training Loop; # Set to train mode; # print train statistics; # print every 32 mini-batches; # Validation Loop; # Set to eval mode; # print val statistics per epoch; # Define predict function; # Set to eval mode; # Store model to file; # Convert the model to torchscript before saving; # Book methods; # Run training, test and evaluation; # Plot ROC Curves",MatchSource.CODE_COMMENT,tutorials/tmva/pytorch/ClassificationPyTorch.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/pytorch/ClassificationPyTorch.py
Testability,test,test,"#!/usr/bin/env python; ## \file; ## \ingroup tutorial_tmva_pytorch; ## \notebook -nodraw; ## This tutorial shows how to do classification in TMVA with neural networks; ## trained with PyTorch.; ##; ## \macro_code; ##; ## \date 2020; ## \author Anirudh Dagar <anirudhdagar6@gmail.com> - IIT, Roorkee; # PyTorch has to be imported before ROOT to avoid crashes because of clashing; # std::regexp symbols that are exported by cppyy.; # See also: https://github.com/wlav/cppyy/issues/227; # Setup TMVA; # create factory without output file since it is not needed; # Load data; # Generate model; # Define model; # Construct loss function and Optimizer.; # Define train function; # Training Loop; # Set to train mode; # print train statistics; # print every 32 mini-batches; # Validation Loop; # Set to eval mode; # print val statistics per epoch; # Define predict function; # Set to eval mode; # Store model to file; # Convert the model to torchscript before saving; # Book methods; # Run training, test and evaluation; # Plot ROC Curves",MatchSource.CODE_COMMENT,tutorials/tmva/pytorch/ClassificationPyTorch.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/pytorch/ClassificationPyTorch.py
Performance,optimiz,optimizer,"#!/usr/bin/env python; ## \file; ## \ingroup tutorial_tmva_pytorch; ## \notebook -nodraw; ## This tutorial shows how to do multiclass classification in TMVA with neural; ## networks trained with PyTorch.; ##; ## \macro_code; ##; ## \date 2020; ## \author Anirudh Dagar <anirudhdagar6@gmail.com> - IIT, Roorkee; # PyTorch has to be imported before ROOT to avoid crashes because of clashing; # std::regexp symbols that are exported by cppyy.; # See also: https://github.com/wlav/cppyy/issues/227; # Setup TMVA; # create factory without output file since it is not needed; # Load data; # Generate model; # Define model; # Set loss and optimizer; # Define train function; # Training Loop; # Set to train mode; # print train statistics; # print every 32 mini-batches; # Validation Loop; # Set to eval mode; # print val statistics per epoch; # Define predict function; # Set to eval mode; # Store model to file; # Convert the model to torchscript before saving; # Book methods; # Run TMVA; # Plot ROC Curves",MatchSource.CODE_COMMENT,tutorials/tmva/pytorch/MulticlassPyTorch.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/pytorch/MulticlassPyTorch.py
Safety,avoid,avoid,"#!/usr/bin/env python; ## \file; ## \ingroup tutorial_tmva_pytorch; ## \notebook -nodraw; ## This tutorial shows how to do multiclass classification in TMVA with neural; ## networks trained with PyTorch.; ##; ## \macro_code; ##; ## \date 2020; ## \author Anirudh Dagar <anirudhdagar6@gmail.com> - IIT, Roorkee; # PyTorch has to be imported before ROOT to avoid crashes because of clashing; # std::regexp symbols that are exported by cppyy.; # See also: https://github.com/wlav/cppyy/issues/227; # Setup TMVA; # create factory without output file since it is not needed; # Load data; # Generate model; # Define model; # Set loss and optimizer; # Define train function; # Training Loop; # Set to train mode; # print train statistics; # print every 32 mini-batches; # Validation Loop; # Set to eval mode; # print val statistics per epoch; # Define predict function; # Set to eval mode; # Store model to file; # Convert the model to torchscript before saving; # Book methods; # Run TMVA; # Plot ROC Curves",MatchSource.CODE_COMMENT,tutorials/tmva/pytorch/MulticlassPyTorch.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/pytorch/MulticlassPyTorch.py
Safety,avoid,avoid,"#!/usr/bin/env python; ## \file; ## \ingroup tutorial_tmva_pytorch; ## \notebook -nodraw; ## This tutorial shows how to do regression in TMVA with neural networks; ## trained with PyTorch.; ##; ## \macro_code; ##; ## \date 2020; ## \author Anirudh Dagar <anirudhdagar6@gmail.com> - IIT, Roorkee; # PyTorch has to be imported before ROOT to avoid crashes because of clashing; # std::regexp symbols that are exported by cppyy.; # See also: https://github.com/wlav/cppyy/issues/227; # Setup TMVA; # create factory without output file since it is not needed; # Load data; # Generate model; # Define model; # Construct loss function and Optimizer.; # Define train function; # Training Loop; # Set to train mode; # print train statistics; # print every 32 mini-batches; # Validation Loop; # Set to eval mode; # print val statistics per epoch; # Define predict function; # Set to eval mode; # Store model to file; # Convert the model to torchscript before saving; # Book methods; # Run TMVA",MatchSource.CODE_COMMENT,tutorials/tmva/pytorch/RegressionPyTorch.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/tmva/pytorch/RegressionPyTorch.py
Energy Efficiency,allocate,allocate,"## \file; ## \ingroup tutorial_vecops; ## \notebook -nodraw; ## In this tutorial we learn how the RVec class can be used to; ## adopt existing memory or allocate some.; ##; ## \macro_code; ## \macro_output; ##; ## \date May 2018; ## \author Danilo Piparo; # We use this class for didactic purposes: upon copy, a line is printed to the terminal.; '''; class UponCopyPrinter {; public:; UponCopyPrinter() = default;; UponCopyPrinter(UponCopyPrinter &&) = default;; UponCopyPrinter(const UponCopyPrinter &) { std::cout << ""Invoking copy c'tor!"" << std::endl; }; };; '''; # One of the essential features of RVec is its ability of adopting and owning memory.; # Let's create an RVec of UponCopyPrinter instances. We expect no printout:; # Let's adopt the memory from v into v2. We expect no printout:; # OK, let's check the addresses of the memory associated to the two RVecs It is the same!; # Now, upon reallocation, the RVec stops adopting the memory and starts owning it. And yes,; # a copy is triggered. Indeed internally the storage of the RVec is an std::vector. Moreover,; # the interface of the RVec is very, very similar to the one of std::vector: you have already; # noticed it when the `data()` method was invoked, right?; # Of course, now the addresses are different.",MatchSource.CODE_COMMENT,tutorials/vecops/vo001_AdoptOrOwnMemory.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/vecops/vo001_AdoptOrOwnMemory.py
Integrability,interface,interface,"## \file; ## \ingroup tutorial_vecops; ## \notebook -nodraw; ## In this tutorial we learn how the RVec class can be used to; ## adopt existing memory or allocate some.; ##; ## \macro_code; ## \macro_output; ##; ## \date May 2018; ## \author Danilo Piparo; # We use this class for didactic purposes: upon copy, a line is printed to the terminal.; '''; class UponCopyPrinter {; public:; UponCopyPrinter() = default;; UponCopyPrinter(UponCopyPrinter &&) = default;; UponCopyPrinter(const UponCopyPrinter &) { std::cout << ""Invoking copy c'tor!"" << std::endl; }; };; '''; # One of the essential features of RVec is its ability of adopting and owning memory.; # Let's create an RVec of UponCopyPrinter instances. We expect no printout:; # Let's adopt the memory from v into v2. We expect no printout:; # OK, let's check the addresses of the memory associated to the two RVecs It is the same!; # Now, upon reallocation, the RVec stops adopting the memory and starts owning it. And yes,; # a copy is triggered. Indeed internally the storage of the RVec is an std::vector. Moreover,; # the interface of the RVec is very, very similar to the one of std::vector: you have already; # noticed it when the `data()` method was invoked, right?; # Of course, now the addresses are different.",MatchSource.CODE_COMMENT,tutorials/vecops/vo001_AdoptOrOwnMemory.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/vecops/vo001_AdoptOrOwnMemory.py
Usability,learn,learn,"## \file; ## \ingroup tutorial_vecops; ## \notebook -nodraw; ## In this tutorial we learn how the RVec class can be used to; ## adopt existing memory or allocate some.; ##; ## \macro_code; ## \macro_output; ##; ## \date May 2018; ## \author Danilo Piparo; # We use this class for didactic purposes: upon copy, a line is printed to the terminal.; '''; class UponCopyPrinter {; public:; UponCopyPrinter() = default;; UponCopyPrinter(UponCopyPrinter &&) = default;; UponCopyPrinter(const UponCopyPrinter &) { std::cout << ""Invoking copy c'tor!"" << std::endl; }; };; '''; # One of the essential features of RVec is its ability of adopting and owning memory.; # Let's create an RVec of UponCopyPrinter instances. We expect no printout:; # Let's adopt the memory from v into v2. We expect no printout:; # OK, let's check the addresses of the memory associated to the two RVecs It is the same!; # Now, upon reallocation, the RVec stops adopting the memory and starts owning it. And yes,; # a copy is triggered. Indeed internally the storage of the RVec is an std::vector. Moreover,; # the interface of the RVec is very, very similar to the one of std::vector: you have already; # noticed it when the `data()` method was invoked, right?; # Of course, now the addresses are different.",MatchSource.CODE_COMMENT,tutorials/vecops/vo001_AdoptOrOwnMemory.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/vecops/vo001_AdoptOrOwnMemory.py
Usability,learn,learn,"## \file; ## \ingroup tutorial_vecops; ## \notebook -nodraw; ## In this tutorial we learn how elements of an RVec can be easily sorted and; ## selected.; ##; ## \macro_code; ## \macro_output; ##; ## \date August 2018; ## \author Stefan Wunsch; # RVec can be sorted in Python with the inbuilt sorting function because; # PyROOT implements a Python iterator; # For convenience, ROOT implements helpers, e.g., to get a sorted copy of; # an RVec ...; # ... or a reversed copy of an RVec.; # Helpers are provided to get the indices that sort the vector and to; # select these indices from an RVec.; # Take can also be used to get the first or last elements of an RVec.; # Because the VecOps helpers return a copy of the input, you can chain the operations; # conveniently.",MatchSource.CODE_COMMENT,tutorials/vecops/vo004_SortAndSelect.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/vecops/vo004_SortAndSelect.py
Performance,perform,perform,"## \file; ## \ingroup tutorial_vecops; ## \notebook -nodraw; ## In this tutorial we learn how combinations of RVecs can be built.; ##; ## \macro_code; ## \macro_output; ##; ## \date August 2018; ## \author Stefan Wunsch; # RVec can be sorted in Python with the inbuilt sorting function because; # PyROOT implements a Python iterator; # To get the indices, which result in all combinations, you can call the; # following helper.; # Note that you can also pass the size of the vectors directly.; # Next, the respective elements can be taken via the computed indices.; # Finally, you can perform any set of operations conveniently.; # However, if you want to compute operations on unique combinations of a; # single RVec, you can perform this as follows.; # Get the indices of unique triples for the given vector.; # Take the elements and compute any operation on the returned collections.",MatchSource.CODE_COMMENT,tutorials/vecops/vo005_Combinations.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/vecops/vo005_Combinations.py
Usability,learn,learn,"## \file; ## \ingroup tutorial_vecops; ## \notebook -nodraw; ## In this tutorial we learn how combinations of RVecs can be built.; ##; ## \macro_code; ## \macro_output; ##; ## \date August 2018; ## \author Stefan Wunsch; # RVec can be sorted in Python with the inbuilt sorting function because; # PyROOT implements a Python iterator; # To get the indices, which result in all combinations, you can call the; # following helper.; # Note that you can also pass the size of the vectors directly.; # Next, the respective elements can be taken via the computed indices.; # Finally, you can perform any set of operations conveniently.; # However, if you want to compute operations on unique combinations of a; # single RVec, you can perform this as follows.; # Get the indices of unique triples for the given vector.; # Take the elements and compute any operation on the returned collections.",MatchSource.CODE_COMMENT,tutorials/vecops/vo005_Combinations.py,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/vecops/vo005_Combinations.py
